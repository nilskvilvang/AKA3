---
bibliography: bibliografi.bib
csl: chicago-author-date.csl
always_allow_html: true
---

# Polynomisk regresjon

```{r echo = FALSE}
options(scipen = 999)
```

R-pakker brukt i dette kapittelet:

```{r}
pacman::p_load(tidyverse, readxl, ggpubr, sjPlot)
```

Anta at vi har lønnsdata fra en bedrift som har regnet ut snittlønn for de 10 stillingskategoriene de har av ansatte. 

```{r echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE}
xfun::embed_file("polydata.xlsx")
```

```{r}
polydata <- read_excel("polydata.xlsx")
```

```{r}
glimpse(polydata)
```

Vi kan se på disse dataene og se en klar ikke-lineær trend. Vi har i koden under vist en modifisert funksjon fra @johnstonQuickEasyFunction2012 som trekker ut parametre fra regresjonslikningen og legger det inn på toppen av plottet. Vi inkluderer også kode til "vanlig plott" kommentert ut med # slik at man kan bruke dette hvis man foretrekker det.

```{r fig.cap = "Ikke-linear forhold mellom stillingskategori og snittlonn", warning = FALSE, message = FALSE}
enkelOLS <- lm(Snittlonn ~ Stillingskategori, data = polydata)
# Kode modifisert fra Susan Johnstone (2012)
ggplotRegression <- function (fit) {
require(ggplot2)
ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
    geom_point() +
    stat_smooth(method = "lm", col = "red") +
    labs(title = paste("R2 = ",signif(summary(fit)$r.squared, 3),
                     ", Intercept =",signif(fit$coef[[1]], 5),
                     ", Slope =",signif(fit$coef[[2]], 5),
                     ", P =",signif(summary(fit)$coef[2,4], 3))) +
    theme_bw()
}
ggplotRegression(lm(Snittlonn ~ Stillingskategori, data = polydata))

# "Vanlig kode for plott:
# ggplot(polydata, aes(x = Stillingskategori, y = Snittlonn), col = "red") +
#     geom_point() +
#     stat_smooth(method = "lm", col = "red") +
#     xlab("Stillingskategori") +
#     ylab("Gjennomsnittslonn") +
#     theme_bw()
```

Siden vi er inne på modifisering av plott med data fra regresjonslikningen kan vi også vise et alternativ til funksjonen fra @johnstonQuickEasyFunction2012. Observante lesere vil se at koden nedenfor avrunder tall som legge i plottet, mens funksjonen i plottet over ikke gjøre det. 

```{r fig.cap = "Alternativt plott med parametere", warning = FALSE, message = FALSE}
ggplot(polydata, aes(x = Stillingskategori, y = Snittlonn), col = "red") + 
    geom_point() + 
    stat_smooth(method = "lm", col = "red") +
    stat_regline_equation() +
    stat_cor(label.y = 900000, 
             aes(label = paste(..rr.label.., ..p.label.., sep = "*`,`~"))) +
    theme_bw()
```

Vi kunne kanskje ane dette ut fra dataene (og fordi det selvsagt er et eksempel der vanlig lineær regresjon ikke skal funke så bra...). Det synes i hvert fall klart at den lineære regresjonslinja sannsynligvis ikke er en veldig god modell for lønnsdataene. 

Fra kapittelet om enkel, lineær regresjon vet vi at vi kan uttrykke regresjonslinja slik:

$y=\alpha + \beta x$

Vi kan videre uttrykke en multippel regresjon slik:

$y=\alpha + \beta_1x_1 + \beta_2x_2 +\ ...\ +\beta_nx_n$

En polynomisk regresjonslikning likner, men her introduserer vi "i n-te ledd":

$y=\alpha + \beta_1x_1 + \beta_2x_1^2+\ ...\ +\beta_nx_1^n$

Selv om dette sikkert er kjent er det greit å illustrere ulike n'te-ledd grafisk:

```{r, warning = FALSE, message = FALSE, fig.cap ="n'te leddsfunksjoner #1"}
funksjonsgrafer <- ggplot(data.frame(x = c(-5, 5)), aes(x = x)) + 
    stat_function(fun = function(x){x^4}, color="red", lwd = 1) +        
    stat_function(fun = function(x){x^3}, color="blue", lwd = 1) + 
    stat_function(fun = function(x){4*x^2}, color="orange", lwd = 1) +
    stat_function(fun = function(x){10*x}, color="purple", lwd = 1) +
    annotate(geom="text", label = "Quartic", x = -4, y = 200, size = 5) + 
    annotate(geom="text", label = "Quadratic", x = -4.6, y = 107, size =5) +
    annotate(geom="text", label = "Cubic", x = -3.8, y = -90, size =5) + 
    annotate(geom="text", label = "Linear", x = -4.6, y = -32, size =5) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    ylim(-100, 200) +
    theme_bw()
funksjonsgrafer
```

Imidlertid er vi sjeldent interessert i negative x-verdier i praktisk bruk (matematisk er det selvsagt interessant, men som sagt sjeldent i anvendt analyse som vi driver med). 
```{r, warning = FALSE, message = FALSE, fig.cap ="n'te leddsfunksjoner #2"}
funksjonsgrafer2 <- ggplot(data.frame(x = c(0, 5)), aes(x = x)) + 
    stat_function(fun = function(x){x^4}, color="red", lwd = 1) +        
    stat_function(fun = function(x){x^3}, color="blue", lwd = 1) + 
    stat_function(fun = function(x){4*x^2}, color="orange", lwd = 1) +
    stat_function(fun = function(x){10*x}, color="purple", lwd = 1) +
    annotate(geom="text", label = "Quartic", x = 4.2, y = 200, size = 5) + 
    annotate(geom="text", label = "Quadratic", x = 4.8, y = 75, size =5) +
    annotate(geom="text", label = "Cubic", x = 4.5, y = 120, size =5) + 
    annotate(geom="text", label = "Linear", x = 4.8, y = 40, size =5) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    ylim(-100, 200) +
    xlim(0, 5) +
    theme_bw()
funksjonsgrafer2
```

Vi skal nedenfor vise hvordan vi kan utvikle polynomisk regresjon, men det er på sin plass å komme med en liten advarsel. Vi kan, som nevnt, i prinsippet ha så mange med så mange ledd vi vil, og muligens vil vi få bedre og bedre "fit" til dataene jo flere vi legger til. Det betyr imidlertid ikke at modellen blir bedre og bedre til å predikere framtidige verdier. Vi kan få det vi kaller overtilpasning ("overfitting"), dvs at modellen vår passer helt utmerket til dataene våre, men at modellen ikke vil klare å predikere nye verdier med særlig god treffsikkerhet. Det er alltid en avveining mellom å få en modell som er godt tilpasset dataene, predikativ evne og modellens kompleksitet/enkelhet (alt annet likt foretrekker vi som regel enklere modeller framfor komplekse modeller). Vi skal altså alltid være bevisst på hva vi gjør i polynomisk regresjon (som med alt annet selvsagt...). 

Et annet punkt, som forsåvidt også gjelder andre modeller, er at vi skal være klar over i hvilket område - x-verdier - modellen kan sies å være gyldig. Som vi ser av grafene ovenfor er det tildels store forskjeller i hva modellene vil predikere, og valg av modell må ses i sammenheng med det området vi har observasjoner og hvilket område vi ønsker å predikere i. Vi skal være ekstra observante når vi tenker på å gjøre prediksjoner i områder vi ikke har modellert, eller i områder vi ikke har hatt data på når vi har modellert.

## Modellering av andregrads ("quadratic") polynomisk regresjon

```{r}
polydata$Stillingskategori2 <- polydata$Stillingskategori^2
glimpse(polydata)
```

Vi har altså laget en kvadrert variabel som heter Stillingskategori2. Lager så polynomial modell med ^2 ledd:

```{r fig.cap = "Polynomial regression ^2"}
poly_reg2 <- lm(Snittlonn ~ ., data = polydata)
ggplot() +
    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn), col = "black") +
    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg2, newdata = polydata)), col = "red") +
    xlab("Stillingskategori") +
    ylab("Snittlønn") +
    theme_bw()
```

## Modellering av tredjegrads ("cubic") polynomisk regresjon

```{r}
polydata$Stillingskategori3 <- polydata$Stillingskategori^3
glimpse(polydata)
```

```{r fig.cap = "Polynomial regression ^3"}
polydata$Stillingskategori3 <- polydata$Stillingskategori^3
poly_reg3 <- lm(Snittlonn ~ ., data = polydata)
ggplot() +
    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn)) +
    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg3, newdata = polydata)), col = "red") +
    ggtitle("Polynomial regression ^3") +
    xlab("Stillingskategori") +
    ylab("Snittlønn") +
    theme_bw()
```

## Modellering av fjerdegrads ("quartic") polynomisk regresjon

```{r fig.cap = "Polynomial regression ^4"}
polydata$Stillingskategori4 <- polydata$Stillingskategori^4
poly_reg4 <- lm(Snittlonn ~ ., data = polydata)
ggplot() +
    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn)) +
    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg4, newdata = polydata)), col = "red") +
    ggtitle("Polynomial regression ^4") +
    xlab("Stillingskategori") +
    ylab("Snittlønn") +
    theme_bw()
```

## Tolkning

Av grafene over får vi en indikasjon på hvilken modell som ser ut til å passe våre data best. Vi kan også se på hvordan de ulike modellene predikerer ulikt. La oss f.eks. anta at man ønsker å innføre en stillingskategori 5.5. Hva er predikert snittlønn for denne kategorien? Vi viser kun for de to "ytterste" regresjonslikningene vi har vist over - enkel OLS og 4'grads polynomfunksjon.

```{r}
# Enkel OLS
pred1 <- predict(enkelOLS, data.frame(Stillingskategori = 5.5))
# ^4 polynomial
pred2 <- predict(poly_reg4, data.frame(Stillingskategori = 5.5,
                              Stillingskategori2 = 5.5^2,
                              Stillingskategori3 = 5.5^3,
                              Stillingskategori4 = 5.5^4))
pred <- as.data.frame(c(pred1, pred2))
pred
```

Vi kan se at det er en vesentlig forskjell i prediksjonene. Når vi ser på grafen for enkel OLS lenger opp ser vi også at regresjonslinja ligger godt over linja/punktene for dataene. Den siste av grafene - polynomial ^4 - ser ut til å treffe ganske bra. 

## Tilnæmring gjennom kryssvalidering - valg av antall grader i polynomialfunksjonen

Dette eksempelet er modifisert fra @zachPolynomialRegressionStepbyStep2019.

Vi kan tilnærme oss valg av riktig polynomialfunksjon gjennom "Mean Squared Error" (MSE) som forteller oss hvor nærme en regresjonslinje er datapunktene. Jo lavere MSE, jo bedre kan vi anta prediksjonsevnen til modellen er. Vi skal ikke gå i dybden på dette, men vise hvordan dette kan se ut for eksempelet ovenfor. I eksempelet brukes det som kalles "k-fold cross-validation" som vi ikke går nærmere inn på (men som vi forklarte i kapittelet om Principal Component Analysis). Utregning av MSE følger av:

$MSE = \frac{1}{n}*\sum(faktisk\ verdi - predikert\ verdi)$

I eksempelet har vi generert data som ser på timer studenter har brukt på et emne og respektive resultater. 

```{r fig.cap = "Timer og resultat"}
set.seed(1)
df <- data.frame(timer = runif(50, 5, 15), resultat=50)
df$resultat <- df$resultat + df$timer^3/150 + df$timer*runif(50, 1, 2)
glimpse(df)
```

Vi kan se på en enkel OLS på datasettet:

```{r}
enkelOLS2 <- lm(resultat ~ timer, data = df)
ggplot() +
    geom_point(aes(x = df$timer, y = df$resultat), col = "red") +
    geom_line(aes(x = df$timer, y = predict(enkelOLS2, newdata = df)), col = "blue") +
    ggtitle("Enkel OLS") +
    xlab("timer") +
    ylab("resultat") +
    theme_bw()
```

Neste steg er å lage 5 modeller (kan selvsagt lage x antall modeller).

```{r}
# Stokker om dataene tilfeldig
df.shuffled <- df[sample(nrow(df)),]
# Setter antall "folds" som skal brukes i "k-fold cross-validation"
K <- 10 
# Definerer antall polynomfunksjoner
degree <- 5
# Lager k antall like store "folds"
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
# Lager matrise som skal fylles med MSE for de 5 modellene
mse = matrix(data=NA,nrow=K,ncol=degree)
# Gjennomfører "K-fold cross validation"
for(i in 1:K){
    # Deler opp i treningsdata og testdata
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    # Evaluerer modellene gjennom "k-fold cv" 
    for (j in 1:degree){
        fit.train = lm(resultat ~ poly(timer,j), data=trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((fit.test- testData$resultat)^2) 
    }
}
colMeans(mse)
```

Modell 2 ser ut til å ha lavest MSE.

Vi kan til slutt visualisere valgt modell og se på modellens parametere. 

```{r fig.cap = "Valgt modell - modell 2"}
ggplot(df, aes(x=timer, y=resultat)) + 
    geom_point(col = "red") +
    stat_smooth(method='lm', formula = y ~ poly(x,2), size = 1) + 
    xlab('timer') +
    ylab('resultat') +
    theme_bw()

bestemodell <- lm(resultat ~ poly(timer, 2, raw = TRUE), data = df)
summary(bestemodell)
tab_model(bestemodell)
```

