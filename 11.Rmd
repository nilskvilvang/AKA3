---
bibliography: bibliografi.bib
csl: "chicago-author-date.csl"
---

```{r}
pacman::p_load(datasets, tidyverse, explore, glmnet)
```

# Regularized regression

I kapittelet om regresjonsanalyse (ikke medregnet logistisk regresjon) har vi brukt minste kvadratsums metode ("Ordinary Least Squares"). Dersom vi har mange forklaringsvariabler i forhold til antall observasjoner eller stor korrelasjon mellom forklaringsvariablene (multikolinearitet) kan det være aktuelt å bruke alternative metoder fordi minste kvadratsums metode vil medføre stor varians i estimatorene. @projectproHowCreateOptimize2021 beskriver dette slik:

"The subset selection methods use ordinary least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. This shrinkage is also known as regularisation."

I OLS-regresjon ("Ordinary Least Squares") - der vi altså søker å predikere gjennom en kombinasjon av prediktorer (uavhengige variabler) (og forutsetter normalfordelte feilledd). Vi estimerer parametrene fra et utvalg siden vi ikke kjenner populasjonens "ekte" parametere gjennom å minimalisere de kvadrerte residualene (jfr kapittel om lineær regresjon). De estimerte koeffisientene kan være skjeve/"feil" på to måter: bias og varians. Variansen er et mål på usikkerheten i estimatene gjennom spredningen. Bias er forskjellen mellom populasjonenens sanne parameter og den forventede parameteren. @aryaBiasVarianceTradeoff2022 illustrerer dette slik:

![Bilde fra kdnuggets.com](biasvarians.png)

Vi ønsker selvsagt at både varians og bias skal være så liten som mulig siden det vil gi oss gode prediksjoner, men like selvsagt er det at det åpenbart varierer. En OLS-estimator er "unbiased", men kan ha en stor varians - spesielt ved multikolinearitet og et høyt antall uavhengige variabler. Det regularized regression søker å gjøre er å redusere variansen, med den implisitte effekten at bias økes noe. Vi kan se på forholdet mellom bias, varians og modellkompleksitet slik:

![Bilde fra Wikipedia](biasvarianstradeoff.png){width=85%}

Ettersom modellens kompleksitet øker reduseres bias (fordi vi antar at våre estimater vil ligge nærmere populasjonens sanne verdi med økt antall variabler). Regularisering er altså et forsøk på å finne det optimale punketet der bias og varians tilsammen gir lavest mulig total feil. 

Som @boehmkeHandsOnMachineLearning2020 uttrykker det:

"Regularization methods provide a means to constrain or regularize the estimated coefficients, which can reduce the variance and decrease out of sample error."

Regularisering innebærer dermed at man legger begrensninger på ikke-betydningsfulle regresjonskoeffiseinter mot 0. Hensikten med dette er bl.a. lettere tolkbare resultater (for L2/lasso - L1/ridge gir ikke denne effekten). Hovedhensikten er også å gjøre modellen mer generaliserbar fordi effekten av ikke-betydningsfulle prediktorer reduseres (ved at koeffisientene til disse reduseres mot 0 eller fjernes). Det primære anvendelsesområdet kan derfor sies å være tilfeller der vi har komplekse modeller - modeller med mange variabler. En effekt - som vi ikke går nærmere inn på her - er også at metodene er raskere enn mange alternativer ved svært store datasett. Dette kan være spesielt interessant i maskinlæringstilfeller. En siste effekt vi kan nevne her er at disse metodene vil fungere også når antallet prediktorer er høyere enn antall observasjoner - noe som er problematisk i andre regresjonsmetoder.

Ofte snakker man om tre metoder for såkalt "Regularized Regression":

1. Ridge regression - også kalt L2-regresjon - søker å minimere koeffisientenes kvadratsum. L2 vil påvirke store koeffisienter mer enn små, siden de koeffisientene som er små/nærme 0 ikke vil reduseres relativt sett like mye som større koeffisienter. Man får dermed en modell med flere koeffisienter nærme 0 (men ikke 0). Ridge/L2 vil normalt fungere bedre enn L1 dersom vi har et stort antall uavhengige variabler som alle bidrar i mdoellen (få/ingen prediktorer som har vesentlig høyere påvirkning enn øvrige prediktorer). 

2. Lasso regression ("Least Absolute Shrinkage and Selection Operator") - også kalt L1-regresjon - søker å minimere den absolutte verdien av regresjonskoeffisientene. L1 vil altså redusere noen koeffisienter til 0 (eller i praksis eksludere dem fra modellen), noe som i realiteten er en implisitt varaibelseleksjon. L1 - lasso regression - er dermed egnet dersom man f.eks. i en eksplorerende analyse ønsker å foreta et utvalg blant mange variabler. Lasso regresjon/L1 vil gi bedre resultater/bedre prediktiv evne enn ridge/L2 dersom man har relativt få uavhengige variabler som er gode prediktorer og øvrige uavhengige variabler ikke har stor påvirkning.

3. Elastic-Net regression - er en kombinasjon av ridge og lasso.

I disse tilnærmingene tilfører vi en "shrinkage" på koeffisientene gjennom en straffeparameter som som regel bete3gnes $\lambda$ (lambda). Jo større $\lambda$-verdien er, jo mer reduserer vi koeffisientene. Som vi vil vise under kan vi teste for hva den optimale $\lambda$-verdien er, f.eks. gjennom kryssvalidering (som vi omtaler mer i detalj andre steder i boka). 

## Ridge regresjon

En av disse er "ridge regression" [@hoerlRidgeRegressionApplications1970; @hoerlRidgeRegressionBiased1970]. Vi tenker det er i overkant teknisk å gå inn i utregning (se f.eks. [her](https://www.publichealth.columbia.edu/research/population-health-methods/ridge-regression) for en rimelig bra forklaring (på engelsk), men poenget er å få estimatorer som er mer "korrekt" under de betingelsene som er nevnt over. Ridge regresjon utvider lineær regresjon ved å ta hensyn til kompleksiteten i modellen ved å introdusere en "straffeparameter" ("penalty parameter") som utgjøres av den kvadrerte størrelsen av koeffisientene.   

For å illustrere dette bruker vi et eksempel fra @zachLassoRegressionStepbyStep2020 og et innebygd og ofte brukt datasett i R ("mtcars"). Den avhengige variabelen er "hp" ("gross horsepower"), og "mpg" ("Miles pr gallon"), "wt" ("weight"), "drat" ("Rear axle ratio") og "qsec" ("1/4 mile time") som de uavhengige variablene. 
Den avhengige variabelen er da:

```{r}
y <- data.matrix(mtcars$hp)
head(y)
```

Og de uavhengige variablene:

```{r}
x <- data.matrix(mtcars[, c('mpg', 'wt', 'drat', 'qsec')])
x
```

Modellen blir da:

```{r message = FALSE, warning = FALSE}
ridgemodell <- glmnet(x, y, alpha = 0)
summary(ridgemodell)
```

Vi kan legge merke til at i koden velger vi alpha = 0. Glmnet = 0 gir ridge regresjon, alpha = 1 gir lasso regresjon, og alpha mellom 0 og 1 gir elastic-net regresjon. Vi kan også merke også at disse typene regresjon forutsetter standardisering, men dette ivaretar glmnet for oss.
Vi kommenterer ikke modellen på dette stadiet. Neste steg blir å velge en såkalt lambdaverdi ($\lambda$) som gir lavest MSE (Mean Square Error). Dette gjør vi gjennom en kryssvalidering ("cv.glmnet" som default velger k=10 folds). 

```{r}
cv_ridgemodell <- cv.glmnet(x, y, alpha = 0)
beste_lambda_ridge <- cv_ridgemodell$lambda.min
beste_lambda_ridge
plot(cv_ridgemodell)
```

Vi kan da optimalisere modellen gjennom den identifiserte beste $\lambda$.

```{r}
beste_modell <- glmnet(x, y, alpha = 0, lambda = beste_lambda_ridge)
coef(beste_modell)
```

Et interessant plott er et "trace-plott" som viser hvordan koeffisientene endrer seg som følge av endret lamda-verdi:

```{r}
plot(ridgemodell, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
```

Til slutt kan vi se på $R^2$ for beste modell:

```{r}
y_prediksjon <- predict(ridgemodell, s = beste_lambda_ridge, newx = x)
sst <- sum((y - mean(y))^2)
sse <- sum((y_prediksjon - y)^2)
rsq <- 1 - sse/sst
rsq
```

Beste modell kan dermed forklare `r round(rsq, 3)*100` % av variansen i den avhengige variabelen.

## Lasso regression

Vi fortsetter med eksempelet ovenfor, og som nevnt kan vi endre alpha-verdien fra 0 til 1 for å skifte fra ridge regresjon til lasso regresjon [@tibshiraniRegressionShrinkageSelection1996].

```{r}
lassomodell <- glmnet(x, y, alpha = 1)
summary(lassomodell)
```

Identifisering av optimal lambda:

```{r}
cv_lassomodell <- cv.glmnet(x, y, alpha = 1)
beste_lambda_lasso <- cv_lassomodell$lambda.min
beste_lambda_lasso
plot(cv_lassomodell)
```

```{r}
plot(lassomodell)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
```

```{r}
beste_lassomodell <- glmnet(x, y, alpha = 1, lambda = beste_lambda_lasso)
coef(beste_lassomodell)
```

Her kan vi se at lassoregresjonen har redusert koeffisienten til "drat" til 0 (variabelen er dermed ikke i beste modell). Lassoregresjon kan altså redusere variablers koeffisienter **til** 0, mens ridgeregresjon reduserer kan redusere koeffisientene **mot** 0. 

Vi kan til slutt se på $R^2$ for beste modell:

```{r}
y_prediksjon2 <- predict(lassomodell, s = beste_lambda_lasso, newx = x)
sst2 <- sum((y - mean(y))^2)
sse2 <- sum((y_prediksjon2 - y)^2)
rsq2 <- 1 - sse2/sst2
rsq2
```

Beste modell kan dermed forklare `r round(rsq2, 3)*100` % av variansen i den avhengige variabelen.

## Elastic-net regresjon

Som vi har sett over vil lasso regresjon fungere som "variabel selection". En utfordring med denne metoden kan oppstå når man to sterkt korrelerte variabler "dyttes" mot null er at den ene kan falle ut av modellen mens den andre forblir i modellen (selv om de altså er sterkt korrelerte). Dette vil også kunne opptre usystematisk. Ridge regresjon kan i slike tilfeller fungere noe bedre, men denne vil altså *ikke* utelukke variabler helt. Kombinasjonen av de to - elastic-net [@zouRegularizationVariableSelection2005] - søker å kombinere egenskapene for de to.

I modellen under setter vi en alpha-verdi mellom 0 og 1. Dersom vi setter den til 0.5 ber vi om at ridge og lasso vektes like tungt. Hvis alpha settes < 0.5 vil modellen lene seg tyngre mot ridge, og med alpha > 0.5 vil den lene seg tyngre mot lasso. 

```{r}
en_modell <- glmnet(x, y, alpha = 0.3)
summary(en_modell)
```

Identifisering av optimal lambda:

```{r}
cv_en_modell <- cv.glmnet(x, y, alpha = 0.3)
beste_lambda_en <- cv_en_modell$lambda.min
beste_lambda_en
plot(cv_en_modell)
```

```{r}
plot(en_modell)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
```

```{r}
beste_en_modell <- glmnet(x, y, alpha = 0.3, lambda = beste_lambda_en)
coef(beste_en_modell)
```

```{r}
y_prediksjon3 <- predict(en_modell, s = beste_lambda_en, newx = x)
sst3 <- sum((y - mean(y))^2)
sse3 <- sum((y_prediksjon3 - y)^2)
rsq3 <- 1 - sse3/sst3
rsq3
```

Beste modell kan dermed forklare `r round(rsq3, 3)*100` % av variansen i den avhengige variabelen.

## Ridge vs Lasso vs Elastic-net

Vi kan sammenblikne $R^2$ for de tre teknikkene:
```{r}
rsq4 <- cbind("R-squared" = c(rsq, rsq2, rsq3))
rownames(rsq4) <- c("Ridge regression", "Lasso regression", "Elastic-net")
print(rsq4)
```

De gir realtivt like $R^2$-verdier. Som tidligere nevnt kan man ikke si at den ene er generelt bedre enn de andre. De lar oss håndtere multikolinearitet, og lasso kan fungere som teknikk for å redusere antall variabler (ridge kan ikke det). Elastic-net søker altså å kombinere de gode egenskapene for de to, men likevel kan man ikke si på generelt grunnlag at den alltid vil være bedre. 

Det kan til slutt også være på sin plass å beskrive noen av de utfordringene med disse teknikkene som ofte nevnes.
Slik teknikkene fungerer må alle variablene være numeriske. Mange pakker/program som kjører teknikkene håndterer manglende verdier dårlig/ikke, så det kan kreve endel bearbeiding av dataene før analyse (på den andre siden - deskriptiv statistikk og databearbeiding bør være et must uansett...). Uteliggere, f.eks., kan være en utfordring som for OLS og her viser vi til grundig diskusjon om tematikken uteliggere i OLS-kapittelet. Og - det er en lineær teknikk med de svakheter dette medfører generelt. 



