---
bibliography: bibliografi.bib  
csl: chicago-author-date.csl
always_allow_html: true
---
# Samvariasjon og korrelasjon

R-pakker brukt i dette kapittelet:

```{r}
pacman::p_load(flextable, tidyverse, officer, readxl, knitr, kableExtra, writexl, car, corrplot, sjPlot)
```

## Samvariasjon/kovarians

Samvariasjon – at noe varierer sammen (omtales også som kovarians/"covariance") – er et mål på en felles variasjon mellom (minst) to variabler. Samvariasjonen kan være enten negativ eller positiv. Hvis vi har data på variablene x og y kan vi undersøke om verdiene i x og y varierer tilfeldig eller om det er en sammenheng mellom de to. Ved positiv samvariasjon vil en økning eller nedgang i en variabel korrespondere med en økning eller nedgang i den andre. Samvariasjon innebærer at det ikke er tilfeldig hvordan to variabler varierer i forhold til hverandre – det er et gjensidig forhold mellom variablene.

I forrige kapittel var begrepet varians et tema. Et alternativt begrep for samvariasjon er kovarians (fra engelsk «covariance»). Kovariansen er den enkleste måten å se om to variabler varierer sammen – altså om endring i den ene variabelen følges av en endring i den andre. Eller, med andre ord, dersom en variabel avviker fra gjennomsnittet forventer vi at den andre også gjør det (enten i samme eller motsatt retning hvis vi tror de kovarierer). Vi ønsker derfor å finne ut hvor stor kovariansen er.

```{r echo = FALSE, message = FALSE, warning=FALSE}

kovarians <- data.frame(Snr = c("1", "2", "3", "4", "5", "Snitt", "Sum"),
                        Hudsyr = c("1", "2", "3", "3", "4", "2.6", ""),
                        Avvik = c("-1.6", "-0.6", "0.4", "0.4", "1.4", "", "0"),
                        Familie = c("1", "3", "3", "5", "5", "3.4", ""),
                        Avvik2 = c("-2.4", "-0.4", "-0.4", "1.6", "1.6", "", "0"),
                        Tverrprodukt = c("3.84", "0.24", "-0.16", "0.64", "2.24", "", "6.80"))

colnames(kovarians) <- c("Student nr", "Antall husdyr", "Avvik fra snitt", "Antall i familien", "Avvik fra snitt", "Tverrproduktavvik")

kovarians <- kovarians %>%
  kbl(caption = "Kovarians", align = "c") %>%
  kable_classic(full_width = F, position = "left", html_font = "Cambria")

column_spec(kovarians, 1:4, width = "3cm")
```

Vi regner altså ut snittet og avviket fra snittet for begge variablene (antall husdyr og antall i familien). Så multipliserer vi avvikene pr rad – og får tverrproduktavviket pr observasjon/respondent. Til slutt summerer vi tverrproduktavvikene. Så regner vi ut kovariansen:

$\frac{6.8}{n-1}=\frac{6.8}{(5-1)}=1.7$

I dette enkle eksempelet kan vi altså si at antall husdyr samvarierer positivt med antall medlemmer i familien. Det er imidlertid ikke så lett å tolke hva kovariansen betyr da det ikke er et standardisert mål. Vi kan altså ikke sammenlikne kovarianser mellom ulike undersøkelser på en objektiv måte [@fieldDiscoveringStatisticsUsing2009]. Det fører oss over på begrepet korrelasjon.

## Korrelasjon og korrelasjonskoeffisient

For å kunne si noe mer fornuftig om samvariasjonen må derfor normalisere kovariansen gjennom å dele på variablenes standardavvik – noe som gir oss *korrelasjonen* mellom variablene [@lovasStatistikkUniversiteterOg2013]. Den standardiserte kovariansen er med andre ord korrelasjonskoeffisienten. I vårt tilfelle ser det da slik ut:

- Vi har allerede funnet kovariansen gjennom å multiplisere avvikene for de to variablene med hverandre (6.8). Vi gjør det samme med standardavvikene. Standardavviket for antall husdyr har vi tidligere regnet vi ut til å være 1.14. For antall medlemmer i familien blir standardavviket:

$\sqrt{\frac{(-2.4)^2 + (-0.4)^2 + (1.6)^2 + (1.6)^2}{(5-1)}} = \sqrt{\frac{5.76 + 0.16 + 0.16 + 2.56 + 2.56}{4}}=1.67$

- Neste skritt blir å multiplisere standardavvikene:

$1.14 * 1.67 = 1.923$

- Til slutt tar vi kovariansen og deler på standardavvikproduktet:

$\frac{1.7}{1.923}=0.88$

Resultatet 0,88 er det som betegnes Pearsons korrelasjonskoeffisient ($r$). For oss betyr det i vårt lille eksempel at antall familiemedlemmer er positivt korrelert med antall husdyr – jo flere i familien, jo flere husdyr. Korrelasjonskoeffsienten er et mål på hvor sterk korrelasjonen er og i hvilken retning korrelasjonen går (om den er positiv eller negativ). 

## Tolkning av korrelasjon og korrelasjonskoeffisenter

Korrelasjonskoeffisienten har alltid en verdi mellom -1 og 1, der 0 betyr ingen korrelasjon. Det er vanlig å bruke følgende retningslinjer for vurdering av koeffisienten:

- Nær x/- 1: Perfekt eller tilnærmet perfekt korrelasjon
- Mellom +/- 0.5 og +/- 1: Sterk korrelasjon
- Mellom +/- 0.3 og +/- 0.49: Moderat korrelasjon
- Under +/- 0.29: Liten korrelasjon

@hinkleAppliedStatisticsBehavioral2003 opererer med en noe mer finmasket inndeling:

```{r echo = FALSE, message = FALSE, warning=FALSE}

cohend <- data.frame(Veldig = c("0.90 - 1.00"),
                     Høy = c("0.70 - 0.90"),
                     Moderat = c("0.50 - 0.70"),
                     Lav = c("0.30 - 0.50"),
                     Ingen = c("0.00 - 0.30"))

colnames(cohend) <- c("Veldig høy", "Høy", "Moderat", "Lav", "Svært lav (om noe)")

cohend<- cohend %>%
  kbl(caption = "Korrelasjonskoeffisient - Pearsons r. Modifisert fra Hinkle et al. (2003)") %>%
  kable_classic(full_width = F, position = "left", html_font = "Cambria")

column_spec(cohend, 1:3, width = "3.5cm")
```

Ofte ønsker vi (og anbefaler) å ikke bare se på verdien av korrelasjonskoeffisienten, men også se på en grafisk framstilling av datane - gjerne omtalt som et spredningsplott ("scatter plot").

![Eksempler på korrelasjoner](korrelasjoner.png){width=90%}

I eksempelet og illustrasjonene over har vi vist Pearsons korrelasjonskoeffisient $r$. Det finnes ulike korrelasjonskoeffisienter for ulike typer datasett. Pearsons korrelasjonskoeffisient brukes som regel på datasett der vi gjennomfører såkalte parametriske tester. For andre tester, ikke-parametriske, kan Spearmans rho ($\rho$) - ofte brukt for ordinale variabler (men kan også brukes på intervall/ratiodata) med parrede data ("matched pair") - og Kendalls tau ($\tau$) - ofte brukt for "nominelle variabler med rangert data ("ranked data") - være gode alternativer. Vi går ikke nærmere inn på disse her.

Vi skal merke oss at en korrelasjonskoeffisient på 0 ikke betyr at det ikke er noen korrelasjon. Som vi kan se av bildet under [@froslieKorrelasjon2022] kan korrelasjonskoeffisienten være 0 og variablene like vel være avhengige (delt her iht Creative Commons: CC BY SA 3.0), jfr illustrasjonen over.

Som sagt, et viktig poeng er at vi skal være veldig forsiktige med å tolke en korrelasjonskoeffisient uten å ha en grafisk framstilling av hvordan datapunktene fordeler seg (hvordan distribusjonen av datapunkter ser ut). Dette fordi en gitt korrelasjonskoeffisient kan representere et uendelig antall mønstre mellom to variabler.  @vanhoveWhatDataPatterns2018 viser dette:

![Eksempler på lik korrelasjon (r = 0.5)](korrelasjoner2.png){width=85%}

Alle 16 eksemplene viser altså mønstre av korrelasjon mellom to variabler som alle har korrelasjonskoeffisienten r=0,5. 

## Spuriøs korrelasjon

Det er viktig å huske på at en korrelasjon mellom to variabler (x og y) ikke er det samme som å si at det er en årsakssammenheng (kausalitet). Selv om to variabler korrelerer perfekt betyr ikke det nødvendigvis at den ene er årsak til den andre. Det kan være en såkalt spuriøs sammenheng. I mange tilfeller er det åpenbart at det er urimelig å anta at det skal være en sammenheng mellom variablene. 

Korrelasjon betyr altså ikke automatisk kausalitet. Dette er utrolig viktig å huske – og en kilde til mye feilinformasjon/feiltolkninger. La oss ta to eksempler fra [Tyler Vigen](https://www.tylervigen.com/spurious-correlations).

![Spuriøs korrelasjon - filmer med Nicolas Cage og dødsfall som følge av drukning i svømmebasseng](Modul_6_Kaus1.png){width=90%}

![Spuriøs korrelasjon - konsum av ost og dødsfall gjennom innvikling i eget sengetøy](Modul_6_Kaus2.png){width=90%}

I det første eksempelet ovenfor er det altså en sterk korrelasjon mellom filmer Nicholas Cage medvirker i og antall mennesker som druknet etter å ha falt i et svømmebasseng. Med mindre man tenker seg at Cage er så dårlig skuespiller at det får folk til å hoppe frivillig ut i et svømmebasseng for å drukne seg virker denne sammenhengen ganske søkt.

I det andre eksempelet virker sammenhengen like sprø. At det skal være en reell og nesten perfekt sammenheng mellom antall mennesker som kveles av deres eget sengetøy og osteforbruket per capita er ekstremt lite troverdig. 

Det synes åpenbart at selv om det er en klar korrelasjon mellom variablene i de to eksemplene virker det fullstendig meningsløst å tro at de faktisk henger sammen. Dette er det vi kaller **spuriøs korrelasjon**.

## Eksempel på bivariat korrelasjonsanalyse

I tittelen på delkapittelet har vi brukt begrepet *bivariat* korrelasjonsanalyse. Det innebærer at vi ser på korrelasjonen mellom to variabler. I neste delkapittel skal vi vise et enkelt eksempel på korrelasjonsanalyse av flere variabler. 

Dette eksempelet er hentet fra @pallantSPSSSurvivalManual2010. Datasettet kan lastes ned fra nett [her](http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.YpzW0KhBw2x).

```{r echo = FALSE}
Pallantmod <- read_excel("Pallant_survey.xlsx")
Pallantmod <- data.frame(Pallantmod$tpstress,Pallantmod$tpcoiss)
Pallantmod <- na.omit(Pallantmod)
write_xlsx(Pallantmod, "Pallantmod.xlsx")
```

Vi har modifisert datasettet til å kun inneholde de to variablene vi skal bruke og fjernet observasjoner med manglende verdier:

```{r echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE}
xfun::embed_file('Pallantmod.xlsx')
```

Datasettet inneholder opprinnelig flere variabler, men det jeg skal se på nå er om oppfattelse av stress ("tpstress") korrelerer med variabelen "tpcoiss" ("Total perceived control over internal states"). Jeg ønsker altså å se på sammenhengen mellom oppfattet stress og hvordan man oppfatter at man har "indre kontroll". 

```{r}
korr <- cor.test(Pallantmod$Pallantmod.tpstress, Pallantmod$Pallantmod.tpcoiss)
korr
```

I resultatet kan vi se at `r round(korr$estimate, 3)`. Siden vi har et negativt fortegn betyr det at vi har en negativ korrelasjon mellom variablene – altså en høy verdi på den ene variabelen er forbundet med en lav verdi på den andre. Vi ser at korrelasjonen er signifikant (`r korr$p.value`). Vi kan også si noe om styrken på korrelasjonen (uavhengig av fortegn på koeffisienten). 

Lenger opp har vi gjengitt forslag til tokning av størrelsen på korrelasjonskoeffisienten. Etter disse har vi en sterk negativ korrelasjon mellom disse to variablene ut fra Cohens inndeling, men en moderat korrelasjon ut fra @hinkleAppliedStatisticsBehavioral2003. 

```{r fig.cap = "Scatterplott for 'tpcoiss' og 'tpstress'"}
korrplott <- ggplot(Pallantmod, aes(x=Pallantmod.tpcoiss, y=Pallantmod.tpstress)) + 
    geom_point(col = "red") +
    theme_bw() 
korrplott
```

Det er vanligvis ikke nødvendig å vise resultater av en korrelasjonsanalyse dersom det ikke er mer enn to variabler. I eksempelet over kan man rapportere (vi har ikke sjekket forutsetninger, men for eksempelets skyld antar vi at vi ikke har funnet problemer der):

> Forholdet mellom oppfattelse av indre kontroll (målt med PCOISS) og oppfattelse av stress (målt med Perceived Stress Scale) ble undersøkt med Pearsons korrelasjonskoeffisient. Innledende analyser avdekket ingen brudd på forutsetningene om normalitet, linearitet og homoskedastisitet. Basert på resultatene av studien er oppfattelse av indre kontroll sterkt negativt korrelert med oppfattelse av stress, r = `r korr$estimate`, n = `rnrow(Pallantmod)`, p = `r korr$p.value`.

## Multivariat korrelasjonsanalyse

Eksempelet her er hentet fra @coghlanUsingMultivariateAnalysis2010 ([Creative Commons Attribution 3.0 License](http://creativecommons.org/licenses/by/3.0/)). Datasettet inneholder data på mengde av 13 ulike kjemikalier i vin fra en region i Italia fra tre ulike produsenter. 

```{r}
wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data",
          sep=",")
```

Vi kan teste enkeltvise korrelasjoner som f.eks. mellom V2 og V3:
```{r}
v2v3korr <- cor.test(wine$V2, wine$V3, method = "pearson")
v2v3korr
```

V2 og V3 har en korrelasjon på `r format(round(v2v3korr$estimate, 3), nsmall = 3)` med en p-verdi på `r format(round(v2v3korr$p.value, 3), nsmall = 3)`. 

```{r}
v2v4korr <- cor.test(wine$V2, wine$V4, method = "pearson")
v2v4korr
```
V2 og V4, på sin side, har en korrelasjon på `r format(round(v2v4korr$estimate, 3), nsmall = 3)` med en p-verdi på `r format(round(v2v4korr$p.value, 3), nsmall = 3)`. 

Ofte viser vi korrelasjoner i en korrelasjonsmatrise. 

```{r}
tab_corr(wine, triangle = "lower", p.numeric = FALSE)
```
p < .0001**** , p < .001*** , p < .01**, p < .05*

```{r echo = FALSE}
p_korrelasjon <- scales::pvalue(v2v4korr$p.value,
                           accuracy = 0.01, 
                           decimal.mark = ".", 
                           add_p = TRUE)
```

Vi kan se at korrelasjonen mellom V2 og V3 framstår som ikke signifikant i korrelasjonsmatrisen, mens V2 og V4 vises som signifikant (`r p_korrelasjon`. 

Og et sammensatt plott (viser her kun for V2 - V5):

```{r fig.cap = "Korrelasjonsplott for 5 kjemikalier"}
scatterplotMatrix(wine[2:5])
```

I spredningsplottet kan vi se at korrelasjonen mellom V2 og V3 ser mest "spredd" ut. Dette kan vi også se i matrisen over. Ut fra bare plottet er det ikke så lett å f.eks. se hvor korrelasjonen er størst, men i matrisen ser vi det selvsagt enkelt (V4 og V5). Så kombinasjonen av en korrelasjonsmatrise og et spredningsplott vil være et godt hjelpemiddel for å se på korrelasjon. 

##  Kausalitet (årsakssammenheng)

Kausalitet – fra latin "causa" -> engelsk "cause" - betyr altså årsak eller grunn. Kausalitet innebærer altså at det må være en årsakssammenheng mellom to hendelser/fenomener.

Vi kan tydeligst (enklest) se kausalitet i naturvitenskapene, der vi snakker om et **deterministisk årsaksforhold** (det er ikke begrenset til naturvitenskapene selvsagt, men naturvitenskapene sysler mye med lovmessigheter vi ikke like lett kan se/påvise i for eksempel samfunnsvitenskapene). En deterministisk årsakssammenheng innebærer at et fenomen B alltid har samme årsak (fenomen A), og at fenomen A alltid fører til fenomen B. Det er således et tidsperspektiv involvert (rekkefølge av hendelser/fenomen).

I samfunnsvitenskapene har man gjerne en litt annerledes tilnærming til kausalitet fordi det er vanskelig å vise/se de lovmessige og deterministiske årsaksforholdene. Her sier man at et fenomen (A) er årsak til fenomen B, dersom det er slik at (med en viss sannsynlighet) A enten fører til eller øker sannsynligheten for B. Dette faller inn under en **stokastisk årsakssammenheng** [@dahlumKausalitet2021]. I samfunnsvitenskapene kan man i mindre grad kontrollere alle ting som påvirker et fenomen B, så man kan også snakke om tendenser – at det er en **tendensiell forståelse** av kausalitet.

I et naturvitenskapelig eksperiment kan vi ofte isolere fenomenene vi undersøker fra all annen påvirkning (selv om dette naturligvis på ingen måte er gjeldende for all naturvitenskapelig forskning). Vi kan kontrollere hva som påvirker hva, i tid og rom. Et kontrollert eksperiment er derfor en slags gullstandard for forskning når man skal si noe om kausale forhold.

I samfunnsvitenskapene er dette ofte i praksis umulig. Det vil være mange mulige påvirkninger på et fenomen, og det kan være vanskelig å si noe sikkert om 1) har vi tenkt på alle ting som kan påvirke, 2) klarer vi å ta hensyn til denne usikkerheten i våre analyser og 3) vet vi egentlig hva som påvirker hva (kan det være sånn at vår antakelse om at A påvirker B faktisk kan være motsatt)? Moralen er nok: Vi skal være veldig varsomme med å dra for bastante slutninger om kausalitet så lenge vi ikke gjennomfører et kontrollert eksperiment der vi kontrollerer omgivelsene og variablene. Det finnes imidlertid (selvsagt) metoder også i samfunnsvitenskapene som gjør at vi kan snakke om kausalitet. Disse kommer vi (litt) tilbake til undervegs i senere kapitler.




