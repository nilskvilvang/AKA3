[{"path":"index.html","id":"introduksjon","chapter":"Introduksjon","heading":"Introduksjon","text":"Dette notatet gir en introduksjon til anvendt kvantitativ analyse rettet mot samfunnsvitenskapene.Innholdet er utvikling og oppdateres jevnlig om enn noe uregelmessig. Tilbakemeldinger og innspill bes gitt til Nils Kvilvang.Bilde: Gjengitt uten kreditering et stort antall steder, men Gleitman, Gross, Reisberg (2011), s.28, fig. 1.8 er illustrasjonen signert Chase","code":""},{"path":"index.html","id":"versjoner","chapter":"Introduksjon","heading":"Versjoner","text":"","code":""},{"path":"hvorfor-dette-notatet.html","id":"hvorfor-dette-notatet","chapter":"Hvorfor dette notatet?","heading":"Hvorfor dette notatet?","text":"ability take data, able understand , process , extract values , visualize , communicate - ’s going hugely important skill next decades… (Varian (2009), Chief Economist Google og Professor Emeritus University California, Berkeley)Dette notatet oppsummerer ulike forelesningsnotater og undervisningsopplegg anvendt kvantitativ analyse. Hensikten er å forsøke å formidle sentrale konsepter og teknikker anvendt kvantitativ analyse, samt gjennom kodeblokkene med R-kode gi “oppskrift” på prosedyre slik leseren kan reprodusere eksempler. Jeg håper det kan tjene som et oppslagsverk og kanskje også som litt inspirasjon?Det finnes mange veldig gode bøker som tar seg introduksjon til og/eller videregående emner kvantitativ analyse. Min gode kollega Christer Thranes bøker er eksempler på det. Det finnes også bøker som hvert fall jeg tenker ikke er spesielt gode (betydningen pedagogiske) formidlingen (dette er selvsagt veldig subjektivt). Likvel - er det virkelig behov enda flere, og er mitt bidrag nødvendigvis bedre/mer pedagogisk? Igjen - det får bli en subjektiv vurdering av leseren. Jeg har hvert fall gjort et ærlig forsøk på å formidle dette stoffet på en måte jeg håper er forståelig etter å ha pløyd gjennom et utall bøker og andre ressurser. Jeg skal heller ikke legge skjul på jeg ikke gjør anspråk på å revolusjonere noe som helst. Tvert mot - jeg velger lett å bruke det jeg oppfatter som gode eksempler ulike temaer fra andre. Sånn sett er mitt bidrag mer å samle gode gjennomganger og eksempler, vise leseren til hvor dette kommer fra slik man kan gå til originalkilden om man ønsker, og formidle dette på et samlet sted, norsk språkdrakt, på en forhåpentligvis forståelig måte.Samtidig er det åpenbart slik det kan være krøkkete formuleringer, dårlige forklaringer, ulne beskrivelser, dårlige eksempler, lite av noe, mye av annet osv. Alle innspill og tilbakemeldinger mottas med stor takk (helt sant!).","code":""},{"path":"hvorfor-dette-notatet.html","id":"programvare","chapter":"Hvorfor dette notatet?","heading":"Programvare","text":"Notatet er skrevet med bruk av R (R Core Team 2021) og RStudio (RStudio Team 2022). R er en velkjent programvare innenfor statistikk, dataanalyse, datamodellering osv. R har noen store fordeler; det er gratis, det kjører på “alle” operativsystemer, og det har et svært stort bruker- og utviklermiljø som hovedsak deler alt gratis. Det er også enkelt å finne løsninger på det meste gjennom veiledninger og brukerforum på nett. Selve R er et programmeringsspråk og utviklermiljø statistikk som gir en kjernefunksjonalitet innenfor datahåndtering, kalkulasjoner, dataanalyse, datamodellering, grafisk framstilling av data o.l.R kommer med 14 basis “pakker”. Det store potensialet ligger imidlertid brukerne av R utvikler tilleggspakker som man bruker R, det finnes pr. april 2022 19000 ulike pakker som bygger på kjernen R. Alle pakkene tilbyr ulike tilrettelagte løsninger ulike problemer/analyser.Den største ulempen med R er brukergrensesnittet er veldig ulikt hva vi kjenner fra Microsoft Office-typen brukergrensesnitt, så det vil ta litt tid å bli kjent med programmet. tillegg er brukergrensesnittet kodebasert og ikke menybasert, og kan (dessverre) virke avskrekkende. Likevel, det er et utrolig kraftig verktøy hvis man tar seg tid til å lære seg det grunnleggende.Brukere av notatet vil sannsynligvis kjøre programvare som enten SPSS eller Stata. Konsepter og eksempler notatet er som sagt laget og kjørt R. Ambisjonen er også å gi dere videoer framgangsmåte SPSS og Stata å få fram tilsvarende analyser på samme eksempler som notatet. Dette kommer på plass gradvis.tillegg er det på sin plass å nevne alternativer til store og dyre programvarepakker som SPSS og Stata som mer og mer framstår som reelle og gode alternativer. av disse, jamovi og JASP, har sterkt voksende bruk (også akademia) rundt om verden. Dette er grafisk tiltalende og funksjonsrike statistikkpakker som kjører med R bakgrunnen (alle analyser jamovi og JASP bruker R), og som også kan inkludere R kode direkte. Det gjør man kan utnytte alle pakkene skrevet R direkte de grafiske brukergrensesnitt. tillegg kan man (varierende grad mellom de ) hente ut R-kode fra analyser man gjør gjennom det grafiske brukergrensesnittet - noe som kan gjøre en overgang/introduksjon til R lettere om man vil den veien.Leseren står selvsagt fritt til å hoppe elegant alle verktøy som ikke er interessante. Det er klare fordeler og ulemper med alle, men forhåpentligvis vil de fleste finne et verktøy de kan bruke utvalget vi har tatt med.Der det er naturlig, som ved bruk av R og RStudio, er kodingen inkludert slik eksempler skal kunne replikeres av leseren, men vi går ikke inn på R utover dette. Kodingen er gjengitt fortløpende der analysen er gjort. Vi har brukt R/RStudio og rmarkdown (Allaire et al. 2022) – en såkalt pakke til R – produksjonen av dette notatet. R baserer seg som sagt på bruk av ulike “pakker” som er utviklet av forskjellige utviklere og som er fritt tilgjengelig. Mange av disse har også datasett inkludert slik det er enkelt å replikere eksempler. Så langt det er mulig har vi basert oss på det vi viser som eksempler skal være replikerbare leseren.Vi har lite fokus på matematikk og formler den forstand vi ikke utleder dybden forklaring på ulike formler. Vi tror det er fullt mulig å ha en praktisk forståelse og anvendt bruk av kvantitative analyser uten å ha dyptgående kjennskap til de matematiske eller statistiske forklaringene. Likevel, det er heller ikke slik vi absolutt skal unngå dette. Vi har derfor inkludert noe bakgrunnskunnskap å skape en\nramme rundt kjernen notatet der vi tenker det kan være interessant de som ønsker å fordype seg noe mer.","code":"\npacman::p_load(flextable, tidyverse, officer, readxl, knitr, kableExtra, writexl, car, readxl, effects, ggplot2, writexl, ggpubr, tidyverse, gridExtra, nortest, knitr, kableExtra, tseries, normtest, flextable, magrittr, ISLR, olsrr, lmtest, rnorsk, qwraps2, sjPlot, sjmisc, sjlabelled, xtable, Hmisc, gt, gtsummary, sjPlot, modelsummary, table1, jtools, interactions, outliers, EnvStats, qqplotr, summarytools, caret)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"grunnleggende-begreper-og-sammenhenger","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"Kapittel 1 Grunnleggende begreper og sammenhenger","text":"Vi skal dette kapittelet gå gjennom en rekke begreper og forhold som vi vil komme tilbake til gjennom flere ulike analyser senere, større eller mindre grad, men de er alle det vi vil kalle grunnleggende begreper vi bør ha en grad av kjennskap til.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"populasjon-og-utvalg","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.1 Populasjon og utvalg","text":"Når vi gjør undersøkelser om «et eller annet» kan vi veldig ofte ikke samle inn informasjon (data) fra alle. Om man gjør en meningsmåling før et valg å anslå utfallet av valget kan man naturligvis ikke spørre alle stemmeberettigede hele landet (+ alle stemmeberettigede som ikke er landet akkurat når man gjennomfører meningsmålingen). Det er praktisk umulig. Alle stemmeberettigede kalles denne sammenhengen populasjonen. Populasjonen er altså begrepet vi bruker på hele gruppen/den totale mengden av objekter vi ønsker å undersøke. en meningsmåling tar man derfor et utvalg fra populasjonen, spør dem, og antar man kan la resultatene fra utvalget snakke /representere hele populasjonen. Men – man kan selvsagt gjøre undersøkelser på hele populasjoner om det er praktisk mulig, det avhenger bare av hva man definerer som populasjonen.figuren har vi illustrert dette. Populasjonen består av et antall (kanskje ukjent) antall individer (N). Gjennomsnittsverdien populasjonen (kalles – µ) en egenskap, som eksempel høyde, er dermed også ukjent. Derfor tar vi et utvalg individer fra populasjonen, måler dem, og kan regne ut gjennomsnittsverdien (kalles x strek, eller «x bar» på engelsk) utvalget. Så lar vi \\(\\overline{x}\\) være et estimat µ, og antar gjennomsnittet utvalget er representativt gjennomsnittet populasjonen.Det er viktig å huske på \\(\\overline{x}\\) er nettopp et estimat. Kanskje treffer vi bra, kanskje treffer vi dårlig. Hvordan vi velger ut utvalget vil derfor være viktig. kvantitativ metode opererer vi som regel med det som kalles sannsynlighetsutvalg (motsetning til strategisk utvalg som ofte brukes kvalitativ metode). Sannsynlighetsutvalg innebærer alle enhetene populasjonen har en gitt sannsynlighet å bli trukket ut utvalget. Det gjør vi innenfor visse feilmarginer kan anta det vi finner utvalget gjelder populasjonen.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"utvelgelse-fra-populasjonen","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.1.1 Utvelgelse fra populasjonen","text":"Ofte deler man måten man foretar sannsynlighetsutvalg inn fire metoder (se eksempel Grønmo (2021)):Enkel tilfeldig utvelgelse. Enhetene trekkes ut helt tilfeldig en og en fra populasjonen («random sampling»). Tilfeldig vil innebære ethvert medlem populasjonen har lik sjanse til å bli trukket ut og hvert objekt trekkes ut uavhengig av hverandre. Også dette er praksis umulig å få til perfekt, så ethvert utvalg vil trolig ha en eller annen form skjevhet («bias»).Enkel tilfeldig utvelgelse. Enhetene trekkes ut helt tilfeldig en og en fra populasjonen («random sampling»). Tilfeldig vil innebære ethvert medlem populasjonen har lik sjanse til å bli trukket ut og hvert objekt trekkes ut uavhengig av hverandre. Også dette er praksis umulig å få til perfekt, så ethvert utvalg vil trolig ha en eller annen form skjevhet («bias»).Systematisk utvelgelse. «Den første enheten utvalget trekkes tilfeldig blant de n første (eksempel de 100 første) enhetene universet. Deretter trekkes systematisk hver n’te enhet universet til utvalget. Hvis den første tilfeldig utvalgte enheten er nummer 83 universet, vil de neste enhetene utvalget være universets enheter nummer 183, 283, 383 og så videre» (Grønmo 2021).Systematisk utvelgelse. «Den første enheten utvalget trekkes tilfeldig blant de n første (eksempel de 100 første) enhetene universet. Deretter trekkes systematisk hver n’te enhet universet til utvalget. Hvis den første tilfeldig utvalgte enheten er nummer 83 universet, vil de neste enhetene utvalget være universets enheter nummer 183, 283, 383 og så videre» (Grønmo 2021).Stratifisert utvelgelse. Man deler først inn populasjonen kategorier (eller strata) før man deretter foretar et tilfeldig eller systematisk utvalg. Kategoriene kan eksempel være kjønn, alder ellerliknende).Stratifisert utvelgelse. Man deler først inn populasjonen kategorier (eller strata) før man deretter foretar et tilfeldig eller systematisk utvalg. Kategoriene kan eksempel være kjønn, alder ellerliknende).Populasjonen deles inn klynger basert på fysisk eller geografisk nærhet mellom enhetene. Deretter foretas et tilfeldig eller systematisk utvalg.Populasjonen deles inn klynger basert på fysisk eller geografisk nærhet mellom enhetene. Deretter foretas et tilfeldig eller systematisk utvalg.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"enheter-variabler-og-verdier","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.2 Enheter, variabler og verdier","text":"","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"enhet","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.2.1 Enhet","text":"En nehet er det vi forsøker å si noe om. eksempel kan et individ være en enhet. Et individ kan vi kalle en enhet på mikronivå. En organisasjon eller en gruppe individer kan også utgjøre en enhet. Dette nivået kaller vi mesonivå. tillegg kan vi ha enheter på makronivå – dette kan være samfunnsgrupper (eksempel klasser, etnisitet og religion).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"variabel","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.2.2 Variabel","text":"Egenskapene ved enheten vi ønsker å si noe om. eksempel egenskaper ved et individ.Ofte vil vi snakke om uavhengig og avhengig variabel. En uavhengig variabel (kan også kalles årsaksvariabel) er en variabel som ikke er påvirket av det vi forsøker å si noe om, men tvert imot påvirker det vi forsøker å si noe om (den avhengige variabelen).En avhengig variabel (også kalt virkningsvariabel) er en variabel som påvirkes av andre variabler (andre uavhengige variabler). Dvs. verdien på den avhengige variabelen er avhengig av verdien på en eller flere uavhengige variabler.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"verdi","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.2.3 Verdi","text":"Hvordan egenskapen måles, hvordan egenskapene ser ut.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"målenivå","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.3 Målenivå","text":"Vi måler altså verdien på variabler. Men variabler er forskjellige, det vil si vi kan måle de på ulik måte ut fra hva de representerer. eksempel måler vi alder år, temperatur grader, inntekt kroner, kjønn og utdanning ulike kategorier. De ulike måle- eller kategoriseringsmåtene gjør oss stand til å gjøre ulike ting med målingene/kategoriene. Vi kan vise dette en tabell:NivåDataForklaringOperasjonerEksempelKategoriskBinær/dikotomKun muligheter/kategorierTelle frekvenser.Til stede/ikke til stede, død/levende, ja/nei, på/avKategoriskNomiellMer enn kategorier som er gjensidig utelukkende. Tallverdi er en 'merkelapp' som ikke sier noe om egenskaper.Telle frekvenser.Nasjonalitet, politiske partier, yrke, studieretningKategoriskOrdinalKategorier som kan rangeres/ordnes, men der avstanden mellom kategoriene er betydningsløs.Arrangere rekkefølge.Likertskalaer (sterkt uenig-sterkt enig), UtdanningsnivåKontinuerligeIntervallKan rangeres og man kan si noe kvantitativt om avstanden mellom verdier. Fast avstand mellom måleverdier – lik avstand på måleskalaen representerer lik avstand fenomenet som måles. Har et kunstig nullpunktAddere, subtrahere og regne ut gjennomsnittTemperatur: 10°C er dobbelt så mye som 5°C, men man kan ikke si 10°C er dobbelt så varmt som 5°C.KontinuerligeSkala/ratio/ forholdstallKan rangere, måle avstand og beregne forholdstall mellom verdier. Har et faktisk nullpunktRegne ut ratio/forholdstall og prosenterInntekt: 200000 er dobbelt så mye som 100000, og 200000 er dobbelt så stor inntekt som 100000. Samtidig er 400000 dobbelt så høy inntekt som 200000.Vi skal imidlertid merke oss et viktig poeng. Såkalte responsskalaer og Likertskalaer (Likert 1932) som brukes mye spørreundersøkelser (typiske 5 eller 7 svaralternativer langs en skala der man velger en verdi) er formelt på ordinalnivå. vil vi eksempel respondentene svare på en skala fra 1-7, der 7 er «Helt enig» og 1 er «Helt uenig» en påstand. Dette gir data som ikke er på intervallnivå (S. S. Stevens 1966). Vi kan umulig si med sikkerhet forskjellen mellom «Helt uenig» og «Uenig» er lik forskjellen mellom «Enig» og «Helt enig». Data på ordinalnivå kan man strengt tatt ikke regne ut gjennomsnitt på. Det er imidlertid svært vanlig å behandle denne typen data som intervalldata, og det finnes gode argumenter litteraturen å gjøre dette – vi går litt mer dybden det påfølgende delkapittelet (du kan leve lenge uten å måtte gå dybden på dette, men vi velger likevel å behandle dette litt mer inngående det påfølgende delkapittelet slik dette er drøftet - så om du trenger argumenter kan du finne det der. Foreløpig nøyer vi oss med å fastslå vi kan behandle denne typen data som intervalldata. en ofte sitert bok sier Tabachnik Fidell (2007):distinction continuous discrete variables always clear. add enough digits digital clock, instance, becomes practical purposes continuous measuring device, whereas time measured analog device can also read discrete categories hours half hours. fact, continuous measurement may rendered discrete (dichotomous) loss information, specifying cutoffs continuous scale.følge Kahler et al. (2008) kan vi ikke bruke parametriske tester på slik data. Noen statistikkbøker Pallant (2010) slår ganske enkelt fast dataene skal være på minimum intervallnivå å tilfredsstille forutsetningene parametriske tester, men diskuterer ikke dette nærmere.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"forutsetninger-om-intervalldata","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.3.1 Forutsetninger om intervalldata","text":"Latente variabler (vi kommer sterkt tilbake til begrepet latente variabler såvel faktoranalyse som SEM-analyse senere kapitler, men kort fortalt er dette variabler vi ikke kan observere eller måle direkte, men som må tilnærmes gjennom andre variabler) måles ofte gjennom skalaer som måler respondentens holdninger eller oppfatninger (“semantic differnatial scales - Jamieson (2004)), jfr. Likert (1932). Slike skalaer, der respondentene velger et av flere svaralternativ som står forhold til hverandre – f.eks.”Svært uenig” - “Uenig” - “Nøytral” - “Enig” - “Svært enig” - anses produserer data på ordinalnivå ifølge kjente klassifiseringer. En ofte sitert klassifisering er S. S. Stevens (1946). Det er umulig å fastslå forskjellen mellom “Sterkt uenig” og “Uenig” er nøyaktig den samme som forskjellen mellom “Enig” og “Sterkt enig”.Stevens’ taksonomi av målenivå er grunnlaget “representational theory”. Michell (1986) påpeker “numbers used measurements represents empirical relations objects” (s.398). Dette innebærer vitenskapelige konklusjoner bør være uforanderlige ift skalaen som er brukt (konklusjoner skal ikke endres med ulike skalaer) (Marcus-Roberts Roberts 1987). Parametriske tester, denne tradisjonen, krever mulighet lineære transformasjoner, noe ordinale data ikke muliggjør (N. H. Anderson 1961). Konsekvensen er ordinale data ikke tilfredsstiller forutsetningene parametriske tester, og derfor ikke kan gjøres (L. Cohen, Manion, Morrison 2000). Dette medfører en konservativ tilnærming til hvilke tester man kan gjøre med ulike typer data som følger klassisk “measurement theory” (Michell 1986).motsetning til den konservative tilnærmingen finnes det en mer liberal tradisjon som hevder tilhengerne av den konservative tilnærmingen blander sammen “measurement theory” og “statistical theory”, og dermed misforstår når det er mulig/hensiktsmessig å kjøre parametriske tester (Gaito 1959, 1960, 1980). Savage (1957) og Gaito (1980) hevder den forbindelsen det ikke finnes noen matematisk grunn til å begrense statistiske prosedyrer til de som involverer aritmetiske operasjoner av kontinuerlige data av de observerte størrelsene. N. H. Anderson (1961) poengterer en statistisk test ikke kan være bevisst den empiriske betydningen/det empiriske innholdet av tallene man putter inn testen, mens Baker, Hardyck, Petrinovich (1966) påpeker en statistisk test svarer på spørsmålet den er ment å svare på uavhengig av om målingene er sterke eller svake – typen statistisk test er dermed uavhengig av det empiriske betydningen av dataene per se (Michell 1986). Tilhengere av denne tradisjonen mener derfor valget av type statistisk test utelukkende bør handle om statistiske vurderinger som “nothing scale type” (N. H. Anderson 1961, s.309). Det sentrale valget av type statistisk test bør heller være vurdering av dataenes distribusjon, utvalgsstørrelse, uavhengighet, bias, robusthet, kontekst og empirisk meningsfullhet (Carifio Perla 2007; Carifio Perla 2008; Hand 1996; Knapp 1990; Muthen Kaplan 1992; Pell 2005). Det er også empirisk vist skalaen på målingene liten grad påvirker variansbaserte statistiske tester Kempthorne (1955). Det er også vist parametriske tester er robuste de fleste forhold (Gardner 1975; Glass, Peckham, Sanders 1972; Norman 2010).Det finnes med andre ord gode begrunnelser både en konservativ og en liberal tilnærming til hvorvidt man kan kjøre parametriske tester på ordinale data som data fra spørreundersøkelser som bruker Likert skalaer. Det er imidlertid ingen praktisk grunn til å anta man ikke kan gjøre det – det finnes gode teoretiske og empiriske argumenter det kan gjøres, men man bør vurdere andre aspekter ved dataene også som nevnt overfor.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"gjennomsnitt-som-modell","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.4 Gjennomsnitt som modell","text":"En mer utførlig forklaring kan finnes f.eks. Miles Shevlin (2001).Hvis vi tenker oss vi har en gruppe på 100 personer vi ikke kjenner, men der vi vet gjennomsnittshøyden er 175 cm – hvor høy vil du gjette en tilfeldig person den gruppa er? kjenner vi kun gjennomsnittshøyden – hva vi kan kalle en parameter. Hvis vi ikke kjenner noen andre karakteristika, vil vår beste antakelse være 175 cm. Dette er gjennomsnittshøyden, og det er rimelig å anta vi vil treffe nærmest hvis vi sier 175 cm hver gang vi blir spurt om hvor høy vi tror en tilfeldig person den gruppa er. Vi kan selvsagt gjette 182 cm første gang, 168 cm andre gang, 171 cm tredje gang osv. og treffe 100%, men det vil være ren flaks. Første person kan være 163 cm, andre person 190 cm, tredje person 182 cm osv. Hver gang vil vi så fall bomme grovt. Gjennomsnittsverdien blir vår “modell”. En modell er en representasjon av virkeligheten (Miles Shevlin 2001). Modellen vil aldri være perfekt – hadde den vært prefekt hadde vi ikke hatt en modell av virkeligheten, men et duplikat av virkeligheten – det vil alltid være feil ved modellen en eller annen grad.vårt eksempel er modellen gjennomsnittshøyden, og vi vet selv om vår beste gjetning når vi blir spurt om høyden på en tilfeldig person er 175 cm vil vi oppleve av vi kun treffer noen få (og kanskje ingen) tilfeller. Modellen vår vil imidlertid søke å minimere feilene vi får slik vi treffer best mulig. Det er mer sannsynlig en tilfeldig person er nærheten av 175 cm enn 195 cm. Vi kan si:\\(Virkeligheten = Modell + Feil\\)Imidlertid vil vi ofte ikke kjenne “virkeligheten” - vi kjenner ikke populasjonsgjennomsnittet. Vi har som regel data om populasjonen fra et tilfeldig utvalg gjort av populasjonen. Vi vil derfor heller uttrykke:\\(Data = Modell + Feil\\)Data (altså en observert verdi – vårt tilfelle en høyde) = x. Gjennomsnittsverdi = \\(\\overline{x}\\). Det er vanlig å benevne feiltermen som \\(e\\). Vi kan derfor første observerte høydeverdi uttrykke dette matematisk som:\\(x_1 = x - e_1\\)Eller på en generell form:\\(x_i = \\overline{x} - e_i\\)Feil en modell vil ofte betegnes residual (fra engelsk: residual = rest/gjenværende). En residual er altså verdien vi sitter igjen med når vi trekker gjennomsnittsverdien fra den observerte verdien. Hvis den observerte høyden er 179 cm og gjennomsnittsverdien er 175 cm er residualen 3 cm. Dette kan vi uttrykke slik:\\(Feil = Data - Modell\\)Eller:\\(e = x - \\overline{x}\\)Residualbegrepet kommer vi sterkt tilbake til når vi skal ta oss regresjonsanalyse.Vi kommer til å snakke mye om modeller. Vi kommer også itl å snakke mye om “hvor god er modellen” - eller med andre ord: har vi klart å lage en modell som representerer “virkeligheten” på en god måte. Vi skal imidlertid huske på “models wrong, useful” (Box 1976). En statistisk modell vil aldri klare å representere den komplekse virkeligheten - vi må alltid forsøke å finne måter å måle og representere enkelte deler av virkeligeheten og lage modeller som kan fortelle oss noe fornuftig og nyttig om denne virkeligheten. Men selv om modellen alltid er feil og imperfekt kan den fortsatt være nyttig.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordeling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.5 Normalfordeling","text":"Når vi snakker om distribusjonen av et datasett tenker vi på hvordan dataene vi har samlet inn fordeler seg forhold til hverandre etter gitte egenskaper. Vi kan eksempel ha målt høyden på 100 mennesker. Disse dataene utgjør da en observert fordeling som vi kan sette inn et histogram å visualisere hvordan datasettet ser ut:\nFigure 1.1: Høydefordeling 100 tilfeldige menn, genererte data\nHvis vi tar et utvalg på 100 andre personer kan fordelingen se slik ut:\nFigure 1.2: Høydefordeling 100 andre tilfeldige menn, genererte data\nHver gang vi måler høyden på 100 tilfeldig utvalgte menn vil fordelingen se ulik ut siden de er observerte fordelinger et utvalg av populasjonen (alle) «norske menn». Hvis vi imidlertid økte antallet utvalget vi målte til 1000 eller 10000 vil vi med større sikkerhet kunne si vi faktisk viser populasjonens fordeling (mulighetene vi tilfeldigvis måler 10000 veldig lave eller veldig høye menn er svært liten). Vi kan derfor, gitt visse forutsetninger om utvalget, si noe om hele populasjonen ut fra utvalget.Hittil har vi snakket om observerte fordelinger – altså hva vi har målt, observert, samlet inn osv. Ut fra dette kan vi si vi kan ha visse forventninger til hvordan fordelingen av ulike populasjoner vil se ut, og vi kan snakke om teoretiske fordelinger – eller sannsynlighetsfordelinger med andre ord. Hvor sannsynlig er det en tilfeldig x-verdi dukker opp dataene?høyde kan vi ha visse forventninger til hvilke sannsynligheter det er en tilfeldig person har en gitt høyde, eller hvor mange prosent av den mannlige befolkningen som har en høyde innenfor et gitt intervall. Det vil si fordelingen har en viss form med visse karakteristika. Vi forventer flest observasjoner befinner seg nærheten av gjennomsnittet, og vi vil se færre og færre observasjoner jo lenger unna gjennomsnittet vi beveger oss. Vi forventer å finne flere norske menn 20 år på rundt 180 cm enn 160 cm eller 210 cm. fordelingen av høydedata vil vi si dette er data som er normalfordelte.En normalfordeling er en sannsynlighetsfunksjon der flesteparten av verdiene fra funksjonen samler seg om en sentral tendens, og der tettheten (hyppigheten, eller “density” på engelsk) av verdier avtar jevnt jo lenger unna den sentrale tendensen man kommer. Grafisk framstilt får fordelingskurven en klokkeform, og normalfordeling omtales også som “bell shaped”. Overraskende mange fenomener viser seg å være nærme en normalfordeling, og den er derfor en helt sentral teoretisk sannsynlighetsfordeling mange sammenhenger kvantitativ metode. Vi bruker dermed normalfordelingen som en modell observerte data.Vi skal ikke bry oss om det matematisk uttrykket sannsynlighetstetthetsfunksjonen. Hvis vi derimot genererer et tenkt datasett etter standard normalfordelingsfunksjon vil det kunne se slik ut:\nFigure 1.3: Genererte standard normalfordelte data\nkan vi legge på en forventningskurve – en teoretisk kurve som viser en standard normalfordeling:\nFigure 1.4: Genererte standard normalfordelte data med normalfordelingskurve\nVi kan ta bort det genererte datasettet og sitte igjen med bare forventningskurven:\nFigure 1.5: Normalfordelingskurve\nDet den standardiserte normalfordelingskurven (også kjent som Gausskurven eller også Bellkurven – “Klokkekurven” fordi den har en klokkeform) – kan brukes til er å si noe om spredningen på forventede verdier – eller hvor langt fra gjennomsnittsverdien man kan forvente å finne de enkelte verdiene.Før vi ser nærmere på egenskaper ved normalfordelingskurven kan det være nødvendig å gå litt inn på begrepene varians og standardavvik som mål på spredningen datasett. Disse begrepene, spesielt standardavvik, vil være helt sentrale videre arbeid med temaet.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"varians-og-standardavvik","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.6 Varians og standardavvik","text":"Variansen en variabel representerer det gjennomsnittlige avviket fra gjennomsnittsverdien (Field, Miles, Field 2012a) og er et mål på spredningen dataene (som navnet antyder: hvor mye dataene variere ut fra den sentrale tendensen). vises et eksempel basert på Field (2009b).La oss anta vi har spurt 5 studenter på høgskolen hvor mange kjæledyr de har. Svarene kan settes opp en enkel tabell. gjennomsnitt har de 2,6 kjæledyr. Vi ønsker imidlertid å se hvor mye avviket er den enkelte fra snittet (siden vi har regnet ut snittet kan vi se på gjennomsnittsverdien som en modell på forholdet mellom studenter og antall kjæledyr). Vi registrerer svarene vi fikk et skjema:StudentnrAntallAvvikAvvik_kvadrert11-1.62.5622-0.60.36330.40.16430.40.16541.41.96Snitt2.6Sum05.20Når vi regner ut avviket (sum deviances) summerer vi avvikene. Siden denne er 0 skulle det innebære det totalt sett “modellen” ikke er avvik mellom modellen og våre virkelige observasjoner. Problemet er det er både positive og negative avvik som nuller hverandre ut. Man må derfor kvadrere avvikene å omgå problemet med fortegn. Imidlertid får vi et nytt problem. La oss anta vi stedet 5 studenter har spurt 500. Da får vi et svært høyt kvadrert avvik fra snitt. Altså – vi må ta høyde antallet observasjoner. Vi deler derfor sum kvadrert avvik fra snitt på antall observasjoner (5,20/5). MEN: vi må foreta et litt teknisk og komplisert tillegg utregningen. Vi må dele på antall observasjoner MINUS 1 (som er antallet frihetsgrader – degrees freedom). Dette vil ikke bli nærmere forklart , men de som ønsker å lese mer om frihetsgrader kan prøve noen andre kilder, f.eks. Walker (1940), Good (1973) eller Pandey Bright (2008). Vi ender altså opp med regnestykket 5,20/(5-1) = 1,3. Dette er variansen. Variansen er altså det gjennomsnittlige avviket mellom gjennomsnittsverdien av de observerte dataene og verdiene til de enkelte observasjonene.Som regel snakker vi imidlertid om standardavviket. Dette finner vi ved å ta kvadratroten av variansen (som vi jo har funnet ved å kvadrere avvikene å unngå fortegnsproblemer). Vi får da vårt tilfelle et standardavvik på 1,14. Variansen og standardavviket forteller oss altså noe om spredningen dataene. Liten varians betyr spredningen er liten (om vi har gjennomført en spørreundersøkelse betyr det respondentene har svart ganske likt). Stor varians betyr stor spredning (respondentene har svart ganske ulikt).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordeling-standardavvik-og-forventninger","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7 Normalfordeling, standardavvik og forventninger","text":"Vi kan nå se nærmere på normalfordelingen.\nFigure 1.6: Normalfordeling med 1 standardavvik\nEtt standardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 68 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan vise det samme 2 og 3 standardavvik:\nFigure 1.7: Normalfordeling med 2 standardavvik\nstandardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 95 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan finne arealet mellom x=-2 og x=2, som er 0.95449971.\nFigure 1.8: Normalfordeling med 3 standardavvik\nTre standardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 99.7 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan finne arealet mellom x=-3 og x=3, som er 0.99730022. Dette utgjør et kjernepunkt statistisk prosesskontroll som vi vil komme mye tilbake til.Oppsummert kan vi framstille normalfodeling og standardavvik slik (Hartmann, Krois, Waske 2018b):\nFigure 1.9: Normalfordeling med standardavvik\nSom nevnt er mange fenomener hverdagen normalfordelte, eller nærme nok normalfordeling til vi kan bruke normalfordeling som teoretisk modell observerte data 3. Det finnes imidlertid mange tilfeller der vi ikke kan bruke normalfordelingen. Hvis dataene er sterkt asymmetriske vil ikke reglene normalfordeling som vi har skissert ovenfor gjelde 4.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"er-dataene-dine-normalfordelte","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1 Er dataene dine normalfordelte?","text":"Vi vil se senere mange tester har en forutsetning om dataene er normalfordelte. Ulike analyser vi gjør statistisk har forutsetninger/bygger på antalkelser om hvordan dataene er fordelt (det gjelder forsåvidt alle statistiske analyser vi gjør). Dvs. mange sammenhenger må de være “tilnærmet” normalfordelte. Mange tester er såkalt “robuste”, altså de tåler avvik fra en nærmest perfekt normalfordeling uten resultatene av testen nødvendigvis blir uåpålitelige. Det er rimelig å si det er tildels stor uenighet om hvor alvorlig avvik fra normalfordelingens teoretiske forventning man kan være likevel å bruke ulike statistiske analyser. Vi skal også være klar utregning av såkalte testverdier (som Shapiro-Wilk eller Anderson-Darling) også bygger på visse forutsetninger.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"histogram","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.1 Histogram","text":"\nFigure 1.10: Høydefordeling 100 tilfeldige menn, genererte data\n","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"qq-plott","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2 QQ-plott","text":"Q-Q plottet (“quantile-quantile plot”) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet (se video forklaring på utregning) innebærer å se distribusjoner mot hverandre – empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om “normal Q-Q plott - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). eksempelet vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med.viser vi typiske mønstre histogram og “tilhørende” qq-plott som kan være til hjelp tolkning av dataene dine. Dette er genererte tall og ikke tallene fra eksempelet :","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordelt","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.1 Normalfordelt","text":"Download QQ_norm.xlsx\nFigure 1.11: Q-Q plott normalfordeling\nVi ser dette Q-Q plottet viser oss vi kan være ganske sikre på dette datasettet er normalfordelt (noe som gir meninig siden vi har brukt R til å lage et normalfordelt datasett).","code":"\nset.seed(89)\n# base R\n# Bruker pakken: tibble (Tidyverse)\nqqnorm <- as_tibble(rnorm(10000, mean=90, sd=5))\n# Bruker pakken: writexl\nwrite_xlsx(qqnorm,\"QQ_norm.xlsx\")\n# Bruker pakken: ggpubr\nggqqplot(qqnorm$value) + ggtitle(\"Normal Q-Q plott\") + labs(x = \"Teoretisk forventning\", y = \"Data\")"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"skjevhet-høyre","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.2 Skjevhet høyre","text":"Download QQ_norm_rs.xlsx\nFigure 1.12: Q-Q plott - fordeling skjevhet høyre\net datasett med høyreskjevhet vil ofte Q-Q plottet vise en bananform med bunnen/midten av bananen ned mot høyre hjørne og endene pekende oppover/utover fra den rette linjen.","code":"\n# Lage datasett med right skew\n# base R\n# Bruker pakken: tibble\nset.seed(90)\nN <- 5000\nqqrightskew <- as_tibble(rnbinom(N, 10, .1))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqrightskew,\"QQ_norm_rs.xlsx\")\n# Plotte histogram og Q-Q plott\"\n# Bruker pakken: ggplot2\nqqrighthist <- ggplot(qqrightskew, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqrightskew_plott <- ggqqplot(qqrightskew$value) + ggtitle(\"Normal Q-Q plott - skjevhet høyre\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqrighthist, qqrightskew_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"skjevhet-venstre","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.3 Skjevhet venstre","text":"Download QQ_norm_ls.xlsx\nFigure 1.13: Q-Q plott - fordeling skjevhet venstre\ndette datasettet har vi generert en kraftig skjevhet til venstre. Q-Q plottet får da en omvendt bananform forhold til høyre skjevhet, altså en topp på midten og ender som svinger nedover ift den rette linja.","code":"\n# Lage datasett med left skew\n# base R\n# Bruker pakken: tibble\nset.seed(91)\nN=5000\nqqleftskew <- as_tibble(rbeta(N,5,1,ncp=0))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqleftskew,\"QQ_norm_ls.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken: ggplot2\nqqlefthist <- ggplot(qqleftskew, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqleftskew_plott <- ggqqplot(qqleftskew$value) + ggtitle(\"Normal Q-Q plott - skjevhet venstre\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqlefthist, qqleftskew_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"tunge-haler","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.4 Tunge haler","text":"Download QQ_ht.xlsx\nFigure 1.14: Q-Q plott - ‘heavy-tail’\n“Heavy-tailed” (fete/tunge haler) har større sannsynlighet ekstreme verdier vil forekomme). Fordelinger med tunge haler vil ofte følge en slags S-form, men den er ofte mer “liggende” enn S-formen til fordeling med lette haler. Den starter med å vokse raskere enn normalfordelingen og ender med å vokse saktere.","code":"\nset.seed(14)\nN=100\n# Bruker pakken: tibble\n# Base R\nqqcauchy <- as_tibble(rcauchy(N, scale = 5)) \n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqcauchy,\"QQ_ht.xlsx\")\n# Bruker pakken: ggplot2\nqqcauchyhist <- ggplot(qqcauchy, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqcauchy_plott <- ggqqplot(qqcauchy$value) + ggtitle(\"Normal Q-Q plott - tung hale\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqcauchyhist, qqcauchy_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"lette-haler","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.5 Lette haler","text":"Download QQ_lt.xlsx\nFigure 1.15: Q-Q plott - ‘light-tail’\n“Light-tailed” (lette haler) har liten sannsynlighet ekstreme verdier og utvalg tenderer til å ikke fravike gjennomsnittet med mye. Q-Q plottet en fordeling med lette haler har ofte en S-form. Dataene vokser saktere enn normalfordelingen starten før den følger vekstraten til normalfordelingen. Mot slutten vokser den raskere enn normalfordelingen. Derfor bøyer den av fra normalfordelingen.","code":"\nset.seed(81)\n# Base R\n# Bruker pakken: tibble\nqqlt <- as_tibble(runif(n = 1000, min = -1, max = 1))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqlt,\"QQ_lt.xlsx\")\n# Bruker pakken: ggplot2\nqqlthist <- ggplot(qqlt, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken ggpubr\nqqlt_plott <- ggqqplot(qqlt$value) + ggtitle(\"Normal Q-Q plott - lett hale\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqlthist, qqlt_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"bimodalitet","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.6 Bimodalitet","text":"Download QQ_bimod.xlsx\nFigure 1.16: Q-Q plott - bimodal\nDen bimodiale fordelingen viser ofte et brudd eller et distinkt knekkpunkt rundt krysning av den rette linja, med en del av linja på hver side av den rette linja.","code":"\nset.seed(10) \n# Base R\n# Bruker pakken: ggplot2\n# Bruker pakken: tibble\nmode1 <- rnorm(50,2,1)\nmode1 <- mode1[mode1 > 0] \nmode2 <- rnorm(50,6,1)\nmode2 <- mode2[mode2 > 0] \nqqbimod <- as_tibble(sort(c(mode1,mode2)))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqbimod,\"QQ_bimod.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken ggplot2\nqqbimodhist <- ggplot(qqbimod, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqbimod_plott <- ggqqplot(qqbimod$value) + ggtitle(\"Normal Q-Q plott - bimodial\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqbimodhist, qqbimod_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"multimodalitet","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.2.7 Multimodalitet","text":"Download QQ_multimod.xlsx\nFigure 1.17: Q-Q plott - multimodal\nMultimodale fordelinger vil som regel vise flere brudd.","code":"\n# Bruker pakken: readxl\n# Bruker pakken: tibble\nqqmultimod <- as_tibble(read_xlsx(\"Multimodal.xlsx\"))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqmultimod,\"QQ_multimod.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken ggplot2\nqqmultimodhist <- ggplot(qqmultimod, aes(x=Verdi)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqmultimod_plott <- ggqqplot(qqmultimod$Verdi) + ggtitle(\"Normal Q-Q plott - multimodial\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqmultimodhist, qqmultimod_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistiske-tester-for-vurdering-av-dataenes-distribusjon","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.3 Statistiske tester for vurdering av dataenes distribusjon","text":"Vi har nå sett på noen typiske eksempler på mønstre Q-Q plott. Det kan imidlertid være vanskelig å bedømme fordelinger som ligger nære normalfordelingen, men likevel ikke perfekt oppå (du vil trolig aldri se en perfekt match med mindre du har generert et normalfordelt datasett med mange datapunkter). Vi kan supplere Q-Q plottene med visse statistiske tester (men husk: disse statistiske testene har sine egne forutsetninger og er heller ikke uten utfordringer).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"anderson-darling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.3.1 Anderson-Darling","text":"Anderson-Darlings test er en test å se om et datasett kommer fra en gitt fordeling, f.eks. normalfordelingen (T. W. Anderson Darling 1952; T. W. Anderson Darling 1954). Testen setter opp hypoteser:\\(H_0\\): Dataene følger normalfordelingen\\(H_1\\): Dataene følger ikke normalfordelingenDownload Anderson-Darling_raw.xlsxSiden vi vet nullhypotesen er datasettet har en normalfordeling vil vi forkaste nullhypotesen dersom vi har en signifikant p-verdi (grensen hva som er signifikant bestemmer vi forsåvidt selv, men vanlige verdier er 0.01, 0.05 og 0.1). Altså - dette tilfellet har vi en p-verdi=0.04. Vi forkaster derfor nullhypotesen og aksepterer \\(H_1\\) som sier dataene er trolig ikke er normalfordelte (med andre ord: p-verdien må være større enn signifikansverdien vi skal si dataene trolig er normalfordelte) - en huskeregel: “p low, null must go” (: “low” = terskelverdien vi har satt, ofte 0.05).Generisk ser dette slik ut (Hartmann, Krois, Waske 2018a):Det er verdt å merke seg Anderson-Darling testen egentlig ikke forteller deg dataene dine er normalfordelte, men det er usannsynlig de ikke er det om testen viser det. Dette synes kanskje som samme sak, men er realiteten en viktig erkjennelse – en tørr gressplen er et bevis det ikke har regnet, men en våt gressplen er ikke bevis det har regnet. En våt gressplen kan skyldes andre ting enn regn. Altså – en signifikant p-verdi på testen gjør vi forkaster \\(H_0\\) og antar fordelingen er ikke-normal. En ikke-signifikant p-verdi på gjør vi med f.eks. 95% konfidens kan si vi ikke har funnet avvik fra normalfordelingen.Det finnes flere andre statistiske tester som kan kjøres å teste normalitet, f.eks. Kolmogorov-Smirnov, Shapiro-Wilks og Cramer Von-Mises test. Anderson-Darling er en modifisering/videreutvikling av Kolmogorov-Smirnov og anses ofte som en bedre test av de . Andre kilder (se f.eks. Razali Wah (2011)) finner Shapiro-Wilks presterer best 10 000 simuleringer på ulike distribusjoner.Tolkning Kolmogorov-Smirnov: Hvis p-verdien er valgte signifikansnivå (f.eks. 0.05) skal vi anta datasettet ikke er normalfordelt. vil testen peke på datasettet ikke er normalfordelt.Tolkning av Shapiro-Wilks og Cramer-von Mieses test er lik som Kolmogorov-Smirnov.Som et siste eksempel på en statistisk test normalitet kan vi bruke Jarque-Bera test. Denne skiller seg litt ut fra de andre ved den spesifikt ser på skjevhet og kurtosis datasettet opp mot hva en normalfordeling vil ha. å gjøre lykken komplett finnes det versjoner av testen:Tolkningen er lik som før - hvis p-verdien er mindre enn valgte signifikansnivå peker det mot datasettet ikke er normalfordelt. , motsetning til de øvrige testene, er p-verdien større enn signifikansnivået (0,05) så det peker mot datasettet er normalfordelt.Dette er altså ikke så enkelt. Det finnes mange statistiske tester, som kan gi motsatte indikasjoner på om et datasett er normalfordelt eller ikke siden de ser på dataene fra “ulik vinkel” (fokuserer på ulike aspekter ved dataene). Vårt råd blir: Start alltid med Q-Q plott. Velg evt en teststatistikk, men vær klar alle teststatistikker bygger på forutsetninger eller tester ulike sider av distribusjonen. Det vi også kan huske på er henhold til sentralgrenseteoremet (“Central Limit Theorem”) vil populasjonens fordeling være av mindre interesse dersom utvalgsstørrelsen er stor nok. Hva er stor nok? De fleste kilder peker mot 30 er “stort nok”.","code":"\n# Bruker pakken: tibble\n# Bruker pakken: readxl\naddata <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Bruker pakken: nortest\nad.test(addata$Values)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  addata$Values\n#> A = 0.74573, p-value = 0.04046\noptions(scipen=999)\n# Bruker pakken: tibble\n# Bruker pakken: readxl\naddata5 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Base R\nks.test(addata5, \"pnorm\")\n#> \n#>  One-sample Kolmogorov-Smirnov test\n#> \n#> data:  addata5\n#> D = 0.88493, p-value = 0.0000000000000171\n#> alternative hypothesis: two-sided\n# Base R\nshapiro.test(addata5$Values)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  addata5$Values\n#> W = 0.87521, p-value = 0.04027\n# bruker pakken: nortest\ncvm.test(addata$Values)\n#> \n#>  Cramer-von Mises normality test\n#> \n#> data:  addata$Values\n#> W = 0.12634, p-value = 0.04326\n# Bruker pakken: tibble\n# bruker pakken: readxl\naddata6 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Bruker pakken: tseries\njarque.bera.test(addata6$Values)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  addata6$Values\n#> X-squared = 2.1953, df = 2, p-value = 0.3337\n# Bruker pakken: normtest\najb.norm.test(addata6$Values, nrepl=2000)\n#> \n#>  Adjusted Jarque-Bera test for normality\n#> \n#> data:  addata6$Values\n#> AJB = 3.1014, p-value = 0.131"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"boxplott","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.4 Boxplott","text":"Et boxplott forteller oss mye om dataenes distribusjon.Selve boksen representerer 50 % av observasjonene/casene, det vil si nedre kant representerer første kvartil (= 25.prosentil) og øvre kant tredje kvartil (= 75.prosentil).Den tykkere horisontale streken boksen viser medianverdien (= andre kvartil = 50.prosentil)Dersom en observasjon ligger utenfor en terkselverdi (jfr figur ) vises dette med en liten sirkel. Dette defineres som uteliggere. Å identifisere uteliggere kan være viktig mange statistiske tester.Galarnyk (2018) illustrerer boxplott slik:","code":"\n# Base R\npar(mfrow=(c(1,2)))\nBoxplot(Field_OLS_data$Adverts, id = list(n=Inf), ylab = \"\", main = \"Adverts\", col = \"Blue\")\n#> [1]  43  87 184\nBoxplot(Field_OLS_data$Sales, id = list(n=Inf), ylab = \"\", main = \"Sales\", col = \"Green\")"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"scatterplott","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.7.1.5 Scatterplott","text":"Et scatterplott viser oss på en god visuell måte hvordan de variablene forholder seg til hverandre (vi plotter hver enkelt observasjon gjennom verdiene de har på de variablene). Mønsteret kan derfor si oss mye om sammenhengen mellom de .En god måte å fremstille et scatterplott på R (gjennom pakken car) er denne:kombinerer vi scatterplott og boxplott. Den rette blå linja er en minste kvadratssums regresjonslinje (OLS). Den stiplede blå linja bruker en ikke-parametrisk tilnærming. tillegg får vi visualisert de fire mest ekstreme tilfellene (lengst vekk fra gjennomsnitt).","code":"\n# Bruker pakken: ggplot2\nggplot(Field_OLS_data, aes(x = Adverts, y = Sales)) +\n  geom_point(colour = 4)\n# Bruker pakken: car\nscatterplot(Adverts ~ Sales, data = Field_OLS_data, id = list(n=4))#> [1]   1  87 169 184"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"binomialfordeling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.8 Binomialfordeling","text":"En distribusjon hvor det kun er mulige utfall av en hendelse kalles en binomial fordeling. Et myntkast er en slik hendelse (gitt vi ser bort fra den fysiske muligheten mynten kan lande stående på høykant). Levende eller død kan også være et eksempel på dette. Det ene utfallet utelukker det andre, men de er uavhengige fordi resultatet ett myntkast ikke påvirker resultatet neste myntkast. Alle myntkastene må derimot være identiske, det vil si sannsynligheten det ene eller det andre resultatet er lik hver gang forsøket eller myntkastet gjennomføres. Hvis vi har lik sannsynlighet, kan en tilfeldig generert binomial distribusjon se slik ut:\nFigure 1.18: Binomialfordeling med lik sannsynlighet\ndiagrammet vises en sannsynlighetsfordeling en binomial fordeling der utfallene suksess/fiasko har lik sannsynlighet. Hvis vi gjennomfører en aktivitet med disse karakteristika 20 ganger kan vi bruke sannsynlighetsfordelingen til å skape en forventning om sannsynligheten antall suksesser/fiaskoer. Hver gang vi gjennomfører aktiviteten blir det enten suksess eller fiasko. Hvis vi har 50% sjanse suksess eller feil hver gang vi gjennomfører aktiviteten er sannsynligheten suksess lik som sannsynligheten fiasko. Vi kan da forvente det er størst sannsynlighet vi 10 av 20 tilfeller får suksess. Det er liten sannsynlighet vi enten får suksess 0 eller 20 av 20 ganger vi gjør aktiviteten.Det er imidlertid verdt å merke seg de utfallene ikke trenger å ha lik sannsynlighet. Da vil den binomiale distribusjonen se annerledes ut:\nFigure 1.19: Binomialfordeling med ulik sannsynlighet\nhar vi bare 20% sannsynlighet suksess, og fordelingen av sannsynligheter vil se annerledes ut. Med 20% sannsynlighet suksess er det veldig liten sannsynlighet vi vil få 10 eller flere suksesser hvis vi gjør forsøket 20 ganger. Det er størst sannsynlighet å få 4 suksesser.Et terningkast (med en vanlig terning med 6 sider) – som ikke er tuklet med – har lik sannsynlighet å lande på hhv 1,2,3,4,5 og 6. Det vil si det er 1/6 sannsynlighet 1, 1/6 sannsynlighet 2 osv. Hvis vi kaster denne terningen 10 ganger kan resultatet se slik ut:\nFigure 1.20: 10 terningkast\nVi ser vi ikke fikk noen 2’ere og 5’ere. Dette kan vi forvente når vi bare har 10 terningkast. Hvis vi imidlertid kaster terningen 100 ganger vil det være svært liten sannsynlighet å ikke få «treff» på alle 6 verdiene på terningen, og vi burde kunne forvente vi får en ganske jevn fordeling på alle 6 verdiene. Nedenfor vises resultatet av 100 terningkast.\nFigure 1.21: 100 terningkast\nVi ser vi har en relativt jevn fordeling. Noe ulikhet er det selvsagt, noe vi vil forvente fra en tilfeldig prosess. Hvis vi gjennomførte 1000 eller 10000 terningkast vil fordelingen bli nærmere og nærmere den teoretisk forventede fordelingen. Vi kan burde, teoretisk, forvente 100 treff på hver mulighet hvis vi kaster terningen 600 ganger, men vi vil sjelden se akkurat 100 treff på hver slik vi ser hvis vi kjører tre runder med 600 terningkast:Runde 1:Runde 2:Runde 3:Selv om vi kjører 6 000 000 terningkast og vil forvente 1 000 000 treff på hver av terningens sider vil vi ikke få en perfekt fordeling iht teoretisk forventning, men resultatet vil være svært nærme og er nærme nok til vi kan bruke sannsynlighetsfordelingen til å lage forventninger om utfall:6 000 000 terningkast:Hvis vi setter resultatet fra 6 000 000 terningkast inn et histogram ser vi resultatet er svært nærme hva vi teoretisk vil forvente:\nFigure 1.22: 6 000 000 terningkast\n","code":"#> terning_runde1\n#>   1   2   3   4   5   6 \n#>  93 102 102 102 108  93#> terning_runde2\n#>   1   2   3   4   5   6 \n#> 101 102  94  91 105 107#> terning_runde3\n#>   1   2   3   4   5   6 \n#> 104 113  92  98  95  98#> minterning\n#>       1       2       3       4       5       6 \n#> 1000492  998250 1000216 1000832 1001422  998788"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"poissonfordeling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.9 Poissonfordeling","text":"Poissonfordelinger finnes situasjoner der hendelser skjer vilkårlig tid (og rom) hvor vi er interessert kun antallet hendelser et gitt tidsintervall. Vi kan f.eks. være interessert hvor mange supporthenvendelser vi får løpet av en time, antallet feilmedisineringer per uke, hvor mange besøk avdelingen får per dag o.l. Andre eksempler kan være antall trafikkulykker langs en angitt veistrekning, antall elgpåkjørlser på en togstrekning, eller antall av en gitt art fugler et definert område et definert tidsrom. En hendelse må være uavhengig tidsmessig av andre hendelser (det er altså ikke økt sannsynlighet en hendelse vil skje fordi en tilsvarende hendelse akkurat har skjedd), sannsynligheten en hendelse et kort perspektiv er lik sannsynligheten et lengre perspektiv, og ettersom et tidsintervall blir kortere og kortere vil sannsynligheten hendelsen gå mot null.Poissonfordeling uttrykker sannsynligheten et gitt antall hendelser inntreffer et gitt tidsintervall (eller et gitt geografisk domene) og vi kjenner gjennomsnittlig hvor ofte hendelsen inntreffer. Denne sannsynligheten uttrykkes som en lambdaverdi (\\(\\lambda\\)).Eksempelet er hentet fra Soage (2020):\nFigure 1.23: Poissonfordelinger\nUt fra hvilken \\(\\lambda\\)-verdi vi setter kan vi si noe om sannsynligheten et antall hendelser inntreffer.Ugarte, Militino, Arnholt (2016) eksemplifiserer Poissonfordeling ved å vise til det gjennomsnitt skåres 2,5 mål en VM-kamp fotball. Denne situasjonen tilfredsstiller forutsetningene å bruke Possionfordeling.Vi kan grafisk framstille sannsynlighetsfordeingen slik:\nFigure 1.24: Poissonfordeling mål VM-kamp fotball\nR kan vi også enkelt regne ut den nøyaktige sannsynligheten x antall mål gitt forutsetningen om det snitt skåres 2.5 mål pr kamp til å være 0. Vi kan bruke sannsynlighetsfordelingen til å regne ut sannsynligheten et gitt antall mål, f.eks.:Sannsynligheten 0 mål = 0.082085Sannsynligheten 1 mål = 0.2052125Sannsynligheten 2 mål = 0.2565156Sannsynligheten 3 mål = 0.213763Sannsynligheten 4 mål = 0.1336019eller f.eks. sannsynligheten det skåres mellom 1 og 3 mål (= 0.6754911).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"geometrisk-fordeling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.10 Geometrisk fordeling","text":"En geometrisk fordeling er en diskret fordeling der man teller antall hendelser/forsøk inntil et gitt resultat forekommer. Resultatet er suksess eller feil, altså hvor mange ganger man har en hendelse før man får en suksess eller feil (avhengig av hva man måler). Et eksempel er hvor mange ganger man må kaste terninger å få 11 sum. Man kaster da terninger til første gang man får 11 (= suksess). En geometrisk distribusjon kan se slik ut (p = 0,4):\nFigure 1.25: Geometrisk fordeling\nstatistisk prosesskontroll er denne typen fordeling til stede når man f.eks. teller antall dager mellom sjeldne hendelser. Man teller antall dager før man f.eks. får et alvorlig avvik på en medisinering, en operasjon e.l. geometrisk fordeling er sannsynligheten et gitt utfall uavhengig av om det har skjedd før. Man kan bruke geometrisk fordeling f.eks. til å estimere hvor mange dager man normalt vil forvente det går mellom en sjelden hendelse. Hvis man gjennom erfaringstall vet sannsynligheten en sjelden hendelse er p = 0.035 vil man forvente det går 1/0.035 \\(\\approx\\) 29 dager mellom hver hendelse. Geometrisk distribusjon kan hjelpe oss en statistisk prosesskontroll å finne normal/unormal variasjon ved sjeldne hendelser.Det kan være verdt å merke seg binomial og geometrisk fordeling skiller seg fra hverandre ved geometrisk fordeling har et ukjent antall hendelser (man fortsetter til man får første suksess/feil), mens binomial fordeling har et gitt antall hendelser. Som vi skal se senere eksempler derfor geometrisk fordeling viktig når vi håndterer sjeldne hendelser, fordi vi ikke kjenner hvor mange dager det f.eks. går før vi får første suksess/feil.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"eksponensiell-fordeling","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.11 Eksponensiell fordeling","text":"En tilfeldig kontinuerlig variabel kan sies å være analog til den geometriske distribusjonen, men kontinuerlige data. Den eksponensielle distribusjonen brukes ofte å modellere tid mellom hendelser. statistisk prosesskontroll vil vi typisk bruke denne distribusjonen hvis vi måler tid mellom sjeldne hendelser. Hvis vi f.eks. måler tiden mellom uventet dødsfall som følge av en type rutineoperasjon på et sykehus vil den ha en eksponensiell distribusjon hvis sannsynligheten hendelsen inntreffer innenfor t gitt tidsintervall er omtrentlig proporsjonal med lengde på tidsintervallet (Taboga 2017). Eksponensielle fordelinger har samme grunnform, men kan ha ulik bratthet avhengig av den såkalte lamdaverdien (= en parameter raten av hendelser). Lambdaverdi er en parameter hvor ofte hendelsene forventes å skje.\nFigure 1.26: Eksponensiell fordeling\n","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"nullhypotese","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.12 Nullhypotese","text":"Vi kommer mye tilbake til hypotesetesting (ulike former), men dette danner grunnlaget å forstå hvorfor vi tester en nullhypotese og forkaster den hvis vi får et signifikant resultat. Vi ønsker å teste hypotesen om M = 53 [51,55] er en god estimator μ. stedet å teste alle muligheter μ vil ligge intervallet, tester vi stedet en presist formulert og testbar nullhypotese om μ ikke vil ligge intervallet [51,55]. Hvis vi får et signifikant resultat på nullhypotesetesten kan vi si sannsynligheten μ vil ligge utenfor [51,55] er svært liten (avhengig av konfidensnivå), og vi derfor har styrket hypotesen om M=53 [51,55] er en god estimator μ. Vi setter med andre ord opp en stråmann: vi vil egentlig teste om våre estimatorer populasjonen er sannsynlige innenfor et konfidensintervall, men tester stedet sannsynligheten de ikke er det håp om å forkaste stråmannen.Nullhypotesen formuleres som regel som en presis og testbar hypotese om ingen forbindelse eller forskjell mellom gitte variabler. Nullhypotesen kan imidlertid være «hva som helst», den forstand det er like gyldig å formulere en nullhypotese som ikke inneholder null betydningen tallet null eller ingen forskjell e.l. Poenget er den må formuleres slik den evt kan forkastes hvis den ikke støttes (“null” kommer, har jeg blitt fortalt, fra det engelske “nullify” - altså “gjøre ugyldig, annullere, oppheve”).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistisk-styrke-statistical-power---og-type-i-og-ii-feil","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.13 Statistisk styrke – “Statistical Power” - og type I og II feil","text":"mange sammenhenger anvendt statistikk leser man om statistisk styrke (“power”). Enkelt forklart er statistisk styrke sannsynligheten en statistisk test vil identifisere en effekt hvis den er der. hypotesetesting referer statistisk styrke til sannsynligheten å få et statistisk signifikant resultat som fører til vi forkaster nullhypotesen når den alternative hypotesen er sann. Når den statistiske styrken øker synker sannsynligheten vi ikke forkaster en feilaktig nullhypotese (type II feil). Alternativt kan man si når den statistiske styrken testen øker, øker sannsynligheten vi korrekt godtar en sann alternativ hypotese.Vi kan uttrykke dette slik:\\(Statistisk\\ styrke = 1 - \\beta\\)En ofte sitert og brukt vurdering rundt nivået på statistisk styrke (som altså er et tall mellom 0 og 1) er J. Cohen (1988). Cohen foreslår 0,8 som et nivå på statistisk styrke som god avveining mellom sannsynligheten type og type II feil. Type feil forekommer når man feilaktig forkaster \\(H_0\\) når den er sann, mens type II feil innebærer å feilaktig beholde \\(H_0\\)når den er usann (eller: vi konkluderer med det ikke er noen effekt når det faktisk er en) (Mayr et al. 2007). Vi kan oppsummere dette slik:Vi kan med andre ord treffe riktig konklusjon av de fire mulighetene, men også feil av de fire mulighetene. å huske forskjellen på type og type II feil pleier jeg å huske:Seeing something (type ) – det vi også kallen en falsk positivNot seeing something (type II) – det vi også kaller en falsk negativP. Ellis (2010) illustrerer dette slik:Cohen postulerer de fleste forskere vil anse type som langt verre enn type II, faktisk 4 ganger så ille. Dersom man velger \\(\\alpha=0.05\\) (95% konfindensnivå) må da \\(\\beta = 0.05*4=0.2\\). Vi får da:\\(Power = 1 - \\beta\\)\n\\(Power = 1 . 0.2 = 0.8\\)En annen måte å si dette på er med statistisk styrke = 0,8 har man 80 % sjanse å detektere en effekt hvis det virkelig er en effekt. Lav statistisk styrke fører altså til ikke-signifikante resultater. Et ikke-signifikant resultat betyr et uavklart resultat: Det kan være en effekt der og det kan hende det ikke er et resultat der. Et ikke-signifikant resultat betyr IKKE det ikke kan være en effekt. Derimot vil et ikke-signifikant resultat oftest føre til man tolker resultatet som det ikke er noen effekt, og det vil være en type II feil hvis det faktisk er en effekt der som vi ikke ser pga lav statistisk styrke.Vi skal ikke gå nærmere inn på type og type II feil . design av undersøkelser og analyser kan man gjøre valg som reduserer sannsynligheten å gjøre en av feilene, men de typene feil henger sammen så hvis man reduserer sannsynligheten den ene øker man samtidig sannsynligheten den andre feilen (og motsatt). må den som foretar undersøkelsen ta noen valg ut fra situasjonen og hvilken feil som vil være mest alvorlig å gjøre, men det er vanlig å regne type feil som mer alvorlig enn type II (vitenskapsteoretisk sett). Grunnen til dette er man anser det som verre å gå glipp av noe som har en faktisk effekt, enn å hevde noe har en effekt når det ikke har det (men eksempel innen medisinsk forskning kan dette være stikk motsatt – det vil eksempel kunne være svært uheldig om man feilaktig konkluderer med en ny medisin eller behandling ikke har negative bivirkninger hvis den faktisk har det).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistisk-styrke---litt-mer","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.14 Statistisk styrke - litt mer","text":"Statistisk styrke kan enten brukes priori eller post hoc – før eller etter.\nå ta det siste først (post hoc). Dette innebærer vi kalkulerer statistisk styrke etter undersøkelsen og analysene er gjort (eller som regel ser på hva f.eks. SPSS forteller oss). Det finnes sterke advarsler mot å gjøre dette (Cumming 2012). Hoenig Heisey (2001) anser dette som fundamentalt feil. Det er likevel rimelig å si dette er vanlig. Man skal hvert fall være klar informasjonen vi får ut av post hoc statistisk styrketester er begrenset og, hevdes det, brukes til dels villedende.Imidlertid er “alle” enige om priori kan statistisk styrke være en viktig del av design av en undersøkelse. Mer spesifikt kan vi bruke “power calculations” å regne ut hvor stort utvalg vi trenger å tilfredsstille et gitt konfidensnivå og antall variabler.å gjennomføre en priori estimering av hvor stor N vi trenger en undersøkelse trenger vi å vite:Hvilken type test vi skal gjennomføre: dette kan gi ulik informasjon man trenger estimering, men uansett trenger man 2-4:Forventet effektstørrelse (f.eks. Cohens d)Ønsket statistisk styrkeSignifikansnivåRetningslinjer effektstørrelse J. Cohen (1988) gir:\nTable 1.1: Effektstørrelser, modifisert fra Cohen (1988)\nEt praktisk hjelpemiddel priori vurderinger rundt design av studier - f.eks. å finne ut hvor stort utvalg (hvor stor N) man bør ha ut fra kriteriene 1-4 ovenfor er programmet G*Power (Faul et al. 2007, 2009) som kan lastes ned .Et eksempel: Vi planlegger å gjennomføre en undersøkelse der vi skal kjøre en multippel lineær regresjonsanalyse. G*Power legger vi følgende verdier: Effect size = 0,15; α = 0,05; Power = 0,8; Number predictors (antall uavhengige variabler) = 3G*Power vil kunne gi oss et plott der vi kan vurdere utvalgsstørrelse:Dette plottet kan vi bruke planlegging av en undersøkelse. Det viser oss nødvendig N (y-aksen) en gitt statistisk styrke med den valgte effektstørrelsen. Vi kan visuelt se hvordan en endring statistisk styrke vil gi utslag nødvendig N. Som vi skal komme tilbake til andre steder notatet er planlegging av en studie viktig slik vi får tilstrekkelig stort utvalg forhold til hva vi ønsker å undersøke (ut fra parametrene ovenfor), men samtidig vi ikke “overdriver” utvalgsstørrelsen. Dette kan også få uønskede konsekvenser (som vi kommer tilbake til allerede neste delkapittel).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"effektstørrelse-og-litt-om-p","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.15 Effektstørrelse (og litt om “p”)","text":"(en god nettressurs de som trenger å regne på effekstørrelser - og som beskriver mange forhold rundt ulike effektmål - er Lenhard Lenhard (2017)).Det er, som vi nå ser, en direkte sammenheng mellom effektstørrelse og statistisk styrke. Jo mindre effekt, jo større statistisk styrke må man ha å oppdage den. P. D. Ellis (2012) peker på effektstørrelse, spesielt samfunnsvitenskapene, det store er svært små (og mye mindre enn man forventer), og peker på - noe som kanskje burde være åpenbart - Effekter eksisterer den virkelige verden. På samme måte som vi bruker utvalgsgjennomsnittet \\(\\overline{x}\\) som estimat på populasjonsgjennomsnittet \\(\\mu\\), er effektstørrelser vi kalkulerer utvalg estimater på populasjonseffekter. Samtidig er det klart effektstørrelse har en vesentlig informasjonsverdi tillegg til p verdi. Ikke bare kan vi si om det er en signifikant effekt, men vi kan si noe om denne effekten er liten eller stor. Uten en formening om effekten er liten eller stor (ikke bare om den er statistisk liten eller stor, men også om den er liten eller stor praksis) er informasjonsverdien av å vite det er en statistisk signifikant effekt begrenset. Likeledes, et ikke-signifikant resultat innebærer ikke det ikke kan være en effekt (det kan godt være en effekt, men vi har ikke hatt nok statistisk styrke til å oppdage den).er det på sin plass med noen (flere) ord om effekt og signifikans. P. D. Ellis (2012) illustrerer sammenhengen med denne likningen:\\(Statistisk\\ signifikans = Effekstørrelse * Utvalgsstørrelse\\)Sammenhenger: Jo større effektstørrelse, jo lavere p verdi (ved uendret utvalg). Ergo: En lav p verdi kan indikere en stor effekt. Men, en lav p verdi kan også skyldes et stort utvalg (og en liten effekt). Det motsatte gjelder selvsagt også. En høy p verdi kan skyldes en lav effekt. Eller et lite utvalg. Eller en kombinasjon. Det er med andre ord umulig å si noe om praktisk eller substansiell signifikans ut fra en p verdi og en statistisk signifikans (P. D. Ellis 2012).Det finnes et stort antall mål effektstørrelser. De kan det store deles inn “familier” (P. D. Ellis 2012):Effektmål som måler forskjeller mellom grupper – d familien. Eksempler: Cohens d, Hedges’ g.Effektmål som måler assosiasjon/forbindelse (hvor sterk er denne forbindelsen mellom x og y) – r familien. Eksempler: Pearsons r, Spearmans rho (\\(\\rho\\)) og Eta squared (\\(\\eta^2\\)).J. Cohen (1988) er en ofte referert kilde terskelverdier vurdering av effektstørrelse (gjengitt fra P. D. Ellis 2012, s.44, tabell 5):\nTable 1.2: Effektstørrelser, modifisert fra Cohen (1988)\nLenhard Lenhard (2016) modifiserer J. Cohen (1988) og gir følgende retningslinjer:\nTable 1.3: Effektstørrelser, modifisert fra Lenhard & Lenhard (2017)\nDet må sies hva som er en liten, middels eller stor effekt er svært kontekstavhengig. Som P. D. Ellis (2012), s.46, sier:proper way view Cohen’s thresholds interpretation tool last resort. might refer basis drawing meaning results. fact used – given raison d’être beyond Cohen’s study teenage girls – speaks volumes inherent difficulties assessing substantive significance results.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"standardisering---transformasjon-av-data-z-skåre","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.16 Standardisering - transformasjon av data (z-skåre)","text":"et tidligere delkapittel har vi vist data kan ha ulike målenivå. Målenivå/skala kan derfor skape utfordringer oss. Variabler er målt på ulike nivåer og ulike skalaer. dette kan skape utfordringer oss. Hvis vi har resultater fra ulike studier kan vi kun sammenlikne resultatene meningsfullt hvis de er gjennomført med samme skalaer.dette delkapittelet bruker vi et eksempel fra Miles Shevlin (2001). Vi kan eksempel ha en studie som undersøker eksamensresultater (avhengig variabel) ut fra antall fagbøker lest (uavhengig variabel) og finner en økning eksamensresultater på x prosentpoeng per leste bok. En annen studie har samme variabler, men presenterer resultatet som en gitt økning på en til F karakterskala per leste bok. Hvordan kan vi sammenlikne disse studiene? Siden vi ikke kan tvinge alle til å måle på nøyaktig samme måte og presentere funnene på nøyaktig samme måte (det ville jo unektelig ha gjort en god del mye lettere…) kan vi standardisere målene som er gjort, det vil si vi transformerer distribusjonen til å ha en gjennomsnittsverdi på 0 og et standardavvik på 1, og benevnes ofte som z-score.Formelen å finne z-scores er:\\(z_i = \\frac{verdien\\ av\\ x\\ på\\ målepkt\\ - gjennomsnittverdien\\ \\ x}{standardavviket}\\)som kan uttrykkes:\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\) populasjoneneller som:\\(z_i = \\frac{x_i - \\overline{x}}{s}\\) utvalget.En z-score er altså det antallet standardavvik en verdi x er fra gjennomsnittet. En z-score verdien xi på 0,45 betyr en standardisert distribusjon ligger observasjonen 0,45 standardavvik fra gjennomsnittet.La oss anta vi har normalfordelte data en variabel x. Gjennomsnittsverdien x er gitt som \\(\\mu\\) og standardavviket som \\(\\sigma\\).grafen til er dette illustrert.Den blå linjen viser en normalfordelt datamengde som har en gjennomsnittsverdi \\(\\mu\\) og standardavviket \\(\\sigma\\). dette eksempelet er dataene høyde norske kvinner 2012 målt på sesjon5. medisinske kretser regnes ofte et standardavvik på høydefordeling som 6 cm6. Tallene eksempelet reflekterer dette. Fordelingskurven kvinners høyde 2012 viser altså en gjennomsnittshøyde på 167 cm (egentlig 167,1 cm) og et standardavvik på 6 cm.Vi antar høyden målt på kvinner på sesjon er representative populasjonen norske kvinner, og kan si eksempel 68 % av norske kvinner er mellom 161 cm og 173 cm høye. 95 % av norske kvinner har en høyde på mellom 156 cm og 179 cm. På bakgrunn av dette kan vi gjøre sannsynlighetsberegninger gjennom å bruke standardverdien z. Vi kan eksempel være interessert å vite hva sannsynligheten er en tilfeldig norsk kvinne er 175 cm.dette tilfellet er det det rødskraverte området av distribusjonen vi er interessert , formulert slik:\\(p=x > 175\\)Dette kan vi omformulere:\\(p=\\frac{x-\\mu}{\\sigma}\\ som\\ gir\\ \\frac{175 - \\mu}{\\sigma}\\)Fra før vet vi :\\(z=\\frac{x-\\mu}{\\sigma}\\)Vi kan dermed uttrykke :\\(p=z>\\frac{175-167}{6}\\ som\\ gir\\ p=z>1.33\\)Med andre ord kan vi si sannsynligheten x er større enn 175 er den samme som sannsynligheten z > 1,33. Vi kan illustrere dette med figuren , der fordelingen er standardisert med gjennomsnitt 0 og standardavvik 1. Området vi er interessert er det rødskraverte som ligger til høyre z = 1,33.Siden vi vet en standardisert normalfordeling har \\(\\sigma=1\\) betyr det vi kan se visuelt det området vi er interessert , sannsynligheten en tilfeldig norsk kvinne er høyere enn 175 cm, ligger utenfor 1 standardavvik.Vi kan bruke en tabell standard normalforfordeling (f.eks. .Merk: På toppen av tabellen står det «Table Values Represent AREA LEFT Z score». Vi er jo interessert området til høyre, så vi kan da si:\\(p=1-0.90824 = 0.09176\\)Vi kan derfor si det er cirka 9,2 % sannsynlighet en norsk kvinne er 175 cm. Hadde vi vært interessert sannsynligheten en tilfeldig norsk kvinne er lavere enn 175 cm kunne vi lest det rett ut av tabellen som 0,90824, altså cirka 91 % sannsynlighet.Tilsvarende kan vi finne sannsynligheten en tilfeldig norsk kvinne er mellom 165 cm og 175 cm.Vi kan uttrykke dette som\\(p(165 < x < 175)\\)Ved å bruke samme framgangsmåte kommer vi fram til:\\(p(\\frac{165-167}{6} < z < \\frac{175-167}{6})\\)som gir:\\(p(-0.33 < z < 1.33)\\)Vi bruker samme tabell og finner verdiene 0,37070 og 0,90824, det vil si sannsynligheten en tilfeldig norsk kvinne er mellom 165 cm og 175 cm er (0,90824 – 0,37070 = 0,53754), det vil si cirka 54 %.Hvis vi ser på dette tallet forhold til hva vi ville forvente ut fra en normalfordeling ser vi vi forventer 68 % av verdiene en normalfordeling ligger innenfor intervallet +/- 1 standardavvik. Det vil si vi forventer 68 % av et utvalg tilfeldige norske kvinner vil ligge høydeintervallet 161 cm til 173 cm. Siden vi har sett på intervallet 165 cm til 175 cm) har vi et mindre intervall noe som gjør vi vil forvente en noe lavere sannsynlighet en tilfeldig kvinne vil ligge vårt intervall forhold til sannsynligheten å ligge intervallet +/- 1 standardavvik. Vår utregning virker derfor rimelig forhold til hva vi kunne forvente.Heldigvis vil alle statistikkprogrammer regne ut dette raskt. det første eksempelet:det andre eksempelet:","code":"\npopgjsnitt <- 167\nsd <- 6\n    \npnorm(175, popgjsnitt, sd, lower.tail = FALSE)\n#> [1] 0.09121122\npnorm(175, popgjsnitt, sd) - pnorm(165, popgjsnitt, sd)\n#> [1] 0.5393474"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"standardisering---transformasjon-av-data-z-skåre---del-2","chapter":"Kapittel 1 Grunnleggende begreper og sammenhenger","heading":"1.16.1 Standardisering - transformasjon av data (z-skåre) - del 2","text":"Så langt har vi brukt standardisering på en “enkel” måte – altså med en variabel. Som nevnt tidligere kan vi ha spesielt nytte av standardisering dersom vi befinner oss en situasjon der vi har datainnsamling av ulike variabler som bruker ulike måleskalaer. Standardisering (som også omtales som normalisering) defineres som en prosess som transformerer data av ulike typer/måleskalaer til en uniform/felles skala slik de kan sammenliknes. Hvis vi eksempel har alder (målt år) og inntekt (målt kroner) vil vi gjennom å standardisere gi begge fordelingene en gjennomsnittsverdi på 0 og et standardavvik på 1. En z-score alder på 0,45 vil innebære 0,45 standardavvik fra gjennomsnittet av alder. Det samme vil en z-score på 0,45 inntekt.Ofte kan man være interessert å lage komposittvariabler av et antall variabler. La oss eksempel si du sitter med data på høyde (cm), vekt (kg) og lengde på øre (mm). Du har altså tre mål på hvert objekt en studie, men alle tre er målt på forskjellige skalaer. Det er standardisering kommer inn bildet dersom man ønsker å lage en variabel som heter “Kroppstype”. Vi standardiserer hver skåre hver observasjon og får nye z-skåre variabler de tre opprinnelige variablene. Vi har dermed fått tre z-skåre variabler som deler de har gjennomsnitt på 0 og standardavvik på 1.Høyde er målt cm, vekt kg og ørelengde mm. Nå kan vi “sammenlikne” de tre variablene på en meningsfull måte fordi alle de tre nye variablene har gjennomsnittsverdi på 0 og standardavvik på 1. Z-scoren gir altså avstand fra gjennomsnittet den enkelte observasjon uavhengig av hvilken skala målingene er gjennomført på. Jeg ønsker nå å lage en komposittvariabel - en ny variabel der jeg bruker de 3 målene en variabel jeg kan kalle kroppstype (vi “later som” høyde, vekt og ørelengde kan si noe meningsfullt om kroppstype). Siden vi har ulike måleskalaer kan vi bruke standardisering.Et annet praktisk anvendelsesområde av standardiserte verdier er tolkning av ulike variablers betydning en regresjonsanalyse. Dette vil vi vise kapittelet der vi gjennomgår regresjonsanalyse.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"samvariasjon-og-korrelasjon","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"Kapittel 2 Samvariasjon og korrelasjon","text":"","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"samvariasjonkovarians","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.1 Samvariasjon/kovarians","text":"Samvariasjon – noe varierer sammen (omtales også som kovarians/“covariance”) – er et mål på en felles variasjon mellom (minst) variabler. Samvariasjonen kan være enten negativ eller positiv. Hvis vi har data på variablene x og y kan vi undersøke om verdiene x og y varierer tilfeldig eller om det er en sammenheng mellom de . Ved positiv samvariasjon vil en økning eller nedgang en variabel korrespondere med en økning eller nedgang den andre. Samvariasjon innebærer det ikke er tilfeldig hvordan variabler varierer forhold til hverandre – det er et gjensidig forhold mellom variablene.forrige kapittel var begrepet varians et tema. Et alternativt begrep samvariasjon er kovarians (fra engelsk «covariance»). Kovariansen er den enkleste måten å se om variabler varierer sammen – altså om endring den ene variabelen følges av en endring den andre. Eller, med andre ord, dersom en variabel avviker fra gjennomsnittet forventer vi den andre også gjør det (enten samme eller motsatt retning hvis vi tror de kovarierer). Vi ønsker derfor å finne ut hvor stor kovariansen er.\nTable 2.1: Kovarians\nVi regner altså ut snittet og avviket fra snittet begge variablene (antall husdyr og antall familien). Så multipliserer vi avvikene pr rad – og får tverrproduktavviket pr observasjon/respondent. Til slutt summerer vi tverrproduktavvikene. Så regner vi ut kovariansen:\\(\\frac{6.8}{n-1}=\\frac{6.8}{(5-1)}=1.7\\)dette enkle eksempelet kan vi altså si antall husdyr samvarierer positivt med antall medlemmer familien. Det er imidlertid ikke så lett å tolke hva kovariansen betyr da det ikke er et standardisert mål. Vi kan altså ikke sammenlikne kovarianser mellom ulike undersøkelser på en objektiv måte (Field 2009a). Det fører oss på begrepet korrelasjon.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"korrelasjon-og-korrelasjonskoeffisient","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.2 Korrelasjon og korrelasjonskoeffisient","text":"å kunne si noe mer fornuftig om samvariasjonen må derfor normalisere kovariansen gjennom å dele på variablenes standardavvik – noe som gir oss korrelasjonen mellom variablene (Løvås 2013). Den standardiserte kovariansen er med andre ord korrelasjonskoeffisienten. vårt tilfelle ser det da slik ut:Vi har allerede funnet kovariansen gjennom å multiplisere avvikene de variablene med hverandre (6.8). Vi gjør det samme med standardavvikene. Standardavviket antall husdyr har vi tidligere regnet vi ut til å være 1.14. antall medlemmer familien blir standardavviket:\\(\\sqrt{\\frac{(-2.4)^2 + (-0.4)^2 + (1.6)^2 + (1.6)^2}{(5-1)}} = \\sqrt{\\frac{5.76 + 0.16 + 0.16 + 2.56 + 2.56}{4}}=1.67\\)Neste skritt blir å multiplisere standardavvikene:\\(1.14 * 1.67 = 1.923\\)Til slutt tar vi kovariansen og deler på standardavvikproduktet:\\(\\frac{1.7}{1.923}=0.88\\)Resultatet 0,88 er det som betegnes Pearsons korrelasjonskoeffisient (\\(r\\)). oss betyr det vårt lille eksempel antall familiemedlemmer er positivt korrelert med antall husdyr – jo flere familien, jo flere husdyr. Korrelasjonskoeffsienten er et mål på hvor sterk korrelasjonen er og hvilken retning korrelasjonen går (om den er positiv eller negativ).","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"tolkning-av-korrelasjon-og-korrelasjonskoeffisenter","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.3 Tolkning av korrelasjon og korrelasjonskoeffisenter","text":"Korrelasjonskoeffisienten har alltid en verdi mellom -1 og 1, der 0 betyr ingen korrelasjon. Det er vanlig å bruke følgende retningslinjer vurdering av koeffisienten:Nær x/- 1: Perfekt eller tilnærmet perfekt korrelasjonMellom +/- 0.5 og +/- 1: Sterk korrelasjonMellom +/- 0.3 og +/- 0.49: Moderat korrelasjonUnder +/- 0.29: Liten korrelasjonHinkle, Wiersma, Jurs (2003) opererer med en noe mer finmasket inndeling:\nTable 2.2: Korrelasjonskoeffisient - Pearsons r, modifisert fra Hinkle et al. (2003)\nOfte ønsker vi (og anbefaler) å ikke bare se på verdien av korrelasjonskoeffisienten, men også se på en grafisk framstilling av datane - gjerne omtalt som et spredningsplott (“scatter plot”).eksempelet og illustrasjonene har vi vist Pearsons korrelasjonskoeffisient \\(r\\). Det finnes ulike korrelasjonskoeffisienter ulike typer datasett. Pearsons korrelasjonskoeffisient brukes som regel på datasett der vi gjennomfører såkalte parametriske tester. andre tester, ikke-parametriske, kan Spearmans rho (\\(\\rho\\)) - ofte brukt ordinale variabler (men kan også brukes på intervall/ratiodata) med parrede data (“matched pair”) - og Kendalls tau (\\(\\tau\\)) - ofte brukt “nominelle variabler med rangert data (”ranked data”) - være gode alternativer. Vi går ikke nærmere inn på disse .Vi skal merke oss en korrelasjonskoeffisient på 0 ikke betyr det ikke er noen korrelasjon. Som vi kan se av bildet (Frøslie 2022) kan korrelasjonskoeffisienten være 0 og variablene like vel være avhengige (delt iht Creative Commons: CC SA 3.0), jfr illustrasjonen .Som sagt, et viktig poeng er vi skal være veldig forsiktige med å tolke en korrelasjonskoeffisient uten å ha en grafisk framstilling av hvordan datapunktene fordeler seg (hvordan distribusjonen av datapunkter ser ut). Dette fordi en gitt korrelasjonskoeffisient kan representere et uendelig antall mønstre mellom variabler. Vanhove (2018) viser dette:Alle 16 eksemplene viser altså mønstre av korrelasjon mellom variabler som alle har korrelasjonskoeffisienten r=0,5.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"spuriøs-korrelasjon","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.4 Spuriøs korrelasjon","text":"Det er viktig å huske på en korrelasjon mellom variabler (x og y) ikke er det samme som å si det er en årsakssammenheng (kausalitet). Selv om variabler korrelerer perfekt betyr ikke det nødvendigvis den ene er årsak til den andre. Det kan være en såkalt spuriøs sammenheng. mange tilfeller er det åpenbart det er urimelig å anta det skal være en sammenheng mellom variablene.Korrelasjon betyr altså ikke automatisk kausalitet. Dette er utrolig viktig å huske – og en kilde til mye feilinformasjon/feiltolkninger. La oss ta eksempler fra Tyler Vigen.det første eksempelet ovenfor er det altså en sterk korrelasjon mellom filmer Nicholas Cage medvirker og antall mennesker som druknet etter å ha falt et svømmebasseng. Med mindre man tenker seg Cage er så dårlig skuespiller det får folk til å hoppe frivillig ut et svømmebasseng å drukne seg virker denne sammenhengen ganske søkt.det andre eksempelet virker sammenhengen like sprø. det skal være en reell og nesten perfekt sammenheng mellom antall mennesker som kveles av deres eget sengetøy og osteforbruket per capita er ekstremt lite troverdig.Det synes åpenbart selv om det er en klar korrelasjon mellom variablene de eksemplene virker det fullstendig meningsløst å tro de faktisk henger sammen. Dette er det vi kaller spuriøs korrelasjon.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"eksempel-på-bivariat-korrelasjonsanalyse","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.5 Eksempel på bivariat korrelasjonsanalyse","text":"tittelen på delkapittelet har vi brukt begrepet bivariat korrelasjonsanalyse. Det innebærer vi ser på korrelasjonen mellom variabler. neste delkapittel skal vi vise et enkelt eksempel på korrelasjonsanalyse av flere variabler.Dette eksempelet er hentet fra Pallant (2010). Datasettet kan lastes ned fra nett .Vi har modifisert datasettet til å kun inneholde de variablene vi skal bruke og fjernet observasjoner med manglende verdier:Download Pallantmod.xlsxDatasettet inneholder opprinnelig flere variabler, men det jeg skal se på nå er om oppfattelse av stress (“tpstress”) korrelerer med variabelen “tpcoiss” (“Total perceived control internal states”). Jeg ønsker altså å se på sammenhengen mellom oppfattet stress og hvordan man oppfatter man har “indre kontroll”.resultatet kan vi se \\(r = - 0.581\\). Siden vi har et negativt fortegn betyr det vi har en negativ korrelasjon mellom variablene – altså en høy verdi på den ene variabelen er forbundet med en lav verdi på den andre. Vi ser korrelasjonen er signifikant (\\(p < .001\\)). Vi kan også si noe om styrken på korrelasjonen (uavhengig av fortegn på koeffisienten).Lenger opp har vi gjengitt forslag til tokning av størrelsen på korrelasjonskoeffisienten. Etter disse har vi en sterk negativ korrelasjon mellom disse variablene ut fra Cohens inndeling, men en moderat korrelasjon ut fra Hinkle, Wiersma, Jurs (2003).Det er vanligvis ikke nødvendig å vise resultater av en korrelasjonsanalyse dersom det ikke er mer enn variabler. eksempelet kan man rapportere (vi har ikke sjekket forutsetninger, men eksempelets skyld antar vi vi ikke har funnet problemer der):Forholdet mellom oppfattelse av indre kontroll (målt med PCOISS) og oppfattelse av stress (målt med Perceived Stress Scale) ble undersøkt med Pearsons korrelasjonskoeffisient. Innledende analyser avdekket ingen brudd på forutsetningene om normalitet, linearitet og homoskedastisitet. Basert på resultatene av studien er oppfattelse av indre kontroll sterkt negativt korrelert med oppfattelse av stress, r = -.58, n = 426, p < .001","code":"\ncor.test(Pallantmod$Pallantmod.tpstress, Pallantmod$Pallantmod.tpcoiss)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  Pallantmod$Pallantmod.tpstress and Pallantmod$Pallantmod.tpcoiss\n#> t = -14.683, df = 424, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.6402679 -0.5139141\n#> sample estimates:\n#>        cor \n#> -0.5805759\n\nplot(Pallantmod$Pallantmod.tpcoiss, Pallantmod$Pallantmod.tpstress, main = 'Spredningsplott ', xlab = 'tpcoiss', ylab = 'tpstress')"},{"path":"samvariasjon-og-korrelasjon.html","id":"multivariat-korrelasjonsanalyse","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.6 Multivariat korrelasjonsanalyse","text":"Eksempelet er hentet fra Coghlan (2010) (Creative Commons Attribution 3.0 License. Datasettet inneholder data på mengde av 13 ulike kjemikalier vin fra en region Italia fra tre ulike produsenter.Vi kan først lage en korrelasjonsmatrise der vi kun tar oss de første fem kjemikaliene (kolonne 2 til 5 datasettet):Og et sammensatt spredninsgplott:spredningsplottet kan vi se korrelasjonen mellom V2 og V3 ser mest “spredd” ut. Dette kan vi også se matrisen . Ut fra baree plottet er det ikke så lett å f.eks. se hvor korrelasjonen er størst, men matrisen ser vi det selvsagt enkelt (V4 og V5). Så kombinasjonen av en korrelasjonsmatrise og et spredningsplott vil være et godt hjelpemiddel å se på korrelasjon.","code":"\nvinkorrelasjon <- as.matrix(cor(wine[2:5]))\nupper <- round(vinkorrelasjon, 2)\nupper[upper.tri(vinkorrelasjon)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       V2   V3   V4 V5\n#> V2     1             \n#> V3  0.09    1        \n#> V4  0.21 0.16    1   \n#> V5 -0.31 0.29 0.44  1\nscatterplotMatrix(wine[2:5])"},{"path":"samvariasjon-og-korrelasjon.html","id":"kausalitet-årsakssammenheng","chapter":"Kapittel 2 Samvariasjon og korrelasjon","heading":"2.7 Kausalitet (årsakssammenheng)","text":"Kausalitet – fra latin “causa” -> engelsk “cause” - betyr altså årsak eller grunn. Kausalitet innebærer altså det må være en årsakssammenheng mellom hendelser/fenomener.Vi kan tydeligst (enklest) se kausalitet naturvitenskapene, der vi snakker om et deterministisk årsaksforhold (det er ikke begrenset til naturvitenskapene selvsagt, men naturvitenskapene sysler mye med lovmessigheter vi ikke like lett kan se/påvise eksempel samfunnsvitenskapene). En deterministisk årsakssammenheng innebærer et fenomen B alltid har samme årsak (fenomen ), og fenomen alltid fører til fenomen B. Det er således et tidsperspektiv involvert (rekkefølge av hendelser/fenomen).samfunnsvitenskapene har man gjerne en litt annerledes tilnærming til kausalitet fordi det er vanskelig å vise/se de lovmessige og deterministiske årsaksforholdene. sier man et fenomen () er årsak til fenomen B, dersom det er slik (med en viss sannsynlighet) enten fører til eller øker sannsynligheten B. Dette faller inn en stokastisk årsakssammenheng (Dahlum Grønmo 2021). samfunnsvitenskapene kan man mindre grad kontrollere alle ting som påvirker et fenomen B, så man kan også snakke om tendenser – det er en tendensiell forståelse av kausalitet.et naturvitenskapelig eksperiment kan vi ofte isolere fenomenene vi undersøker fra annen påvirkning (selv om dette naturligvis på ingen måte er gjeldende naturvitenskapelig forskning). Vi kan kontrollere hva som påvirker hva, tid og rom. Et kontrollert eksperiment er derfor en slags gullstandard forskning når man skal si noe om kausale forhold.samfunnsvitenskapene er dette ofte praksis umulig. Det vil være mange mulige påvirkninger på et fenomen, og det kan være vanskelig å si noe sikkert om 1) har vi tenkt på alle ting som kan påvirke, 2) klarer vi å ta hensyn til denne usikkerheten våre analyser og 3) vet vi egentlig hva som påvirker hva (kan det være sånn vår antakelse om påvirker B faktisk kan være motsatt)? Moralen er nok: Vi skal være veldig varsomme med å dra bastante slutninger om kausalitet så lenge vi ikke gjennomfører et kontrollert eksperiment der vi kontrollerer omgivelsene og variablene. Det finnes imidlertid (selvsagt) metoder også samfunnsvitenskapene som gjør vi kan snakke om kausalitet. Disse kommer vi (litt) tilbake til undervegs senere kapitler.","code":"\npacman::p_load(summarytools)"},{"path":"univariat-analyse.html","id":"univariat-analyse","chapter":"Kapittel 3 Univariat analyse","heading":"Kapittel 3 Univariat analyse","text":"Univariat analyse handler om analyse av en enkelt variabel - dvs. vi kan godt gjøre analysen på flere variabler samtidig, men vi ser kun på karakteristika ved den enkelte variabel, ikke variabler sammenheng eller forhold til hverandre). hovedsak gjør vi dette på tre måter:Deskriptiv statistikk (“Summary statistics”)FrekvenstabellerDiagrammer / plott","code":""},{"path":"univariat-analyse.html","id":"deskriptiv-statistikk","chapter":"Kapittel 3 Univariat analyse","heading":"3.1 Deskriptiv statistikk","text":"Deskriptiv statistikk kan vi gjerne gruppere hovedgrupper:Tendens (“location measures”)Spredning (“dispersion measuresd”)Typiske karakteristika vi kan være interessert å se på er gjennomsnitt, median, maksimums- og minimumsverdier (“range”), kvartiler / interkvartil avstand (“IQR”), varians / standardavvik, manglende verdier (“missing values” / “NAs”), skjevhet og kurtosis.Vi fram igjen datasettet vi brukte kapittel 2 der vi hadde genererte høydedata 100 tilfeldige menn.","code":"#> Descriptive Statistics  \n#> hoyde100  \n#> N: 100  \n#> \n#>                     hoyde100\n#> ----------------- ----------\n#>              Mean     177.79\n#>           Std.Dev      16.93\n#>               Min     132.05\n#>                Q1     166.99\n#>            Median     177.84\n#>                Q3     188.10\n#>               Max     220.57\n#>               MAD      15.73\n#>               IQR      20.99\n#>                CV       0.10\n#>          Skewness       0.15\n#>       SE.Skewness       0.24\n#>          Kurtosis      -0.20\n#>           N.Valid     100.00\n#>         Pct.Valid     100.00"},{"path":"univariat-analyse.html","id":"frekvenstabell","chapter":"Kapittel 3 Univariat analyse","heading":"3.2 Frekvenstabell","text":"Det gir ikke særlig mening å lage en frekvenstabell de genererte høydedataene siden alle verdeiene er unike (altså får vi frekvens 1 på alle 100 observasjonene). La oss derfor lage et enkelt eksempel:","code":"\nx <- c(1, 1, 2, 3.5, 4, 4, 4, 5, 5, 6.5, 7, 7.4, 8, 13, 14.2)\n# Bruker pakken: summarytools\nfreq(x)\n#> Frequencies  \n#> x  \n#> Type: Numeric  \n#> \n#>               Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#> ----------- ------ --------- -------------- --------- --------------\n#>           1      2     13.33          13.33     13.33          13.33\n#>           2      1      6.67          20.00      6.67          20.00\n#>         3.5      1      6.67          26.67      6.67          26.67\n#>           4      3     20.00          46.67     20.00          46.67\n#>           5      2     13.33          60.00     13.33          60.00\n#>         6.5      1      6.67          66.67      6.67          66.67\n#>           7      1      6.67          73.33      6.67          73.33\n#>         7.4      1      6.67          80.00      6.67          80.00\n#>           8      1      6.67          86.67      6.67          86.67\n#>          13      1      6.67          93.33      6.67          93.33\n#>        14.2      1      6.67         100.00      6.67         100.00\n#>        <NA>      0                               0.00         100.00\n#>       Total     15    100.00         100.00    100.00         100.00"},{"path":"univariat-analyse.html","id":"diagrammer-plott","chapter":"Kapittel 3 Univariat analyse","heading":"3.3 Diagrammer / plott","text":"Det er en rekke diagrammer / plott vi kan lage som gir oss informasjon om en enkelt variabel.Histogram:Boxplott:Fordeling:Vi kommer tilbake flere steder til mer utførlig tolkning av ulike diagrammer ulike kapitler der de brukes en kontekst.","code":"\nhist(x)\nboxplot(x)\nplot(density(x), main = \"Density plot\")"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"kjikvadrattest---analyse-av-kategoriske-data","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","text":"Dette kapittelet tar seg kjikvadrattester. Dersom vi har kategoriske variabler - jamfør tidligere kapittel der vi tok oss målenivå - er kjikvadrattester en god måte å gjøre bivariat analyse på (dersom vi har metriske/kontinuerlige variabler bruker vi t-test som vi kommer tilbake til et senere kapittel).Vi skal vise tre tilfeller, det vil si tre måter å gjør kjikvadrattester:Vi har kategoriske variabler og ønsker å se om det er en sammenheng mellom dem - (“Test association”).Vi har en kategorisk variabel og ønsker å se om den representerer en kjent populasjon eller forventning (“Goodness fit”).Vi har en kategorisk variabel en gruppe mennesker / en gruppe observasjoner målt på f.eks. ulike tidspunkt eller ulike forhold (“Paired samples - McNemars test”).","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"test-of-association---sammenheng-mellom-to-kategorisek-variabler","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.1 Test of association - sammenheng mellom to kategorisek variabler","text":"Vi bruker et generert datasett:Download Modul 5_Krysstabell_Fargeblind2.xlsxDette datasettet består av 1626 observasjoner av gutter og jenter der vi også har registrert fargeblindhet. Vi ønsker å se om det er en sammenheng mellom disse variablene - altså om det er en statistisk signifikant sammenheng.Vi kan sette opp en krysstabell (“contigency table”/“crosstab”).kjikvadrattester setter vi derfor opp en hypotese om en eller annen sammenheng mellom de variablene vi ønsker å undersøke. dette eksempelet blir vår hypotese det er en sammenheng. Teknisk sett vil testen faktisk undersøke det motsatte - altså det ikke er noen sammenheng. Testen vil vise oss om vi kan forkaste denne hypotsene - ingen sammenheng. Om vi kan det er vi styrket troen på det faktisk er en sammenheng. Viser testen oss vi ikke kan forkaste hypotesen ingen sammenheng er vi svekket troen på det er en sammenheng.Vi setter da opp det som kalles en nullhypotses og en alternativ hypotses (og det er altså nullhypotsesen vi tester):\\(H_0 = Ingen\\ sammenheng\\ mellom\\ kjønn\\ og\\ fargeblindhet\\)\\(H_a = Sammenheng\\ mellom\\ kjønn\\ og\\ fargeblindhet\\)R gir oss som default test med en såkalt “Yates continuity correction” som gjør verdien kan virke annerledes enn programvare som ikke gjør denne korreksjonen. Yateskorreksjonen tar utgangspunkt det er en forutsetning om normalfordeling dataene, noe som ikke kan forutsettes binære data som vi har . Dette skal ikke være et stort problem større utvalg (mer enn 5-10 forventede hver celle krysstabellen) (SAGE 2019).vårt konkrete tilfelle ser vi med korreksjon er p-verdien 0.0528378. Hvis vi kjører testen uten korreksjonen får vi:er altså p-verdien 0.0353085. dette eksempelet er altså forskjellen mellom de ikke ubetydelig, da uten korreksjon er 0.05 og med korreksjon er 0.05. En tommelfingerreglel tolkning av p-verdien er:p low, null must go. p high, null flies.Med andre ord: Er p-verdien 0.05 (“low”) vil vi forkaste nullhypotesen, er p-verdien 0.05 (“high”) vil vi ikke forkaste nullhypotesen.tilegg til p-verdien er vi spesielt interessert kjikvadratverdien (\\(\\chi^2\\)). Kjikvadratverdien sammenlikner vi med kritisk verdi. Kritisk verdi kan vi finne tabeller på nett (f.eks. ). Hvis vi går inn tabellen lenka fører til og ser på 0.95 - 1 frihetsgrad (1 df) finner vi 3.841. Statistikkprogrammer vil gi oss antall frihetsgrader (df), men vi kan også regne ut dette en krysstabell slik:\\(df = (rader - 1) * (kolonner - 1) = (2-1)*(2-1)=1\\)Mange programmer vil uansett lett gi deg den kritiske verdien:Hvis kjikvadratverdien er større eller lik den kritiske verdien er resultatene våre statistisk signifikante. Den kritiske verdien dette eksempelet er 3.84, mens kjikvadratverdien er hhv. 3.75 og 4.43.Vi kan altså bruke både p-verdien og kjikvadratverdien til å vurdere om vi skal forkaste nullhypotesen eller ikke. Hvis vi tar fram igjen krysstabellen vår:Vi vet dersom antall forventede hver celle er minst 5, og gjerne 10, er det greit å kjøre test uten Yates korreksjon. så fall indikerer det vi kan forkaste nullhypotesen (“null must go”), og vår hypotese om det er en statistisk singifikant forskjell mellom kjønnene når det gjelder fargeblindhet er styrket.En mulighet er også å kjøre en såkalt “Fisher’s exact test” dersom de forventede celletallene er 5. Det er ikke tilfelle , men vi viser likevel prosedyren. Først vil vi sjekke hva forventede celletall er:Forventede verdier er regnet ut matematisk ved:\\(\\frac{radsum * kolonnesum}{totalsum}\\)Vi har krysstabellen:vårt tilfelle gutt - fargeblind:\\(\\frac{802*36}{1626}=17.75\\)Dersom vi får forventede 5 kan vi altså kjøre en kjikvadrattest med korreksjon, eller en Fishers eksakt test.Tolkningen er den samme som kjikvadrattesten.En siste vurdering vi kan gjøre er å se på den såkalte phi-koeffisienten.Phi-koeffisienten kan tolkes som en korrelasjonskoeffisient (Pearson r).\nTable 4.1: Phi-koeffisient - uavhengig av fortegn\ndette eksempelet har vi altså ingen eller neglisjerbar sammenheng.Oppsummert vårt eksempel: Når vi undersøker forventede verdier hver celle ser vi det minste forventede tallet er 17.75. Det er godt et minimum på 5 og en anbefalt grense på 10. Derfor kan vi gjennomføre en kjikvadrattest uten Yates korreksjon. vårt tilfelle forkaster vi altså nullhypotesen. Selv om vi finner en statistisk sammengeng mellom kjønn og fargeblindhet er denne liten eller neglisjerbar.","code":"dim(kji1)\n#> [1] 1626    2\ntable(kji1$Kjønn)\n#> \n#>  Gutt Jente \n#>   802   824\ntable(kji1$Fargeblind)\n#> \n#>   Ja  Nei \n#>   36 1590krysstab <- addmargins(table(kji1$Kjønn, kji1$Fargeblind),c(1,2))\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626kjitest1 <- chisq.test(kji1$Kjønn, kji1$Fargeblind)\nkjitest1\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  kji1$Kjønn and kji1$Fargeblind\n#> X-squared = 3.749, df = 1, p-value = 0.05284kjitest2 <- chisq.test(kji1$Kjønn, kji1$Fargeblind, correct = FALSE)\nkjitest2\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  kji1$Kjønn and kji1$Fargeblind\n#> X-squared = 4.4302, df = 1, p-value = 0.03531\nkritiskverdi <- qchisq(p=.05, df=1, lower.tail=FALSE)\nkritiskverdi\n#> [1] 3.841459\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626\nkjitest1$expected\n#>           kji1$Fargeblind\n#> kji1$Kjønn       Ja      Nei\n#>      Gutt  17.75646 784.2435\n#>      Jente 18.24354 805.7565\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626test <- fisher.test(table(kji1$Kjønn, kji1$Fargeblind))\ntest\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  table(kji1$Kjønn, kji1$Fargeblind)\n#> p-value = 0.04246\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  0.9953805 4.6127889\n#> sample estimates:\n#> odds ratio \n#>   2.086476krysstab2 <- table(kji1$Kjønn, kji1$Fargeblind) \n# Bruker pakken: psych\nphi(krysstab2, digits = 3)\n#> [1] 0.052"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"utregning-av-kjikvadratverdi","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.1.1 Utregning av kjikvadratverdi","text":"Når vi vet faktisk tall og forventet tall, og har krysstabell med rad- og kolonnesummer, kan vi regne ut kjikvadratverdien manuelt:Siste steg er å summere \\(2.19 + 2.13 + 0.05 + 0.05 = 4.42\\).\nDifferansen mellom 4.43 og 4.42 sjykdes avrundinger.","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"goodness-of-fit---sammenheng-mellom-kategorisk-variabel-og-kjentforventet-datafordeling","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.2 Goodness of fit - sammenheng mellom kategorisk variabel og kjent/forventet datafordeling","text":"Kjikvadrattest brukes også på en annen måte. Vi kan bruke den å teste om en variabel kommer fra en spesifikk datadistribusjon, eller med andre ord om de empiriske (innsamlede/observerte) dataene stemmer overens med en teoretisk datadistribusjon (som eksempel normalfordelingen). Det vil si vi kan vurdere om et utvalg er representativt en populasjon.Som den første måten å bruke kjikvadrattest tester vi en hypotese. Hypotesen sier det er en signifikant forskjell mellom verdiene på de empiriske/observerte dataene og de forventede/teoretiske verdiene. Nullhypotesen blir da det ikke er signifikant forskjell.La oss lage et eksempel. En produsent av et skrapelodd hevder det er null gevinst 80% av loddene, en liten gevinst 15% av loddene, en litt større gevinst 4% av loddene og en stor gevinst 1% av loddene.Download lotteri.csv\nTable 4.2: Lovet fordeling av gevinster\nVi trekker ut 100 tilfeldige lodd og finner: 85 uten gevinst, 10 med liten gevinst, 3 med en middels gevinst og 2 med stor gevinst.\nTable 4.3: Faktisk fordeling av gevinster\nStemmer de resultatene vi fikk det tilfeldige utvalget på 100 med det produsentene lover?Stegene videre analyse blir:Regne ut frihetsgraderRegne ut forventede verdierRegne ut kjikvadratverdien","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"frihetsgrader","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.2.1 Frihetsgrader","text":"\\(df = k - 1 = 4 - 1 = 3\\)\n(k = antall nivåer den kategoriske variabelen)","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"forventede-verdier","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.2.2 Forventede verdier","text":"Disse har vi forsåvidt allerede tabellen - de produsenten har lovet er det forventede. Utregningen blir:\\(F_1 = n * p_1 = 100 * 0.80 = 80\\)\n\\(F_2 = n * p_2 = 100 * 0.15 = 15\\)\nosv.","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"kjikvadratverdi","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.2.3 Kjikvadratverdi","text":"\\(\\chi^2=\\sum\\left(\\frac{(O_i - E_i)^2}{E_i}\\right)\\)der O = Observert og E = Forventet (“Expected”)\\(\\chi^2=\\frac{(85-80)^2}{80}+\\frac{(10-15)^2}{15}+\\frac{(3-4)^2}{4}+\\frac{(2-1)^2}{1} = 0.31 + 1.67 + 0.25 + 1 = 3.23\\)Vi kan sammenlikne denne med kritisk verdi:Hvis kjikvadratverdien er større eller lik den kritiske verdien er resultatene våre statistisk signifikante. Den kritiske verdien dette eksempelet er 7.81, mens kjikvadratverdien er 3.23. Vi vil derfor anta resultatene ikke er statistisk signifikante. Dette viser seg også p-verdien. vil vi vise en nettressurs å regne ut p-verdien. Den finner du .Vi kan også kjøre testen R:P-verdien er sannsynligheten kjikvadratverdien med 3 frihetsgrader er høyere enn 3.23. Huskeregel: «p low, null must go». er ikke p lav (ikke 0.05). Det vil si nullhypotesen ikke kan forkastes. Med andre ord – vi kan konkludere med det er sannsynlig utvalget representerer populasjonen – eller dette tilfellet: basert på dette tilfeldige utvalget har produsentene produsert lodd iht det som er lovet fordi vi ikke kan forkaste nullhypotesen om det ikke er forskjell mellom utvalget vi fikk og det vi ville forvente.","code":"\nkritiskverdi2 <- qchisq(p=.05, df=3, lower.tail=FALSE)\nkritiskverdi2\n#> [1] 7.814728\nobserved <- c(85, 10, 3, 2) \nexpected <- c(.8, .15, .04, .01)\n\nchisq.test(x=observed, p=expected)\n#> Warning in chisq.test(x = observed, p = expected): Chi-\n#> squared approximation may be incorrect\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 3.2292, df = 3, p-value = 0.3576"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"paired-samples-mcnemars-test","chapter":"Kapittel 4 Kjikvadrattest - Analyse av kategoriske data","heading":"4.3 Paired samples (McNemars test)","text":"Download Modul 5_McNemar.xlsxMcNemars test brukes når vi ikke har uavhengige variabler, men derimot har tilfeller der vi har “paired data”/“repreated measures” av kategoriske data. Et eksempel på dette er en pasientgruppe som er testet ved anledninger. tabellen har vi laget data som viser et tenkt forsøk med en ny type behandling en gruppe mennesker. De er spurt om de har smerter før og etter de er gitt en behandling.Det vi jo vil undersøke er om man kan si behandlingen har en effekt.Vi kan tilnærme oss dette først litt teoretisk ved å sette opp samme tabell uten tall:denne testen er rutene med og d ikke interessante, de svarte det samme før og etter behandling. Hvis behandlingen ikke har noen effekt bør antallet respondenter som går fra ja til nei, og fra nei til ja, være likt. Derfor er vi interessert cellene b og c, dette er respondentene som beveget seg fra ja til nei eller motsatt. Hvis behandlingen har en effekt kan vi forvente antallet som går fra ja til nei vil være større, og hvis behandlingen har en negativ effekt vil vi forvente motsatt (flere fra ja til nei enn andre veg).Vi kan derfor sette opp følgende hypoteser:\\(H_0: p_b = p_c\\)\\(H_1: p_b \\neq p_c\\)der p er sannsynligheten tallet cellene med b og c. Nullhypotesen, altså den vi vil teste kjikvadrattesten, går altså på det er like mange som går fra ja til nei, som fra nei til ja. Den alternative hypotesen sier det er en forskjell.Når vi kjører testen vil vi altså forkaste \\(H_0\\) dersom p-verdien er terskelverdien (“p low, null must go”. Hvis vi forkaster nullhypotesen betyr det vi mener hypotesen om behandlingen har en effekt er styrket.Vi kan regne ut testverdien manuelt ganske enkelt ut fra krysstabellen:\\(\\chi^2=\\frac{(b-c)^2}{b+c}=\\frac{(75-785)^2}{75+785}=586.163\\)Testverdien kan sammenliknes med kritisk verdi.Vi kan se både på kjikvadratveri mot kritiske verdi og p-verdi vi har gode grunner til å forkaste nullhypotesen. Det kan derfor se ut til behandlingen virker.","code":"krysstab2 <- addmargins(table(mcnemar$Før, mcnemar$Etter),c(1,2))\nkrysstab2\n#>               \n#>                Ikke_smerter Smerter  Sum\n#>   Ikke_smerter          215      75  290\n#>   Smerter               785     380 1165\n#>   Sum                  1000     455 1455\nkrysstab2\n#>               \n#>                Ikke_smerter Smerter  Sum\n#>   Ikke_smerter          215      75  290\n#>   Smerter               785     380 1165\n#>   Sum                  1000     455 1455\nqchisq(p=.05, df=1, lower.tail=FALSE)\n#> [1] 3.841459\nmcnemar3 <- matrix(c(215, 785, 75, 380), nrow = 2,\n                   dimnames = list(\"Før behandling\" = c(\"Nei\", \"Ja\"),\n                                   \"Etter behandling\" = c(\"Nei\", \"Ja\")))\n\nmcnemar.test(mcnemar3, correct=FALSE) \n#> \n#>  McNemar's Chi-squared test\n#> \n#> data:  mcnemar3\n#> McNemar's chi-squared = 586.16, df = 1, p-value <\n#> 0.00000000000000022"},{"path":"t-tester.html","id":"t-tester","chapter":"Kapittel 5 T-tester","heading":"Kapittel 5 T-tester","text":"En t-test brukes når man vil sammenlikne gjennomsnittsverdier og utvalget er relativt lite. Vi kan se oss tre tilfeller:Vi sammenlikner en gruppe mot en kjent gjennomsnittsstørelse og tester om gruppas gjennomsnitt er signifikant forskjellig fra det kjente gjennomsnittet («One sample t-test»).\nVi sammenlikner uavhengige gruppers gjennomsnitt å se om det er signifikant forskjell på gjennomsnittene en variabel («Independent samples t-test»)\nVi sammenlikner samme gruppe på ulike tidspunkt – observasjonene er altså ikke uavhengige av hverandre («Paired samples t-test»)\nEn t-test handler altså om å undersøke om det er signifikant forskjell på gjennomsnittsverdiene sett med data. Vi setter derfor opp en nullhypotese som sier det ikke er forskjell:\\(H_0: \\mu_1 = \\mu_2\\)Hvis vi denne nullhypotesen ikke kan forkastes (vi konkluderer med gjennomsnittene er like) betyr det eksempel en gruppe som har fått «ekte» medisin ikke skiller seg fra en gruppe som har fått placebo. Hvis vi derimot forkaster nullhypotesen vil vi konkludere med det er signifikant forskjell mellom de gruppene (på en eller annen verdi vi måler). T-testen tester denne nullhypotesen – det ikke er forskjell.en t-test får vi en testverdi. Dersom p-verdien denne testen er mindre enn 0.05 (gitt vi bruker \\(\\alpha = 0.05\\)) forkaster vi nullhypotesen (p low, null must go»), og vi vil anta det er signifikant forskjell mellom gruppene (og denne forskjellen ikke skyldes tilfeldigheter). Er p høyere enn valgt α vil vi beholde nullhypotesen.","code":""},{"path":"t-tester.html","id":"students-t-test","chapter":"Kapittel 5 T-tester","heading":"5.1 Students t-test","text":"T-test, eller “Student’s t-test” som den ofte omtales som, baserer seg på en såkalt t-fordeling. En t-fordeling er ganske lik en normaldistribusjon, men har tyngre haler. Fordelingen vil variere med antall frihetsgrader, men likere og likere en normaldistribusjon ettersom utvalgsstørrelsen øker:Så hvorfor behovet en t-distribusjon? William Sealy Gosset, .k.. “Student”, fant ut hvis man ikke er helt sikker på hva standardavviket er må man bruke et estimat på standardavviket som gjør fordelingen endrer seg litt fra normalfordelingen. Det vi omtaler som t-test er en test av en statistisk hypotese som baserer seg på Students t-distribusjon.","code":"\ncurve(dt(x, df = 2), from = -4, to = 4, col = \"blue\", ylim = c(0, 0.41))\ncurve(dt(x, df = 5), from =-4, to = 4, col = \"brown\", add = TRUE)\ncurve(dt(x, df=20), from = -4, to = 4, col = \"black\", add = TRUE)\ncurve(dnorm, -4, 4, col = \"red\", add = TRUE)\nlegend(-4, .3, legend = c(\"df=2\", \"df=10\", \"df=20\", \"Normal\"),\n       col = c(\"blue\", \"brown\", \"black\", \"red\"), lty = 1, cex = 1.2)"},{"path":"t-tester.html","id":"one-sample-t-test","chapter":"Kapittel 5 T-tester","heading":"5.2 One sample t-test","text":"La oss anta vi har en gruppe på 20 studenter som gjennomfører et nettbasert kurs anvendt kvantitativ analyse basert på bruk av en pakke med digitale læringsressurser som legger opp til mange selvøvelser. Vi tester denne gruppa opp mot en gjennomsnittsskåre på en test på 67.5 alle andre studenter (skåre 0-100) som har gjennomført samme kurs tidligere der man ikke har hatt samme tilgang til digitale øvingsoppgaver. Skårer denne testgruppa signifikant bedre enn resten av studentene?Download t-test_onesample.csvVi regner ut teststatstikken (t) slik:\\(t=\\frac{\\overline{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\)der:\\(t = t-verdi\\)\\(\\overline{x} = observert\\ gjennomsnitt\\)\\(\\mu = teoretisk/forventet\\ gjennomsnitt\\)\\(s = standardavviket\\ \\ utvalget/observerte\\)\\(n=utvalgsstørrelse/antall\\ observerte\\)Vi henter nødvendige verdier fra datasettet:Dette gir da:\\(t=\\frac{72.3-67.5}{\\frac{9.52}{\\sqrt{20}}}=\\frac{4.8}{2.129}=2.255\\)Vi sammenlikner t-verdien 2.255 med kritisk verdi, f.eks. . Vi finner verdien 1.729. Hvis t-verdien er større enn kritisk verdi: forkast nullhypotesen. forkaster vi nullhypotsesen fordi 2.255 er større enn 1.729. Vår alternative hypotese om det er signifikant forskjell er styrket.R bruker vi:Vi ser også t-testveridien er større enn kritisk verdi (2.255 > 1.729). tillegg ser vi p-verdien er < 0.05 (“p low, null must go”).","code":"\n# Bruker pakken: summarytools\ndescr(ttestonesample$Score)\n#> Descriptive Statistics  \n#> ttestonesample$Score  \n#> N: 20  \n#> \n#>                      Score\n#> ----------------- --------\n#>              Mean    72.30\n#>           Std.Dev     9.52\n#>               Min    50.00\n#>                Q1    66.00\n#>            Median    75.00\n#>                Q3    79.00\n#>               Max    89.00\n#>               MAD     9.64\n#>               IQR    13.00\n#>                CV     0.13\n#>          Skewness    -0.45\n#>       SE.Skewness     0.51\n#>          Kurtosis    -0.50\n#>           N.Valid    20.00\n#>         Pct.Valid   100.00\nt.test(ttestonesample$Score, mu = 67.5, alternative = \"two.sided\")\n#> \n#>  One Sample t-test\n#> \n#> data:  ttestonesample$Score\n#> t = 2.2547, df = 19, p-value = 0.03615\n#> alternative hypothesis: true mean is not equal to 67.5\n#> 95 percent confidence interval:\n#>  67.84422 76.75578\n#> sample estimates:\n#> mean of x \n#>      72.3\nqt(0.05, 19, lower.tail=FALSE)\n#> [1] 1.729133"},{"path":"t-tester.html","id":"sjekk-av-forutsetninger-for-one-sample-t-test","chapter":"Kapittel 5 T-tester","heading":"5.2.1 Sjekk av forutsetninger for one sample t-test","text":"Tilfeldig utvalg fra en definert/gitt populasjonVariabelen må være kontinuerligPopulasjonen er normalfordeltVi ser spesielt på nr 3. Det finnes flere måter å se på normalitetsforutsetningen, både grafisk og formelle statistiske tester. Vi skal vise en formell test - Shapiro-Wilks som ofte brukes. Andre eksempler er Kolmogorov-Smirnov og Anderson-Darling. Razali Wah (2011) finner en sammenlinende studie Shapiro-WIlks fungerer bra.Vi sammenlikner testverdien med 0,05 (gitt vi bruker 0,05 som signifikansnivå). Dersom testverdien er 0,05 indikerer det dataene er normalfordelte. Hvis testeverdien er 0,05 indikerer det dataene avviker fra normalfordelingen. Dette er ikke tilfelle (0.5855703).","code":"\nshapirotest <- shapiro.test(ttestonesample$Score)\nshapirotest\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ttestonesample$Score\n#> W = 0.96205, p-value = 0.5856"},{"path":"t-tester.html","id":"paired-samples-t-test","chapter":"Kapittel 5 T-tester","heading":"5.3 Paired samples t-test","text":"Paired samples t-test brukes når gruppene/målingene ikke er uavhengige av hverandre. tilfeller hvor vi eksempel har en gruppe som er testet ganger på ulike tidspunkt er ikke observasjonene uavhengige av hverandre.Paired samples t-test og one sample t-test er på en måte “samme sak” - altså, der en one sample t-test sammenlikner gjennomsnittet fra et utvalg med en kjent størrelse kalkulerer en paired samples t-test forskjellen mellom de gjennomsnittene deretter å gjennomføre en one sample t-test på forskjellen.La oss anta vi har en gruppe på 15 studenter som vi har testet ganger. Imellom testene har de gjennomført en aktivitet som skal trigge hukommelsen.Download t-test_paired.csvVi kan regne ut testverdien slik:\\(t=\\frac{\\sum{d}}{\\frac{n(\\sum{d^2})-(\\sum{d}^2)}{(n-1)}}\\)der\\(t = testverdi\\)\\(d=differansen\\ innad\\ \\ hvert\\ par\\)\\(n=antall\\ \\ utvalget\\)Gjennom en manuell utregning finner vi \\(t=0.02\\)Vi gjennomfører en t-test R:ser vi p er 0,05. Dvs vi kan ikke forkaste nullhypotesen. Vi kan derfor ikke si det er en signifikant forskjell mellom de gruppene.Av grafen og tabellen ser vi spredningen er mindre etter aktiviteten (se f.eks. på standardavviket tabellen).Vi kan se fra ulike tester ovenfor forutsetningen om normalfordeling ser ut til å være innfridd. Normalt trenger vi ikke gjøre alle tre testene, men vi har tatt de med å vise relevant kode om man skulle ha behov den ene eller den andre - de tester normalitet fra ulike vinkler, og svært mange tilfeller vil man se Shapiro-Wilks brukt. Robusthetstester viser også Shapiro-Wilks viser bra robusthet og egenskaper sammenliknet med alternativer (Razali Wah 2011).","code":"\npairedsamples <- read.csv(\"t-test_paired.csv\")\n# Bruker pakken: summarytools\ndescr(pairedsamples)\n#> Descriptive Statistics  \n#> pairedsamples  \n#> N: 15  \n#> \n#>                       Post      Pre\n#> ----------------- -------- --------\n#>              Mean    76.19    76.21\n#>           Std.Dev     3.69     4.87\n#>               Min    70.76    67.82\n#>                Q1    72.78    70.81\n#>            Median    77.34    77.60\n#>                Q3    78.68    79.57\n#>               Max    83.02    83.36\n#>               MAD     3.47     5.02\n#>               IQR     5.50     7.25\n#>                CV     0.05     0.06\n#>          Skewness    -0.09    -0.39\n#>       SE.Skewness     0.58     0.58\n#>          Kurtosis    -1.18    -1.31\n#>           N.Valid    15.00    15.00\n#>         Pct.Valid   100.00   100.00\nt.test(pairedsamples$Pre, pairedsamples$Post, paired = TRUE, alternative = \"two.sided\")\n#> \n#>  Paired t-test\n#> \n#> data:  pairedsamples$Pre and pairedsamples$Post\n#> t = 0.020764, df = 14, p-value = 0.9837\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.934057  2.991423\n#> sample estimates:\n#> mean of the differences \n#>              0.02868286\nggpaired(pairedsamples, cond1 = \"Pre\", cond2 = \"Post\",\n    fill = \"condition\")\n# Bruker pakken: summarytools\ndescr(pairedsamples)\n#> Descriptive Statistics  \n#> pairedsamples  \n#> N: 15  \n#> \n#>                       Post      Pre\n#> ----------------- -------- --------\n#>              Mean    76.19    76.21\n#>           Std.Dev     3.69     4.87\n#>               Min    70.76    67.82\n#>                Q1    72.78    70.81\n#>            Median    77.34    77.60\n#>                Q3    78.68    79.57\n#>               Max    83.02    83.36\n#>               MAD     3.47     5.02\n#>               IQR     5.50     7.25\n#>                CV     0.05     0.06\n#>          Skewness    -0.09    -0.39\n#>       SE.Skewness     0.58     0.58\n#>          Kurtosis    -1.18    -1.31\n#>           N.Valid    15.00    15.00\n#>         Pct.Valid   100.00   100.00\nforskjell <- pairedsamples$Post - pairedsamples$Pre\nshapiro.test(forskjell)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  forskjell\n#> W = 0.94997, p-value = 0.5239\n#Bruker pakken: nortest\nad.test(forskjell)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  forskjell\n#> A = 0.44249, p-value = 0.2483\n# Bruker pakken: tseries\njarque.bera.test(forskjell)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  forskjell\n#> X-squared = 0.26188, df = 2, p-value = 0.8773"},{"path":"t-tester.html","id":"independent-samples-t-test","chapter":"Kapittel 5 T-tester","heading":"5.4 Independent samples t-test","text":"En vanlig situasjon er vi har grupper som skal sammenliknes. Hvis gruppene er uavhengige av hverandre (motsetning til paired samples t-test), kan vi bruke independent samples t-test.eksempelet ønsker vi å sammenlikne grupper studenter: en gruppe har ren nettundervisning, mens den andre gruppa har hybrid undervisning (nett og fysisk). Vi har eksamensresultater begge gruppene. Kan vi ut fra eksamensresultatene si om gruppene er signifikant forskjellige fra hverandre?Vi ser datasettet består av 200 observasjoner av studenter som enten har gjennomført ren nettundervisning eller en hybrid undervisning (nett og fysisk undervisning).Vi kan regne ut testverdien:\\(t=\\frac{\\overline{x}_A - \\overline{x}_B}{\\sqrt{\\biggl({\\frac{(\\sum ^2-\\frac{(\\sum )^2}{n_A})+(\\sum B^2-\\frac{(\\sum B)^2}{n_B})}{n_A+n_B-2}\\biggr)}*\\biggl(\\frac{1}{n_A}+\\frac{1}{n_B}\\biggr)}}\\)der:\\(= Variabel 1 - \\ vårt\\ tilfelle\\ \"nett\"\\)\\(B = Variabel 2 - \\ vårt\\ tilfelle\\ \"hybrid\"\\)\\((\\sum )^2 = Summen\\ av\\ 'ene\\ kvadrert\\)\\((\\sum B)^2 = Summen\\ av\\ B'ene\\ kvadrert\\)\\(A_2 = Enkeltverdien\\ \\ kvadrert\\ (hver\\ enkelt\\ verdi)\\)\\(B_2 = Enkeltverdien\\ B\\ kvadrert\\ (hver\\ enkelt\\ verdi)\\)\\(\\sum ^2 = Summen\\ av\\ de\\ kvadrerte\\ 'ene\\)\\(\\sum B^2 = Summen\\ av\\ de\\ kvadrerte\\ B'ene\\)\\(\\overline{x}_A = Gjennomsnitt\\ \\ variabel\\ \\)\\(\\overline{x}_B = Gjennomsnitt\\ \\ variabel\\ B\\)\\(n_A = antall\\ \\ variabelen\\ \\)\\(n_B = antall\\ \\ variabelen\\ B\\)Vi kan legge merke til t-testen R velger Welch t-test stedet Student t-test. Forskjellen ligger Welch t-test ikke forutsetter lik varians, mens Student t-test gjør dette.dette eksempelet har vi ingen forskjell mellom Welch og Student t-test. Noen programmer vil automatisk gi deg den ene eller den andre, alternativt begge . Forskjellen Forskjellen på Student’s t og Welch’s t er altså førstnevnte forutsetter begge gruppene har likt standardavvik (“«”assumption equal variances”). virkeligheten er dette ofte ikke tilfelle (det er liten grunn til å tro gruppene har likt standardavvik hvis de ikke har lik gjennomsnittsverdi). slike tilfeller er Welch’s t en mer robust test.Ut fra testen kan vi si det er en statistisk signifikant forskjell mellom de gruppene. Hvis vi ser på resultatet fra testen ser vi hybridgruppa har høyere snitt enn nettgruppa. Dette forteller oss hybridgruppa skårer signifikant høyere enn nettgruppa.","code":"\nindependent <- read.csv(\"t-test_independent.csv\")\nHeadTail <- function(independent){rbind(head(independent),tail(independent))}\nHeadTail(independent)\n#>     Studentnr   Type    Score\n#> 1           1   Nett 77.36150\n#> 2           2   Nett 79.39069\n#> 3           3   Nett 71.88117\n#> 4           4   Nett 80.00219\n#> 5           5   Nett 81.99973\n#> 6           6   Nett 73.35320\n#> 195       195 Hybrid 93.46116\n#> 196       196 Hybrid 83.60819\n#> 197       197 Hybrid 79.41945\n#> 198       198 Hybrid 78.83346\n#> 199       199 Hybrid 94.84745\n#> 200       200 Hybrid 86.55348\nttestind <- t.test(Score ~ Type, independent)\nt.test(Score ~ Type, independent, var.equal=TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  Score by Type\n#> t = 10.98, df = 198, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means between group Hybrid and group Nett is not equal to 0\n#> 95 percent confidence interval:\n#>   7.216698 10.376497\n#> sample estimates:\n#> mean in group Hybrid   mean in group Nett \n#>             84.58566             75.78906\nttestind$estimate\n#> mean in group Hybrid   mean in group Nett \n#>             84.58566             75.78906"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"variansanalyse---anova-analysis-of-variance","chapter":"Kapittel 6 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"Kapittel 6 Variansanalyse - ANOVA (“Analysis of Variance”)","text":"kapittel 2 så vi på korrelasjon mellom enkeltvariabler (bivariat korrelasjon) ) og sammenlikninger mellom grupper eller en gruppe på tidspunkter (kjikvadrattester og t-tester). Regresjonsanalyser, som vi kommer itlbake til neste kapittel, kan også ses på som analyser av forholdet mellom enkeltvariabler.Variansanalyse - heretter ANOVA - er en samlebetegnelse på flere statistiske metoder der man tester likheter mellom eller flere utvalg. Har man grupper vil ANOVA og en t-test gi samme resultat (hypotesen \\(H_0: \\mu_1 = \\mu_2\\) mot \\(H_A: \\mu_1 \\neq \\mu_2\\)). Man kan faktisk (prinsippet) gjennomføre t-tester x antall kombinasjoner av y antall grupper, men risikoen type-feil øker sammenliknet med en ANOVA test på samme data.en ANOVA snakker man om elementer som utgjør den totale variansen: varians innad gruppen og varians mellom gruppene. Det er derfor vanlig å dele ANOVA inn hovedgrupper: enveis og toveis ANOVA. Enveis analyser ser kun på en egenskap som varierer mellom gruppene, mens toveis inkluderer egenskaper som kan variere mellom enhetene gruppene.Det ANOVA innbærer - helt grunnleggende - er å teste om variansen mellom gruppene er større enn variansen innad gruppene.","code":""},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"enveis-mellom-grupper-anova-one-way-between-groups-anova","chapter":"Kapittel 6 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"6.1 Enveis mellom grupper ANOVA (“One-way between-groups ANOVA”)","text":"Enveis ANOVA (one-way ANOVA) er analyser der vi har en uavhengig variabel som er målt på/har flere nivåer. Hvis vi f.eks. vil undersøke effekt av ulike opplæringstiltak selgere (den uavhengige variabelen er opplæringsmetode) som kan bestå av tre grupper (metoder opplæring): e-læring/egenlæring, gruppeopplæring og observasjon/mentorering av erfaren selger) vil den avhengige variabelen kunne være ukentlig salg (kroner, enheter e.l.).\nAnalysen sammenlikner variansen mellom gruppene (metodene opplæring) med variansen internt hver gruppe. Teststatistikken kalles F (F ratio):\\(F = \\frac{Varians\\ mellom\\ gruppene}{Varians\\ innad\\ \\ gruppen}\\)En høy F-verdi vil innebære det er høyere varians mellom gruppene enn internt gruppene. En signifikant F-verdi betyr vi kan forkaste hypotesen (\\(H_0\\))) om det gjennomsnittene er like.dette eksempelet skal vi bruke et datasett fra Pallant (2010) og trekker ut de variablene vi trenger eksempelet. Vi gjør også om variabelen “agegp3” til faktor (dette er nødvendig R, men andre programmer “fikser” dette selv).Download Pallant_Survey_ANOVA1.xlsxVi skal bruke en variabel datasettet som deler respondentene inn tre aldersgrupper (“agegp3”) og en variabel som måler total optimisme (“toptim”) der respondentene skårer på en skala fra 6 til 30 (30 er høyeste nivået av optimisme).Hypotesen er altså:\\(H_0: \\mu_1 = \\mu_2,\\ dvs.\\ gjennomsnittet\\ til\\ de\\ ulike\\ gruppene\\ er\\ like\\)\\(H_A: \\mu_1 \\neq \\mu_2\\ dvs.\\ gjennomsnittet\\ til\\ de\\ ulike\\ gruppene\\ er\\ ulike\\)Vi kan først se på datasettet:Vi kan se gjennomsnittene er forskjellige de tre gruppene, men vi vet ikke om denne forskjellen er statistisk signifikant.Vi kan også se på dette grafisk:Om vi skal anta noe ut fra grafen vil det kunne være gruppe 1 og 2 ikke er signifikant ulike, mens gruppe 3 kanskje skiller seg statistisk signifikant ut.Siden p-verdien er lavere enn 0.05 kan vi konkludere med det er signifikante forskjell et sted (mellom eller flere grupper) variabelen (agegp3). Men vi kan ikke ut fra dette si hvor – altså hvilken gruppe som er signifikant forskjellig fra de andre).Vi bør sjekke forutsetningen om homogenitet variansen. Homogen (lik) varians har vi når standardavvikene ulike grupper er omtrent like.Bartletts test brukes dersom vi har normalfordelte data. Vi kan derfor sjekke dette:Vi kan teste normalfordeling:Siden testverdien er < 0.05 må vi anta dataene er signifikant forskjellig fra normalfordelingen. Dette betyr vi bør bruke Levenes og/eller Fligner-Killeen.En annen test av homogenitet varians som framholdes som robust avvik fra normalfordeling er altså “Fligner-Killeen test”:Tolkningen av alle tre testene homogenitet varians er den samme: Hvis p < 0.05 er variansen ikke lik. Det fremheves imidlertid dersom gruppene er tilnærmet like store er ikke denne forutsetningen kritisk (eller sågar nødvendig) - ANOVA (og t-tester) er generelt robuste forhold til brudd på forutsetningen om homogen varians dersom gruppene er relativt like (Solutions 2013). vårt eksempel viser f.eks. Levenes test (p = 0.4899) vi har homogenitet variansen.Et alternativ er å gjøre en såkalt Welch enveis test. Welch test forutsetter ikke homogen varians:Vi kan deretter se nærmere på hvilke grupper som er statistisk signifikant forskjellige:Vi ser av tabellen gruppene 1 og 3 er signifikant forskjellige. De andre parene - 1-2 og 2-3 ikke har signifikante forskjeller.\nDette kan også visualiseres:Gruppene 1-2 og 2-3 har konfiendsintervall som inneholder 0, mens 1-3 ikke har det.Til slutt kan vi være interessert å vurdere hvor stor effektstørrelsen (eta squared = \\(\\eta^2\\)).J. Cohen (1988) angir følgende forslag på grenseverdier tolkning av \\(\\eta^2\\):Liten effekt: 0.01Middels effekt: 0.06Stor effekt: 0.14I vårt eksempel er det altså en statistisk signifikant, men liten forskjell (noe vi sannsynligvis fikk en mistanke om plottet lenger opp der vi antok det kanskje kunne være en forskjell gruppe 3), noe vi også fikk en indikasjon på tabellen med forskjellene gjennomsnittsverdier gruppene. Dette er ikke uvanlig - vi har 435 observasjoner, og store utvalg kan selv små forskjeller gi statistisk signifikans og vi bør tolke resultatene med det øye. en tolkning bør vi også vurdere hvilken praktisk forskjell det er mellom gruppene selv om vi har funnet en statistisk signifikant forskjell.","code":"\ngroup_by(optimisme, agegp3) %>%\n  summarise(\n    count = n(),\n    mean = mean(toptim, na.rm = TRUE),\n    sd = sd(toptim, na.rm = TRUE)\n  )\n#> # A tibble: 3 x 4\n#>   agegp3 count  mean    sd\n#>   <fct>  <int> <dbl> <dbl>\n#> 1 1        149  21.4  4.55\n#> 2 2        153  22.1  4.15\n#> 3 3        137  23.0  4.49\nggboxplot(optimisme, x = \"agegp3\", y = \"toptim\", \n          color = \"agegp3\", palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          order = c(\"1\", \"2\", \"3\"),\n          ylab = \"Total optimisme\", xlab = \"Aldersgruppe\")\nresultataov1 <- aov(toptim ~ agegp3, data = optimisme)\nsummary(resultataov1)\n#>              Df Sum Sq Mean Sq F value Pr(>F)  \n#> agegp3        2    179   89.53   4.641 0.0101 *\n#> Residuals   432   8334   19.29                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 4 observations deleted due to missingness\nbartlett.test(toptim ~ agegp3, data = optimisme)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  toptim by agegp3\n#> Bartlett's K-squared = 1.4561, df = 2, p-value =\n#> 0.4828\nshapiro.test(optimisme$toptim)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  optimisme$toptim\n#> W = 0.97471, p-value = 0.0000007366\nleveneoptimisme <- leveneTest(toptim ~ agegp3, data = optimisme)\nleveneoptimisme\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value Pr(>F)\n#> group   2  0.7147 0.4899\n#>       432\nfligner.test(toptim ~ agegp3, data = optimisme)\n#> \n#>  Fligner-Killeen test of homogeneity of variances\n#> \n#> data:  toptim by agegp3\n#> Fligner-Killeen:med chi-squared = 1.5588, df = 2,\n#> p-value = 0.4587\nwelchoptimisme <-oneway.test(toptim ~ agegp3, data = optimisme)\nwelchoptimisme\n#> \n#>  One-way analysis of means (not assuming equal\n#>  variances)\n#> \n#> data:  toptim and agegp3\n#> F = 4.38, num df = 2.00, denom df = 284.51, p-value =\n#> 0.01338\nTukeyHSD(resultataov1)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = toptim ~ agegp3, data = optimisme)\n#> \n#> $agegp3\n#>          diff        lwr      upr     p adj\n#> 2-1 0.7440309 -0.4489781 1.937040 0.3080109\n#> 3-1 1.5950113  0.3636472 2.826376 0.0069296\n#> 3-2 0.8509804 -0.3687705 2.070731 0.2296685\nplot(TukeyHSD(resultataov1, conf.level=.95), las = 2)\n# Bruker pakken: lsr\netaSquared(resultataov1)\n#>            eta.sq eta.sq.part\n#> agegp3 0.02103477  0.02103477"},{"path":"regresjonsanalyse---ols.html","id":"regresjonsanalyse---ols","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"Kapittel 7 Regresjonsanalyse - OLS","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"innledning","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.1 Innledning","text":"Regresjonsanalyse er et særtilfelle av variansanalyse, og er følge Mehmetoglu Mittner (2020) muligens den mest brukte analysemetoden dataanalyse, eller arbeidshesten forskning på økonomiske og sosiale forhold (Thrane 2019). Det er først og fremst metodens er fleksibilitet som en hovedgrunn til dette.En regresjonsanalyse er en statistisk analyse som undersøker sammenhengen mellom en kontinuerlig avhengig variabel og en eller flere kontinuerlige og/eller kategoriske uavhengige variabler. Selv om korrelasjon kan være veldig hjelpsomt å forstå vil en regresjonsanalyse søke å ta vår forståelse av sammenhengen litt videre, til eksempel å forsøke å predikere nivået en avhengig variabel ut fra nivået på den/de uavhengige variabler. Hvis vi lykkes med dette vil vi kunne klare å si noe om forventet verdi på et fenomen vi er interessert ut fra kjente verdier på andre variabler. La oss anta vi har variabler som beliggenhet (avstand fra sentrum), areal, etasje, solforhold, antall rom, antall bad, standard på bad og liknende en leilighet kan vi bruke disse uavhengige variablene til å predikere en salgssum denne boligen (som en avhengig variabel). Vi lager da en modell dette forholdet - dette tilfellet en regresjonsmodell. Vi går dermed fra å spørre om det er en sammenheng til å spørre hvilken sammenheng det er.La oss forsøke å illustrere prinsippet med regresjonsanalyse gjennom et såkalt Venndiagram.Den gule sirkelen illustrerer det forholdet vi er interessert å “finne ut noe om”. Den representerer det vi kaller den avhengige variabelen - fordi det vi ønsker å finne ut er avhengig av andre forhold (andre variabler). Vi kan si den gule sirkelen viser variasjonen drivstofforbruket til alle biler vi har med undersøkelsen vår, og vi betegner denne variabelen \\(Y\\). Biler har ulikt drivstofforbruk, så vi har altså en variasjon drivstofforbruket mellom bilene. Den blå sirkelen viser variasjonen motorstørrelse (vi kaller denne variabelen \\(x_1\\)). Ulike biler har ulik motorstørrelse, og vi tenker større motor betyr mer drivstofforbruk enn mindre motor. Den grønne sirkelen representerer en variabel vi har kalt kjørestil (\\(x_2\\)).Vi har en hypotese om vi kan predikere (forutsi) drifstofforbruket til en gitt bil ut fra motorstørrelse og kjørestil. Så det vi ønsker å se på er hvor mye av korrelasjonen mellom drivstofforbruk og motorstørrelse skyldes faktisk motorstørrelse, og hvor mye skyldes kjørestil. Vi tenker også kjørestil og drivstofforbruk er korrelert (det er naturlig å tenke seg personer med en aggresiv kjørestil har biler med større motorer - det er altså en korrelasjon mellom kjørestil og motorstørrelse). Vi ser dette figuren . Korrelasjonen mellom drivstofforbruk og motorstørrelse er gitt områdene merket 1 og 2. Korrelasjonen mellom drivstofforbruk og kjørestil er gitt områdene 2 og 3. Korrelasjonen mellom kjørestil og motorstørrelse er gitt 2 og 4.Området 2 viser den delte variasjonen mellom drivstofforbruk, motorstørrelse og kjørestil. Det vil innebære vi kan bruke regresjonsanalsye til å isolere ut område 1 ved å se på motorstørrelsens totale korrelasjon med drivstofforbruk og trekke fra den delen av den totale korrelasjonen som deles med kjørestil (område 2). Da finner vi motorstørrelsens (\\(x_1\\)’s) unike bidrag.Det samme kan vi gjøre kjørestil, der det unike bidraget utgjøres av område 3. Vi kan selvsagt ha flere prediktorer (uavhengige variabler) - noe vi veldig ofte vil ha. Det vi gjør er prinsippet det samme: vi tar bort biter av korrelasjonen mellom motorstørrelse og drivstofforbruk som skyldes samvariasjon med andre variabler slik vi får isolert den delen av korrelasjonen som utelukkende skyldes motorstørelse. Man kan tenke seg en ny variabel med rød sirkel. Igjen - regresjonsanalysen forsøker å isolere den unike delen korrelasjonen mellom motorstørrelse og drivstofforbruk (og det samme de andre variablene: den unike delen). Når vi klarer å isolere den unike korrelasjonen kan vi også si vi har isolert den unike kausale effekten motorstørrelse har på drivstofforbruket (gitt vi har inkludert alle relevant uavhengige variabler modellen, noe vi praksis sjelden vil klare).Den videre innledningen til regresjonsanalyse tar utgangspunkt eksempelet Løvås (2013) (boka kom ut 4. utgave 2018). Illustrasjonene som er brukt er hentet fra bokas nettressurser. Løvås’ bok «Statistikk universiteter og høgskoler» kan anbefales som introduksjonsbok til statistikk på universitets- og høgskolenivået. En annen bok som fungerer fint til dette formålet er Jan Ubøes “Statistikk økonomifag” (vi har brukt 4. utgave, 2014 - 5. utgave kom 2015) (Ubøe 2014).","code":""},{"path":"regresjonsanalyse---ols.html","id":"teori","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.2 Teori","text":"Eksempelet Løvås (2013) dreier seg om sammenhengen mellom motorstørrelse og drivstofforbruk. Vi kan måle motorstørrelse hestekrefter (hk) og drivstofforbruk liter/mil.å vise sammenhengen kan vi sette opp ligningen \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) der\\(Y=drivstofforbruket\\)\\(\\alpha=konstantleddet\\) (krysningspunktet på y-aksen, altså Y-verdien om x er 0)\\(\\beta=linjens\\:stigningstall\\) (dersom x øker med 1, øker y med \\(\\beta\\)\\(e=forstyrrelsen\\) (vi antar det er flere ting som forstyrrer forholdet mellom motorstørrelse og drivstofforbruk - drivstofforbruket er ikke bare avhengig av motorstørrelse). Vi skal snakke mye om residualer regresjonsanalyse - residualer er dette restleddet/feilleddet/forstyrrelsen. En av forutsetningene regresjonsanalyse er knyttet til fordelingen av disse residualene, men det kommer vi tilbake til.Dersom vi ikke hadde hatt et feilledd kunne vi framstilt denne ligningen slik:Når vi plotter inn et antall observasjoner av motorstørrelse og drivstofforbruk kan det se slik ut:Det vi en lineær regresjonsanalyse gjør er å finne den rette linja som best passer til disse observasjonene. Vi ønsker altså å finne en rett linje som best «beskriver» observasjonene. Tenk deg vi trekker den rette linja som samlet sett ligger nærmest punktene og deretter tar bort punktene. Det vi sitter igjen med er regresjonslinja. Denne linja gir oss da “tilgang til” alle punkter som ligger på linja som en modell på sammenhengen mellom de variablene. Selv om vi bare hadde noen observasjoner på gitte punkter på x-aksen har vi gjennom regresjonslinja fått tilgang til alle tenkelige punkter på x-linja og kan anta et drivstofforbruk ut fra det (ved å gå opp fra x-aksen, finne skjæringspunktet med regresjonslinja, og deretter gå inn på y-aksen og lese av drivstofforbruket). Den prediksjonen vi da gjør er vår beste gjetning på hvor stort drivstofforbruket vil være en gitt motorstørrelse. Dette vil selvsagt være en kvalifisert gjetning - nettopp fordi det er en modell. Og alle modeller er feil, men noen modeller er nyttige likevel.eksempelet kan vi eksempel tenke oss mulige linjer:Begge linjene er forsøk på å lage en rett linje som har kortest mulig avvik. Vi kan deretter legge sammen de absolutte vertikale avstandene (de stiplede linjene) fra observasjonspunktene ned til den rette linja. prinsippet er da den rette linja som medfører minst samlet avstand fra observasjonspunktene den rette linja som best representerer observasjonspunktene, og vi kan si vi har laget en modell sammenhengen mellom motorstørrelse og drivstofforbruk. Siden vi har en sammenhengende rett linje har vi også mulighet til å mene noe om drivstofforbruk på motorstørrelser vi ikke har målt/har observasjoner på. Vi har med andre ord en modell å predikere drivstofforbruk ut fra motorstørrelse. Uavhengige variabler regresjonsanalyser kalles også ofte prediktorer, fordi vi bruker de til å predikere en verdi den avhengige variabelen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"minste-kvadratsum-ordinary-least-squares---ols","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.2.1 Minste kvadratsum (Ordinary Least Squares - OLS)","text":"Imidlertid er absoluttverdier matematisk problematiske (Løvås 2013). Pre-datamaskiner ble det derfor utviklet en alternativ måte som kalles «minste kvadraters metode» - derav begrepet OLS («Ordinary Least Squares»). Det finnes andre måter å tilnærme seg dette, men dette kurset går vi kun inn på OLS-regresjon. Hvis vi fortsetter eksempelet kan vi tenke oss en mengde forslag på ulike linjer som forsøker å beskrive sammenhengen mellom de variablene:Man regner deretter ut kvadratene som dannes av hvert punkt og avstanden til den rette linja. Den linja som har den laveste kvadratsummen («least squares») er den linja som best representerer datapunktene og som derfor er den beste lineære modellen av forholdet mellom variablene. Regresjonslinja er således en modell. Som Thrane (2019) beskriver: den diagonale linja oppsummerer den typiske trenden det statistiske forholdet mellom de variablene - en linje vi kjenner som regresjonslinja.\nHvis vi har et stort antall datapunkter er dette selvsagt en omfattende prosess å gjøre manuelt. Det statistikkprogrammer gjør oss er å regne ut kvadratsummen et stort antall mulige linjer og deretter fortelle oss hvilken som har lavest kvadratsum.Hvis vi tenker tilbake til formelen modellen vår: \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) kan vi nå fylle ut med verdier fra eksempelet.Det vi egentlig har gjort når vi finner den rette linja som gir minste kvadratsum er å identifisere \\(\\alpha\\) (skjæringspunktet på y-aksen) og \\(\\beta\\) (stigningstallet). Statistikkprogrammer vil gi oss verdiene på dette. Vi går ikke inn på en manuell utregning , men bruker de verdiene Løvås (2013) viser (\\(\\alpha=0.211\\) og \\(\\beta=0.00576\\)). Vår modell ser da slik ut: \\(Y_i=0,211\\:+0,00576x_i\\).Som sagt har vi ønsket å lage en modell som predikerer drivstofforbruk ut fra motorstørrelse – eller sagt på en annen måte: hvilket drivstofforbruk kan vi forvente med en motor på 100 hk? Vi får da: \\[Y_i=0,211\\:+0,00576x_i=0,211\\:+\\:0,00576\\times100\\:=\\:0,787\\]Dette blir vårt “best guess”, vår antakelse (vår prediksjon av verdien på y-aksen som er drivstofforbruket ut fra verdien på x-aksen som er motorstørrelse), om forventet drivstofforbruk en motor med 100 hk basert på den modellen vi har laget om sammenhengen mellom motorstørrelse og drivstofforbruk (som er basert på de observasjonene vi har).Vi kan naturligvis umiddelbart tenke drivstofforbruket er avhengig av mange andre faktorer enn motorstørrelse, eksempel bilens design (luftmotstand), vekt, rullemotstand, temperatur, type motor og så videre. Dette belyser så vidt et sentralt problem når vi ønsker å lage modeller prediksjon: Virkeligheten er utrolig sammensatt, mange relevante variabler er vanskelig å måle, og man ønsker en modell som er enkel nok til å kunne brukes og sammensatt nok til å gi relevante prediksjoner. Tenk eksempel bare på «klimamodellene» som brukes å analysere og predikere temperatur, issmelting, global oppvarming og liknende. Det er klart de fleste tilfeller trenger vi flere prediktorer enn en – og regresjonssammenheng snakker vi da om multippel regresjonsanalyse. Vi kan ha som en tommelfingerregel vi skal ha med så mange variabler modellen har praktisk verdi, men likevel så få som mulig.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konfidensintervall","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.2.2 Konfidensintervall","text":"Avslutningsvis denne introduksjonen til regresjonsanalyse kan vi se kort på begrepet konfidensintervall (se eksempel Løvås (2013) eller Hinkle, Wiersma, Jurs (2003)). de som ønsker å fordype seg effektstørrelser og konfidensintervaller anbefales Cumming Calin-Jageman (2017) “Introduction new statistics: Estimation, open science, & beyond”.Man kan si estimatet på stigningstallet \\(\\beta\\) er det viktigste resultatet en regresjonsanalyse fordi dette sier noe om hvor sterk sammenhengen mellom de variablene er. Løvås (2013) illustrerer dette slik:De røde stiplede linjene utgjør konfidensgrensene 95 % konfidensintervall. grafen har vi kun 5 observasjoner, noe som selvsagt er lite. Vi kan gå litt dypere inn hvordan konfidensgrensene framkommer en regresjonsanalyse.La oss anta vi har et datasett der vi har plottet korrelasjonen mellom en prediktor (den uavhengige variabelen) på x-aksen og en avhengig varaibel på y-aksen (se graf ). Den røde prikken markerer verdien x=8. Verdien på y-aksen (10,458) er vår prediksjon (vår buest guess) på hva verdien den avhengige variabelen vil være ved den observerte verdien x=8.punktet x=8 har vi en hel populasjon av mulige normalfordelte verdier. Vårt beste estimat av gjennomsnittsverdien denne populasjonen er 10,458. Dette er et punktestimat. Det tilhørende intervallestimatet er vårt konfidensintervall. Vi må tenke på konfidensintervallet som en vertikal linje. Så stedet å tenke punkt- og intervallestimat slikKan vi tenke det slik:Overført til vårt eksempel får vi:Den røde streken er vårt 95 % konfidensintervall punktestimatet. Hvis vi legger på 95 % konfidensintervaller på alle punktestimatene (alle punktene som utgjør regresjonslinja) kan vi lage de stiplede linjene som toucher endepunktene på alle konfidensintervallene. Disse stiplede linjene utgjør da konfidensgrensene regresjonslinja.Vi ser konfidensgrensene er lett buede mot hverandre med minst avstand mellom dem “på midten”. Vi skal kort se på hvorfor det er slik. Vi har nå lagt på et nytt kryss grafen . Dette krysset markerer punktet der gjennomsnittene av X og Y krysser. Regresjonslinja må gå gjennom dette punktet, slik alle alternative regresjonslinjer må pivotere rundt dette punktet. Dette medfører det er litt større usikkerhet rundt punktestimatenes konfidensintervaller endene forhold til midten. Konfidensintervallene hvert enkelt punkt blir derfor litt lenger jo lenger ut fra krysningspunket vi går, og resultatet blir en form buet linje.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-i-analyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.2.3 Steg i analyse","text":"Vi anbefaler en analyse går gjennom disse stegene:Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterVi skal det følgende gå gjennom disse stegene en regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"enkel-lineær-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3 Enkel, lineær regresjonsanalyse","text":"Dette eksempelet bygger på Field (2009a). Hvis vi ønsker å se på hvilken grad vi kan predikere salgstall gjennom hvor mye vi bruker på reklame før lansering kan vi gjøre en lineær regresjonsanalyse med salg som avhengig variabel og reklame (adverts) som uavhengig variabel.Du kan laste ned datasettet ulike formater :Download Field_datasett_OLS.xlsxDownload Field_datasett_OLS.savDownload Field_datasett_OLS.dtaDatasettet kan også finnes herVi skal nå gå gjennom våre anbefalte steg analysen, og starter med en analyse av dataene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-1-analyse-av-dataene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.1 Steg 1: Analyse av dataene","text":"“soon collected data, compute statistics, look data. Data screening data snooping. opportunity discard data change values favor hypotheses. However, assess hypotheses without examining data, risk publishing nonsense” (Wilkinson Task Force Statistical Inference 1999).Vi ser på datasettet og noen nøkkeltall datasettet.Descriptive Statistics\nField_OLS_data\nN: 200Vi kan se datasettet består av 200 obervasjoner av 4 variabler. Hver av observasjonene er en CD:Adverts: Dette er summen brukt på reklame før lanseringsdatoSales: Dette er salgtall per ukeAirplay: Antall ganger et spor fra CDen ble spilt på radio uka før lanseringsdatoImage: En rating på hvor attraktiv gruppen/artisten (positivt image)Ofte er det imidlertid mer hensiktsmessig å se på dataene grafisk en utforskende hensikt (Tukey 1977).","code":"\n# Bruker pakken: readxl\nField_OLS_data <- read_excel(\"Field_datasett_OLS.xlsx\")\n# Bruker pakken: summarytools\nsummarytools::descr(Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"histogram-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.1.1 Histogram","text":"Det kan se ut som salgstallene er rimelig normalfordelte, mens reklamevariabelen er klart skjev.","code":"\n# Bruker pakkene: ggplot2 og gridExtra\n\nannotations <- data.frame(\n  x = c(round(min(Field_OLS_data$Adverts), 2), round(mean(Field_OLS_data$Adverts), 2), round(max(Field_OLS_data$Adverts), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n  \nplott1 <- ggplot(Field_OLS_data, aes(Adverts)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Adverts)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\n\nannotations2 <- data.frame(\n  x = c(round(min(Field_OLS_data$Sales), 2), round(mean(Field_OLS_data$Sales), 2), round(max(Field_OLS_data$Sales), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n\nplott2 <- ggplot(Field_OLS_data, aes(Sales)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Sales)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations2, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\ngrid.arrange(plott1, plott2, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"quantile-quantile-plott-qq","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.1.2 Quantile-Quantile plott (qq)","text":"Q-Q plottet (“quantile-quantile plot”) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet innebærer å se distribusjoner mot hverandre – empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om “normal Q-Q plott” - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). eksempelet vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med.Som vi fikk indikert gjennom histogrammene er salgsvariabelen rimelig normalfordelt, mens reklamevariabelen viser avvik fra normalfordelingen - dette tilfellet (ut fra histogram og qq-plott vil vi si den er høyreskjev).","code":"\n# Bruker pakken: car\npar(mfrow=(c(1,2)))\nqqSales2 <- car::qqPlot(Field_OLS_data$Adverts)\nqqAdverts2 <- car::qqPlot(Field_OLS_data$Sales)"},{"path":"regresjonsanalyse---ols.html","id":"steg-2-evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.2 Steg 2: Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Dette er ikke relevant en enkel lineær regresjonsanalyse. Når vi skal gjøre en multippel regresjonsanalyse - altså vi har eller flere uavhengige variabler (prediktorer) vil analysen av dataene våre - og hvilken rekkefølge vi legger de uavhengige variablene inn regresjonsmodellen (mer om det eksempelet multippel regresjon) kunne informeres av analysen vi gjør forkant. Derfor viser vi dette nå selv om det altså ikke er relevant enkel regresjonsanalyse.Vi lager en korrelasjonstabell de tre uavhengige og den avhengige variabelen (Sales, Adverts, Airplay, Image):p < .0001**** , p < .001*** , p < .01**, p < .05*standard multippel regresjonsanalyse legger vi alle de uavhengige variablene inn samtidig. Dersom vi skal gjøre en stegvis regresjonsanalyse vil vi legge de uavhengige variablene inn en og en ut fra statistiske kriterier - som hvor stor korrelasjonen er. Den uavhengige variabelen med størst korrelasjon legges inn først og så videre. det eksempelet vi har ser vi Sales korrelerer høyest med Airplay, og nesten like mye med Adverts. Korrelasjonen med Image er noe lavere.","code":"\n# Bruker pakken: sjPlot\ntab_corr(Field_OLS_data, triangle = \"lower\")"},{"path":"regresjonsanalyse---ols.html","id":"steg-3-lage-modell-og-kjøre-regresjonsanalysen","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.3 Steg 3: Lage modell (og kjøre regresjonsanalysen)","text":"Vi ønsker å se om Adverts kan predikere Sales. Grafisk kan vi vise dette:","code":"\n# Bruker pakken: olsrr\nFieldOLS_reg <- ols_regress(Sales ~ Adverts, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"steg-4-analyse-av-resultatene-diagnostikk","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.4 Steg 4: Analyse av resultatene (diagnostikk)","text":"","code":"\n# Bruker pakken: olsrr\nFieldOLS_reg\n#>                          Model Summary                           \n#> ----------------------------------------------------------------\n#> R                       0.578       RMSE                 65.991 \n#> R-Squared               0.335       Coef. Var            34.157 \n#> Adj. R-Squared          0.331       MSE                4354.870 \n#> Pred R-Squared          0.323       MAE                  50.869 \n#> ----------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                  ANOVA                                   \n#> ------------------------------------------------------------------------\n#>                    Sum of                                               \n#>                   Squares         DF    Mean Square      F         Sig. \n#> ------------------------------------------------------------------------\n#> Regression     433687.833          1     433687.833    99.587    0.0000 \n#> Residual       862264.167        198       4354.870                     \n#> Total         1295952.000        199                                    \n#> ------------------------------------------------------------------------\n#> \n#>                                     Parameter Estimates                                     \n#> -------------------------------------------------------------------------------------------\n#>       model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n#> -------------------------------------------------------------------------------------------\n#> (Intercept)    134.140         7.537                 17.799    0.000    119.278    149.002 \n#>     Adverts      0.096         0.010        0.578     9.979    0.000      0.077      0.115 \n#> -------------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-mye-forklarer-modellen-vår","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.4.1 Hvor mye forklarer modellen vår?","text":"Det første vi kan se på er \\(R^{2}\\) som forteller oss hvor stor del av den totale variansen modellen forklarer. dette tilfellet er \\(R^{2} = 0.3346\\). Det innebærer modellen vår kan forklare 33.46 % av den totale variansen. Det betyr reklame forklarer («accounts ») 33.5 % av variansen salget. Det er med andre ord mange andre faktorer som kan forklare hvorfor noen plater selger bedre enn andre, men reklame kan forklare drøye 33 % av den totale variansen. Dette kan vi også se er statistisk signifikant p < .001.Endel programmer vil også gi en R verdi. Siden vi kun har en uavhengig variabel (en prediktor) vil verdien R utgjøre den bivariate korrelasjonen (korrelasjonskoeffisienten mellom de variablene - vi ser dette er samme verdi som tabellen korrelasjonskoeffisienter lenger opp).Adjusted \\(R^{2}\\) er en “modifisert versjon” av \\(R^{2}\\) der det legges inn en korreksjon antall prediktorer modellen. Motivasjonen dette er det å legge til flere prodeiktorer alltid vil øke \\(R^{2}\\) verdien (Navarro Foxcroft 2019). Navarro Foxcroft (2019) påpeker imidlertid man ikke kan tolke adjusted \\(R^{2}\\) like rett fram som \\(R^{2}\\), og anbefaler man bruker \\(R^{2}\\). Det vi også kan si er dersom verdiene på henholdsvis \\(R^{2}\\) og adjusted \\(R^{2}\\) er nærme hverandre (eller like) indikerer dette en god kryssvaliditet modellen, noe som kan gjøre oss sikrere generalisering av funnene våre.","code":""},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-og-regresjonslikning","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.4.2 Modellens koeffisienter og regresjonslikning","text":"Koeffisientene vil fortelle oss mer multippel regresjonsanalyse, men gir oss noen interessante opplysninger også . introduksjonen snakket vi om punktet der regresjonslinja skjærer y-aksen (konstanten). Fra analysen ser vi (Intercept) = 134.14 og Adverts = 0.096. 134.14 er punktet på y-aksen regresjonslinja “begynner” (der x = 0). Altså, ettersom x-aksen angir verdier hva vi bruker på reklame er Intercept estimatet antallet plater vi kan forvente å selge dersom vi bruker 0 kroner på reklame. Estimatet på «Adverts» på 0,096 er stigningstallet regresjonslinja – hvis prediktoren (reklame) stiger med 1 enhet stiger salget med 0,096 plater. Vi kan da lage følgende likning: \\[ Sales=134,14\\:+\\:0,096\\left(Adverts\\right) \\]","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.4.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi ønsker å ha en formening om hvor god modellen er («goodness fit»). Altså, er regresjonsmodellen vår bedre enn en modell der vi ikke vet noe om forholdet mellom reklame og platesalg? Vi kan bruke gjennomsnitt av salgstallene som en modell ingen sammenheng mellom reklame og platesalg, og deretter sammenlikne regresjonsmodellen med gjennomsnittsmodellen. Sammenlikningen mellom modellene skjer gjennom å se på forskjellene mellom de observerte målingene (salgstall) og verdier predikert de ulike modellene. Dersom regresjonsmodellen signifikant predikerer bedre er det en bedre modell enn alternativet.Analysen vår har gitt oss F verdien 99.59 med p < .001. Vi ser verdien er statistisk signifikant. F verdien er et mål på forbedring prediksjonen sett opp mot unøyaktigheter modellen (alle modeller er unøyaktige (eller “feil”)).\nVi kan sjekke F-verdien opp mot antall frihetsgrader (df) gjennom tabeller som ofte finnes statistikkbøker, eller bruke onlineressurser som herVi ser av resultatene fra analysen antall df teller er 1 og antall df nevner er 198. Hvis vi leser av tabellen ser vi df 1/df 200 er kritisk verdi 3,89 α = 0,05 og 11,15 α = 0,001. Vår F er med andre ord langt kritisk verdi. Vi kan derfor si vår regresjonsmodell gir en signifikant bedre prediksjon av platesalg enn alternativet. Reklame er med andre ord en god prediktor platesalg.Helt nøyaktig kan vi regne ut kritisk verdi (som vi har gjort R) df 1 og df 198:Ut fra dette kan vi si det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.Til slutt vil vi se på konfidensintervallet:","code":"\n# Base R\nqf(p=.05, df1=1, df2=198, lower.tail=FALSE)\n#> [1] 3.888853\n# Bruker pakken: car\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\nConfint(FieldOLS_reg)\n#>                 Estimate        2.5 %      97.5 %\n#> (Intercept) 134.13993781 119.27768082 149.0021948\n#> Adverts       0.09612449   0.07712929   0.1151197"},{"path":"regresjonsanalyse---ols.html","id":"steg-5-sjekk-av-forutsetningene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5 Steg 5: Sjekk av forutsetningene","text":"Dette er et tema som behandles og framstilles på noe ulike måter litteraturen. Det som er klart er brudd på forutsetningene kan gjøre modellen vår mer usikker opp til et punkt hvor regresjonsanalyse ikke bør gjennomføres. Noen av forutsetningene er empirisk testbare (vi kan få ut en eller annen form analyse av et statistikkprogram som SPSS, Stata, R og så videre) mens noen er ikke empirisk testbare (det vil si vi må bruke egen vurdering). Vi skal dette delkapittelet gå gjennom forutsetningene lineær regresjon. Selv om noen ikke er aktuelle enkel regresjon tar vi med alle forutsetningene oversiktens skyld.Ved regresjonsanalyse gjør vi en rekke sjekker av datamaterialet vi har å avgjøre om regresjonsanalyse er en egnet teknikk og hvorvidt vi mener vi kan generalisere funnene. Dersom forutsetningene brytes gjør det vi kan sette spørsmålstegn ved hvor nærme regresjonskoeffisienten er populasjonskoeffisienten – eller med andre ord: Hvis regresjonskoeffisienten er helt forventningsrett («unbiased», dvs 0) så vil regresjonskoeffisienten være lik populasjonskoeffisienten («estimatet er lik virkeligheten»). Nå vil det praksis aldri være tilfelle, men ved å sette visse forutsetninger kan vi fastslå om våre data egner seg regresjonsanalyse og hvor sikre vi føler oss funnene kan generaliseres. Det er verdt å merke seg hva Field et. al (2012b, s.298) skriver:“’s worth remembering can perfectly good model data (outliers, influential cases, etc.) can use model draw conclusions sample, even assumptions violated. However, ’s much interesting generalize regression model assumptions become important. violated generalize findings beyond sample.”Med andre ord: Vi kan ha brudd på forutsetningene og likevel si noe meningsfullt om vårt utvalg/våre data, men resultatene våre blir mer usikre og vi skal være veldig forsiktige med å kreve generaliserbarhet dersom vi har brudd på forutsetningene. Så alt håp er ikke ute med brudd på forutsetningene, men vi skal behandle konklusjonene våre deretter.Regresjonsforutsetninger behandles ulikt av ulke kilder, og får ulik plass diskusjoner om regresjon. Vi har undersøkt en rekke kilder å framstille dette (blant annet Green (1991), Berry (1993), Miles Shevlin (2001), Hinkle, Wiersma, Jurs (2003), Tabachnik Fidell (2007), Eikemo Clausen (2007), Hair Jr. et al. (2010), Lomax Hahs-Vaughn (2012)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.1 Kausalitet","text":"Forutsetningen om kausalitet hviler fagunnskap og teoretiske vurderinger. Det sier seg så vidt selv vi ikke er interesserte å ha med irrelevante variabler modellen. Med irrelevant menes variabler som korrelerer med den avhengige variabelen, men hvor korrelasjonen er ikke har noe med årsakssammenheng å gjøre (sammenhenger medfører ikke seg selv kausalitet som vi har vært inne på en tidligere modul).Kausalitet er dermed en forutsetning. Den uavhengige variabelen må variere korrelert med de avhengige, det er en kausal sammenheng (hvis ikke det er kausalitet har den/de uavhengige variablene ingen effekt på den avhengige – det kan like gjerne være motsatt). Når vi velger en avhengig variabel og en eller flere uavhengige variabler har vi også gjort en antakelse om kausalitet og retning på kausaliteten, og forutsatt denne er tilstede - basert på teoretisk kunnskap om det vi undersøker. Men det er viktig å understreke verken korrelasjon eller regresjon indikerer kausalitet.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil våre data. Thoresen (2003) viser eksempel til en studie av MacMahon et al. (1990) der de fant en 60% sterkere sammneheng mellom blodtrykk og hjerte-karsykdommer en stor metastudie når de korrigerte skjevhet estimatene de tidligere studiene (som var inkludert metastudien).Vi skal også være oppmerksom på utfordringer dersom feil den ene variabelen korrelerer med feil en annen variabel. “Dersom målt eksponering og målt helseutfall er rammet av avhengige feil, blir resultatet oftest en falskt forhøyet sammenheng mellom de . Slik resultatskjevhet er sannsynligvis ikke uvanlig tverrsnittsstudier, hvor data om eksponering og utfall skaffes til veie gjennom spørreskjema” (Kristensen 2005).","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.3 Relevante og irrelevante variabler","text":"Alle relevante uavhengige variabler må være inkludert modellen, og alle irrelevante uvhengige variabler er fjernet/er ikke med modellen. Man kan si en hovedgrunn til vi veldig ofte kjører mulitippel regresjonsanalyse stedet bivariat regresjonsanalyse er å unngå vi ikke inkluderer relevante variabler (såkalt “omitted variable bias”) (Thrane 2019). Imidlertid, som Thrane (2019) påpeker, er dette praksis umulig, så det vi tilstreber er å inkludere de mest relevante variablene. Dette faller igjen tilbake på teoretiske betraktninger og faglig kjennskap til området man holder på med. Du skal hvert fall kunne begrunne valget av hvilke uavhengige variabler som er inkludert og hvilke som kanskje kunne tenkes å være inkludert, men som du har valgt å ikke inkludere.Teoretisk sett skal vi også forsikre oss om ikke-relevante variabler ikke er inkludert modellen. Igjen er dette delvis umulig og delvis forvirrende/unøyaktig. Det er delvis umulig fordi vi vanskelig kan vite eksakt hvilke potensielle variabler som er relevante og ikke. Det er delvis forvirrende/unøyaktig fordi det kan være viktig å identifisere variabler som ikke har noen effekt - dette kan være viktig policyrevisjon/-utforming (jfr Thrane (2019)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Forholdstallet mellom respondenter/caser/observasjoner og uavhengig variabler er av stor betydning dersom man skal gjennomføre en multippel regresjon. Dette gjelder spesielt ved skjevdistribusjon av den avhengige variabelen, effektstørrelsen er forventet liten eller man kan mistenke vesentlige målefeil (Tabachnik Fidell 2007).Det finnes ulike anbefalte normer vurdering av forholdstallet (vi tar utgangspunkt standard multippel regresjonsanalyse - stegvis multippel regresjonsanalyse kan gi andre vurderinger rundt forholdstallet). viser vi noen eksempler på hvordan man kan vurdere dette:\nTable 7.1: Forholdstall\nTable 7.1: ForholdstallKildeAntallMarks (1966, Harris (2013)Minimum 200 uansettSchmidt (1971)15-1 til 25-1Nunally (1978)2-3 IV = minst 100, 9-10 IV = 300-400Stevens (1996)15 pr IVGreen (1991)N=50+8m (m=antall uavhengige variabler) ved «medium-sized relationship IVs DV, =.05 ß=.20Miles & Shevlin (2001)Som Green (2001). Utvalgsstørrelse avhenger av størrelse på effekt og statistisk styrke (se Cohen, 1988 effektstørrelser). Stor effekt: 80 respondenter er alltid nok opptil 20 IVs. Middels effekt: 200 respondenter vil alltid være nok opptil 20 IVs, 100 er nok opptil 6 eller færre IVs. Lav effekt: Minst 600.Det er også verdt å merke seg det ikke er ønskelig med mange respondenter, da et svært stort antall respondenter vil gi statistisk signifikans nesten enhver multippel korrelasjon – “statistical practical reasons, , one wants measure smallest number cases decent chance revealing relationship specified size” (Tabachnik Fidell 2007, s.123). Dette har med andre ord mye å si hvordan man planlegger en studie. Miles Shevlin (2001), som angitt siste rad tabellen , ser på sammenhengen mellom effektstørrelse, statistisk styrke og utvalgsstørrelse. Field (2009a, s.223, figur 7.10) har modifisert grafisk denne sammenhengen:","code":""},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Denne forutsetningen gjelder om man har minst uavhengige variabler - altså multippel regresjon. Med dette menes vi må forvente variasjonen den avhengige variabelen er en funksjon av sum av endringer de uavhengige variablene. Vi kan også uttrykke dette som effekten av den uavhengige variabelen \\(x_1\\) på den avhengige variabelen \\(y\\) er uavhengig av eventuelle andre uavhengige variablers effekt på \\(y\\). Forutsetningen om additivitet betyr altså det ikke er interaksjon mellom eller flere uavhengige variabler.\nMed andre ord, hvis effekten av \\(x_1\\) på \\(y\\) er avhengig av hvordan \\(x_2...x_n\\) påvirker \\(y\\) brytes forutsetningen om additivitet. praksis er det imidlertid ikke uvanlig denne forutsetningen brytes en eller annen grad.La oss vise dette med et eksempel fra Thrane (2019) (se figur 6.1). Dersom vi har data kvinners og menns inntekt relatert til antall års utdannelse vil vi kunne se en regresjonsanalyse antall års utdanning predikerer inntekt. Forutsetningen om additivitet sier da antall års utdanning har lik effekt på inntekt menn og kvinner. Slik er det imidlertid ikke. Generisk kan det framstilles som grafen :Vi ser linjene har ulikt stigningstall. Kvinner starter menn, men har et høyere stigningstall. Det vil si kvinner har større effekt av et (ekstra) års utdanning enn menn. Da er forutsetningen om additivitet brutt. Som Thrane (2019) påpeker: “Additivity thus means parallel regression lines”.Vi kan altså sjekke dette ved å kjøre regresjonsanalyser. En annen måte, som vi ikke går inn på , er å lage en interaksjonsvariabel som vi dernest bruker modellen. En interaksjonsvariabel er et produkt av (minst) uavhengige variabler.La oss se på et annet eksempel å vise interaksjonsvariabel og -effekt basert på et eksempel fra Thomas (2017).Datasettet inneholder ti variabler tillegg til det vi vil bruke som avhengig variabel: Sales. Vi vil altså se om vi kan bruke de ni variablene til å predikere salg.Denne modellen kan altså forklare 87 % av variansen Sales.\nVi kan imidlertid mistenke det kan være en interaksjon mellom variablene Income og Population - jo større befolkning, jo større inntekt tilgjengelig.Forskjellen mellom modellene ligger altså den nederste koeffisienten (Income:Population) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Vi ser en klar interaksjon ved linjene krysser (jfr. Thranes merknad om forutsetningen om additivitet gir parallelle linjer).R har vi et hjelpemiddel pakken interactions:Tolkningen av plottet fra pakken interactions er den samme: Parallelle linjer indikerer fravær av interaksjoneseffekt, mens ikke-parallelle linjer indikerer tilstedeværelse av interaksjoneseffekt.Vi ser tillegg interaksjonseffekten (Income:Population) er statistisk signifikant.","code":"\n# Bruker pakken: ISLR\ndata(Carseats)\n# Bruker pakken: summarytools\nsummarytools::descr(Carseats, stats = \"common\")\n#> Non-numerical variable(s) ignored: ShelveLoc, Urban, US\n#> Descriptive Statistics  \n#> Carseats  \n#> N: 400  \n#> \n#>                   Advertising      Age   CompPrice   Education   Income   Population    Price    Sales\n#> --------------- ------------- -------- ----------- ----------- -------- ------------ -------- --------\n#>            Mean          6.64    53.32      124.97       13.90    68.66       264.84   115.80     7.50\n#>         Std.Dev          6.65    16.20       15.33        2.62    27.99       147.38    23.68     2.82\n#>             Min          0.00    25.00       77.00       10.00    21.00        10.00    24.00     0.00\n#>          Median          5.00    54.50      125.00       14.00    69.00       272.00   117.00     7.49\n#>             Max         29.00    80.00      175.00       18.00   120.00       509.00   191.00    16.27\n#>         N.Valid        400.00   400.00      400.00      400.00   400.00       400.00   400.00   400.00\n#>       Pct.Valid        100.00   100.00      100.00      100.00   100.00       100.00   100.00   100.00\n# Bruker pakken: olsrr\nsaleslm <- ols_regress(Sales~. ,data = Carseats)\nsaleslm\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.935       RMSE                1.019 \n#> R-Squared               0.873       Coef. Var          13.592 \n#> Adj. R-Squared          0.870       MSE                 1.038 \n#> Pred R-Squared          0.865       MAE                 0.804 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                 Sum of                                                \n#>                Squares         DF    Mean Square       F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression    2779.441         11        252.676    243.372    0.0000 \n#> Residual       402.834        388          1.038                      \n#> Total         3182.275        399                                     \n#> ----------------------------------------------------------------------\n#> \n#>                                      Parameter Estimates                                      \n#> ---------------------------------------------------------------------------------------------\n#>           model      Beta    Std. Error    Std. Beta       t        Sig      lower     upper \n#> ---------------------------------------------------------------------------------------------\n#>     (Intercept)     5.661         0.603                   9.380    0.000     4.474     6.847 \n#>       CompPrice     0.093         0.004        0.504     22.378    0.000     0.085     0.101 \n#>          Income     0.016         0.002        0.157      8.565    0.000     0.012     0.019 \n#>     Advertising     0.123         0.011        0.290     11.066    0.000     0.101     0.145 \n#>      Population     0.000         0.000        0.011      0.561    0.575    -0.001     0.001 \n#>           Price    -0.095         0.003       -0.799    -35.700    0.000    -0.101    -0.090 \n#>   ShelveLocGood     4.850         0.153        0.703     31.678    0.000     4.549     5.151 \n#> ShelveLocMedium     1.957         0.126        0.345     15.516    0.000     1.709     2.205 \n#>             Age    -0.046         0.003       -0.264    -14.472    0.000    -0.052    -0.040 \n#>       Education    -0.021         0.020       -0.020     -1.070    0.285    -0.060     0.018 \n#>        UrbanYes     0.123         0.113        0.020      1.088    0.277    -0.099     0.345 \n#>           USYes    -0.184         0.150       -0.031     -1.229    0.220    -0.479     0.111 \n#> ---------------------------------------------------------------------------------------------\n# Base R\nsaleslm2 <- lm(Sales~. ,Carseats)\nsaleslm1 <- lm(Sales~.+Population*Income, Carseats)\n# Bruker pakken: car\ncompareCoefs(saleslm2, saleslm1)\n#> Calls:\n#> 1: lm(formula = Sales ~ ., data = Carseats)\n#> 2: lm(formula = Sales ~ . + Population * Income, data = \n#>   Carseats)\n#> \n#>                     Model 1   Model 2\n#> (Intercept)           5.661     6.195\n#> SE                    0.603     0.644\n#>                                      \n#> CompPrice           0.09282   0.09262\n#> SE                  0.00415   0.00413\n#>                                      \n#> Income              0.01580   0.00797\n#> SE                  0.00185   0.00387\n#>                                      \n#> Advertising          0.1231    0.1237\n#> SE                   0.0111    0.0111\n#>                                      \n#> Population         0.000208 -0.001811\n#> SE                 0.000370  0.000952\n#>                                      \n#> Price              -0.09536  -0.09511\n#> SE                  0.00267   0.00266\n#>                                      \n#> ShelveLocGood         4.850     4.859\n#> SE                    0.153     0.152\n#>                                      \n#> ShelveLocMedium       1.957     1.964\n#> SE                    0.126     0.125\n#>                                      \n#> Age                -0.04605  -0.04566\n#> SE                  0.00318   0.00317\n#>                                      \n#> Education           -0.0211   -0.0216\n#> SE                   0.0197    0.0196\n#>                                      \n#> UrbanYes              0.123     0.133\n#> SE                    0.113     0.112\n#>                                      \n#> USYes                -0.184    -0.216\n#> SE                    0.150     0.150\n#>                                      \n#> Income:Population            2.88e-05\n#> SE                           1.25e-05\n#> \n# Bruker pakken: ggplot2\nggplot(data=Carseats, aes(x=Income, y=Sales, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(Population,Sales), method=lm, se=F,color=\"black\")+xlab(\"Income and Population\")+labs(\n        title=\"Inntekt i blått - Befolkning i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\n# Bruker pakken: interactions\ninteract_plot(saleslm1, pred = Population, modx = Income)\n# Bruker pakken: jtools\nsumm(saleslm1)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.6 Linearitet","text":"Vi tar, som navnet “linær regresjonsanalyse” ganske klart indikerer, utgangspunkt forholdet mellom den/de uavhengige varaibelen(e) og den avhengige variabelen kan beskrives som en lineær funksjon (se eksempel Ringdal (2007)). Sammenhengen mellom variablene må ikke være perfekt lineær, men må hvert fall være tilnærmet lineær.Vi har allerede sett en grafisk framstilling punktet analyse av dataene som lar oss visuelt vurdere denne forutsetningen:Den stiplede linjen som buer ca midt det skraverte området gjør ingen forutsetninger, men plotter bare dataene (ofte kalt “scatterplot smoother”). Vi kan vurdere om denne er nærme eller langt fra en rett linje. grafen ligger det både en stiplet buet linje og en rett linje (regresjonslinje). vårt tilfelle vil vi raskt konkludere med forholdet mellom de variablene er tilnærmet lineært.","code":"\n# Bruker pakken: car\nscatterplot(Adverts ~ Sales, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.7 Residualene skal være normalfordelte","text":"Forutsetningen er residualene modellen er tilfeldige, normalfordelte variabler med gjennomsnittsverdi 0 (Field, Miles, Field 2012b), hvilket innebærer forskjellen mellom modellen og de observerte dataene er 0 eller nær 0 de fleste tilfeller (og ulikhet skyldes tilfeldigheter). Poenget er dersom regresjonsmodellen er god skal det være omtrent like stor sannsynlighet den underestimerer som den overestimerer. Er den det vil fordelingen være tilnærmet symmetrisk (perfekt normalfordeling vil praksis ikke inntreffe).En tilnærming til å se på denne forutsetningen er å se på et histogram residualene. Når vi lagrer regresjonsmodellen som et objekt (R) kan vi se hvilke parametere som lagres modellen:Residualene lagres altså modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nnames(FieldOLS_reg)\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"      \n#>  [4] \"rank\"          \"fitted.values\" \"assign\"       \n#>  [7] \"qr\"            \"df.residual\"   \"xlevels\"      \n#> [10] \"call\"          \"terms\"         \"model\"\n# Base R\n\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\n\nhist(FieldOLS_reg$residuals)\n\nFieldOLSresid <- FieldOLS_reg$residuals\n\nplott3 <- ggplot(data = FieldOLS_reg, aes(FieldOLS_reg$residuals)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") +\n    theme_classic() +\n    labs(title = 'Histogram av residualer', x = 'Residualer', y = 'Antall')\n\nplott3\n# Bruker pakken: olsrr\nols_plot_resid_qq(FieldOLS_reg)\nols_test_normality(FieldOLS_reg)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9899         0.1757 \n#> Kolmogorov-Smirnov        0.0634         0.3970 \n#> Cramer-von Mises          16.055         0.0000 \n#> Anderson-Darling          0.4298         0.3057 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.8 Fravær av multikolinearitet","text":"Multikolinearitet innebærer det er korrelasjon mellom de uavhengige varaiblene (multippel regresjon). Det kan ofte forekomme vi har en viss korrelasjon, men hvis korrelasjonen blir stor blir det vanskelig å skille mellom effekten den enkelte uavhengige variabel har på den avhengige variabelen.Multikolinearitet inntreffer dersom vi har sterk korrelasjon mellom eller flere av de uavhengige variablene. Multikollinearitet handler altså om det innbyrdes forholdet mellom de uavhengige variablene. Hvis disse er høyt korrelerte har vi multikollinearitet, altså det kan være (tilnærmet) perfekt linearitet mellom uavhengige variabler (Berry 1993) hvilket innebærer muligheten ingen av korrelasjonskoeffisientene er signifikante pga størrelsen på standardfeil. En perfekt kolinearitet har koeffisienten 1.Berry (1993) angir angir \\(r=.9\\) gir en dobling av standardfeil regresjonskoeffisienten, og selv \\(0.5\\) og \\(0.6\\) kan gi utfordringer tolkningen av regresjonskoeffisientene. Field (2009a) anser \\(0.8\\) til \\(0.9\\) som høy korrelasjon.Samtidig sier Pallant (2010) det (naturligvis) bør være en viss korrelasjon mellom de uavhengige og den avhengige variabelen, og hevder de bør være på \\(0.3\\), men samtidig bivariat korrelasjon mellom de uavhengige variablene ikke bør være \\(0.7\\).Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de tre uavhengige variablene korrelerer mellom \\(0.33\\) og \\(0.60\\) med den avhengige. De bivariate korrelasjonene mellom de uavhengige variablene erpå hhv. \\(0.08\\), \\(0.10\\) og \\(0.18\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Dette er aktuelt multippel regresjonsanalyse, så å vise dette lager vi en slik datasettet til Field der vi inkluderer tre uavhengige variabler:Vi kan også se på Variance Inflation Factor (VIF), som måler hvor mye variansen til en estimert regresjonskoeffisient øker pga. multikollinearitet.Myers (1990) og Hair Jr. et al. (2010) opererer med en akseptabel grense VIF på 10.0 (toleranse på .1). er rådene (også) litt ulike. En VIF-verdi på 10 VIF anses f.eks. pakken “olsrr” R (som vi har brukt ) som et tegn på alvorlig multikolinearitet (jfr. Belsley, Kuh, Welsch 1980). Der settes VIF på 4 som en grense der man bør se nærmere på om multikolinearitet kan være et problem. Bowerman O’Connell (1990) tilføyer snittet av VIF de uavhengige variablene ikke bør være vesentlig 1.","code":"\n# Base R\nFieldKorr <- cor(Field_OLS_data, method = \"pearson\")\nround(FieldKorr, 2)\n#>         Adverts Sales Airplay Image\n#> Adverts    1.00  0.58    0.10  0.08\n#> Sales      0.58  1.00    0.60  0.33\n#> Airplay    0.10  0.60    1.00  0.18\n#> Image      0.08  0.33    0.18  1.00\n# Base R\nFieldOLS_mult_reg <- lm(Sales ~ ., data = Field_OLS_data)\nbrief(FieldOLS_mult_reg)\n#>            (Intercept) Adverts Airplay Image\n#> Estimate         -26.6 0.08488   3.367 11.09\n#> Std. Error        17.4 0.00692   0.278  2.44\n#> \n#>  Residual SD = 47.1 on 196 df, R-squared = 0.665\n# Bruker pakken: olsrr\nols_vif_tol(FieldOLS_mult_reg)\n#>   Variables Tolerance      VIF\n#> 1   Adverts 0.9856172 1.014593\n#> 2   Airplay 0.9592287 1.042504\n#> 3     Image 0.9629695 1.038455"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.9 Fravær av heteroskedasisitet","text":"Variansen til residualene de uavhengige variablene skal være lik (Miles Shevlin 2001). Heteroskedasitsitet innebærer residualene ikke har konstant varianse (motsatt: vi ønsker lik varians residualene alle x-verdier og kaller dette homoskedastisitet). Hvis vi har homoskedastisitet predikerer modellen vår likt på alle predikerte verdier av Y, noe vi ønsker.Variansen til residualen kan altså ikke avhenge av de uavhengige variablene, men være lik på alle nivåer av verdier prediktorene. Dersom vi har heteroskedastisitet vil spredningen rundt regresjonslinja variere med X.Forutsetningen om homoskedastisitet er godt illustrert av Miles Shevlin (2001), s.100-101, fig. 4.22, 4.23 og 4.24:Figuren til venstre viser et scatterplot residualer. midten har vi samme fordeling av residualer med et antall normalfordelingskurver. Til høyre ser vi et alternativt scatterplot residualer som bryter med forutsetningen om homoskedastisitet.Løvås (2013) illustrerer det samme slik eksempelet om motorstørrelse og drivstofforbruk:Variasjonen residualene er like stor uansett verdien av x.Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi p-verdien er \\(0.4027\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Base R\nFieldOLS_mult2 <- lm(Sales ~ Adverts + Airplay + Image, data = Field_OLS_data)\n# Bruker pakken: lmtest\nbptest(FieldOLS_mult2, ~ Adverts*Airplay*Image + I(Adverts^2) + I(Airplay^2)+ I(Image^2), data = Field_OLS_data)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  FieldOLS_mult2\n#> BP = 10.44, df = 10, p-value = 0.4027"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.10 Fravær av autokorrelasjon","text":"Dette er typisk et problem tidsserieanalyser eller geografiske analyser (Eikemo Clausen 2007, .124), der “verdien på variabel X enhet N stor grad er bestemt av verdien på variabel X enhet N-1”. Et annet kjent eksempel er fra aksjemarkedet: Hvis en aksje stiger dag er det mer sannsynlig den stiger morgen. Verdien av aksjen morgen avhenger (delvis) av verdien dag. Verdiene dataene autokorrelerer. Grad av autokorrelasjon er således et mål på forholdet mellom en variabels verdi på tidspunkt X og verdien på tidspunkt før X.tilfeldige observasjoner bør ikke ha korrelasjon residualen. Dette kan testes gjennom en Durbin-Watson test (Durbin Watson 1951).\nDurbin-Watson testen er en test på autokorrelasjon residualene, og vil ha en verdi på mellom 0 og 4. Verdien 2 indikerer ingen autokorrelasjon. Verdier mellom 0 og 2 indikerer en positiv autokorrelasjon, mens verdier mellom 2 og 4 indikerer en negativ autokorrelasjon. En konservativ tommelfingerregel sier verdier 1 og 3 er bekymringsfullt (Field, Miles, Field 2012b).viser verdien \\(2.03\\) noe som ikke gir grunn til bekymring.","code":"\n# Bruker pakken: car\ndurbinWatsonTest(FieldOLS_reg)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1     -0.04394305      2.032324   0.754\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene 1 og 169 identifiseres som statistisk signifikante uteliggere","code":"\n# Bruker pakken: olsrr\ncar::qqPlot(FieldOLS_reg, id.method=\"identify\", main=\"Q-Q Plott\")#> [1]   1 169"},{"path":"regresjonsanalyse---ols.html","id":"hampel-filter","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.11.1 Hampel filter","text":"Hampel filter innebærer man ser alle observasjoner som ligger utenfor intervallet \\(median +/- 3 median\\ absolute\\ deviation\\ (MAS)\\).","code":"\n#Base R\nnedregrense <- median(Field_OLS_data$Adverts) - 3*(mad(Field_OLS_data$Adverts, constant = 1))\nnedregrense\n#> [1] -457.7405\n\novregrense <- median(Field_OLS_data$Adverts) + 3*(mad(Field_OLS_data$Adverts, constant = 1))\novregrense\n#> [1] 1521.572\n\nuteligger_ind <- which(Field_OLS_data$Adverts < nedregrense |Field_OLS_data$Adverts > ovregrense)\nuteligger_ind\n#>  [1]  11  23  28  43  55  87  88  93 126 175 184"},{"path":"regresjonsanalyse---ols.html","id":"grubbs-test","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.11.2 Grubbs’ test","text":"Grubbs’ test ser om den høyeste verdien variabelen bør regnes som en uteligger (hvis den høyeste ikke er det vil ingen andre heller være det).","code":"\n# Bruker pakken: outliers\ngrubbstest <- grubbs.test(Field_OLS_data$Adverts)\ngrubbstest\n#> \n#>  Grubbs test for one outlier\n#> \n#> data:  Field_OLS_data$Adverts\n#> G = 3.41281, U = 0.94118, p-value = 0.05396\n#> alternative hypothesis: highest value 2271.86 is an outlier"},{"path":"regresjonsanalyse---ols.html","id":"rosners-test","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.11.3 Rosners test","text":"angir vi det antallet vi tror er uteliggere, f.eks. fra et box plott.","code":"\n# Bruker pakken: EnvStats\nrosnerstest <- rosnerTest(Field_OLS_data$Adverts,\n  k = 3\n)\nrosnerstest$all.stats\n#>   i   Mean.i     SD.i    Value Obs.Num    R.i+1 lambda.i+1\n#> 1 0 614.4123 485.6552 2271.860     184 3.412808   3.605525\n#> 2 1 606.0834 472.3432 2000.000      43 2.951068   3.604019\n#> 3 2 599.0434 462.9555 1985.119      87 2.993971   3.602505\n#>   Outlier\n#> 1   FALSE\n#> 2   FALSE\n#> 3   FALSE"},{"path":"regresjonsanalyse---ols.html","id":"influential-cases","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.5.12 Influential cases","text":"Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):Det som grafen kalles “hat values” er et vanlig mål å finne observasjoner/caser som er relativt langt fra senter av prediksjonsrommet og som derfor potensielt har stor innflytelse på OLS-regresjonskoeffisientene (“leverage”) (Fox Weisberg 2019). Huber (1981) anbefaler følgende grenseverdier: verdier \\(0.2\\) er ønskelig, verdier \\(0.5\\) uønskede, og verdier mellom \\(0.2\\) og \\(0.5\\) problematiske.“Hat values” er altså et mål på potensiell innflytelse. Neste mål - DfBetas - er et mål på observasjonens effekt på regresjonskoeffisinenten hver variabel med og uten den innflytelsesrike observasjonen - eller med andre ord: observasjonenes innflytelse på variablene. Belsley, Kuh, Welsch (1980) anbefaler 2 som cut-verdi å indikere innflytelsesrike observasjoner.Pakken legger automatisk inn cutoff-verdi (dette tilfellet 0.14). Formelen utregning av dfbeta cutoff-verdi er \\(\\frac{2}{\\sqrt{n}}\\), der n=antall observasjoner.Vi kan også se på dffit (Welsch Kuh 1977).Cutoff-verdi dette tilfellet er 0.2. Formel utregning er \\(2*\\frac{\\sqrt{(k+1)}}{(n-k-1)}\\), der k = antall prediktorer og n = antall observasjoner.Det siste målet vi ønsker å se på (og trolig den mest brukte) er “Cook’s distance”, som gir et mål på observasjonens totale innflytelse på regresjonsmodellen.grafen vises Cook’s distance - et mål på vektet kvadratsum forskjellene mellom de individuelle elementene til koeffisienten. Sagt på en annen måte: Vi bruker Cook’s distance til å se hvilke observasjoner/caser som kan påvirke modellen vår uforholdsmessig mye (totalt sett). Dersom vi har mange caser med høy verdi på Cook’s distance kan det være en indikasjon på lineær regresjon kanskje ikke er en egnet analyse det foreliggende datasettet.Så hva er høy verdi på Cook’s distance? Kilder som Cook Weisberg (1982) og Tabachnik Fidell (2007) angir verdier 1 er bekymringsfullt. Andre, som Fox (2020), advarer mot en ren numerisk vurdering (og fremhever viktigheten av både grafisk presentasjon og vurdering av hvert enkelt tilfelle). En tilnærming som er anbefalt (se f.eks. Zach (2019)) er å bruker forholdstallet \\(4/N\\) - vårt tilfelle \\(4/200=0.02\\).La oss hente opp Cook’s distance de største verdiene de enkelte observasjoner:kjenner vi igjen observasjonene 1, 169 og 42 som de med høyest verdi på Cook’s distance, men også casene 10, 55 og 125 har verdier anbefalingen som kommer fra \\(4/n\\). Men vi ser også verdien er relativt lave hvis man tar utgangspunkt 1 som bekymringsfullt. å vise eventuell justering av modellen som følge av uteliggere viser vi likevel framgangsmåte.Vi bør også undersøke “added variable plots” - en regresjon kan observasjonene ha både en individuell og en sammensatt/felles påvirkning.sier Fox Weisberg (2019), s.44 “Points extreme left right plot correspond cases high leverage corresponding coefficients consequenlty potentially influential”.","code":"\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_reg, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(FieldOLS_reg, print_plot = TRUE)\n# Bruker pakken: olsrr\nols_plot_dffits(FieldOLS_reg)\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_reg, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier <- cooks.distance(FieldOLS_reg)\nmineCDverdier <- round(mineCDverdier, 3)\nhead(sort(mineCDverdier, decreasing = TRUE), n = 10)\n#>     1   169    42    10    55   125     3   148    86    72 \n#> 0.057 0.051 0.041 0.024 0.024 0.023 0.018 0.018 0.017 0.016\n# Bruker pakken: car\navPlots(FieldOLS_reg, id=list(cex=0.75, n=3, method=\"mahal\"))"},{"path":"regresjonsanalyse---ols.html","id":"steg-6-eventuell-revisjon-av-modell","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.6 Steg 6: Eventuell revisjon av modell","text":"kan vi eksempel se hvordan modellens presterer ved bortfall av visse ekstreme verdier (spesielt innflytelsesrike observasjoner/caser, jfr. diskusjon regresjonsforutsetnigner) eller ved inkludering/eksklusjon av gitte variabler modellen (først og fremst ved multippel regresjonsanalyse).Vi bør vurdere punktene og vurdere om vi ønsker å lage en revidert modell der vi tar ut veldig innflytelsesrike caser/observasjoner. Som tidligere nevnt har vi ikke svært store verdier , men la oss som et eksempel si vi ønsker å se om en modell uten observasjon 169. Vi anbefaler å ta bort en og en observasjon siden (som nevnt) observasjonene/casene har både en individuell og felles påvirkning.Som forventet ser vi ikke de store forskjellene. Vi kan ta bort de andre observasjonene illustrasjonens skyld:Igjen, ikke de store endringene. Vi kan se Intercept (\\(\\beta\\)) går litt ned etter hvert som vi tar bort caser, og betydningen av Adverts går litt opp (men det er marginalt).La oss, eksempelets skyld, manipulere datasettet slik en analyse av Cook’s distance ser slik ut:Hvis vi nå kjører denne modellen opp mot en modell der vi tar bort 11 og 23 får vi:ser vi koeffisienten Adverts stiger fra 0.01 til 0.09, eller en endring på 11.1%.","code":"\nFieldOLS_reg2 <- update(FieldOLS_reg, subset = -169)\n# Bruker pakken: car\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> \n#>             Model 1 Model 2\n#> (Intercept)  134.14  131.76\n#> SE             7.54    7.39\n#>                            \n#> Adverts     0.09612 0.09826\n#> SE          0.00963 0.00942\n#> \nFieldOLS_reg3 <- update(FieldOLS_reg, subset = -c(1, 42, 169, 184))\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2, FieldOLS_reg3)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> 3: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -c(1, 42, 169, 184))\n#> \n#>             Model 1 Model 2 Model 3\n#> (Intercept)  134.14  131.76  126.14\n#> SE             7.54    7.39    7.28\n#>                                    \n#> Adverts     0.09612 0.09826 0.10461\n#> SE          0.00963 0.00942 0.00942\n#> \n# Bruker pakken: readxl\nField_OLS_data2 <- read_excel(\"Field_datasett_OLS2.xlsx\")\n# Base R\nFieldOLS_man <- lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_man, vars = c(\"Cook\"), id = list(n=3))\nFieldOLS_man2 <- update(FieldOLS_man, subset = -c(11, 23))\n# Bruker pakken: car\ncompareCoefs(FieldOLS_man, FieldOLS_man2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data2,\n#>    subset = -c(11, 23))\n#> \n#>             Model 1 Model 2\n#> (Intercept)  183.77  134.14\n#> SE             5.97    7.64\n#>                            \n#> Adverts     0.01207 0.09621\n#> SE          0.00298 0.00999\n#> "},{"path":"regresjonsanalyse---ols.html","id":"steg-7-eventuell-analyse-av-revidert-modell","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.7 Steg 7: Eventuell analyse av revidert modell","text":"vil vi prinsippet bare gjenta samme analyser som ved analyse av den opprinnelige modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-8-konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.8 Steg 8: Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Vi viser rapportering av en enkel lineær regresjonsanalyse etter APA-standard:En enkel lineær regresjon ble gjennomført å predikere salgstall per uke basert på sum brukt på reklame uka før lansering. Vi fant en signifikant regresjonslikning (F(1,198) = 99.59, \\(\\beta\\) = 134.14p < .001) med en \\(R^2\\) på .335, 95% CI [119.28, 149.00].","code":"\n# Bruker pakken: table1\ntable1::label(Field_OLS_data$Adverts) <- \"Adverts\"\ntable1::label(Field_OLS_data$Sales) <- \"Sales\"\ntable1::table1(~Adverts + Sales, data = Field_OLS_data)\n# Bruker pakken: sjPlot\nFieldOLSkorr <- tab_corr(Field_OLS_data, triangle = \"lower\")\nFieldOLSkorr\n# Bruker pakken: sjPlot\ntab_model(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"til-slutt-for-r-brukere","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.3.9 Til slutt for R-brukere…","text":"Mehmetoglu Mittner (2020) har skrevet en veldig god bok om på norsk: “Innføring R statistiske analyser” som vi varmt kan anbefale. Den kommer med en pakke (“rnorsk”) som kan lastes ned gjennom kommandoen devtools::install_github(“ihrke/rnorsk”) (forutsetter pakken “devtools” er på plass, hvis ikke så kjør “package.install(”devtools”)).Forfatterne har laget en samling av regresjonsdiagnostikk som vi viser på Fields data der vi laget en multippel regresjonsmodell:Analysen gir en samlet oversikt et antall parametere og søker å hjelpe til med beslutning om forutsetninger er ok/ikke ok, men vi vil understreke kunnskap om hva som ligger bak de ulike testene og kriteriene er essensielt. Mer informasjon om parameterene finner dere .","code":"\noptions(scipen=999)\n# Bruker pakken: rnorsk\nregression.diagnostics(FieldOLS_mult_reg)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 1/11 (9.1 %) checks failed\n#> \n#> \n#> Identified problems: \n#>  functional form\n#> Summary:\n#> # A tibble: 11 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   6.19e+0  0.103   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   3.03e-1  0.582   0.05 No Pro~\n#>  3 multicoll~ Adverts  Vari~   1.01e+0 NA       5    No Pro~\n#>  4 multicoll~ Airplay  Vari~   1.04e+0 NA       5    No Pro~\n#>  5 multicoll~ Image    Vari~   1.04e+0 NA       5    No Pro~\n#>  6 normality  global   Shap~   9.95e-1  0.725   0.01 No Pro~\n#>  7 model spe~ global   Stat~  -6.99e-5  0.916   0.05 No Pro~\n#>  8 functiona~ global   RESE~   3.72e+0  0.0261  0.05 Problem\n#>  9 outliers   global   Cook~   7.08e-2 NA       1    No Pro~\n#> 10 outliers   global   Bonf~   3.16e+0  0.362   0.05 No Pro~\n#> 11 autocorre~ global   Durb~   2.70e-3  0.744   0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"standard-multippel-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4 Standard multippel regresjonsanalyse","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-standard-multippel-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.1 Eksempel standard multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på multippel regresjonsanalyse, ved å følge stegene .Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultater","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.2 Analyse av dataene","text":"Vi skal bruke et datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaDatasettet er stort til å vises fram sin helhet, men vi skal bruke uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) mot den avhengige variabelen tpstress (Perceived stress).Vi ønsker altså å se om en gruppe studenters (N = 439) egenrapporterte oppfattelse av sin kontroll eksterne forhold som kan skape stress og deres evne til å kontrollere deres følelser, tanker og fysiske reaksjoner (Pallant 2000). Vi kan derfor se nærmere på variablene.","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\n# Base R\nPallant_survey2 <- select(Pallant_survey, tmast, tpcoiss, tpstress)\nPallant_survey2 <- na.omit(Pallant_survey2) \n# Bruker pakken: summarytools\nsummarytools::descr(Pallant_survey2, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey2  \n#> N: 426  \n#> \n#>                    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- ----------\n#>            Mean    21.74     60.56      26.75\n#>         Std.Dev     3.97     11.96       5.84\n#>             Min     8.00     20.00      12.00\n#>          Median    22.00     62.00      26.00\n#>             Max    28.00     88.00      46.00\n#>         N.Valid   426.00    426.00     426.00\n#>       Pct.Valid   100.00    100.00     100.00\n# Base R\npar(mfrow=(c(2,2)))\nhisttmast <- with(Pallant_survey2, hist(tmast))\nhisttpcoiss <- with(Pallant_survey2, hist(tpcoiss))\nhisttpstress <- with(Pallant_survey2, hist(tpstress))\n# Bruker pakken: car\nqqtmast <- car::qqPlot(~ tmast, data = Pallant_survey2)\nqqtpcoiss <- car::qqPlot(~ tpcoiss, data = Pallant_survey2)\nqqtpstress <- car::qqPlot(~ tpstress, data = Pallant_survey2)\n# Base R\nboxplot(Pallant_survey2)"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.3 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.4 Lage modell (kjøre regresjonsanalysen)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.5 Analyse av resultatene (diagnostikk)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"antall-prediktoreruavhengige-variabler-overfitting-og-predicted-r-square","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.5.1 Antall prediktorer/uavhengige variabler, overfitting og predicted R-square","text":"multippel regresjonsanalyse har vi mer enn en prediktor/uavhengig variabel. Som regel kan vi ønske å inkludere mer enn en prediktor fordi vi sjeldent klarer å fange nok av variansen en avhengig variabel gjennom en prediktor. Samtidig ønsker vi ikke flere uavhengige variabler enn nødvendig å lage en så enkel modell som mulig som gir oss prediksjoner vi kan bruke. Matematisk er det også slik enhver uavhengig variabel som har en grad av korrelasjon med den avhengige variabelen vil bidra til å øke \\(R^2\\) uten det nødvendigvis gjør modellen “riktigere” (men kan gjøre den vanskeligere å tolke). Dersom vi legger til unødvendige uavhengige variabler risikerer vi det som kalles overfitting - altså modellen blir god på å beskrive tilfeldige feil dataene heller enn å beskrive forholdet mellom variablene. Resultatet er modellen ikke kan generaliseres. Vi kan også se tilbake på regresjonsforutsetningene og vil se et stort antall uavhengige variabler er en invitasjon til multikolinearitet.En måte å sjekke dette er å dele datasettet slik man gjør regresjonsanalysen på en del av datasettet og deretter tester modellen på den andre delen av datasettet. Dette kalles kryssvalidering. En annen måte er å se på “predicted R-square” som innebærer følgende prosedyre (som statistikkprogrammer gjør oss naturligvis):Et datapunkt/observasjon tas ut av datasettetRegresjonslikningen kalkuleresModellens evne til å predikere det datapunktet/observasjonen som ble tatt ut evalueres (altså - hvor nærme datapunktet kommer vår modell sin prediksjon?)Dette gjentas alle datapunktene datasettetTolkningen av dette er ganske grei. Man sammenlikner R-square med predicted R-square. Dersom det er liten forskjell mellom disse verdiene har man trolig liten sannsynlighet du har overfitting av modellen. Dersom forskjellen er stor er det grunn til å tro man kan ha overfitting.å unngå dette er det viktig å tenke på forholdstallene som ble diskutert foregående punkt om regresjonsforutsetninger. Kjennskap til tidligere forskning og resultater vil gi informasjon om og et teoretisk grunnlag hvilke variabler som bør inkluderes modellen.Vi skal illustrere overfitting basert på et eksempel fra Frost (2022) (dette eksempelet har altså ikke noe direkte med analysen vi er inne - Pallant sitt datasett - men er tatt med å illustrere poenget med overfitting).Datasettet inneholder variabler: Hvordan historikere rangerer amerikanske presidenter (Historians.rank) og hvor stor generell støtte presidenter har hatt befolkningen (Approval.High).Vi ser R-squared er \\(0.0068\\) - hvilket vi vil tolke som det praksis ikke er noen sammenheng mellom variablene.Vi kan så bruke en polynomisk likning (har vi brukt \\(x^3\\) å lage regresjonslinjen):Vi har nå en R-squared på \\(0.66\\) denne modellen mot \\(0.0068\\) den lineære modellen. Dette betyr den polynomiske regresjonsmodellen forklarer drøye 66% av variansen den avhengige variabelen. Modellen vår er ut fra dette en god modell våre data.\nImidlertid gir en analyse av Predicted R-square oss et annet bilde av modellen:realiteten forteller både verdien på predicted r-square på 0, og forskjellen mellom R-square (\\(0.664\\)) og predicted R-square \\(0\\), vi har en seriøs overfitting. Vi har med andre ord funnet en modell som beskriver dataene våre veldig godt, men som ikke kan brukes på andre data enn de vi har (vel, den kan jo brukes, men vil ikke kunne gi oss noe av verdi). Vi kan altså ikke predikere noe ut fra modellen.Vi ser \\(R^2 = 0.466\\). Modellen kan altså forklare \\(46.6%\\) av variansen den avhengige variabelen. Vi ser vi får en noe lavere verdi «Adjusted R Squared» forhold til «R Squared». Når vi legger til en uavhengig variabel en regresjonsanalyse er det lite sannsynlig korrelasjonen mellom den nye uavhengige variabelen og den avhengige variabelen vil være nøyaktig 0. Den vil stedet fluktuere rundt 0. Pga. denne tilfeldige fluktuasjonen rundt 0 vil \\(R^2\\) alltid øke litt når man legger til en ny uavhengig variabel. Adjusted \\(R^2\\) søker å kompensere dette å få fram en mer korrekt verdi. Jo større antall uavhengige variabler, jo større forskjell vil man se mellom \\(R^2\\) og Adjusted \\(R^2\\). Det samme vil være tilfelle ved mindre utvalgsstørrelser fordi variasjonen rundt 0 vil være større mindre utvalg.Vi kan også legge merke til Predicted R-Squared er \\(0.456\\) og dermed svært lik R-squared.","code":"\n# Base R\nPresidentRanking <- read.csv(\"PresidentRanking.csv\")\n\n# Bruker pakken: summarytools\nsummarytools::descr(PresidentRanking)\n#> Descriptive Statistics  \n#> PresidentRanking  \n#> N: 12  \n#> \n#>                     Approval.High   Historians.rank\n#> ----------------- --------------- -----------------\n#>              Mean           78.75             17.00\n#>           Std.Dev            8.01             10.90\n#>               Min           67.00              2.00\n#>                Q1           72.00              8.50\n#>            Median           79.00             15.00\n#>                Q3           85.50             25.00\n#>               Max           90.00             38.00\n#>               MAD           10.38             11.86\n#>               IQR           12.25             15.75\n#>                CV            0.10              0.64\n#>          Skewness           -0.05              0.37\n#>       SE.Skewness            0.64              0.64\n#>          Kurtosis           -1.58             -1.20\n#>           N.Valid           12.00             12.00\n#>         Pct.Valid          100.00            100.00\n# Base R\npresidentlm <- lm(formula = Historians.rank ~ Approval.High, data = PresidentRanking)\nplot(Historians.rank ~ Approval.High, data = PresidentRanking)\nabline(presidentlm, lwd = 2, col = \"red\")\nsumm(presidentlm)\n# Base R\npresidentlm2 <- lm(Historians.rank ~ poly(Approval.High, degree=3), data=PresidentRanking)\n# Bruker pakken: ggplot2\nggplot(data=PresidentRanking, aes(Approval.High,Historians.rank)) +\n    geom_point() + \n    geom_smooth(method=\"lm\", formula=y~I(x^3)+I(x^2))\nsumm(presidentlm2)\n# Bruker pakken: olsrr\nols_pred_rsq(presidentlm2)\n#> [1] -0.2162257\n# Bruker pakken: summarytools\nPallantOLS_reg <- lm(tpstress ~ tmast + tpcoiss, data = Pallant_survey2)\nsumm(PallantOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.5.2 Modellens koeffisienter","text":"Vi ser først på tallene “Std.Beta” “Parameter Estimated”. Vi ser tmast bidrar større grad enn tpcoiss (fortegn er denne sammenheng irrelevant). Begge bidrar signifikant.","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.5.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 184.5. Vi kan regne ut (bruke R) til å finne kritiske verdi.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=2, df2=423, lower.tail=FALSE)\n#> [1] 3.017049"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.3 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\n# Base R\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Det kan se ut som en interaksjonseffekt ved linjene ikke er parallelle. Det kan imidlertid være noe vanskelig å tolke interaksjonseffekt. Man kan f.eks. ha en interaksjonseffekt som ikke er statistisk signifikant. R kan vi bruke pakken jtools som hjelp:Vi ser interaksjonen tmast:tpcoiss ikke er statistisk signifikant.Tolkningen er “som før”: Parallelle linjer indikerer fravær av interaksjonseffekt.","code":"\n# Base R\nPallantOLS_reg2 <- lm(tpstress ~ tmast + tpcoiss, Pallant_survey2)\nPallantOLS_inter <- lm(tpstress ~.+tmast*tpcoiss, Pallant_survey2)\n# Bruker pakken: car\ncompareCoefs(PallantOLS_reg2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ tmast + tpcoiss, data = \n#>   Pallant_survey2)\n#> 2: lm(formula = tpstress ~ . + tmast * tpcoiss, data = \n#>   Pallant_survey2)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     50.83   52.66\n#> SE               1.27    4.34\n#>                              \n#> tmast         -0.6207 -0.7093\n#> SE             0.0614  0.2103\n#>                              \n#> tpcoiss       -0.1747 -0.2071\n#> SE             0.0204  0.0763\n#>                              \n#> tmast:tpcoiss         0.00154\n#> SE                    0.00348\n#> \n# Bruker pakken: ggplot2\nggplot(data=Pallant_survey2, aes(x=tmast, y=tpstress, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(tmast,tpcoiss), method=lm, se=F,color=\"black\")+xlab(\"tmast og tpcoiss\")+labs(\n        title=\"tmast i blått - tpcoiss i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\n# Bruker pakken: jtools\nsumm(PallantOLS_inter)\n# Bruker pakken: interactions\ninteract_plot(PallantOLS_inter, pred = tmast, modx = tpcoiss)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.6 Linearitet","text":"Vi kan ikke se på samme type scatterplott sjekk av linearitet som vi gjorde enkel OLS (på et todimensjonalt plott). Vi kan imidlertid lage “added variable plots” (Mosteller Tukey 1977).X-aksene representerer en enkelt uavhengig variabel (per graf ovenfor). Y-aksen = den avhengige variabelen. Den blå linjen viser sammenhengen mellom den uavhengige variabelen og den avhengige variabelen når alle andre uavhengige variabler holdes konstant. Jo sterkere lineær sammenheng plottene, jo sterkere er den respektive uavhengige variabelens bidrag modellen.vårt tilfelle vil vi nok konkludere med forutsetningen om linearitet er (nok) oppfylt.","code":"\n# Bruker pakken: olsrr\nols_plot_added_variable(PallantOLS_reg2, print_plot = TRUE)\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.7 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nhist(PallantOLS_reg2$residuals)\n# Bruker pakken: olsrr\nols_plot_resid_qq(PallantOLS_reg2)\nols_test_normality(PallantOLS_reg2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9911         0.0115 \n#> Kolmogorov-Smirnov        0.0509         0.2197 \n#> Cramer-von Mises         31.6502         0.0000 \n#> Anderson-Darling          1.0978         0.0070 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.8 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.58\\) og \\(r=-0.61\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.53\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\n# Base R\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\", use=\"pairwise.complete.obs\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\n# Bruker pakken. olsrr\nols_vif_tol(PallantOLS_reg2)\n#>   Variables Tolerance      VIF\n#> 1     tmast 0.7220785 1.384891\n#> 2   tpcoiss 0.7220785 1.384891"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.9 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Bruker pakken: lmtestbptest(PallantOLS_reg2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.10 Fravær av autokorrelasjon","text":"viser verdien \\(1.826\\) noe som ikke gir grunn til bekymring.","code":"\n# Bruker pakken: car\ndurbinWatsonTest(PallantOLS_reg2)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08185218      1.825972   0.074\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:dffit:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\n# Bruker pakken: car\ncar::qqPlot(PallantOLS_reg2, id = list(n=3))#>  22 194 269 \n#>  21 190 263\n# Bruker pakken: car\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(PallantOLS_reg2, print_plot = TRUE)\n# Bruker pakken: olsrr\nols_plot_dffits(PallantOLS_reg2, print_plot = TRUE)\n# Bruker pakken: car\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier2 <- cooks.distance(PallantOLS_reg2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22     268      23     191     413     194 \n#> 0.09556 0.05833 0.05434 0.04374 0.03689 0.03601"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.6.12 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\n# Bruker pakken: rnorsk\nregression.diagnostics(PallantOLS_reg2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/10 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 10 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   3.95     0.139   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   2.91     0.0881  0.05 No Pro~\n#>  3 multicoll~ tmast    Vari~   1.38    NA       5    No Pro~\n#>  4 multicoll~ tpcoiss  Vari~   1.38    NA       5    No Pro~\n#>  5 normality  global   Shap~   0.991    0.0115  0.01 No Pro~\n#>  6 model spe~ global   Stat~   0.00495  0.568   0.05 No Pro~\n#>  7 functiona~ global   RESE~   0.419    0.658   0.05 No Pro~\n#>  8 outliers   global   Cook~   0.0956  NA       1    No Pro~\n#>  9 outliers   global   Bonf~   3.56     0.177   0.05 No Pro~\n#> 10 autocorre~ global   Durb~   0.0819   0.0620  0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.7 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.8 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.9 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:En multippel lineær regresjonsanalyse ble gjennomført å teste om respondentenes oppfattede nivå av indre kontroll og kontroll på ytre faktorer predikerer totalt nivå av oppfattet stress. Analyser ble gjennomført å sikre det ikke var brudd på forutsetningene en multippel lineær regresjonsanalyse. En statistisk signifikant regresjonsmodell ble funnet (F (2, 423) = 184.5, p < .001), med en \\(R^2\\) på .466. Både nivå av indre kontroll og kontroll på ytre faktorer var signifikante prediktorer.","code":"\n# Bruker pakken: table1\ntable1::label(Pallant_survey$tmast) <- \"tmast\"\ntable1::label(Pallant_survey$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey$tpstress) <- \"tpstress\"\ntable1::table1(~tmast + tpcoiss + tpstress, data = Pallant_survey)\n# Bruker pakken: sjPlot\nPallantkorr1 <- tab_corr(Pallant_survey2, triangle = \"lower\")\nPallantkorr1\n# Bruker pakken: sjPlot\ntab_model(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"modellens-evne-til-å-predikere","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.10 Modellens evne til å predikere","text":"Et siste tema vi skal ta oss multippel regresjonsanalyse inkluderes ofte ikke lærebøkers kapitler om regresjonsanalyse. Det omtales gjerne andre sammenhenger lærebøker som “predictive analytics” eller “predictive modelling”, og hvis man skal lære seg om temaer som maskinlæring vil dette naturlig element der. Selv synes jeg dette er naturlig å omtale regresjonsanalyse også fordi:“Model fit” handler kun om hvor god modellen vår beskriver/passer til dataene vi har dette tilfelletDette sier ikke nødvendigvis så mye om hvor god modellen vår er på andre data. Så hvis vi vil lage en modell basert på våre data som vi også ønsker skal kunne predikere andre data (er vår modell generaliserbar?) trenger vi en tilleggsanalyse.Dette tillegget kaller vi kryssvalidering. Metoden vi skal bruke kalles “k fold cross validation”.","code":""},{"path":"regresjonsanalyse---ols.html","id":"kryssvalidering","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.10.1 Kryssvalidering","text":"Kryssvalidering innebærer vi deler datasettet : en del vi utvikler modeller på, og en del vi tester vår utvalgte modell på. På den måten unngår vi vi bruker hele datasettet på å grave fram (litt sjansepreget) en modell som er ok uten å vite om vi bare har hatt flaks. Hvis vår valgte modell har en god fit med den delen av dataene vi tester på kan vi si noe mer sikkert om vår modell.Grafisk kan vi se oss prosessen slik:Datasettet deles først tilfeldig treningsdata og testdata. Ofte brukes 80/20 eller 70/30 fordeling, men dette kan variere. Treningsdataene deles igjen tilfeldig inn utvalg.Metoden vi ser på kalles “k fold Cross Validation”. k utgjør antall “folds”. Et “fold” er et sett utvalg. Det er ikke noen formell regel hvor mange utvalg man bør ha. Kuhn Johnson (2013) hevder “choice k usually 5 10, formal rule” (s.70).Hvis vi tenker oss vi har tre utvalg (U1-3) treningsdatasettet får vi da:Modell 1: Trent på U1 og U2, validert på U3Modell 2: Trent på U1 og U3, validert på U2Modell 3: Trent på U2 og U3, vlaidert på U1Modellene syntetiseres og til slutt kjøres modellen på testdatasettet. Da utsetter vi modellen data den ikke har “sett” før å se hvor nærme de virkelige verdiene (de observerte/faktiske) testdatene modellen klarer å predikere.","code":""},{"path":"regresjonsanalyse---ols.html","id":"kryssvalidering-av-vår-multiple-regresjonsmodell","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.4.10.2 Kryssvalidering av vår multiple regresjonsmodell","text":"Oppsummeringen av vår kryssvalidering viser:Resultaten vi er spesielt interessert er:RMSE: Root Mean Square Error. Dette er et mål på gjennomsnittlig forskjell mellom prediksjonene gjort av modellen (treningsdelen av datasettet) og de faktiske observasjonene (testdelen av datasettet). Jo lavere RMSE, jo bedre klarer modellen å predikere verdien på ukjente data (testdelen).Rsquared: Er korrelasjonen mellom prediksjonene og observasjoneneMAE: Mean Absolute Error. Dette er et mål på forskjellen mellom prediksjonene og observasjonene absolutte tall. Jo lavere MAE, jo nærmere observasjonene er prediksjonene.skulle vi selvsagt gjerne hatt noen absolutte grenseverdier å forholde seg til RMSE og MAE (Rsquared er litt enklere å forholde seg til som en korrelasjonskoeffisient). Dessverre er det ikke slik vi kan gi slike klare grenseverdier om modellen predikerer bra eller ikke. stedet må vi sammenlikne verdien på hhv. RMSE og MAE med skalaene på variablene. Hvis vi f.eks. predikerer boligpriser en modell vil RMSE gi oss avviket relatert til akkurat boligprisene (motsetning til f.eks. prosent). å fortsette boligpriseksempelet: Hvis vi får en RMSE på 15 000 et datasett der snittprisen på boliger er 15 000 000 vil det absolutt måtte betegnes osm en god (=lav) RMSE. Men samme RMSE på 15 000 et datasett der snittprisen på boliger er 100 000 vil være langt mindre god enn den første. Så man må rett og slett sette seg ned og gjøre en vurdering.","code":"\n# Bruker pakken: caret\nset.seed(156)\nPallant_survey <- na.omit(Pallant_survey)\n# Spesifiserer cross-validation parametre \nkontrollspes <- trainControl(method = \"cv\", number = 10, savePredictions = \"final\")\n# Lager modell\nset.seed(156)\nmodell1 <- train(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey, method = \"lm\", trControl = kontrollspes)\nprint(modell1)\n#> Linear Regression \n#> \n#> 333 samples\n#>   4 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 299, 298, 300, 300, 300, 300, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   4.358655  0.4674404  3.388348\n#> \n#> Tuning parameter 'intercept' was held constant at a\n#>  value of TRUE"},{"path":"regresjonsanalyse---ols.html","id":"hierarkisk-multippel-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5 Hierarkisk multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på hierarksik multippel regresjonsanalyse, ved å følge stegene . Hierarkisk vil dere også kunne se omtalt som blockwise entry fordi data legges inn modellen blokker).Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterEksempelet utvider eksempelet standard multippel regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.1 Analyse av dataene","text":"Vi skal bruke samme datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaI tillegg til de uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) vil vi bruke variablene alder (age) og “social desirability” (tmarlow) som kontrollvariabler.“Social desirability bias” er et kjent fenomen der respondenter har en tendens til å ønske å framstå et bedre lys heller enn et sant/reelt lys (Preiss et al. 2015). Respondenter kan f.eks. ønske å framstå som mer ærlige enn de realiteten er, og vil derfor også svare deretter på spørsmål. tillegg kan man anta alder kan ha påvirkning på opplevelsen av totalt stressnivå.Vi velger å lage et nytt datasett der vi trekker ut de relevante variablene, og fjerne observasjoner med NA (ingen verdi).denne hierarkiske multipple regresjonsanalysen ønsker vi derfor å bruke alder og sosial ønskverdighet som kontrollvariabler. Vi legger første blokk inn de variablene vi ønsker å kontrollere , før vi blokk legger inn de samme uavhengige variablene som forrige eksempel. Hensikten med dette er vi ønsker å fjerne mulige effekter av de “kontrollvariablene”.","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\nPallant_subset <- Pallant_survey[c(\"age\", \"tmarlow\", \"tmast\", \"tpcoiss\", \"tpstress\")]\n# Bruker pakken: summarytools\nsummarytools::descr(Pallant_subset, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_subset  \n#> N: 439  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.44      5.30    21.76     60.63      26.73\n#>         Std.Dev    13.20      2.04     3.97     11.99       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   439.00    433.00   436.00    430.00     433.00\n#>       Pct.Valid   100.00     98.63    99.32     97.95      98.63\n# Base R\nPallant_survey3 <- select(Pallant_survey, age, tmarlow, tmast, tpcoiss, tpstress)\nPallant_survey3 <- na.omit(Pallant_survey3)\nsummarytools::descr(Pallant_survey3, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey3  \n#> N: 423  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.32      5.30    21.77     60.56      26.74\n#>         Std.Dev    13.09      2.02     3.97     12.00       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   423.00    423.00   423.00    423.00     423.00\n#>       Pct.Valid   100.00    100.00   100.00    100.00     100.00\n# Base R\npar(mfrow=(c(1,2)))\nhistage <- with(Pallant_survey3, hist(age))\nhisttmarlow <- with(Pallant_survey3, hist(tmarlow))"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.2 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt kontrollvariabler og uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.3 Lage modell (kjøre regresjonsanalysen)","text":"lager vi tre modeller: Modell 0 inneholder kun Intercept (som en referansemodell). Modell 1 inneholder kun age og tmarlow, modell 2 inneholder tillegg tmast og tpcoiss.","code":"\n# Base R\nmodell0 <- lm(tpstress ~ 1, data = Pallant_survey3)\nmodell1 <- lm(tpstress ~ age + tmarlow, data = Pallant_survey3)\nmodell2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey3)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.4 Analyse av resultatene (diagnostikk)","text":"Den første modellen er modellen med kun alder og social desirability som forklarer 5,9 % av variansen. Den andre modellen, som består av både kontrollvariablene og de tidligere uavhengige variablene forklarer til sammen 47 %.Vi kan oppsummere modellene:Modell 0: \\(SS_Total\\) = 14450 (ingen prediktorer)Modell 1: \\(SS_Residual\\) = 13598.0, \\(SS_Difference\\) = 852.4, \\(F\\)(2, 420) = 23.26, \\(p\\)<.001 (etter å ha lagt til age og tmarlow)Modell 2: \\(SS_Residual\\) = 7658.8, \\(SS_Difference\\) = 5939.2, \\(F\\)(2, 418) = 162.07, \\(p\\)<.001 (etter å ha lagt til tmast og tpcoiss)Det vi jo ønsket å finne ut av var hvor mye våre uavhengige variabler forklarer etter effektene fra de kontrollvariablene er tatt bort. Dette finner vi «R Square Change» - altså hvor mye \\(R^2\\) endrer seg fra modell 1 til modell 2. Vi har tillegg med hvor mye \\(R^2\\) endrer seg fra modell 0 ti lmodell 1.modell 2 er verdien 0.411. Altså – de uavhengige variablene forklarer 41.1 % av variansen den avhengige variabelen etter vi har kontrollert alder og sosial ønskverdighet. Vi kan så se på de uavhengige variablenes unike bidrag (altså bidrag etter interaksjonseffekter er tatt bort).","code":"\n# Base R\nsumm(modell1)\nsumm(modell2)\n# Base R\nanova(modell0, modell1, modell2)\n#> Analysis of Variance Table\n#> \n#> Model 1: tpstress ~ 1\n#> Model 2: tpstress ~ age + tmarlow\n#> Model 3: tpstress ~ age + tmarlow + tmast + tpcoiss\n#>   Res.Df     RSS Df Sum of Sq      F                Pr(>F)\n#> 1    422 14450.3                                          \n#> 2    420 13598.0  2     852.4  23.26       0.0000000002642\n#> 3    418  7658.8  2    5939.2 162.07 < 0.00000000000000022\n#>      \n#> 1    \n#> 2 ***\n#> 3 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Base R\nrsq_diff1 <- summary(modell1)$r.squared - summary(modell0)$r.squared\nrsq_diff2 <- summary(modell2)$r.squared - summary(modell1)$r.squared\nrsq_diff1\n#> [1] 0.05898557\nrsq_diff2\n#> [1] 0.4110042"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.4.1 Modellens koeffisienter","text":"Vi ser på tallene “Std.Beta” “Parameter Estimates”. Vi ser alder og social desirability ikke bidrar signifikant seg selv. Beta verdiene tabellen representerer de unike bidragene hver variabel etter overlappende effekter fra de andre variablene er fjernet. tmast bidrar større grad (beta = -0,631) enn pcoiss (beta = -0,160).","code":"\n# Bruker pakken: olsrr\nmodell2x <- ols_regress(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey)\nmodell2x\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.686       RMSE                4.280 \n#> R-Squared               0.470       Coef. Var          16.011 \n#> Adj. R-Squared          0.465       MSE                18.323 \n#> Pred R-Squared          0.455       MAE                 3.277 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                  Sum of                                               \n#>                 Squares         DF    Mean Square      F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression     6791.514          4       1697.879    92.666    0.0000 \n#> Residual       7658.831        418         18.323                     \n#> Total         14450.345        422                                    \n#> ----------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                    \n#> ----------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n#> ----------------------------------------------------------------------------------------\n#> (Intercept)    51.716         1.371                 37.713    0.000    49.020    54.411 \n#>         age    -0.021         0.017       -0.046    -1.201    0.231    -0.054     0.013 \n#>     tmarlow    -0.147         0.111       -0.051    -1.328    0.185    -0.364     0.071 \n#>       tmast    -0.631         0.063       -0.429    -9.997    0.000    -0.756    -0.507 \n#>     tpcoiss    -0.160         0.022       -0.328    -7.311    0.000    -0.203    -0.117 \n#> ----------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.4.2 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 92.67.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=4, df2=418, lower.tail=FALSE)\n#> [1] 2.393283"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.\n#### Variablene er uten målefeilVi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.2 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.3 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\n# Base R\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.4 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten (vi gjentar ikke den grafiske framstillingen som er lik som forrige analyse).","code":"\n# Base R\nPallantOLS2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey)\nPallantOLS_inter <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss + tmast*tpcoiss, Pallant_survey)\n# Bruker pakken: car\ncompareCoefs(PallantOLS2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss, data = Pallant_survey)\n#> 2: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss + tmast * tpcoiss, data = Pallant_survey)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     51.72   53.80\n#> SE               1.37    4.38\n#>                              \n#> age           -0.0206 -0.0206\n#> SE             0.0171  0.0171\n#>                              \n#> tmarlow        -0.147  -0.148\n#> SE              0.111   0.111\n#>                              \n#> tmast         -0.6314 -0.7324\n#> SE             0.0632  0.2111\n#>                              \n#> tpcoiss       -0.1600 -0.1969\n#> SE             0.0219  0.0768\n#>                              \n#> tmast:tpcoiss         0.00175\n#> SE                    0.00349\n#> "},{"path":"regresjonsanalyse---ols.html","id":"linearitet-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.5 Linearitet","text":"Se forrige regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.6 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik (jfr. )En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nhist(PallantOLS2$residuals)\n# Bruekr pakken: olsrr\nols_plot_resid_qq(PallantOLS2)\nols_test_normality(PallantOLS2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9926         0.0345 \n#> Kolmogorov-Smirnov        0.038          0.5743 \n#> Cramer-von Mises         31.3398         0.0000 \n#> Anderson-Darling          0.8705         0.0255 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(PallantOLS2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.7 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.52\\) og \\(r=-0.58\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.52\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\n# Base R\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\n# Bruker pakken: olsrr\nols_vif_tol(PallantOLS2)\n#>   Variables Tolerance      VIF\n#> 1       age 0.8636921 1.157820\n#> 2   tmarlow 0.8729179 1.145583\n#> 3     tmast 0.6897936 1.449709\n#> 4   tpcoiss 0.6290247 1.589763"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.8 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Bruker pakken: lmtest\nbptest(PallantOLS2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  PallantOLS2\n#> BP = 40.384, df = 5, p-value = 0.0000001249"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.9 Fravær av autokorrelasjon","text":"viser verdien \\(1.817\\) med p < 0.05 noe som indikerer vi har autokorrelasjon.","code":"\n# Bruker pakke: car\ndurbinWatsonTest(lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey))\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08634585      1.817403   0.034\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.10 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\n# Bruker pakke: car\ncar::qqPlot(PallantOLS2, id = list(n=3))#> [1]  22 194 269\n# Bruker pakke: car\ninfluenceIndexPlot(PallantOLS2, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(PallantOLS2, print_plot = TRUE)\n# Bruker pakke: car\ninfluenceIndexPlot(PallantOLS2, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier2 <- cooks.distance(PallantOLS2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22      23     268     389     191     413 \n#> 0.05880 0.04913 0.03618 0.02859 0.02728 0.02530"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.5.11 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\n# Bruekr pakken: rnorsk\nregression.diagnostics(PallantOLS2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/12 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 12 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   7.01     0.135   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   1.86     0.173   0.05 No Pro~\n#>  3 multicoll~ age      Vari~   1.16    NA       5    No Pro~\n#>  4 multicoll~ tmarlow  Vari~   1.15    NA       5    No Pro~\n#>  5 multicoll~ tmast    Vari~   1.45    NA       5    No Pro~\n#>  6 multicoll~ tpcoiss  Vari~   1.59    NA       5    No Pro~\n#>  7 normality  global   Shap~   0.993    0.0345  0.01 No Pro~\n#>  8 model spe~ global   Stat~   0.00772  0.366   0.05 No Pro~\n#>  9 functiona~ global   RESE~   0.897    0.409   0.05 No Pro~\n#> 10 outliers   global   Cook~   0.0588  NA       1    No Pro~\n#> 11 outliers   global   Bonf~   3.54     0.190   0.05 No Pro~\n#> 12 autocorre~ global   Durb~   0.0863   0.0680  0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.6 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.7 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-1","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.5.8 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:","code":"\n# Bruker pakken: table1\ntable1::label(Pallant_survey3$age) <- \"age\"\ntable1::label(Pallant_survey3$tmarlow) <- \"tmarlow\"\ntable1::label(Pallant_survey3$tmast) <- \"tmast\"\ntable1::label(Pallant_survey3$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey3$tpstress) <- \"tpstress\"\ntable1::table1(~age + tmarlow + tmast + tpcoiss + tpstress, data = Pallant_survey3)\n# Bruker pakken: sjPlot\nPallantkorr <- tab_corr(Pallant_survey3, triangle = \"lower\")\nPallantkorr\n# Bruker pakken: sjPlot\ntab_model(modell0, modell1, modell2)"},{"path":"regresjonsanalyse---ols.html","id":"stegvis-mutlippel-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6 Stegvis mutlippel regresjonsanalyse","text":"Stegvis regresjon er en iterativ multippel regresjonsanalyse der prosessen gradvis (steg steg = stegvis) setter inn eller tar bort uavhengige variabler som ikke bidrar til forklaring av variansen den avhengige variabelen. Man kan f.eks. ha en situasjon der man har et stort antall uavhengige variabler der problemet er å avgjøre hvor mange og hvilke variabler som skal være med regresjonsanalysen. Det finnes ulike strategier og teknikker å velge de uavhengige variablene som til slutt skal gå inn modellen. Målet er å finne den kombinasjonen av uavhengige variabler blant et større antall variabler som best forklarer variansen den avhengige variabelen.“Stepwise selection” kan foretas på ulike måter: forward, backward og bidirectional (toveis).“Forward Selection” (også omtalt som «Step-») innebærer altså man starter med null variabler. Først settes den variabelen med lavest p-verdi F. hvert steg deretter settes den variabelen som ennå ikke er lagt inn modellen med lavest p-verdi F inn. Når ingen av de gjenværende variablene er signifikante stoppes prosessen. Denne prosedyren kan ha sin nytte en analyse av et stort antall «kandidatvariabler» til en regresjonsmodell. Det som gjøres er:Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.“Backward selection” (også omtalt som “Step-” eller “Backward Elimination”) starter motsatt ende av forward. puttes alle de uavhengige variablene inn først. Deretter tar man hver gang bort variabelen som har høyest p-verdi F inntil man ikke har flere ikke-signifikante variabler igjen modellen.\n“Bidirectional” utvelgelse er en kombinasjon av de foregående. Den starter som forward selection, men hver gang en variabel settes inn blir alle variablene modellen kontrollert å se om p-verdien F har endret seg til en definert terskelverdi som følge av den nye variabelen. Hvis en ikke-signifikant variabel da blir funnet fjernes den. Når ingen variabler tilfredsstiller terskelverdiene enten inkludering eller ekskludering modellen stopper prosessen. Dette krever definerte signifikansnivåer: En å inkludere og en å ekskludere, der terskelverdien å inkludere må være lavere enn å ekskludere (med mindre man ønsker en uendelig loop der man aldri finner en løsning).Det skal bemerkes dette er en rent matematisk/statistisk operasjon. Det ligger ingen teoretiske vurderinger til grunn. Man har selvsagt ingen garanti de uavhengige variablene man ender opp med er fornuftige eller har noen praktisk signifikans. Ethvert resultat fra en stegvis regresjon må derfor vurderes kritisk. Man står også fare såvel overfitting som underfitting (jfr. tidligere punkt om overfitting) (Field 2009a). Det mangler ikke på advarsler om å bruke stegvis regresjon, f.eks. fra Miles Shevlin (2001) som advarer om stegvis regresjonsanalyse “used extreme caution” (s.38). Singer Willett (2003) hevder på sin side “Never let computer select predictors mechanically. computer know research questions literature upon rest. distinguish predictors direct substantive interest whose effects want control”. Man skal hvert fall se på resultatene med et veldig kritisk blikk siden “data analyst knows computer” (Henderson Velleman 1981, s.391).Et alvorlig problem med stegvis regresjonsanalyse er man får en \\(R^2\\)-verdi som har svært unøyaktig høy (Miles Shevlin 2001).Dette skyldes statistikkprogrammet tråler gjennom et stort antall uavhengige variabler på søken etter statistisk signifikante variabler og velger ut alle disse. En andel variabler vil være signifikante av tilfeldighet noe som øker verdien på \\(R^2\\).Forutsetningene er de samme som standard multippel regresjon. Spesielt skal man være observant på uteliggere. En tommelfingerregel som angis er minimum 5 caser per variabel (50 variabler = minimum 250 caser).Vi skal se på et eksempel som er hentet fra  (van den Berg 2018).\nDu kan laste ned datasettet ulike formater :Download magazine_reg.xlsxDownload magazine_reg.savDownload magazine_reg.dta","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-stegvis-multippel-regresjonsanalyse","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.1 Eksempel stegvis multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på stegvis multippel regresjonsanalyse, ved å følge stegene 1-4 .Analyse av dataeneAnalyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.2 Analyse av dataene","text":"","code":"\n# Base R\nmagazine_data <- as.data.frame(read_excel(\"magazine_reg.xlsx\"))\n# Bruker pakken: summarytools\nsummarytools::descr(magazine_data, stats = \"common\")\n#> Non-numerical variable(s) ignored: age\n#> Descriptive Statistics  \n#> magazine_data  \n#> N: 637  \n#> \n#>                     educ    filt1   gender     intnr     mis1     prof     sat1     sat2     sat3\n#> --------------- -------- -------- -------- --------- -------- -------- -------- -------- --------\n#>            Mean     5.11     0.99     1.13   1267.38     0.31     2.31     4.11     4.37     3.77\n#>         Std.Dev     1.13     0.10     0.34    712.79     0.76     0.72     0.88     0.75     0.95\n#>             Min     1.00     0.00     1.00     46.00     0.00     1.00     1.00     2.00     1.00\n#>          Median     5.00     1.00     1.00   1259.00     0.00     2.00     4.00     4.00     4.00\n#>             Max     6.00     1.00     2.00   2497.00     6.00     5.00     6.00     6.00     6.00\n#>         N.Valid   637.00   637.00   637.00    637.00   637.00   637.00   637.00   637.00   637.00\n#>       Pct.Valid   100.00   100.00   100.00    100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                     sat4     sat5     sat6     sat7     sat8     sat9    satov   whours\n#> --------------- -------- -------- -------- -------- -------- -------- -------- --------\n#>            Mean     4.00     4.67     4.41     4.41     3.91     3.97     7.49     4.33\n#>         Std.Dev     1.06     0.59     0.91     0.85     0.92     0.99     0.77     0.65\n#>             Min     1.00     2.00     1.00     1.00     2.00     1.00     5.00     1.00\n#>          Median     4.00     5.00     5.00     5.00     4.00     4.00     8.00     4.00\n#>             Max     6.00     6.00     6.00     5.00     6.00     6.00    10.00     5.00\n#>         N.Valid   637.00   637.00   637.00   637.00   637.00   637.00   637.00   477.00\n#>       Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00    74.88\n# Base R\npar(mfrow=(c(2,3)))\nhisttsat1 <- with(magazine_data, hist(sat1))\nhisttsat2 <- with(magazine_data, hist(sat2))\nhisttsat3 <- with(magazine_data, hist(sat3))\nhisttsat4 <- with(magazine_data, hist(sat4))\nhisttsat5 <- with(magazine_data, hist(sat5))\nhisttsat6 <- with(magazine_data, hist(sat6))\nhisttsat7 <- with(magazine_data, hist(sat7))\nhisttsat8 <- with(magazine_data, hist(sat8))\nhisttsat9 <- with(magazine_data, hist(sat9))"},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.3 Lage modell (kjøre regresjonsanalysen)","text":"Hvilke av de 9 uavhengige variablene (sat1…sat9), som er ulike mål på kunders oppfatning av produktet, bidrar signifikant til å forklare variansen hvordan kundene totalt sett skårer produktet (magasinet) («rate magazine altogether?»). De ulike målene er f.eks. grundighet, objektivitet, lesbarhet og pålitelighet. Spørsmålet er hvilke aspekter (sat1…sat9) har størst innflytelse på kundetilfredsheten?","code":"\n# Base R\nmagazine_intercept <- lm(satov ~ 1, data = magazine_data)\nmagazine_all <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9, data = magazine_data)\nmagazine_forward <- step(magazine_intercept, direction = \"forward\", scope = formula(magazine_all), trace = 0)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---forward","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.4 Analyse av resultatene (diagnostikk) - forward","text":"Første modell er kun intercept. Vi ser på AIC verdiene tabellen . AIC er forkortelse Akaike Information Criterion som er en estimator prediksjonsfeil og brukes som et mål på relativ kvalitet statistiske modeller (hvor godt modellen passer til dataene modellen ble laget av - derfor er AIC egnet til å sammenlikne ulike mulige modeller fra det samme datasettet).skal vi legge merke til fortegnet til AIC verdien er irrelevant - vi ser på absoluttverdien, der lavere AIC verdi er bedre enn høyere AIC verdi.\nNeste steg er alle mulige modeller med en prediktor testes, og prediktoren som produserer den laveste AIC verdien inkluderes modellen. Det er vårt tilfelle sat1. Neste steg er å teste alle modeller/kombinasjoner med prediktorer, hvor den ene er sat1. Den neste prediktoren som inkluderes er da sat3 fordi det er prediktoren som gir lavest AIC av alle mulige kombinasjoner av modeller med prediktorer. Neste steg innebærer å teste alle modeller/kombinasjoner med tre prediktorer, der den første er sat1 og den andre er sat3. Da blir sat5 lagt til, osv.Modellen blir med denne prosedyren:R-brukere kan vi kjøre hele analysen en linje med pakken olsrr:","code":"\n# Base R\nmagazine_forward$anova\n#>     Step Df   Deviance Resid. Df Resid. Dev       AIC\n#> 1        NA         NA       636   381.2308 -325.0134\n#> 2 + sat1 -1 71.7435240       635   309.4872 -455.8202\n#> 3 + sat3 -1 33.8817998       634   275.6054 -527.6782\n#> 4 + sat5 -1 20.9219246       633   254.6835 -575.9685\n#> 5 + sat7 -1  9.7484194       632   244.9351 -598.8295\n#> 6 + sat9 -1  6.5282509       631   238.4069 -614.0379\n#> 7 + sat2 -1  2.2298960       630   236.1770 -618.0240\n#> 8 + sat6 -1  0.8045547       629   235.3724 -618.1977\n#> 9 + sat4 -1  1.0320596       628   234.3403 -618.9969\n# Base R\nmagazine_forward$coefficients\n#> (Intercept)        sat1        sat3        sat5        sat7 \n#>  3.78591494  0.17370968  0.17786494  0.19804309  0.14463696 \n#>        sat9        sat2        sat6        sat4 \n#>  0.10736930  0.08912079 -0.05363069  0.04538282\n# Bruker pakken: olsrr\nols_step_forward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Forward Selection Method \n#> ------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered: \n#> \n#> - sat1 \n#> - sat3 \n#> - sat5 \n#> - sat7 \n#> - sat9 \n#> - sat2 \n#> - sat6 \n#> - sat4 \n#> \n#> No more variables to be added.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#>                          Selection Summary                          \n#> -------------------------------------------------------------------\n#> Variable       AIC       Sum Sq       RSS       R-Sq      Adj. R-Sq \n#> -------------------------------------------------------------------\n#> sat1         1353.907     71.744    309.487    0.18819      0.18691 \n#> sat3         1282.050    105.625    275.605    0.27706      0.27478 \n#> sat5         1233.759    126.547    254.684    0.33194      0.32878 \n#> sat7         1210.898    136.296    244.935    0.35751      0.35345 \n#> sat9         1195.690    142.824    238.407    0.37464      0.36968 \n#> sat2         1191.704    145.054    236.177    0.38049      0.37459 \n#> sat6         1191.530    145.858    235.372    0.38260      0.37573 \n#> sat4         1190.731    146.890    234.340    0.38531      0.37748 \n#> -------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---backward","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.5 Analyse av resultatene (diagnostikk) - backward","text":"","code":"\n# Bruker pakken: olsrr\nols_step_backward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Backward Elimination Method \n#> ---------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Removed: \n#> \n#> - sat8 \n#> \n#> No more variables to be removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                      Backward Elimination Summary                     \n#> --------------------------------------------------------------------\n#> Variable        AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> --------------------------------------------------------------------\n#> Full Model    1191.783    233.992    147.239    0.38622      0.37741 \n#> sat8          1190.731    234.340    146.890    0.38531      0.37748 \n#> --------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---bidirectional","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.6 Analyse av resultatene (diagnostikk) - bidirectional","text":"Vi kan se modellene gir likt resultat alle tre måtene, men det trenger absolutt ikke hende. Vi kan få ulike modeller ved de tre metodene. Husk inkludering/eksludering av variabler skjer rent matematisk.","code":"\n# Bruker pakken: olsrr\nols_step_both_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Stepwise Selection Method \n#> -------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered/Removed: \n#> \n#> - sat1 added \n#> - sat3 added \n#> - sat5 added \n#> - sat7 added \n#> - sat9 added \n#> - sat2 added \n#> - sat6 added \n#> - sat4 added \n#> \n#> No more variables to be added or removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                                 Stepwise Summary                                \n#> ------------------------------------------------------------------------------\n#> Variable     Method       AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> ------------------------------------------------------------------------------\n#> sat1        addition    1353.907    309.487     71.744    0.18819      0.18691 \n#> sat3        addition    1282.050    275.605    105.625    0.27706      0.27478 \n#> sat5        addition    1233.759    254.684    126.547    0.33194      0.32878 \n#> sat7        addition    1210.898    244.935    136.296    0.35751      0.35345 \n#> sat9        addition    1195.690    238.407    142.824    0.37464      0.36968 \n#> sat2        addition    1191.704    236.177    145.054    0.38049      0.37459 \n#> sat6        addition    1191.530    235.372    145.858    0.38260      0.37573 \n#> sat4        addition    1190.731    234.340    146.890    0.38531      0.37748 \n#> ------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-3","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.6.1 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 49.206.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=8, df2=628, lower.tail=FALSE)\n#> [1] 1.953131"},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-2","chapter":"Kapittel 7 Regresjonsanalyse - OLS","heading":"7.6.7 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*","code":"\n# Bruker pakken: table1\ntable1::label(magazine_data$sat1) <- \"sat1\"\ntable1::label(magazine_data$sat2) <- \"sat2\"\ntable1::label(magazine_data$sat3) <- \"sat3\"\ntable1::label(magazine_data$sat4) <- \"sat4\"\ntable1::label(magazine_data$sat5) <- \"sat5\"\ntable1::label(magazine_data$sat6) <- \"sat6\"\ntable1::label(magazine_data$sat7) <- \"sat7\"\ntable1::label(magazine_data$sat8) <- \"sat8\"\ntable1::label(magazine_data$sat9) <- \"sat9\"\ntable1::label(magazine_data$satov) <- \"satov\"\ntable1::table1(~sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9 + satov, data = magazine_data)\n# Bruker pakken: sjPlot\nmagazine_data2 <- magazine_data[, c(\"sat1\", \"sat2\", \"sat3\", \"sat4\", \"sat5\", \"sat6\", \"sat7\", \"sat9\", \"satov\")]\nmagazinekorr <- tab_corr(magazine_data2, triangle = \"lower\")\nmagazinekorr\n# Bruker pakken: sjPlot\nmagazine_finalmod <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat9, data = magazine_data)\ntab_model(magazine_finalmod)"},{"path":"iv-regresjon.html","id":"iv-regresjon","chapter":"Kapittel 8 IV regresjon","heading":"Kapittel 8 IV regresjon","text":"","code":""},{"path":"iv-regresjon.html","id":"innledning-1","chapter":"Kapittel 8 IV regresjon","heading":"8.1 Innledning","text":"Multippel OLS-regresjon av tverrsnittsdata kan kalles en tradisjonell empirisk metode å se på sammenhengen mellom en uavhengig variabel og en eller flere avhengige variabler. regresjonen finner vi den gjennomsnittlige endringen en uavhengig variabel - Y - når en avhengig variabel X endres med en enhet, og vi samtidig holder alle andre variabler konstante. Dette (koeffisienten X) er en betinget korrelasjon mellom X og Y, men det er alltid en usikkerhet rundt hvilken grad dette er et godt estimat den kausale sammenhengen. Vi vil sannsynligvis kunne si vi ikke vet om vi har utelatt variabler som påvirker både X og Y. Det kan også være sånn ikke bare påvirker X -> Y, en kanskje også Y -> X. En mulighet “forkludring” av estimatet kausalitet kan også være vi ikke helt forstår kontrollvariablers påvirkning på X, slik deler av X’s effekt på Y forsvinner. Selv om kontrollvariabler kan klargjøre X’s effekt på Y og dermed variasjonen Y kan det tåkelegge den kausale effekten mellom variablene. Vi kjenner til fra kapittelet om OLS-regresjon det alltid er et feilledd regresjonslikningen. Feil, som beskrevet ovenfor, kan føre til en uavhenigig variabel korrelerer med feilleddet og dermed påvirker kausaleffekten. IV-regresjon (Instrumentell Variabel regresjon) kan ses på som et kvasi-eksperiment (se f.eks. Galiani et al. (2017), Andriano Monden (2019) og DiPrete, Burik, Koellinger (2018)).En definisjon av IV er “use additional ‘instrumental’ variables, contained equation interest, estimate unknown parameters equation” (Stock Trebbi 2003, s.179). La oss tenke oss vi har en avhengig variabel Y - inntekt - som påvirkes av sosioøkonomisk status - X. Sosioøkonomisk status påvirker altså inntekten (X -> Y), men samtidig kan inntekt påvirke sosioøkonomisk status (Y -> X). En såkalt instrumentvariabel - Z - som må korrelere sterkt med X, men ikke med Y på andre måter enn gjennom X kan gi oss en omveg rundt problemet med retning på kausaliteten. Å finne en god instrumentvariabel er derimot krevende og vil trolig forutsette svært god kunnskap om tematikken som undersøkes.Oppsummert blir da en IV-regresjon man først gjennomfører en regresjon der X-verdiene predikeres av Z. Deretter gjennomfører man en regresjon med de predikerte X-verdiene mot Y. IV regresjon kalles derfor også ofte “two-stage least squares”.Det hviler forutsetninger dette: Z må være korrelert med X (det såkalte relevanskriteriet) og Z kun påvirker Y gjennom X - altså ingen direkte påvirkning fra Z til Y (det såkalte eksluderingskriteriet).","code":""},{"path":"iv-regresjon.html","id":"et-konstruert-eksempel-for-konseptualisering-av-iv-regresjon","chapter":"Kapittel 8 IV regresjon","heading":"8.2 Et konstruert eksempel for konseptualisering av IV-regresjon","text":"La oss tenke oss vi ønsker å undersøke sammenhengen mellom lengde på utdanning og inntekt. Er det slik et år ekstra utdanning gir deg x mer inntekt, eller x+…? mer inntekt? Dette eksempelet er basert på Masten (2015).La oss videre tenke oss vi har data:Vi ser det er en klar trend disse dataene folk med lengre utdanning har høyere inntekt enn folk med kortere utdanning. Betyr det lengde på utdanning har en kausal effekt på inntekt? Svaret er nei. Fordi lengde på utdanning er ikke tilfeldig (“randomly assigned”) - folk velger hvor lang utdannelse de vil ta. Likevel ser vi jo en klar trend/sammenheng dataene. Så hvorfor gjør vi det?En forklaring kan være denne sammenhengen skyldes en uobservert variabel som “forstyrrer” bildet. La oss videre eksempelet si dette er IQ, og vi har data på dette.Lengde på utdanning ser ut til å henge sammen med IQ.Men også inntekt ser ut til å henge sammen med IQ, uavhengig av hvor mye utdannelse de har. virkeligheten vil vi kanskje ikke se dette - vi har data på lengde på utdannelse og inntekt (vi har generert dataene nettopp å illustrere). Men virkeligheten kan korrelasjonen vi kanskje faktisk ser og som vi viste første diagrammet - korrelasjonen mellom lengde på utdanning og inntekt - altså være drevet av en underliggende, uobservert variabel og IKKE av en kausal sammenheng mellom lengde på utdanning og inntekt.å mitigere dette problemet kommer instrumentet inn bildet. eksempelet fra Masten (2015) vises det til avstand fra hjemadressen til nærmeste studiested (heretter: avstand) kan være et slikt instrument. Vi bygger videre på samme måte.Et instrument må altså kunne påvirke Y gjennom X (og ikke direkte), og være korrelert med X. vårt eksempel betyr det avstand må påvirke inntekt gjennom lengde på utdannelse, og avstand må være korrelert med lengde på utdannelse.Vi må derfor kunne se på X, Y og Z slik:Lav avstand har sammenheng med lengre utdannelse. Forhold som trekkes fram denne sammenhengen kan være større kjennskap til universitetet, lavere kostnad bolig og reise, større påvirkning av eldre studenter nærmiljøet gjennom oppveksten (og sikkert flere) - som sum antyder det kan være en kausal sammenheng mellom avstand og om folk studerer og hvor lenge. Altså en korrelasjon mellom Z og X (som vi sa var en forutsetning).Sammenhengen mellom IQ og avstand er imidlertid tilfeldig. Vi antar det ikke er noen grunn til IQ har sammenheng med avstand. Folk er født “og der” med ulik IQ. Vi ser bort fra det kanskje faktisk kan være en sammenheng… ved universitets ansatte bor nærheten av iuniversitete, og om IQ er arvelig kan man kanskje tenke seg det bor flere unge med høyere IQ nærheten av nuviersitetet enn lenger unna - vi går ikke inn denne “problemstillingen” , men legger til grunn dataene våre viser det ikke er slik. Om det faktisk er sånn det er en sammenheng vil det gjøre det vanskelig å bruke avstand som et instrument.Den siste forutsetningen er avstand ikke kan ha en direkte kausal effekt på inntekt. Dette virker rimelig, da det er vanskelig å se arbeidsgivere har noe forhold til hvor langt unna et unviersitet folk har vokst opp, og avstand dermed skulle kunne ha en kausal effekt på inntekt.Vi kan si instrumentet “avstand” tilfredsstiller forutsetningene, og vi kan se på korrelasjonen mellom avstand og inntekt.Vi ser avstand og inntekt korrelerer. Vi kan dermed si korrelasjonen mellom avstand og inntekt representerer en kausal effekt av lengde på utdanning og inntekt. Vi ser en kausal effekt mellom X og Y gjennom korrelasjonen mellom Z og Y.","code":"#> Descriptive Statistics  \n#> IVregrdata1  \n#> Label: jamovi data set  \n#> N: 161  \n#> \n#>                      Inntekt   Utdanning\n#> --------------- ------------ -----------\n#>            Mean    795578.74       15.20\n#>         Std.Dev    153427.64        1.52\n#>             Min    501846.67       12.02\n#>          Median    801972.01       15.24\n#>             Max   1049757.30       17.89\n#>         N.Valid       161.00      161.00\n#>       Pct.Valid       100.00      100.00#> Descriptive Statistics  \n#> IVregrdata2  \n#> N: 161  \n#> \n#>                       IQ   Utdanning\n#> --------------- -------- -----------\n#>            Mean   119.27       15.20\n#>         Std.Dev    11.25        1.52\n#>             Min    90.01       12.02\n#>          Median   122.55       15.24\n#>             Max   134.95       17.89\n#>         N.Valid   161.00      161.00\n#>       Pct.Valid   100.00      100.00"},{"path":"iv-regresjon.html","id":"eksempel-1-med-data-fra-pakken-aer","chapter":"Kapittel 8 IV regresjon","heading":"8.3 Eksempel 1 med data fra pakken “AER”","text":"datasettet har vi en rekke variabler:Vi skal se på sammenhengen mellom “packs” (= y -> antall sigarettpakker per capita)som avhengig variabel, “price” (= x -> gjennomsnittlig pris pr år inkl skatter og avgifter) som uavhengig variabel, og “taxs” (= z -> gjennomsnittlig skatt pr år) som instrumentvariabel.Forutsetningene er altså:z og x må være korrelerte: pris og skatte-/avgiftsnivå korrelerer virker rimeligz må ikke påvirke y direkte: skatte-/avgiftsnivået ikke seg selv påvirker antall sigarettpakker som selges direkte virker rimelig, men skatte-/avgiftsnivået påvirker antall sigarettpakker som selges gjennom prisnivåt virker også rimelig (dette er en forutsetning som alltid kan diskuteres, men eksempelets skyld antar vi dette).","code":"\ndata(\"CigarettesSW\")\n\n# Bruker pakken: summarytools\nsummarytools::descr(CigarettesSW, stats = \"common\")\n#> Non-numerical variable(s) ignored: state, year\n#> Descriptive Statistics  \n#> CigarettesSW  \n#> N: 96  \n#> \n#>                      cpi         income    packs    population    price      tax     taxs\n#> --------------- -------- -------------- -------- ------------- -------- -------- --------\n#>            Mean     1.30    99878735.74   109.18    5168866.32   143.45    42.68    48.33\n#>         Std.Dev     0.23   120541138.18    25.87    5442344.66    43.89    16.14    19.33\n#>             Min     1.08     6887097.00    49.27     478447.00    84.97    18.00    21.27\n#>          Median     1.30    61661644.00   110.16    3697471.50   137.72    37.00    41.05\n#>             Max     1.52   771470144.00   197.99   31493524.00   240.85    99.00   112.63\n#>         N.Valid    96.00          96.00    96.00         96.00    96.00    96.00    96.00\n#>       Pct.Valid   100.00         100.00   100.00        100.00   100.00   100.00   100.00\ncor(CigarettesSW$price, CigarettesSW$taxs)\n#> [1] 0.9203278"},{"path":"iv-regresjon.html","id":"modell-1","chapter":"Kapittel 8 IV regresjon","heading":"8.3.1 Modell 1","text":"Vi ser på koeffisientene log(price) er signifikant, og pris påvirker salget (antall sigarettpakker) negativt (negativ koeffisient).","code":"\nmodel1 <- ivreg(log(packs) ~ log(price) | taxs, data = CigarettesSW)\nsummary(model1)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) | taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.597468 -0.105477 -0.008609  0.101021  0.525352 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value\n#> (Intercept)  7.66548    0.34514  22.210\n#> log(price)  -0.60993    0.07004  -8.708\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)     0.000000000000103 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1845 on 94 degrees of freedom\n#> Multiple R-Squared: 0.4324,  Adjusted R-squared: 0.4264 \n#> Wald test: 75.83 on 1 and 94 DF,  p-value: 0.0000000000001025"},{"path":"iv-regresjon.html","id":"modell-2","chapter":"Kapittel 8 IV regresjon","heading":"8.3.2 Modell 2","text":"legger vi inn en eksogen variabel - inntekt - altså en variabel hvis verdi er bestemt utenfor modellen så å legges inn modellen. Pris ser vi på som en endogen variabel, altså en variabel hvis verdi bestemmes modellen. En tilfeldig endogen variabel en modell korrelerer med feilleddet (som vi forsåvidt har beskrevet begynnelsen av dette kapittelet), mens en eksogen variabel ikke er korrelert med feilleddet (hvilket er naturlig siden vi sier verdien er bestemt utenfor modellen).Vi kan legge merke til log(income) ikke er signifikant.","code":"\nmodel2 <- ivreg(log(packs) ~ log(price) + log(income) | log(income) + taxs, data = CigarettesSW)\nsummary(model2)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) + log(income) | log(income) + \n#>     taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.599938 -0.104454 -0.007223  0.101973  0.525855 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value\n#> (Intercept)  7.698702   0.399560  19.268\n#> log(price)  -0.605735   0.075878  -7.983\n#> log(income) -0.003015   0.018990  -0.159\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)      0.00000000000368 ***\n#> log(income)                0.874    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1853 on 93 degrees of freedom\n#> Multiple R-Squared: 0.4335,  Adjusted R-squared: 0.4214 \n#> Wald test: 37.67 on 2 and 93 DF,  p-value: 0.000000000001038"},{"path":"iv-regresjon.html","id":"eksempel-2","chapter":"Kapittel 8 IV regresjon","heading":"8.4 Eksempel 2","text":"Data dette eksempelet er hentet fra Becker Lincoln (2021) (“Mroz.csv”).","code":"\nMrozdata <- read.csv(file = 'Mroz.csv', na.strings = \".\")\nMrozdata <- subset(Mrozdata, is.na(wage) == FALSE) \nhead(Mrozdata)\n#>   inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs\n#> 1    1  1610       1       0  32   12 3.3540    2.65   2708\n#> 2    1  1656       0       2  30   12 1.3889    2.65   2310\n#> 3    1  1980       1       3  35   12 4.5455    4.04   3072\n#> 4    1   456       0       3  34   12 1.0965    3.25   1920\n#> 5    1  1568       1       2  31   14 4.5918    3.60   2000\n#> 6    1  2032       0       0  54   12 4.7421    4.70   1040\n#>   husage huseduc huswage faminc    mtr motheduc fatheduc\n#> 1     34      12  4.0288  16310 0.7215       12        7\n#> 2     30       9  8.4416  21800 0.6615        7        7\n#> 3     40      12  3.5807  21040 0.6915       12        7\n#> 4     53      10  3.5417   7300 0.7815        7        7\n#> 5     32      12 10.0000  27300 0.6215       12       14\n#> 6     57      11  6.7106  19495 0.6915       14        7\n#>   unem city exper  nwifeinc     lwage expersq\n#> 1  5.0    0    14 10.910060 1.2101540     196\n#> 2 11.0    1     5 19.499980 0.3285121      25\n#> 3  5.0    0    15 12.039910 1.5141380     225\n#> 4  5.0    0     6  6.799996 0.0921233      36\n#> 5  9.5    1     7 20.100060 1.5242720      49\n#> 6  7.5    1    33  9.859054 1.5564800    1089"},{"path":"iv-regresjon.html","id":"modell-1-standard-ols","chapter":"Kapittel 8 IV regresjon","heading":"8.4.1 Modell 1: Standard OLS","text":"Vi ønsker å kjøre en regresjon med kvinners lønn (lwage) som avhengig variabel og utdanning (educ) som uavhengig variabel. Eksempelet tar utgangspunkt Wooldridge (2016) (se kap. 15).Vi ser vi får et statistisk signifikant resultat og et års lengre utdanning øker lønn med nesten 11 % (Esitmate = 0.1086). Vi kan imidlertid forvente utdanning korrelerer med mange individuelle karakteristika hos et individ som er viktige den personens lønn, men som ikke fanges opp av utdanningsvariabelen og derfor ikke fanges opp denne modellen. dette tilfellet, og å følge eksempelet Wooldridge (2016) velger vi fars utdanning (fatheduc) som instrument (vi diskuterer ikke instrumentet og forutsetningene ).","code":"\nIVregmod1 <- lm(lwage ~ educ, data = Mrozdata)\nsummary(IVregmod1)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.10256 -0.31473  0.06434  0.40081  2.10029 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept)  -0.1852     0.1852  -1.000             0.318\n#> educ          0.1086     0.0144   7.545 0.000000000000276\n#>                \n#> (Intercept)    \n#> educ        ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.68 on 426 degrees of freedom\n#> Multiple R-squared:  0.1179, Adjusted R-squared:  0.1158 \n#> F-statistic: 56.93 on 1 and 426 DF,  p-value: 0.0000000000002761"},{"path":"iv-regresjon.html","id":"modell-2-iv-regresjon","chapter":"Kapittel 8 IV regresjon","heading":"8.4.2 Modell 2: IV regresjon","text":"Vi kan se “verdien” av et års ekstra utdanning nå kun er 5.9% økning, og signifikant p < 0.1.","code":"\nIVregmod2 <- ivreg(lwage ~ educ | fatheduc, data = Mrozdata)\nprint(summary(IVregmod2))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ | fatheduc, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0870 -0.3393  0.0525  0.4042  2.0677 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.44110    0.44610   0.989   0.3233  \n#> educ         0.05917    0.03514   1.684   0.0929 .\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6894 on 426 degrees of freedom\n#> Multiple R-Squared: 0.09344, Adjusted R-squared: 0.09131 \n#> Wald test: 2.835 on 1 and 426 DF,  p-value: 0.09294"},{"path":"iv-regresjon.html","id":"modell-3-ols-med-flere-prediktorer","chapter":"Kapittel 8 IV regresjon","heading":"8.4.3 Modell 3: OLS med flere prediktorer","text":"Estiemrt koeffisient educ er nå 0.1075 med standardfeil 0.0141","code":"\nIVregmod3 <- lm(lwage ~ educ + age + exper + expersq, data = Mrozdata)\nsummary(IVregmod3)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ + age + exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.08521 -0.30587  0.04946  0.37523  2.37077 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept) -0.5333762  0.2778430  -1.920           0.05557\n#> educ         0.1075228  0.0141745   7.586 0.000000000000212\n#> age          0.0002836  0.0048553   0.058           0.95344\n#> exper        0.0415623  0.0131909   3.151           0.00174\n#> expersq     -0.0008152  0.0003996  -2.040           0.04195\n#>                \n#> (Intercept) .  \n#> educ        ***\n#> age            \n#> exper       ** \n#> expersq     *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6672 on 423 degrees of freedom\n#> Multiple R-squared:  0.1568, Adjusted R-squared:  0.1489 \n#> F-statistic: 19.67 on 4 and 423 DF,  p-value: 0.000000000000007328"},{"path":"iv-regresjon.html","id":"modell-4-iv-regresjon-med-flere-to-instrumenter","chapter":"Kapittel 8 IV regresjon","heading":"8.4.4 Modell 4: IV regresjon med flere (to) instrumenter","text":"Vi legger til både fars og mors utdannelse som instrumenter:fatheduc og motheduc er instrumentene, exper og expersq er eksogene uavhengige variabler.Også ser vi en vesentlig reduksjon estimatet educ fra OLS til IV regresjon; fra 0.1075 med standardfeil 0.0141 til 0.0613 med standardfeil 0.0314. IV estiamtene vil ha større standardfeil enn OLS estimater.","code":"\nIVregmod4 <- ivreg(lwage ~ educ  +exper + expersq | fatheduc + motheduc + exper + expersq, data = Mrozdata)\nprint(summary(IVregmod4))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ + exper + expersq | fatheduc + motheduc + \n#>     exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0986 -0.3196  0.0551  0.3689  2.3493 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  0.0481003  0.4003281   0.120  0.90442   \n#> educ         0.0613966  0.0314367   1.953  0.05147 . \n#> exper        0.0441704  0.0134325   3.288  0.00109 **\n#> expersq     -0.0008990  0.0004017  -2.238  0.02574 * \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6747 on 424 degrees of freedom\n#> Multiple R-Squared: 0.1357,  Adjusted R-squared: 0.1296 \n#> Wald test: 8.141 on 3 and 424 DF,  p-value: 0.00002787"},{"path":"iv-regresjon.html","id":"test-av-relevansen-av-instrumentene","chapter":"Kapittel 8 IV regresjon","heading":"8.4.5 Test av relevansen av instrumentene","text":"Det er selvsagt avgjørende instrumentet/-ene er relevante. Som beskrevet må instrumentene være tilstrekkelig sterkt korrelerte med den/de endogene uavhengige variablene. dette eksempelet (modell 4) må altså fatheduc og motheduc forklare en tilstrekkelig del av variasjonen educ å kunne være relevant instrumenter. Dette kan vi gjøre gjennom en vanlig F-test:F-verdien er 54.943 med p < .001. Vi kan finne kritisk F-verdi:Vi kan klart forkaste nullhypotesen om instrumentene er irrelevante.","code":"\nsteg1 <- lm(educ ~ age + exper + expersq + fatheduc + motheduc, data = Mrozdata)\n\ninstrumentF <- waldtest(steg1, .~. -fatheduc -motheduc)\ninstrumentF\n#> Wald test\n#> \n#> Model 1: educ ~ age + exper + expersq + fatheduc + motheduc\n#> Model 2: educ ~ age + exper + expersq\n#>   Res.Df Df      F                Pr(>F)    \n#> 1    422                                    \n#> 2    424 -2 54.943 < 0.00000000000000022 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nqf(0.05, df1 = 422, df2 = 424)\n#> [1] 0.8520168"},{"path":"logistisk-regresjon.html","id":"logistisk-regresjon","chapter":"Kapittel 9 Logistisk regresjon","heading":"Kapittel 9 Logistisk regresjon","text":"","code":"\npacman::p_load(readxl, car, rgl, flextable, plotly, latex2exp, ggfortify, gridExtra, factoextra, corrplot, Directional, tidyverse, palmerpenguins, psych, paran, kableExtra, multiUS, xtable, GPArotation, EFAtools, nFactors, rstatix, calibrate, Matrix, summarytools, lavaan, lavaanPlot, haven, skimr, psycho, semPlot, gridExtra, kfa)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"innledning-2","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.1 Innledning","text":"“En faktoranalyse er en analyseteknikk som brukes å forstå korrelasjonsstrukturen et sett av observerte variabler” (Bjerkan 2007, s.221). følge Mehmetoglu Mittner (2020) brukes disse statistiske teknikkene praksis som metoder som reduserer et større antall variabler til et mindre antall variabler uten å miste vesentlig informasjon om dataene prosessen. De omtales derfor ofte som datareduksjonsteknikker (“data reduction” eller “dimension reduction”). Ofte vil vi bruke faktoranalyse å kunne si noe om såkalte latente (eller skjulte) variabler samfunnsvitenskapene - forhold vi ikke kan måle direkte, men som vi kan uttrykke gjennom å måle/observere en rekke andre forhold/variabler som vi så “samler” en konstruert variabel gjennom nettopp faktoranalyse. Målet med faktoranalysen er da følge Tinsley Tinsley (1987) “achieve parsimony using smallest number explanatory concepts explain maximum amount common variance correlation matrix”. Det vi ønsker å finne er variabler som er korrelerte med hverandre, men relativt ukorrelerte med andre grupper/subset av variabler (som igjen er interkorrelerte egen gruppe/subset) (Tabachnik Fidell 2007). Den grunnleggende utfordringen er vi ønsker å representere et stort antall variabler på en enklere måte, men hvis vi velger få faktorer mister vi informasjon (noe som går ut påliteligheten) og hvis vi velger mange kan vi ende opp med en modell som er komplisert og vanskelig å tolke.Innledningsvis er det nødvendig å gjøre en grunnleggende begrepsavklaring rundt begrepet faktoranalyse da det mange sammenhenger framstår som om begreper blandes sammen og det kan være uklart hva man egentlig snakker om. Begrepene faktoranalyse og komponentanalyse (“Factor Analysis” - FA - og “Principal Component Analysis” - PCA) brukes ofte om hverandre og noen ganger er det uklart hva som er hva (eller hvert fall hva forfatteren mener). En klargjørende framstilling kan man f.eks. finne Jöreskog, Olsson, Wallentin (2016). Vi vil det følgende skille mellom faktoranalyse (EFA og CFA), og komponentanalyse (PCA).Felles begge er:de er metoder datareduksjonde brukes å uttrykke multivariate data gjennom færre dimensjoner enn det opprinnelige datasettetde er metodikker å identifisere mønstre korrelasjonen mellom variabler.En grunnleggende forskjell er PCA er en deskriptiv teknikk mens faktoranalyse er modelleringsteknikker (Unkel Trendafilov 2010). PCA blir dermed “en empirisk oppsummering av datamaterialet” (Bjerkan 2007, s.225).\nStrengt tatt er ikke PCA en faktoranalyse, men omtales (dessverre noe forvirrende) som sagt ofte som en faktoranalyse. SPSS heter f.eks. menypunktet «Data reduction» og man velger «Factor» neste valg hvilket innebærer en EFA. SPSS er begrenset til EFA, og man trenger AMOS SPSS som strengt tatt er et SEM-program (SEM = Structural Equation Modelling - en relatert metodikk til CFA som vi behandler et senere kapittel).Grafisk kan forskjellene vises slik:venstre del (PCA) kombineres fire målte variabler (\\(X_1...X_4\\)) til en komponent (\\(C\\)). Pilene indikerer det er variablene som bidrar til å skape komponenten, og de kan gjøre det med ulike styrke (vekt), som er vist med \\(w_1...w_4\\). Variablene \\(X_1...X_4\\) utgjør altså ulike størrelser på bidraget til komponenten \\(C\\).figurens høyre del ser vi en faktor \\(F\\) som skaper de fire målte variablene (\\(Y_1...Y_4\\)). Dette vises ved pilene går fra \\(F\\) til \\(Y_1...Y_4\\). \\(F\\) kan typisk være en latent variabel vi ikke kan observere direkte - som f.eks. intelligens eller angst. Også er det ulike vekter, så \\(F\\) kan påvirke \\(Y_1...Y_4\\) med ulik styrke. tillegg har vi et feilledd (\\(e_1...e_4\\)). \\(e_1\\) representerer f.eks. den delen av variansen \\(Y_1\\) som ikke forklares av \\(F\\). Vi kan uttrykke sammenhengen en enkelt varaibel som \\(Y_1\\) som en regresjonslikning: \\(Y_1 = b_1*F + e_1\\) (og tilsvarende \\(Y_2...Y_4\\)).Som illustrert figuren består variansen til variabelen X av tre deler: felles varians med andre variabler, unik varians selve variabel X og målefeil. PCA søker å forklare varians variabel X som en komponent (derav komponentanalyse), mens faktoranalyse kun søker å forklare den delen av den totale variansen som er felles (eller med andre ord: korrelasjonen mellom variablene).Modifisert fra Bjerkan (2007), s. 225, fig. 10.1Matematisk er forskjellen mellom faktoranalyse og PCA altså hvordan varians blir analysert – PCA blir varians analysert, faktoranalyse blir kun delt varians («shared variance») analysert. Eller med andre ord: PCA analyserer varians (felles varians, unik varians og målefeil), FA analyserer kovarians (variabelens felles varians med andre variabler).Teoretisk er forskjellen mellom de FA ses faktoren som årsaker til variabelen, mens PCA ses variablene som årsaken til komponentene; PCA er det ingen teoretisk forventning om hvilke variabler som forbindes med hvilke komponenter – det er kun empirisk assosiert (Tabachnik Fidell 2007).En annen måte å illustrere forskjellen mellom PCA og EFA er gitt av Bastos (2021).Fra Bastos (2021)figuren representerer ’ene spesifikk varians, B’ene felles varians og C’ene feilvarians (jfr. figuren fra Bjerkan (2007) lenger opp). Mens vi PCA vil bruke varians (, B og C) bruker vi kun B EFA.Praktisk er det imidlertid ikke helt trivielt å avgjøre om man skal bruke PCA eller EFA. Guadagnoli Velicer (1988) konkluderer også en litteraturundersøkelse med resultatene fra PCA stor grad er like som resultatene fra faktoranalyse. Med minst 30 variabler vil løsningene være mer eller mindre like, men med 20 variabler kan forskjeller inntreffe (J. P. Stevens 2002). Som Field (2009a) oppsummerer: “However, non-statistician difference principal component factor may difficult conceptualize (linear models), difference arises largely calculation” (s.760).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2 Principal Component Analysis (PCA)","text":"Tabachnik Fidell (2007) foreslår dersom du ønsker en «empirisk oppsummering» av datasettet og redusere et større antall variabler til et mindre antall komponenter er PCA riktig valg. en PCA transformeres et antall korrelerte variabler til et mindre antall «principal components».","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.1 PCA gjennom et lite eksempel","text":"La oss se på hva PCA er. Vi tar utgangspunkt et lite, kontruert datasett der vi har registrert karakterer ulike fag på 10 studenter. Dette eksempelet er modifisert fra Starmer (2018), med innslag fra Pittard (2012).studentnrmatte1792763784755426457418469501049Vi kan plotte den ene variabelen - matte.Vi kan se studentgruppen deler seg klare grupper, en gruppe med høy score og en gruppe med lavere score. Studentene gruppa med høy score likner mer på hverandre enn på studenter den andre gruppa, og motsatt.Hvis vi legger til en variabel - f.eks. engelskkarakterer - får vi denne tabellen:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Som vi også kan plotte:Vi kan nå si vi har dimensjoner hver student: den første dimensjonen - x-aksen - inneholder mattekarakterer, den andre dimensjonen - y-aksen - inneholder engelskkarakterer. Vi kan se også med dimensjoner det er tydelige clustere.Så kan vi legge til nok en karakter - denne gangen norsk.studentnrmatteengelsknorsk17979672766868378716947570665425348645554674162478466050950645110494949Med tre dimensjoner må vi har tre akser plottet:Hvis vi nå legger til fysikkarakterer tillegg kan vi ikke lenger plotte dette siden det vil kreve fire dimensjoner. PCA stepper inn og lager et 2D plott av flere dimensjoner. PCA vil også kunne si oss noe om hvilken av karakterene (= hvilken av variablene) som er viktigst å skape klyngene/grupperingene av studenter.","code":"\n# Base R\nplot(karakterer$matte, xlab = \"studentnr\", ylab = \"matte\")\n# Base R\nplot(karakterer$matte, karakterer$engelsk, xlab = \"matte\", ylab = \"engelsk\")\n# Bruker pakken: plotly\nfig <- plot_ly(karakterer, x = matte, y = norsk, z = engelsk)\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'Matte'),\n                                   yaxis = list(title = 'Norsk'),\n                                   zaxis = list(title = 'Engelsk')))\nfig"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sentrering","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.1.1 Sentrering","text":"Vi går tilbake til datasettet med karakterer (dimensjoner).studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi regner så ut gjennomsnittet de variablene matte og engelsk:Vi bruker disse gjennomsnittsverdiene til å kalkulere senterpunktet (x = 58.1, y = 63.1) som vi deretter - ved å beholde de innbyrdes avstandene x- og y-planet mellom datapunktene - sentrerer alle observasjonene rundt.Vi ser datapunktene ligger nøyaktig likt forhold til hverandre, men senterpunktet er nå (0, 0). Neste steg er å lage en regresjonslinje som passer best mulig til dataene. Vi begynner med å legge på en hvilken som helst linje som går gjennom (0, 0) og deretter roterer vi linja med (0, 0) som pivoteringspunkt til man finner linja som passer best (dette tilfellet den røde stiplede linja):Og hvordan vet PCA hvilken linje som passer best? å se på det skal vi ta en liten omveg ut av eksempelet vårt.","code":"\n# Base R\nkartabell2#> [1] 58.1\n#> [1] 63.1\n# Base R\ncenter_scale <- function(x) {\n    scale(x, scale = FALSE)\n}\nsentrert <- as.data.frame(center_scale(karakterer))  \nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,3), col = \"blue\", lty = 3)\nabline(coef = c(0,2.5), col = \"blue\", lty = 3)\nabline(coef = c(0,2), col = \"blue\", lty = 3)\nabline(coef = c(0,1.5), col = \"blue\", lty = 3)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nabline(coef = c(0,0.75), col = \"red\", lty = 3)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forskjell-i-utregning-av-avvik---ols-og-pca","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.1.2 Forskjell i utregning av avvik - OLS og PCA","text":"Med utgangspunkt Long (2010) kan vi illustrere den prinsippielle forskjellen hvordan hhv. OLS (lineær regresjon) og PCA kalkulerer beste tilpasning.en OLS (jfr. teori kapittel om regresjonsanalyse) vil man søke å finne beste tilpasning:OLS forsøker å minimere feilleddet mellom den avhengige variabelen og modellen ved å regne på alle avstandene (og kvadrere dem) mellom datapunktene og modellen. grafen er dette illustrert med oransje strek av datapunktene.Hvis vi bytter om på den avhengige og uavhengige variabelen ser det OLS slik ut av datapunktene:OLS forsøker alltid minimere y-avstanden (feilleddet = \\(y-\\hat{y}\\)). PCA vil minimere feilleddet ortogonalt (90\\(^\\circ\\) på modellen):PCA vil rotere modellen (“regresjonslinja”) rundt et senterpunkt og hele tiden kalkulere summen av de ortogonale feilleddene. Når du har en litt større mengde uavhengige variabler vil PCA gi deg hvilke lineære kombinasjoner som teller mest. En snasen forklaring og illustrasjon kan de se herÅ gå dybden på den matematiske utregningen av eigenvalues og eigenvectors er på grensen av dybdekunnskap et notat med tittel “Anvendt…” seg, og trolig kan man leve godt uten denne dybdekunnskapen. En anbefalt kilde å gå dybden kan være Smith (2002). Likevel vil vi skissere hvordan dette gjøres det enkle eksempelet.","code":"\n# Base R\nset.seed(2)\nx <- 1:100\ny <- 20 + 3 * x\ne <- rnorm(100, 0, 60)\ny <- 20 + 3 * x + e\n\n# OLS-regresjon y ~ x\nplot(x,y)\nyx.lm <- lm(y ~ x)\nlines(x, predict(yx.lm), col=\"red\")\narrows(x0 = 61, x1= 61, y0 = 100, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 54, x1= 54, y0 = 260, y1 = 180, lwd = 2, col = \"orange\")\n# Base R\n# OLS-regresjon x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"blue\")\narrows(x0 = 84, x1= 55, y0 = 190, y1 = 190, lwd = 2, col = \"orange\")\narrows(x0 = 32, x1= 44, y0 = 137, y1 = 137, lwd = 2, col = \"orange\")\n# Base R\n# PCA x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"green\")\narrows(x0 = 54, x1= 57, y0 = 260, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 42, x1= 38, y0 = 32, y1 = 109, lwd = 2, col = \"orange\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel---del-2","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2 PCA gjennom et lite eksempel - del 2","text":"Vi tar utgangspunkt eksempelet der vi hadde variabler som er sentrert:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi legger på en tilfeldig linje som går gjennom (0, 0).Ettersom vi roterer linja vil avstanden \\(\\) ikke forandre seg. Lengden på både \\(b\\) og \\(c\\) vil imidlertid endre seg relativt til hverandre. Når \\(b\\) blir lengre, blir \\(c\\) kortere og motsatt. samme tanke som OLS-regresjon (se ovenfor) ønsker vi \\(b\\) skal være så kort som mulig da det betyr linja ligger så nærme datapunktet som mulig. å få \\(b\\) så kort som mulig jobber iidlertid PCA å maksimere \\(c\\) (det har samme effekt: når \\(c\\) er på sitt maksimale er \\(b\\) på sitt minimale). PCA finner altså den beste linja ved å maksimere den kvadrerte avstanden \\(c\\) (kvadrert pga. Pythagoras… \\(^2 + b^2 = c^2\\)).PCA summerer da \\(c^2\\) alle de 10 punktene dette eksempelet. Siden verdiene er kvadrerte slipper vi også problemet med positive og negative verdier nuller hverandre ut (som OLS - Ordinary Least Squares selv om vi ikke regner squares men avstand). Summen = sum squared distances SS(distances).","code":"\n# Base R\nkartabell2\n# Base R}\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\n# Base R}\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\ntext(10, -8, TeX('$\\\\c_{1}^2$ ... $\\\\c_{10}^2$ = SS(distances)'))"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"identifikasjon-av-principal-component-1-pc1-eigenvectors-og-loading-scores","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.1 Identifikasjon av Principal Component 1 (PC1), eigenvectors og loading scores","text":"PCA roterer som sagt på linja pivotert (0, 0) og til slutt finner den linja som maksimaliserer SS(Distances). Denne linja kalles Principal Component 1 (PC1).PC1 har et stigningstall på 0.4639586. Det betyr en økning på 1 x (matte) gir en økning på 0.46 y (engelsk). Det betyr matte har større innvirkning på grupepringen av studentene enn engelsk.PCA skaleres \\(c\\) alltid til 1. Matematisk gjør vi det ved å dele hver side av Pythagoras med \\(c\\) (1.1).Disse verdiene - 0.91 og 0.42 - kalles Eigenvector PC1, og de verdiene kalles gjerne Loading Scores. Som vist grafen er \\(SS(Distances) = Eigenvalue PC1\\). Eigenvalue er et begrep vi kommer tilbake til PCA når vi skal velge ut hvor mange komponenter vi skal beholde.Det neste vi kan se på da er Principal Component 2 (PC2). PC2 er linja som går vinkelrett på PC1 og gjennom (0, 0). PC2 er altså linja som reflekterer den nest største kilden til variasjon dataene, men som er ortogonal (vinkelrett) på PC1. Dette betyr eigenvectoren er “snudd” og blir -0.42 og 0.91 (som altså er loading scores PC2).å få fram det endelige PCA-plottet roteres løsningen slik PC1 utgjør x-aksen og PC2 y-aksen.Punktene plottes deretter på det roterte diagrammet. Til dette brukes trigonometri. La oss se på et enkelt eksempel ett punkt - dette gjør vi selvsagt ikke manuelt.Vi har det opprinnelige plottet et koordinatsystem og har funnet PC1 og PC2, og snur først aksene. Så må datapunktene posisjoneres etter de roterte aksene. La oss vise med ett punkt (1,0) - altså punktet 1 på x-aksen, 0 på y-aksen.Punkt (1,0) blir da - forutsatt \\(\\alpha=30\\):\\(y'= sin(\\alpha) = 0.5\\)\\(x'= cos(\\alpha) = 0.9\\)(1,0) blir da rotert (0.5, 0.9). Slik gjør man alle datapunkter.","code":"\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, TeX('Pythagoras gir: a = 1, b = 0.46, c = 1.1'))\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, \"Pythagoras gir: a = 1, b = 0.46, c = 1.1\", cex = 0.75)\ntext(11, -4, \"PCA-skalerte verdier: a = 0.91, b = 0.42, c = 1\", cex = 0.75)\ntext(11, -6.5, \"SS(Distances) = Eigenvalue for PC1\", cex = 0.75)\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\nabline(a = 0, b = -4.5, col = \"blue\") \ntext(-4, 8, \"PC2\", srt = 20)\n# Bruker pakken: calibrate\nkaraktererX2 <- within(karakterer2, rm(studentnr))\n# Skalerer data\nstandardize <- function(x) {(x - mean(x))}\nskalert_karaktererX2 <- apply(karaktererX2,2,function(x) (x-mean(x)))\nplot(skalert_karaktererX2, cex=0.9, xlim=c(-20,20))\n# Finner Eigenvalues fra kovariansematrisen\nmy.cov <- cov(skalert_karaktererX2)\nmy.eigen <- eigen(my.cov)\nrownames(my.eigen$vectors) <- c(\"matte\",\"engelsk\")\ncolnames(my.eigen$vectors) <- c(\"PC1\",\"PC\")\n# Sum Eigenvalues = den totale variansen i dataene\nsum(my.eigen$values)\n#> [1] 357.9778\nvar(skalert_karaktererX2[,1]) + var(skalert_karaktererX2[,2])\n#> [1] 357.9778\n# Eigenvektorene er principal components.\nloadings <- my.eigen$vectors\npc1.slope <- my.eigen$vectors[1,1]/my.eigen$vectors[2,1]\npc2.slope <- my.eigen$vectors[1,2]/my.eigen$vectors[2,2]\nabline(0,pc1.slope,col=\"red\")\nabline(0,pc2.slope,col=\"blue\")\ntextxy(12,10,\"(-0.710,-0.695)\",cx=0.9,dcol=\"red\")\ntextxy(-12,10,\"(0.695,-0.719)\",cx=0.9,dcol=\"blue\")\n# Hvor mye varians foklarer hver eigenvector\npc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=2)\npc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)\nxlab=paste(\"PC1 - \",pc1.var,\" % of variation\",sep=\"\")\nylab=paste(\"PC2 - \",pc2.var,\" % of variation\",sep=\"\")\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")\n# Base R\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\n\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"litt-mer-i-detalj-om-rotasjon","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.2 Litt mer i detalj om rotasjon","text":"Det vi har gjort ovenfor med et lite eksempel kjøres på hele datasettet det verktøyet vi bruker (R, Stata, SPSS, osv.).Med utgangspunkt en datamatrise roteres dataene en gitt vinkel gjennom en rotasjonsmatrise til et ny datamatrise (PC1). Denne igjen roteres på samme måte til PC2.Vi kan se på komponentene:Og matrisen etter en rotasjon som vi kan plotte (som vi kjenner igjen fra litt lenger oppe):","code":"\n# Base R\nut1 <- princomp(karaktererX2, cor = T)\nsummary(ut1)\n#> Importance of components:\n#>                           Comp.1     Comp.2\n#> Standard deviation     1.3533352 0.41046786\n#> Proportion of Variance 0.9157581 0.08424193\n#> Cumulative Proportion  0.9157581 1.00000000\n# Base R\nutdata <- ut1$score\nutdata\n#>           Comp.1      Comp.2\n#>  [1,]  2.2283619 -0.34268382\n#>  [2,]  1.2036707  0.41133586\n#>  [3,]  1.5364460  0.25900821\n#>  [4,]  1.3202598  0.20452296\n#>  [5,] -1.5428918  0.09028815\n#>  [6,] -1.2458551  0.06392291\n#>  [7,] -0.8603493 -0.68247820\n#>  [8,] -0.7964907 -0.29521764\n#>  [9,] -0.2926411 -0.43817195\n#> [10,] -1.5505104  0.72947352\nplot(utdata)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"utvelgelse-av-antall-komponenter","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.3 Utvelgelse av antall komponenter","text":"Et sentralt element PCA er valg av antall komponenter man vil beholde. Dette kan vi gjøre gjennom variansen komponentnen (: PC1 og PC2). kommer vi tilbake til begrepet eigenvalues som vi definerte lenger opp. Vi kan se på eigenvalue en PC som hvor mange variabler som representeres av den respektive PC. Eigenvalues forholder seg til forklart varians slik:\\(Forklart\\ varians = \\frac{Eigenvalue}{antall\\ opprinnelige\\ variabler}\\)Dette kan også uttrykkes slik:\\(\\frac{SS(Distances PC1)}{n - 1} = Varians PC1\\)\\(\\frac{SS(Distances PC2)}{n - 1} = Varians PC2\\)Et viktig poeng er en PCA ikke reduserer antall variabler seg selv. Hvis du opprinnelig har 13 variabler vil du få 13 PC, men spørsmålet er hvor mange du faktisk trenger å se på å forklare variansen (hvilket du ønsker skal være færre enn det opprinnelige antall variabler). Og det er vi kommer fram til hele poenget med PCA: Hvir mange komponenter skal vi beholde?å vise uilke metoder å vurdere antall komponenter som bør beholdes bruker vi et datasett med flere variabler enn vårt eksempel ovenfor, modifisert fra DataViz (2020)) - vi går ikke inn på hva dataene er.","code":"\n# Bruker pakken: palmerpenguins\npenguins_data <- penguins[,c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")]\npenguins_data <- na.omit(penguins_data)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.3.1 Kaisers kriterium","text":"Kaiser (1960) kriterium baserer seg på å beholde alle komponenter med eigenvalue på 1,0. Enhver komponent med eigenvalue 1 forklarer mer varianse enn en enkeltvariabel. Med andre ord – ut fra denne måten å vurdere ønsker vi ikke å beholde komponenter som forklarer mindre varians enn enkeltvariabler. Det er generelt anbefalt man ikke bruker Kaisers kriterium alene da metoden har en tendens til å overvurdere antallet komponenter. Samtidig hevdes det det er umulig å tillegge en komponent med verdi 1,01 som viktig og en annen med verdi 0,99 som uviktig (Fabrigar et al. 1999).Ut fra dette bør vi beholde 1 komponent, men vi ser komponent 2 er svært nærme 1.","code":"\n# Base R\npca_obj <- prcomp(drop_na(penguins_data), scale. = TRUE)\npca_obj_eigen <- ((pca_obj$sdev)^2)\npca_obj_eigen\n#> [1] 2.77008681 0.99348866 0.77191746 0.36520940 0.09929767"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.3.2 Scree plott","text":"Et hjelpemiddel dette er scree plot (Cattell 1966). Et Scree Plot er en grafisk framstilling av komponentene langs x-aksen og de korresponderende eigenvalues på y-aksen. Et scree plot viser hvor stor del av variansen variabelen forklarer rundt f.eks. PC1.Når vi vurderer Scree Plot ønsker vi å identifisere knekkpunktet (også kalt «albuen»).La oss forutsette variansen PC1 = 13 og PC2 = 4. Dvs. den totale variansen = 17. Videre beytr det PC1 forklarer \\(\\frac{13}{17}=0.765\\) - altså 76.5% av den totale variansen. PC2 forklarer på sin side \\(\\frac{4}{17}=0.235\\). Et scree plot er en grafisk framstilling av denne variansen.vårt eksempel kan vi dermed fremstille dette scree plottet:Cattell (1966) beskriver framgangsmåten som man finner albuen og dropper alle komponenter etter komponenten som starter albuen (eller sagt på en annen måte: vi beholder alle komponentene knekkpunktet). vårt tilfelle indikerer det vi beholder 1 komponent. vårt scree plott er det et tydelig knekkpunkt, men mange tilfeller er det ikke så tydelig, og det kan være vanskelig å identifisere «det rette» knekkpunktet.En alternativ, og ofte brukt meetode å illlustrere et scree plot på, er å kombinere det med et histogram (eksempel fra Szczęsna (2022)).","code":"\n# Bruker pakken: psych\nvar_explained_df <- data.frame(PC= paste0(\"PC\",1:5),                           var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2))\nvar_explained_df %>%\n  ggplot(aes(x=PC,y=var_explained, group=1))+\n  geom_point(size=4)+\n  geom_line()+\n  labs(title=\"Scree plott\")\nsummary(pca_obj)\n#> Importance of components:\n#>                          PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     1.664 0.9967 0.8786 0.60433 0.31512\n#> Proportion of Variance 0.554 0.1987 0.1544 0.07304 0.01986\n#> Cumulative Proportion  0.554 0.7527 0.9071 0.98014 1.00000\n# Bruker pakken: factoextra\nfviz_eig(pca_obj, col.var=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.2.3.3 Parallell analyse","text":"Parallell anlayse (PA) (Horn 1965) sammenlikner korrelasjonsmatrisen fra våre data med tilfeldig genererte korrelasjonsmatriser med samme antall variabler og observasjoner, å sammenlikne eigenvalues de genererte med den observerte. eksempelet ber vi om 5000 tilfeldige korrelasjonsmatriser.","code":"\n# Bruker pakken: paran\nparan(penguins_data, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           2.617826    2.770086      0.152260\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (1 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel-pca","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3 Eksempel PCA","text":"En anvendelse av PCA kan være vi ønsker å se på den underliggende strukturen en skalavariabel. skal vi bruke Pallants datasett som vi også brukte deler av kapittelet om regresjonsanalyse som du finner . PCA-eksempelet skal vi se på en av skalaene - PANAS - og har modifisert datasettet til å kun inneholde spørsmålene knyttet til denne skalaen.Download Pallant_survey_PANAS.xlsxDownload Pallant_survey_PANAS.savDownload Pallant_survey_PANAS.dtaDatasettet består av 20 spørsmål som utgjør PANAS skalaen (“Positive Negative Affect Schedule”).","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey_PANAS <- as.data.frame(read_excel(\"Pallant_survey_PANAS.xlsx\"))\nPallant_survey_PANAS <- na.omit(Pallant_survey_PANAS)\nsummarytools::descr(Pallant_survey_PANAS)\n#> Descriptive Statistics  \n#> Pallant_survey_PANAS  \n#> N: 435  \n#> \n#>                        pn1     pn10     pn11     pn12     pn13     pn14     pn15     pn16     pn17\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.79     1.86     2.43     2.87     3.26     1.66     3.19     1.68     3.37\n#>           Std.Dev     0.91     1.07     1.04     1.19     1.00     0.95     1.12     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     2.00     3.00     1.00     2.00     1.00     3.00\n#>            Median     4.00     2.00     2.00     3.00     3.00     1.00     3.00     1.00     3.00\n#>                Q3     4.00     2.00     3.00     4.00     4.00     2.00     4.00     2.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     1.48     1.48     0.00     1.48     0.00     1.48\n#>               IQR     1.00     1.00     1.00     2.00     1.00     1.00     2.00     1.00     1.00\n#>                CV     0.24     0.58     0.43     0.41     0.31     0.57     0.35     0.60     0.30\n#>          Skewness    -0.67     1.16     0.47    -0.01    -0.33     1.59    -0.29     1.53    -0.35\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.26     0.54    -0.40    -0.94    -0.35     2.10    -0.65     1.67    -0.24\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                       pn18     pn19      pn2     pn20      pn3      pn4      pn5      pn6      pn7\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.42     2.19     2.56     1.72     1.74     3.16     1.41     3.72     3.60\n#>           Std.Dev     0.99     1.17     1.20     1.08     1.00     1.11     0.82     1.01     1.10\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     1.00     1.00     2.00     1.00     3.00     3.00\n#>            Median     3.00     2.00     2.00     1.00     1.00     3.00     1.00     4.00     4.00\n#>                Q3     4.00     3.00     3.00     2.00     2.00     4.00     2.00     4.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     0.00     0.00     1.48     0.00     1.48     1.48\n#>               IQR     1.00     2.00     1.00     1.00     1.00     2.00     1.00     1.00     1.00\n#>                CV     0.29     0.54     0.47     0.63     0.58     0.35     0.58     0.27     0.31\n#>          Skewness    -0.48     0.74     0.40     1.47     1.41    -0.27     2.33    -0.66    -0.53\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.02    -0.45    -0.81     1.25     1.25    -0.69     5.31     0.11    -0.33\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                        pn8      pn9\n#> ----------------- -------- --------\n#>              Mean     2.20     3.29\n#>           Std.Dev     1.17     1.01\n#>               Min     1.00     1.00\n#>                Q1     1.00     3.00\n#>            Median     2.00     3.00\n#>                Q3     3.00     4.00\n#>               Max     5.00     5.00\n#>               MAD     1.48     1.48\n#>               IQR     2.00     1.00\n#>                CV     0.53     0.31\n#>          Skewness     0.77    -0.38\n#>       SE.Skewness     0.12     0.12\n#>          Kurtosis    -0.33    -0.12\n#>           N.Valid   435.00   435.00\n#>         Pct.Valid   100.00   100.00\n# Base R\nresultat.pca <- prcomp(Pallant_survey_PANAS, scale = TRUE)\nsummary(resultat.pca)\n#> Importance of components:\n#>                           PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     2.4973 1.8443 1.1063 1.07642 0.94816\n#> Proportion of Variance 0.3118 0.1701 0.0612 0.05793 0.04495\n#> Cumulative Proportion  0.3118 0.4819 0.5431 0.60104 0.64599\n#>                            PC6    PC7     PC8     PC9\n#> Standard deviation     0.88714 0.8556 0.81001 0.80669\n#> Proportion of Variance 0.03935 0.0366 0.03281 0.03254\n#> Cumulative Proportion  0.68534 0.7219 0.75475 0.78729\n#>                           PC10    PC11    PC12    PC13\n#> Standard deviation     0.77118 0.76610 0.70723 0.70109\n#> Proportion of Variance 0.02974 0.02935 0.02501 0.02458\n#> Cumulative Proportion  0.81702 0.84637 0.87138 0.89595\n#>                           PC14    PC15    PC16    PC17\n#> Standard deviation     0.62728 0.61274 0.57483 0.54724\n#> Proportion of Variance 0.01967 0.01877 0.01652 0.01497\n#> Cumulative Proportion  0.91563 0.93440 0.95092 0.96589\n#>                           PC18    PC19    PC20\n#> Standard deviation     0.53320 0.47228 0.41805\n#> Proportion of Variance 0.01422 0.01115 0.00874\n#> Cumulative Proportion  0.98011 0.99126 1.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"valg-av-antall-komponenter","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.1 Valg av antall komponenter","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-og-eigenvalues","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.1.1 Kaisers kriterium og eigenvalues","text":"Ut fra Kaisers kriterium beholder vi fire komponenter.","code":"\n# Base R\neig.val <- as.data.frame(get_eigenvalue(resultat.pca))\neig.val\n#>        eigenvalue variance.percent\n#> Dim.1   6.2365752       31.1828761\n#> Dim.2   3.4015750       17.0078752\n#> Dim.3   1.2239359        6.1196796\n#> Dim.4   1.1586859        5.7934294\n#> Dim.5   0.8990120        4.4950602\n#> Dim.6   0.7870241        3.9351204\n#> Dim.7   0.7320491        3.6602453\n#> Dim.8   0.6561184        3.2805918\n#> Dim.9   0.6507440        3.2537200\n#> Dim.10  0.5947166        2.9735829\n#> Dim.11  0.5869064        2.9345320\n#> Dim.12  0.5001778        2.5008892\n#> Dim.13  0.4915331        2.4576653\n#> Dim.14  0.3934826        1.9674130\n#> Dim.15  0.3754471        1.8772354\n#> Dim.16  0.3304249        1.6521243\n#> Dim.17  0.2994713        1.4973567\n#> Dim.18  0.2843020        1.4215102\n#> Dim.19  0.2230511        1.1152553\n#> Dim.20  0.1747675        0.8738377\n#>        cumulative.variance.percent\n#> Dim.1                     31.18288\n#> Dim.2                     48.19075\n#> Dim.3                     54.31043\n#> Dim.4                     60.10386\n#> Dim.5                     64.59892\n#> Dim.6                     68.53404\n#> Dim.7                     72.19429\n#> Dim.8                     75.47488\n#> Dim.9                     78.72860\n#> Dim.10                    81.70218\n#> Dim.11                    84.63671\n#> Dim.12                    87.13760\n#> Dim.13                    89.59527\n#> Dim.14                    91.56268\n#> Dim.15                    93.43992\n#> Dim.16                    95.09204\n#> Dim.17                    96.58940\n#> Dim.18                    98.01091\n#> Dim.19                    99.12616\n#> Dim.20                   100.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-1","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.1.2 Scree plott","text":"Dette skulle indikere vi beholder komponenter.","code":"\n# Bruker pakken: factoextra\nfviz_eig(resultat.pca, addlabels = TRUE)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-1","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.1.3 Parallell analyse","text":"Dette peker også mot vi bør beholde komponenter.","code":"\n# Bruker pakken: paran\nparan(Pallant_survey_PANAS, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           5.836032    6.236575      0.400542\n#> 2           3.074520    3.401575      0.327054\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (2 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-låst-til-to-komponenter","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.2 PCA låst til to komponenter","text":"Vi ønsker å se på modellen med komponenter på tre parametere. Først ønsker vi mindre enn 50% av residualene skal ha absoluttverdi > 0.05.vårt tilfelle er 12.5% av residualene > 0.05.Den neste parameteren er model fit som bør være > 0.9.er verdien 0.8705564.Til slutt ser vi på “communalities”.Pallant (2010) foreslår å se etter verdier på 0.3. En lav verdi indikerer den respektive variabelen ikke passer godt sammen med de andre variablene sin respektive komponent. Man kan vurdere å se om modellen blir bedre ved å ta vekk variabler med lav verdi (f.eks. 0.3). vårt tilfelle er variabelen pn5 terskelverdien på 0.3. Vi kan prøve å ta den bort. Fra før ser vi modellen forklarer 48% (se “Cumulative Var” tabellen).Vi ser ingen forbedring kumulativ varians forklart.Model fit er marginalt bedre.","code":"\n# Bruker pakken: psych\npca2 <- psych::principal(Pallant_survey_PANAS, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca2\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.14 0.50 0.50 1.1\n#> pn2  -0.15  0.70 0.52 0.48 1.1\n#> pn3  -0.11  0.73 0.54 0.46 1.0\n#> pn4   0.54 -0.12 0.31 0.69 1.1\n#> pn5  -0.12  0.49 0.26 0.74 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.16  0.73 0.55 0.45 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.2\n#> pn10 -0.01  0.60 0.35 0.65 1.0\n#> pn11 -0.15  0.65 0.44 0.56 1.1\n#> pn12  0.76 -0.04 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.73 0.55 0.45 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.58 0.35 0.65 1.1\n#> pn17  0.82 -0.12 0.69 0.31 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.04  0.79 0.62 0.38 1.0\n#> pn20 -0.08  0.71 0.51 0.49 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.89 4.75\n#> Proportion Var        0.24 0.24\n#> Cumulative Var        0.24 0.48\n#> Proportion Explained  0.51 0.49\n#> Cumulative Proportion 0.51 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  830.29  with prob <  2.5e-94 \n#> \n#> Fit based upon off diagonal values = 0.95\n# Base R\nantall_over <- length(pca2$residual[pca2$residual>0.05])\nantall_residualverdier <- nrow(pca2$residual)*ncol(pca2$residual)\n(antall_over/antall_residualverdier)*100\n#> [1] 12.5\n# Base R\npca2$fit\n#> [1] 0.8705564\n# Base R\nsort(pca2$communality)\n#>       pn5       pn4      pn16      pn10       pn6      pn11 \n#> 0.2574873 0.3064322 0.3512776 0.3542041 0.3751836 0.4394038 \n#>       pn7      pn15       pn9       pn1      pn20       pn2 \n#> 0.4503231 0.4608460 0.4746693 0.5034580 0.5064648 0.5160759 \n#>      pn13       pn3      pn14       pn8      pn18      pn12 \n#> 0.5377154 0.5422017 0.5471146 0.5534260 0.5717316 0.5848691 \n#>      pn19      pn17 \n#> 0.6201048 0.6851615\n# Base R\nPallant_survey_PANAS2 <- subset(Pallant_survey_PANAS, select = -(pn5))\npca3 <- psych::principal(Pallant_survey_PANAS2, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca3\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS2, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.15 0.51 0.49 1.1\n#> pn2  -0.14  0.71 0.53 0.47 1.1\n#> pn3  -0.11  0.74 0.56 0.44 1.0\n#> pn4   0.54 -0.11 0.31 0.69 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.15  0.74 0.57 0.43 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.1\n#> pn10 -0.01  0.60 0.36 0.64 1.0\n#> pn11 -0.14  0.65 0.45 0.55 1.1\n#> pn12  0.76 -0.05 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.74 0.56 0.44 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.55 0.31 0.69 1.1\n#> pn17  0.82 -0.13 0.68 0.32 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.03  0.80 0.64 0.36 1.0\n#> pn20 -0.08  0.72 0.52 0.48 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.87 4.55\n#> Proportion Var        0.26 0.24\n#> Cumulative Var        0.26 0.50\n#> Proportion Explained  0.52 0.48\n#> Cumulative Proportion 0.52 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  708.34  with prob <  6.1e-79 \n#> \n#> Fit based upon off diagonal values = 0.95\n# Base R\npca2$fit\n#> [1] 0.8705564\npca3$fit\n#> [1] 0.8785797"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forutsetninger","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.3 Forutsetninger","text":"Så langt har vi ikke sett på hvilke forutsetninger som må ligge til grunn å kunne kjøre en PCA. Det skal vi nå.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"størrelse-på-datasettetutvalgsstørrelse","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.3.1 Størrelse på datasettet/utvalgsstørrelse","text":"Antall cases (sample size) og forholdstallet mellom antall respondenter og antall variabler kan være av betydning en faktoranalyse. små utvalg er korrelasjonskoeffisientene mellom variablene mindre pålitelige/konsistente.Det finnes ulike anbefalinger, og Hogarty et al. (2005) hevder det ikke finnes et minimum hva angår \\(N\\) og \\(\\frac{N}{variabler}\\) å oppnå en god faktoranalyse. Arrindell van der Ende (1985) fant verken et bestemt forholdstall eller et minimumsantall observasjoner hadde påvirkning på faktorstabiliteten. Guadagnoli Velicer (1988) viste en faktor med fire eller flere faktorladninger på 0,6 eller høyere er stabil uavhengig av utvalgsstørrelsen. En faktor med 10 eller flere ladninger større enn 0,4 var stabil dersom utvalgsstørrelsen er minst 150. Antallet caser kan imidlertid ses opp mot hvor sterkt variablene lader på faktorene (Tabachnik Fidell 2007) og korrelasjonene (MacCallum et al. 1999) – høyere korrelasjoner (>.80) krever mindre sample size (Guadagnoli Velicer 1988).vårt datasett har vi 435 caser/observasjoner. Vi får da forholdstallet 22.9. Antallet og forholdstallet skulle utgangspunktet ikke være til hinder en faktoranalyse . Både Hair Jr. et al. (2010) og Nunally (1978) anbefaler et forholdstall på 10:1.\n##### Sphericity - er datasettet “faktoriserbart”Vi gjennomfører tester: Bartletts test sfæritet og KMO. Først Bartletts:ser vi p-verdien er terskelverdi på 0.05, så vi får dermed indikert dataene er egnet PCA etter dette kriteriet.Deretter KMO. KMO er en utregning som indikerer andelen av varians skalavariabelen som kan forklares av de underliggende faktorene. En høy verdi indikerer en faktoranalyse er mulig.Pallant (2010) anbefaler en grenseverdi på 0.60.\nKaiser (1974) anbefaler følgende retningslinjer KMO-verdier:KMO er derfor “respektabel”.","code":"\n# Bruker pakken: psych\nkorrelasjonsmatrise <- cor(Pallant_survey_PANAS2)\ncortest.bartlett(korrelasjonsmatrise, n = nrow(Pallant_survey_PANAS2))\n#> $chisq\n#> [1] 3781.425\n#> \n#> $p.value\n#> [1] 0\n#> \n#> $df\n#> [1] 171\n# Bruker pakken: psych\nKMO(Pallant_survey_PANAS2)\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> \n#> -- Kaiser-Meyer-Olkin criterion (KMO) ----------------------\n#> \n#> v The overall KMO value for your data is meritorious.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.876\n#> \n#>   For each variable:\n#>   pn1   pn2   pn3   pn4   pn6   pn7   pn8   pn9  pn10  pn11 \n#> 0.936 0.863 0.795 0.947 0.884 0.950 0.889 0.936 0.820 0.868 \n#>  pn12  pn13  pn14  pn15  pn16  pn17  pn18  pn19  pn20 \n#> 0.893 0.899 0.800 0.870 0.918 0.905 0.915 0.827 0.803"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"anti-image-korrelasjonsmatrise","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.2.3.3.2 Anti-image korrelasjonsmatrise","text":"resultatet har vi kun hentet ut de diagonale verdiene anti-image matrisen (som er de vi er interessert ) og sortert disse synkende rekkefølge. Disse verdiene er KMO verdier de individuelle variablene. Disse bør ifølge Field (2009a) være på 0,5. Verdier 0,5 kan bety vi bør ta denne variabelen ut.Siden denne matrisen blir stor har vi kun hentet ut antall korrelasjonsverdier som er 0.9 (og 1.0 siden det matrisen alltid vil være 1.0 diagonalene - der variablene korrelerer med seg selv). Field (2009a) peker på ingen korrelasjo-ner bør være 0.9. Samtidig bør det være godt med korrelasjoner 0.3. Det finnes ingen absolutte krav til hvor mange/hvor stor andel av korrelasjonene som bør være 0.3, men hvis man har få 0.3 indikerer det dataene kanskje ikke egner seg PCA.tabellen har vi fjernet alle korrelasjonsverdier +/- 0.3 og 1 å gjøre det litt lettere å se.Det kan se ut som vi har et greit antall korrelasjoner 0.3.","code":"\n# Bruker pakken: multiUS\nantiimage <- antiImage(Pallant_survey_PANAS2)$AIR\nantiimage2 <- as.matrix(diag(antiimage), row.names = FALSE)\nsort(antiimage2, decreasing = TRUE)\n#>  [1] 0.9499796 0.9469195 0.9364141 0.9359988 0.9182026\n#>  [6] 0.9146945 0.9048130 0.8987604 0.8931080 0.8887645\n#> [11] 0.8837913 0.8703518 0.8682341 0.8629180 0.8265146\n#> [16] 0.8197264 0.8028766 0.8001013 0.7945513\n# Base R\ncorPallant_survey_PANAS2 <- cor(Pallant_survey_PANAS2)\ncorPallant_survey_PANAS2 <- round(corPallant_survey_PANAS2, 2)\nlength(corPallant_survey_PANAS2[corPallant_survey_PANAS2>0.9 & corPallant_survey_PANAS2<1])\n#> [1] 0\n# Base R \nunder03<- as.data.frame(apply(corPallant_survey_PANAS2, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\nunder03\n#>       pn1  pn2  pn3  pn4  pn6  pn7  pn8  pn9 pn10 pn11 pn12\n#> pn1                 0.34 0.35 0.41      0.41           0.48\n#> pn2            0.46                0.64      0.41  0.5     \n#> pn3       0.46                     0.49           0.33     \n#> pn4  0.34                     0.33       0.4           0.31\n#> pn6  0.35                     0.33      0.43            0.4\n#> pn7  0.41           0.33 0.33           0.48            0.4\n#> pn8       0.64 0.49                          0.38 0.46     \n#> pn9  0.41            0.4 0.43 0.48                     0.41\n#> pn10      0.41                     0.38           0.58     \n#> pn11       0.5 0.33                0.46      0.58          \n#> pn12 0.48           0.31  0.4  0.4      0.41               \n#> pn13 0.49           0.33 0.33 0.39      0.43           0.58\n#> pn14      0.41 0.81                0.46           0.32     \n#> pn15 0.41           0.32      0.33      0.36           0.51\n#> pn16      0.31 0.33                0.38      0.31 0.35     \n#> pn17 0.56           0.37 0.39 0.49      0.46           0.64\n#> pn18 0.47           0.34 0.45 0.46      0.46           0.47\n#> pn19      0.46 0.56                0.48      0.34 0.39     \n#> pn20      0.42 0.42                0.43      0.37 0.37     \n#>      pn13 pn14 pn15 pn16 pn17 pn18 pn19 pn20\n#> pn1  0.49      0.41      0.56 0.47          \n#> pn2       0.41      0.31           0.46 0.42\n#> pn3       0.81      0.33           0.56 0.42\n#> pn4  0.33      0.32      0.37 0.34          \n#> pn6  0.33                0.39 0.45          \n#> pn7  0.39      0.33      0.49 0.46          \n#> pn8       0.46      0.38           0.48 0.43\n#> pn9  0.43      0.36      0.46 0.46          \n#> pn10                0.31           0.34 0.37\n#> pn11      0.32      0.35           0.39 0.37\n#> pn12 0.58      0.51      0.64 0.47          \n#> pn13           0.36      0.55 0.58          \n#> pn14                0.36           0.56 0.45\n#> pn15 0.36                0.62 0.41          \n#> pn16      0.36                     0.35     \n#> pn17 0.55      0.62           0.58          \n#> pn18 0.58      0.41      0.58               \n#> pn19      0.56      0.35                0.75\n#> pn20      0.45                     0.75"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"faktoranalyse","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3 Faktoranalyse","text":"Det er flere teknikker assosiert med begrepet faktoranalyse, men hovedsak kan vi dele disse inn typer: eksplorerende og konfirmerende (Hoyle 2000; Hurley et al. 1997).Eksplorerende faktoranalyse som søker å gruppere variabler et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og mindre grad drevet av teori - “one can always subject data set EFA necessarily CFA” (Schriesheim Hurley et al. 1997, s.672).Konfirmerende faktoranalyse starter andre enden - med forhåndshypoteser om dataene og strukturen. ønsker vi å bekrefte (konfirmere) en antatt datastruktur vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksplorerende-faktoranalyse-efa","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1 Eksplorerende faktoranalyse (EFA)","text":"Hensikten med en ekspolerende faktoranalyse er altså å undersøke om vi har variabler som korrelerer med hverandre og se om disse kan grupperes på en meningsfull måte. Vi ser på graden av korrelasjon - variabler som er sterkt korrelerte grupperes og skilles fra andre som er mindre korrelerte (som igjen kan inneholde grupper av relativt sterkt korrelerte variabler). Målet er altså å få grupper av variabler som internt er sterkt korrelerte med hverandre, og lite korrelert med variabler utenfor gruppen.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"hva-er-en-faktor","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.1 Hva er en faktor?","text":"En faktor kan ses på som en skjult variabel - en variabel vi ikke kan observere eller måle direkte, men som påvirker flere andre synlige/målbare variabler.Variablene , B, C osv er med andre ord observerbare/målbare fenomen underliggende, skjulte faktorer. Når vi grupperer variabler som er høyt korrelerte antar vi deres variasjon og korrelasjon skyldes den underliggende/skjulte variabelen.Et typisk eksempel er den såkalte “five-factor model” (McCrae John 1992) som antar personlighetstrekk kan grupperes fem faktorer: Åpenhet, planmessighet, ekstroversjon, omgjengelighet og nevrotisisme (se f.eks. Kennair (2021) en kort introduksjon til modellen på norsk). Man kan imidlertid ikke måle disse fem faktorene direkte, men man antar de påvirker en rekke målbare fohold. Disse kan man spørre om/måle/observere. Faktoren planmessighet kan eksempel påvirke spørsmål/atferd som “Jeg er alltid forberedt”, “Jeg følger en plan” eller “Jeg utfører mine oppgaver med en gang de er gitt”.La oss se på dette visuelt på en forenklet framstilling. Vi har en teoretisk modell, der vi sier positive tilbakemeldinger på jobben predikerer jobbtilfredshet, økonomi predikerer tilfredshet hjemmet, og tilsammen predikerer jobbtilfredshet og tilfredshet hjemmet den totale personlige tilfredsheten:Dette er det vi teoretisk forventer og vår modell. Når vi samler data ser vi alltid dataene (selvsagt) aldri passer perfekt inn vår teoretiske modell. figuren er våre faktiske (empiriske) data fra vår undersøkelse representert gjennom de fargede sirklene som knyttes til sin respektive variabel.Så det vi faktisk ser - empirisk - er egentlig dette:Ikke alle målte variabler måler sterkest på den underliggende faktoret, det er overlapping mellom variablene og hva de måler (og det kan se langt verre ut enn figuren ).Faktoranalysen vil hjelpe oss å rydde litt opp dette, ved å se på hvilke variabler som faktisk (ikker teoretisk) korrelerer sterkt med hvilke, og hvilke som korrelerer svakt, så å hjelpe oss strukturere modellen vi tester.figuren kan vi se vi nok har en struktur av sterkt korrelerte variabler som måler “sine” underliggende faktorer, men vi ser også det er en gruppe som teoretisk burde ligge nørmere sine respektive faktorer, men som ser ut til å klumpe seg midten. Kanskje dette er en bedre representasjon?må vi trolig gå tilbake til vårt teoretiske utgangspunkt og spørreskjemate å se på om vi har funnet en ny faktor, eller om vi skal utelate enkelte målinger/sørsmål å få en bedre modell.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.2 Eksempel","text":"Vi skal bruke et generert datasett gjennomgang av eksplorerende faktoranalyse.Download fa_spm.xlsxDownload fa_spm.savDownload fa_spm.dtaVi kan se oss vi har stilt en rekke mennesker 9 spørsmål der de har svart på en skala fra 1-4. Det vi ønsker å se er om disse spørsmålene kan si noe om en eller flere latente variabler. De 9 spørsmålene er altså direkte målt, og vi vil se om vi kan si noe om latente variabler ut fra dette.Inngangsverderdiene en faktoranalyse er korrelasjonsmatrisen som “tygges” (programvare) til en struktur/et mønster.","code":"\n# Base R\n# Bruker pakken: readxl\nfa_spm <- as.data.frame(read_excel(\"fa_spm.xlsx\"))\nsummarytools::descr(fa_spm)\n#> Descriptive Statistics  \n#> fa_spm  \n#> N: 366  \n#> \n#>                      spm_1    spm_2    spm_3    spm_4    spm_5    spm_6    spm_7    spm_8    spm_9\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     1.42     1.28     1.58     2.10     2.86     2.64     1.94     2.48     1.95\n#>           Std.Dev     0.66     0.59     0.72     0.98     0.91     0.89     0.91     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     1.00     1.00     1.00     1.00     2.00     2.00     1.00     2.00     1.00\n#>            Median     1.00     1.00     1.00     2.00     3.00     3.00     2.00     2.50     2.00\n#>                Q3     2.00     1.00     2.00     3.00     4.00     3.00     3.00     3.00     3.00\n#>               Max     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00\n#>               MAD     0.00     0.00     0.00     1.48     1.48     1.48     1.48     0.74     1.48\n#>               IQR     1.00     0.00     1.00     2.00     2.00     1.00     2.00     1.00     2.00\n#>                CV     0.46     0.47     0.46     0.47     0.32     0.34     0.47     0.41     0.52\n#>          Skewness     1.50     2.24     1.04     0.32    -0.35     0.05     0.59     0.00     0.64\n#>       SE.Skewness     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13\n#>          Kurtosis     1.86     4.63     0.43    -1.08    -0.74    -0.84    -0.62    -1.11    -0.86\n#>           N.Valid   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"korrelasjonsmatrise","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.3 Korrelasjonsmatrise","text":"Vi ser på korrelasjonsmatrisen.Henter fram antall korrelasjoner 0.9.Videre tar vi vekk verdier 0.3 og de som = 1:","code":"\n# Bruker pakken: rstatix\nfa_spm_cor <- round(cor(fa_spm, use=\"complete.obs\"),2)\npull_lower_triangle(fa_spm_cor, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4  0.19  0.22  0.21                              \n#> 5   spm_5  0.07  0.17  0.12  0.46                        \n#> 6   spm_6  0.17  0.23  0.21  0.47   0.7                  \n#> 7   spm_7  0.34  0.33  0.27  0.18  0.11  0.26            \n#> 8   spm_8  0.26  0.27  0.25  0.25  0.29  0.34  0.52      \n#> 9   spm_9  0.35   0.3  0.23  0.17  0.22  0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9\nlength(fa_spm_cor[fa_spm_cor>0.9 & fa_spm_cor<1])\n#> [1] 0\n# Bruker pakken: rstatix\nunder03x<- as.data.frame(apply(fa_spm_cor, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\npull_lower_triangle(under03x, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4                                                \n#> 5   spm_5                    0.46                        \n#> 6   spm_6                    0.47   0.7                  \n#> 7   spm_7  0.34  0.33                                    \n#> 8   spm_8                                0.34  0.52      \n#> 9   spm_9  0.35                          0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kmo-og-bartletts","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.4 KMO og Bartletts","text":"Vi bruker KMO og Bartletts til å se på “faktoriserbarheten”. Kaiser (1974) anbefaler 0.60 som cut-verdi. Bartletts test er signifkant. Bartletts test sammenligner våre faktiske korrelasjonsmatrise med en “identity matrix”. “Identity matrix” er en konstruert korrelasjonsmatrise med verdi 1 diagnoalen og 0 på alle andre korrelasjoner:Vi forventer selvsagt korrelasjon vår korrelasjonsmatrise, og siden Bartletts test bruker nullhypotsene om det ikke er korrelasjon, forteller en signifikant test det er meningsfullt å gjennomføre en datareduksjonsteknikk. Hvis vår korrelasjonsmatrise ikke er signifikant forskjellig fra en matrise med null korrelasjon mellom variablene gir det ingen mening å se etter strukturer av korrelasjoner.tillegg kan vi se på “determinant”:En positiv verdi indikerer også datasettet egner seg faktoranalyse.","code":"\n# Bruker pakken: EFAtools\nKMO(fa_spm_cor)\n#> \n#> -- Kaiser-Meyer-Olkin criterion (KMO) ----------------------\n#> \n#> v The overall KMO value for your data is middling.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.779\n#> \n#>   For each variable:\n#> spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 \n#> 0.791 0.790 0.821 0.870 0.659 0.729 0.775 0.863 0.783\nBARTLETT(fa_spm_cor, N = nrow(fa_spm))\n#> \n#> v The Bartlett's test of sphericity was significant at an alpha level of .05.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   <U+0001D712>²(36) = 1157.84, p < .001#> [1] \"Identity Matrix\"\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]    1    0    0    0    0    0    0\n#> [2,]    0    1    0    0    0    0    0\n#> [3,]    0    0    1    0    0    0    0\n#> [4,]    0    0    0    1    0    0    0\n#> [5,]    0    0    0    0    1    0    0\n#> [6,]    0    0    0    0    0    1    0\n#> [7,]    0    0    0    0    0    0    1\n# Bruker pakken: Matrix\ndet(fa_spm_cor)\n#> [1] 0.04052472"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"antall-faktorer","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.5 Antall faktorer","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-1","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.6 Kaisers kriterium","text":"Kaisers kriterium tilsier 3 faktorer.","code":"\n# Bruker pakken: nFactors\neigenComputes(fa_spm)\n#> [1] 3.5289174 1.6219116 1.2294074 0.6032709 0.5332548\n#> [6] 0.4581451 0.4093023 0.3446454 0.2711450"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-2","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.7 Scree plott","text":"Det kan se ut til albuen/knekkpunktet tilsier 3 faktorer.","code":"\n# Bruker pakken: EFAtools\nSCREE(fa_spm_cor, eigen_type = \"EFA\")\n#> \n#> Eigenvalues were found using EFA."},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-2","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.8 Parallell analyse","text":"Dette peker mot tre faktorer.","code":"\n# Bruker pakken: EFAtools\nPARALLEL(fa_spm, eigen_type = \"EFA\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> Parallel Analysis performed using 1000 simulated random data sets\n#> Eigenvalues were found using EFA\n#> \n#> Decision rule used: means\n#> \n#> -- Number of factors to retain according to ----------------\n#> \n#> ( ) EFA-determined eigenvalues:  3"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"analyse","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.9 Analyse","text":"Vi kjører en faktoranalyse med 3 faktorer uten rotasjon.Vi kan se på Unrotated loadings hvordan de ulike spørsmålene lader på de tre faktorene. Det er ikke gitt bildet er helt enkelt å tolke. noen spørsmål - som 1, 2, og 3 - ser vi loadings på alle tre faktorene. andre er det klarere loading på en faktor, eller positivt på en og negativt på en annen. Et hjelpemiddler å tolke modellen er rotasjon. Rotasjon innebærer egnetlig brae å se på variablene og faktorene fra en annen vinkelt. Som Hartmann, Krois, Waske (2018a) påpeker: “purpose rotation produce factors mix high low loadings moderate-sized loadings. idea give meaning factors, helps interpret . mathematical viewpoint, difference rotated unrotated matrix. fitted model , uniquenesses , proportion variance explained ”.Det finnes hovedgrupper rotasjoner: ortogonal og oblikk. Blant ortogonale rotasjonsteknikker finner vi varimax, quartimax og equimax. (Direct) oblimin og promax er vanlige oblikke rotasjoner. En hovedforskjell er ved ortogonal rotasjon tillates ikke faktorene er korrelerte, mens ved oblikk rotasjon kan faktorene korrelere. må vi altså gå tilbake til vår teoretiske forståelse av hva vi undersøker. Svært ofte samfunnsvitenskapene (vil vi hevde) ønsker vi å tillate faktorene kan korrelere ved rotasjon (vi antar veldig mange tilfeller vil dette være teoretisk fornuftig). så fall bør vi bruke oblikk rotasjon. Hvis vi har teoretiske vurderinger som tilsier faktorene ikke korrelerer velger vi ortogonalt.kapittelet om PCA viste vi ortogonal rotasjon. Dette innebærer aksene forblir ortogonale på hverandre, mens ved oblikk rotasjon kan aksenens vinkler på hverandre variere.Når vi kjører en ny faktoranalyse med 3 faktorer ser det slik ut:Vi ser en klar struktur hvilke spørsmål som lader på hvilke faktorer (dette eksempelet er konstruert å vise en veldig klar faktorstruktur, ofte vil det være større rom tolkning).","code":"\n# Bruker pakken: EFAtool\nurotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\nurotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'none'.\n#> \n#> -- Unrotated Loadings --------------------------------------\n#> \n#>           F1      F2      F3  \n#> spm_1     .502    .454    .338\n#> spm_2     .556    .371    .418\n#> spm_3     .470    .318    .378\n#> spm_4     .499   -.241    .119\n#> spm_5     .635   -.592    .056\n#> spm_6     .711   -.428    .016\n#> spm_7     .594    .395   -.364\n#> spm_8     .605    .151   -.282\n#> spm_9     .638    .289   -.386\n#> \n#> -- Variances Accounted for ---------------------------------\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> -- Model Fit -----------------------------------------------\n#> \n#> <U+0001D712>²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50\n# Bruker pakken: EFAtool\nmrotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\", rotation = \"promax\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\nmrotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'promax'.\n#> \n#> -- Rotated Loadings ----------------------------------------\n#> \n#>           F1      F3      F2  \n#> spm_1    -.066    .076    .732\n#> spm_2     .056   -.022    .782\n#> spm_3     .049   -.043    .688\n#> spm_4     .521   -.018    .140\n#> spm_5     .903   -.055   -.069\n#> spm_6     .788    .094    .004\n#> spm_7    -.103    .814    .039\n#> spm_8     .142    .623   -.010\n#> spm_9     .013    .808   -.027\n#> \n#> -- Factor Intercorrelations --------------------------------\n#> \n#>       F1      F2      F3  \n#> F1    1.000   0.380   0.268\n#> F2    0.380   1.000   0.504\n#> F3    0.268   0.504   1.000\n#> \n#> -- Variances Accounted for ---------------------------------\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> -- Model Fit -----------------------------------------------\n#> \n#> <U+0001D712>²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"model-fit","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.10 Model fit","text":"Vi ser vi presenteres flere mål på model fit (se f.eks. Finch (2020)). De ulike indeksene måler ulike aspekter av model fit. Vi skal gå inn på av dem. Husk dette datasettet er generert å vise en utmerket modell - med andre data vil du sjeldent oppleve så gode verdier på model fit som .CFI = Comparative Fit Index. Verdiene kan være mellom 0 og 1, og verdier 0.9 regnes som en god fit (Hu Bentler 1999) (en mer konservativ terskel kan være 0.95).RMSEA = Root Mean Square Error Approximation. oppgis ofte verdiene 0.01, 0.05 og 0.08 som henhodsvis utmerket, god og middels. Finch (2020) viser til 0.05 som en cut-verdi.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sammenlikning-uten-rotasjon-med-ortogonal-varimax-og-oblikk-promax-rotasjon","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.1.11 Sammenlikning uten rotasjon, med ortogonal (varimax) og oblikk (promax) rotasjon","text":"","code":"\nfa_ingenrot <- factanal(fa_spm, factors = 3, rotation = \"none\")\nfa_varimax <- factanal(fa_spm, factors = 3, rotation = \"varimax\")\nfa_promax <- factanal(fa_spm, factors = 3, rotation = \"promax\")\n\npar(mfrow = c(1,3))\nplot(fa_ingenrot$loadings[,1], \n     fa_ingenrot$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Uten rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_varimax$loadings[,1], \n     fa_varimax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med varimax rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_promax$loadings[,1], \n     fa_promax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med promax rotasjon\")\nabline(h = 0, v = 0)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"konfirmerende-faktoranalyse-cfa","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2 Konfirmerende faktoranalyse (CFA)","text":"Gjennomgangen av Confirmatory Factor Analysis (CFA) er basert på Lin (2021). Vi håper vår gjennomgang kan framstå som like god som Lins original.forrige del beskrev vi forskjellen på EFA og CFA på følgende måte:Eksplorerende faktoranalyse som søker å gruppere variabler et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og mindre grad drevet av teori - “one can always subject data set EFA necessarily CFA” (Schriesheim Hurley et al. 1997, s.672).Konfirmerende faktoranalyse starter andre enden - med hypoteser om dataene og strukturen. ønsker vi å bekrefte (konfirmere) en antatt datastruktur vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"gjennomgang-av-teori-med-eksempel","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.1 Gjennomgang av teori med eksempel","text":"Data eksempelet er hentet fra Field (2009a) (dataene er konstruerte).Du kan laste ned datasettet SPSS-format - fila heter SAQ.sav.Vi har modifisert datasettet ved å ta vekk noen unødvendige variabler vårt formål og gitt variablene nye navn, og kalt denne fila SAQ2.sav:Download SAQ2.savDet grunnleggende ved faktoranalyse er det ser på korrelasjoner mellom spørsmål/enheter. vårt datasett - SAQ2 - har vi denne korrelasjonsmatrisen:CFA (og SEM) bruker vi imidlertid kovariansmatrise, ikke korrelasjonsmatrise.Vi kan legge merke til diagonalen korrelasjonsmatrisen er 1 (en variabel korrelerer alltid perfekt (1) med seg selv). en kovariansmatrise er diagnoalen ikke 1. Korrelasjon viser hvordan variabler er relatert til hverandre, kovarians viser hvordan variabler er ulike (korrelasjon er standardisert kovarians ved kovariansen deles på standardavviket hver variabel).","code":"\nSAQ <- read_sav(\"SAQ.sav\")\nSAQ2 <- select(SAQ, -c(Question_09:FAC4_2))\nSAQ2 <- rename(SAQ2, Spm1 = Question_01, Spm2 = Question_02, Spm3 = Question_03, Spm4 = Question_04, Spm5 = Question_05, Spm6 = Question_06, Spm7 = Question_07, Spm8 = Question_08)\nwrite_sav(SAQ2, \"SAQ2.sav\")\nSAQ2cor<-round(cor(SAQ2[,1:8]),2)\nupper <- SAQ2cor\nupper[upper.tri(SAQ2cor)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm1  Spm2  Spm3 Spm4 Spm5 Spm6 Spm7 Spm8\n#> Spm1     1                                     \n#> Spm2  -0.1     1                               \n#> Spm3 -0.34  0.32     1                         \n#> Spm4  0.44 -0.11 -0.38    1                    \n#> Spm5   0.4 -0.12 -0.31  0.4    1               \n#> Spm6  0.22 -0.07 -0.23 0.28 0.26    1          \n#> Spm7  0.31 -0.16 -0.38 0.41 0.34 0.51    1     \n#> Spm8  0.33 -0.05 -0.26 0.35 0.27 0.22  0.3    1\nSAQ2cov <- round(cov(SAQ2[,1:8]),2)\nupper <- SAQ2cov\nupper[upper.tri(SAQ2cov)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm1  Spm2  Spm3 Spm4 Spm5 Spm6 Spm7 Spm8\n#> Spm1  0.69                                     \n#> Spm2 -0.07  0.72                               \n#> Spm3  -0.3  0.29  1.16                         \n#> Spm4  0.34 -0.09 -0.39  0.9                    \n#> Spm5  0.32  -0.1 -0.32 0.37 0.93               \n#> Spm6   0.2 -0.07 -0.27  0.3 0.28 1.26          \n#> Spm7  0.28 -0.15 -0.45 0.43 0.36 0.64 1.22     \n#> Spm8  0.24 -0.04 -0.24 0.29 0.23 0.22 0.29 0.76"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"fra-lineær-regresjon-til-faktoranalyse","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.2 Fra lineær regresjon til faktoranalyse","text":"Fra kapittelet om regresjonsanalyse kjenner vi til:\\(y=b_0 + b_1x + e\\)der\\(y=den\\ avhengige\\ variabelen\\)\\(b_0=intercept\\)\\(b_1=stigningstallet\\ (slope)\\)\\(x=uavhengig\\ observert\\ variabel/prediktor\\)\\(e=feilledd/residual\\)faktoranalyse har vi tilsvarende:\\(y_1 = \\tau_1 + \\lambda_1\\eta_1 + e_1\\)der\\(y_1 = \"enheten\"/spørsmålet\\)\\(\\tau_1 = intercept\\ (= \"tau\")\\)\\(\\lambda_1 = koeffisient\\ (= lambda\\ , på\\ en\\ måte\\ som\\ slope\\ \\ lineær\\ regresjon,\\ men\\ som\\ \\ CFA\\ kalles\\ \"loading\")\\)\\(\\eta_1=faktoren\\ (=eta\\ ,uobservert/latent,\\ \\ motsetning\\ til\\ en\\ prediktor\\ \\ lineær\\ regresjon)\\)\\(e_1=feilleddet/residualen\\)En distinkt forskjell mellom OLS og CFA er dermed OLS er prediktoren observert, mens CFA er faktoren uobservert.Vi har altså:\n\\[y_1 = \\tau_1 + \\lambda_1\\eta_1 + e_1\\]\\[y_2 = \\tau_2 + \\lambda_2\\eta_1 + e_2\\]\\[y_3 = \\tau_3 + \\lambda_3\\eta_1 + e_3\\]Disse tre likningene (som kan være de tre første spørsmålene vårt datasett) er dermed tre separate regresjonslikninger.\nSom vi kan skrive slik på matriseform:\\[\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\end{pmatrix} = \\begin{pmatrix}\\tau_1\\\\\\tau_2\\\\\\tau_3\\end{pmatrix} + \\begin{pmatrix}\\lambda_1\\\\\\lambda_2\\\\\\lambda_3\\end{pmatrix}(\\eta_1) + \\begin{pmatrix}e_1\\\\e_2\\\\e_3\\end{pmatrix}\\]motsetning til OLS, som er univariat, er CFA multivariat. hver \\(y\\) har vi en intercept (\\(\\tau\\)), en loading (\\(\\lambda\\)) og en residual (\\(e\\)). Det alle \\(y\\) har felles er faktoren \\(\\eta\\). Og det vi egentlig sier er den felles faktoren predikerer alle \\(y\\).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"frihetsgrader-1","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.3 Frihetsgrader","text":"La oss anta vi ser på spm 3, 4 og 5, og sier vårt teoretiske utgangspunkt er (den uobserverte/latente) faktoren SPSS-angst beskriver de observerte skårene på de tre spørsmålene. Grafisk kan vi illustrere dette slik:Vi modifiserer datasettet til å kun inneholde disse tre spørsmålene.å se på antall frihetsgrader begynner vi med å se på antallet kjente størrelser (=totalt antall parametere = “known values”). Dette finner vi ved:\\(p(p+1)/2\\)vår tenkte modell har vi \\(p=3\\), altså 6 parametere. Dette kan vi forsåvidt se kovariansmatrisen også, men en stor kovariansmatrise er det raskere å bruke utregningen.Imidlertid ser vår modell slik ut:\\[\\Sigma(\\theta) = \\begin{pmatrix}\\lambda_{1} \\\\\\lambda_{2} \\\\\\lambda_{3}\\end{pmatrix}\\begin{pmatrix}\\psi_{11}\\end{pmatrix}\\begin{pmatrix}\\lambda_{1} & \\lambda_{2} & \\lambda_{3}\\end{pmatrix} + \\begin{pmatrix}\\theta_{11} &  0 & 0 \\\\\\theta_{21} & \\theta_{22} & 0 \\\\\\theta_{31} &  \\theta_{32} & \\theta_{33} \\\\\\end{pmatrix}\\]der\\(\\lambda\\) er kjent fra før\\(\\psi\\) er variansen faktorenog siste ledd likningen er kovariansmatrisen.modellen vår må vi finne antall unike parametere, som dette tilfellet er 10 (3 x \\(\\lambda\\), 1 x \\(\\psi\\) og 4 x \\(\\theta\\)). Vi kan da finne antallet frie parametere som er \\(Antall\\ unike\\ parametere\\ - antall\\ faste\\ parametere = 10 - 0 = 10\\)Antall frihetsgrader vil da være: \\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 6 - 10 = -4\\)Dette er uheldig - vi kan ikke ha et større antall frie parametere enn antall kjente parametere, da får vi et negativt antall frihetsgrader og en umulig modell. Dette kalles “underdefined model”, og kan sammenliknes med å skulle løse \\(x + y = 9\\): Det finnes uendelig mange løsninger den likningen. Der vi har 0 frihetsgrader er modellen “just-identified” (dette eer tilfelle regresjonsmodeller), og positivt antall frihetsgrader innebærer “-identified model”.Løsningen på en “underdefined model” er “fixed parameters” - altså vi låser et antall parametere til en verdi slik de ikke kan variere. Hvis vi f.eks. låser 3 x \\(\\lambda\\) og 1 x \\(\\psi\\) får vi 4 låste parametere som gir:\\(Antall\\ unike\\ parametere\\ - antall\\ faste\\ parametere = 10 - 4 = 6 (frie parrametere)\\)som gir\\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 10 - 6 = 4\\).Antall frihetsgrader blir da:\\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 6 - 4 = 2\\)","code":"\nSAQ3 <- select(SAQ2, -c(Spm1, Spm2, Spm6:Spm8))\nSAQ3cov <- round(cov(SAQ3[,1:3]),2)\nupper <- SAQ3cov\nupper[upper.tri(SAQ3cov)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm3 Spm4 Spm5\n#> Spm3  1.16          \n#> Spm4 -0.39  0.9     \n#> Spm5 -0.32 0.37 0.93"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kjøring-av-modell","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.4 Kjøring av modell","text":"tabellen ser vi resultatet av vår modell. Ladningene (“loadings”) - \\(\\lambda\\) - er gitt “Latent Variables” - “Estimates”. “Estimates” “Variances”, f.eks. 0.815 Spm3 er residualen Spm3.\nR-pakken lavaan som er brukt , låses ladning første spørsmål til 1. Vi ser dette “Latent Variables” der “Estimate” Spm3 = 1, noe vi også kan se grafisk . Siden ladningen til Spm3 er låst til 1 (“fixed parameter”) betyr det \\(\\lambda\\) de andre spørsmålene er vist relasjon til/skalert til Spm3. Dette gjør tolkning vanskelig.stedet å låse første ladning kan vi låse variansen faktoren:Vi ser \\(\\lambda\\) - ladningen - Spm3 ikke lenger er 1.","code":"\nmodell1 <- 'SPSSangst =~ Spm3 + Spm4 + Spm5'\nSPSS_3var <- cfa(modell1, data = SAQ3)\nsummary(SPSS_3var)\n#> lavaan 0.6-11 ended normally after 23 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         6\n#>                                                       \n#>   Number of observations                          2571\n#>                                                       \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 0.000\n#>   Degrees of freedom                                 0\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm3              1.000                           \n#>     Spm4             -1.139    0.073  -15.652    0.000\n#>     Spm5             -0.945    0.056  -16.840    0.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm3              0.815    0.031   26.484    0.000\n#>    .Spm4              0.458    0.030   15.359    0.000\n#>    .Spm5              0.626    0.025   24.599    0.000\n#>     SPSSangst         0.340    0.031   11.034    0.000\n# Bruker pakken: semPlot\nnodenavnf <- c(\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_3var,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 10,\n         nodeNames = nodenavnf,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 8,\n         sizeLat = 10\n)\nmodell2 <- \"SPSSangst =~ Spm3 + Spm4 + Spm5\"\nSPSS_3var_2 <- cfa(modell2, data = SAQ3, std.lv = TRUE)\nsummary(SPSS_3var_2)\n#> lavaan 0.6-11 ended normally after 14 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         6\n#>                                                       \n#>   Number of observations                          2571\n#>                                                       \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 0.000\n#>   Degrees of freedom                                 0\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm3              0.583    0.026   22.067    0.000\n#>     Spm4             -0.665    0.026  -25.605    0.000\n#>     Spm5             -0.551    0.024  -22.800    0.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm3              0.815    0.031   26.484    0.000\n#>    .Spm4              0.458    0.030   15.359    0.000\n#>    .Spm5              0.626    0.025   24.599    0.000\n#>     SPSSangst         1.000\n# Bruker pakken: semPlot\nnodenavn <- c(\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_3var_2,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 10,\n         nodeNames = nodenavn,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 8,\n         sizeLat = 10\n)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"full-modell-på-datasettet","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.5 Full modell på datasettet","text":"Vi ønsker å lage en CFA-modell SPSSangst med alle åtte spørsmålene.En noe mer oversiktlig tabell parametrene, modifisert fra Dudek (2019):Table 10.1: FaktorladningerVi kan isolere/hente ut ladningene (\\(\\lambda\\) = lambda):Vi kan også se isolert på residualenes varians (\\(\\theta\\) = theta):Dere finner igjen lambda- og thetaverdiene såvel tabellearisk som grafisk output ovenfor.","code":"\n# Bruker pakken: lavaan\nmodell3 <- \"SPSSangst =~ Spm1 + Spm2 + Spm3 + Spm4 + Spm5 + Spm6 + Spm7 + Spm8\"\nSPSS_8 <- cfa(modell3, data = SAQ2, std.lv = TRUE)\nsummary(SPSS_8, fit.measure = TRUE, standardized = TRUE)\n#> lavaan 0.6-11 ended normally after 15 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        16\n#>                                                       \n#>   Number of observations                          2571\n#>                                                       \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                               554.191\n#>   Degrees of freedom                                20\n#>   P-value (Chi-square)                           0.000\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                              4164.572\n#>   Degrees of freedom                                28\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    0.871\n#>   Tucker-Lewis Index (TLI)                       0.819\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)             -26629.559\n#>   Loglikelihood unrestricted model (H1)             NA\n#>                                                       \n#>   Akaike (AIC)                               53291.118\n#>   Bayesian (BIC)                             53384.751\n#>   Sample-size adjusted Bayesian (BIC)        53333.914\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.102\n#>   90 Percent confidence interval - lower         0.095\n#>   90 Percent confidence interval - upper         0.109\n#>   P-value RMSEA <= 0.05                          0.000\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.055\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm1              0.485    0.017   28.942    0.000\n#>     Spm2             -0.198    0.019  -10.633    0.000\n#>     Spm3             -0.612    0.022  -27.989    0.000\n#>     Spm4              0.632    0.019   33.810    0.000\n#>     Spm5              0.554    0.020   28.259    0.000\n#>     Spm6              0.554    0.023   23.742    0.000\n#>     Spm7              0.716    0.022   32.761    0.000\n#>     Spm8              0.424    0.018   23.292    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     0.485    0.586\n#>    -0.198   -0.233\n#>    -0.612   -0.570\n#>     0.632    0.667\n#>     0.554    0.574\n#>     0.554    0.494\n#>     0.716    0.650\n#>     0.424    0.486\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm1              0.450    0.015   30.734    0.000\n#>    .Spm2              0.685    0.019   35.300    0.000\n#>    .Spm3              0.780    0.025   31.157    0.000\n#>    .Spm4              0.499    0.018   27.989    0.000\n#>    .Spm5              0.623    0.020   31.040    0.000\n#>    .Spm6              0.951    0.029   32.711    0.000\n#>    .Spm7              0.702    0.024   28.678    0.000\n#>    .Spm8              0.581    0.018   32.849    0.000\n#>     SPSSangst         1.000                           \n#>    Std.lv  Std.all\n#>     0.450    0.656\n#>     0.685    0.946\n#>     0.780    0.675\n#>     0.499    0.555\n#>     0.623    0.670\n#>     0.951    0.756\n#>     0.702    0.578\n#>     0.581    0.764\n#>     1.000    1.000\nparameterEstimates(SPSS_8,standardized = FALSE) %>% \n    filter(op==\"=~\") %>%\n    select(Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue) %>%\n    knitr::kable(digits=3,booktabs=TRUE,format=\"markdown\",caption=\"Faktorladninger\")\n# Bruker pakken: semPlot\nnodenavn2 <- c(\n    \"Statistics make me cry\",\n    \"My friends will think I'm stupid for not being able to cope with SPSS\",\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"I have little experience of computers\",\n    \"All computers hate me\",\n    \"I have never been good at mathematics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_8,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 8,\n         nodeNames = nodenavn2,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         manifests = paste0(\"Spm\", 1:8),\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 6,\n         sizeLat = 10\n)\n# Bruker pakken: semPlot\nround(semMatrixAlgebra(SPSS_8, LY), digits = 3)\n#> model set to 'lisrel'\n#>      SPSSangst\n#> Spm1     0.485\n#> Spm2    -0.198\n#> Spm3    -0.612\n#> Spm4     0.632\n#> Spm5     0.554\n#> Spm6     0.554\n#> Spm7     0.716\n#> Spm8     0.424\nthetaverdier <- semMatrixAlgebra(SPSS_8, TE)\n#> model set to 'lisrel'\nupper <- round(thetaverdier, digits = 3)\nupper[upper.tri(thetaverdier)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>      Spm1  Spm2 Spm3  Spm4  Spm5  Spm6  Spm7  Spm8\n#> Spm1 0.45                                         \n#> Spm2    0 0.685                                   \n#> Spm3    0     0 0.78                              \n#> Spm4    0     0    0 0.499                        \n#> Spm5    0     0    0     0 0.623                  \n#> Spm6    0     0    0     0     0 0.951            \n#> Spm7    0     0    0     0     0     0 0.702      \n#> Spm8    0     0    0     0     0     0     0 0.581"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"tolkning-av-resultatene","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.6 Tolkning av resultatene","text":"Det første vi kan se er “Estimate” SPSSangst er 1.000. Dette er fordi vi har låst denne parameteren, hvilket innebærer variansen er standardisert. Så hvis vi har en enhets økning SPSSangst, kan vi lese endringen de åtte spørsmålene. F.eks. vil Spm1 øke med 0.485 Spm1’s skala (Spm1:Spm8 er ikke standardisert, kun variansen) (se “Latent Variables” - “Estimate”), og Spm2 vil gå ned med 0.198 (“Estimate” er -0.198). den grafiske presentasjonen av modellen ser vi det samme, også illustrert med blå og rød farge dette tilfellet positiv eller negativ “Estimate”.å gjøre tolkningen lettere kan vi standardisere både variansen faktoren (til 1) og enhetene/spørsmålene seg selv:Table 10.2: FaktorladningerHer ønsker vi å se på kolonnen “Std.” originaloutput (vår forenkle tabell = “Std.Beta”), som vi kan sammenlikne med standardiserte betaverdier lineær regresjon. Tolkningen nå blir: ett standardavviks økning SPSSangst øker Spm1 med 0.586 standardavvik, mens Spm går ned med 0.233 standardavvik (-0.233). Dette vil vi (som regel) kalle standardiserte ladninger (“standardized loadings”). En fordel med standardiserte ladninger er vi kan lettere sammenlikne ladningen til hverandre fordi de er nettopp standardiserte.","code":"\nparameterEstimates(SPSS_8,standardized = TRUE) %>% \n    filter(op==\"=~\") %>%\n    select(Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue, Std.Beta = std.all) %>%\n    knitr::kable(digits=3,booktabs=TRUE,format=\"markdown\",caption=\"Faktorladninger\")\nsemPaths(SPSS_8,\n         what = \"std\",\n         whatLabels = \"std\",\n         style = \"lisrel\",\n         residScale = 8,\n         nodeNames = nodenavn,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         manifests = paste0(\"Spm\", 1:8),\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 6,\n         sizeLat = 10\n)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"hvor-god-er-modellen-vår-model-fit-statistics","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.7 Hvor god er modellen vår (“Model Fit Statistics”)?","text":"Et viktig poeng før vi gir oss kast med vurdering av hvor god modellen er, er å kunne si noe om dette må modellen ha positivt antall frihetsgrader (jfr. delkapittel om frihetsgrader lenger opp).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kjikvadrattest","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.8 Kjikvadrattest","text":"en CFA ønsker vi p-verdien kjikvadratverdien er større enn 0.05 - vi ønsker altså ikke å forkaste nullhypotesen, men med en p-verdi < 0.001 må vi forkaste nullhypotesen (nullhypotsen innebærer det ikke er forskjell mellom vår modell og populasjonen, men det ønsker vi jo heller ikke - vi ønsker vår modell representerer populasjonen). Tilsynelatende har vi derfor forhold til kjikvadrattesten en dårlig modell. må vi imidlertid være klar store utvalgsstørrelser gir en stor risiko vi får en signifikant kjikvadrattest. dette tilfellet har vi 2571 observasjoner, noe som er høyt.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"approximate-fit-index---rmsea-cfi-og-tli","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.9 “Approximate Fit Index” - RMSEA, CFI og TLI","text":"Kjikvadrattest er en “Exact Fit”-test, hvilket innebærer vi hypotetiserer utvalget/modellen er lik populasjonen. Det finnes imidlertid andre tester som går betegnelsen “Approximate Fit Index”, hvilket innebærer man tilnærmer seg dette med “nærme nok” (altså ikke absolutt). Vi skal ikke gå inn detaljer på hvordan disse indeksene regnes ut, men begrense oss til å si disse bygger på å sammenlikne en verst mulig modell (= ingen kovariasjoner er med, kun varianser den enkelte enhet/spørsmål) med en best mulig modell (= alle varianser og kovarianser er med -> “just-identified” modell), og deretter plasserer vår modell inn dette bildet.CFI = “Confirmatory Factor Index” ønsker vi skal være 0.95TLI = “Tucker Lewis Index” ønsker vi skal være 0.90RMSEA = “Root Mean Square Error Approximation” bør være eller lik 0.05 “close fit”, og mellom 0.05 og 0.08 “reasonable approximate fit”. RMSEA 0.08 er “poor fit”.Alle disse tre målene på Model Fit er ikke særlig gode. Modellen vår er trolig ikke den beste modellen SPSSangst. Et tiltak vi kan se på er hvor store de enkelte standardiserte ladningen er. Vi ser (f.eks. det siste diagrammet) Spm2 lader med -0.23, noe som er en god del lavere enn de fleste andre. Hva skjer om vi tar bort dette spørsmålet en revidert modell?Vi ser forbedringene ikke er store, og modellen vår ikke ser kjempebra ut (den er ikke elendig, men cut-verdiene vi normalt opererer med).","code":"\nmodell4 <- \"SPSSangst =~ Spm1 + Spm3 + Spm4 + Spm5 + Spm6 + Spm7 + Spm8\"\nSPSS_7 <- cfa(modell4, data = SAQ2, std.lv = TRUE)\nfitMeasures(SPSS_7, c(\"cfi\",\"tli\", \"rmsea\"))\n#>   cfi   tli rmsea \n#> 0.906 0.859 0.100\n# summary(SPSS_7, fit.measure = TRUE, standardized = TRUE)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kryssvalidering-cross-validation","chapter":"Kapittel 10 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"10.3.2.10 Kryssvalidering (“cross-validation”)","text":"Dette er også omtalt delkapittelet om multippel regresjonsanalyse så en litt mer utførlig beskrivelse gis der.Kryssvalidering innebærer vi deler datasettet : en del vi utvikler modeller på, og en del vi tester vår utvalgte modell på. På den måten unngår vi vi bruker hele datasettet på å grave fram (litt sjansepreget) en modell som er ok uten å vite om vi bare har hatt flaks. Hvis vår valgte modell har en god fit med den delen av dataene vi tester på kan vi si noe mer sikkert om vår modell.R kan vi bruke pakken kfa som gjør dette ganske lett oss.hvert utvalg kjøres en EFA på alle utvalg unntatt et uvalg som er et valideringsutvalg. EFA-utvalgene og valideringsutvalget utgjør et “fold”. Neste iterasjon kjøres EFA på alle utvalg testdataene unntatt et nytt utvalg som blir valideringsutvalg. Dette utgjør et nytt “fold”. Slik kjøres prosedyren til alle utvalg har vært valideringsutvalg de andres EFA.Verdiene hhv. CFI og RMSEA kan sammenliknes med foreslåtte cutoff-verider lenger opp å bidra til vurderingen av modellen. Resultatene dette tilfellet bekrefter på mange måter inntrykket vi har før cross-validation.","code":"\nkfamodell <- kfa(\n    SAQ2,\n    k = 5,\n    m = 1,\n    seed = 1243)\n#> [1] \"Using 7 cores for parallelization.\"\n\nk_model_fit(kfamodell, index = \"default\", by.fold = FALSE)\n#> $`1-factor`\n#>   fold chisq.scaled df.scaled cfi.scaled rmsea.scaled\n#> 1    1    104.07114  16.31447  0.6960799   0.10219978\n#> 2    2     77.99306  16.11308  0.7438522   0.08627022\n#> 3    3     88.36224  15.57886  0.7165712   0.09543103\n#> 4    4     80.31738  16.19342  0.7728829   0.08785816\n#> 5    5     87.17257  15.55239  0.6907808   0.09465371"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller","chapter":"Kapittel 11 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"Kapittel 11 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","text":"","code":""},{"path":"referanser.html","id":"referanser","chapter":"Referanser","heading":"Referanser","text":"","code":""},{"path":"vedlegg-b---sentralgrenseteoremet-central-limit-theorem.html","id":"vedlegg-b---sentralgrenseteoremet-central-limit-theorem","chapter":"Vedlegg B - Sentralgrenseteoremet (Central Limit Theorem)","heading":"Vedlegg B - Sentralgrenseteoremet (Central Limit Theorem)","text":"Koden brukt dette eksempelet er stor grad hentet fra Fedit (2018).Dette er et noe komplisert begrep som vi ikke skal gå veldig dybden på, men det har et par viktige konsekvenser oss når vi skal tenke på distribusjon av populasjoner og utvalg. belyser vi forhold som følger av sentralgrenseteoremet:Gjennomsnittsverdien (mean) av tilfeldige utvalg fra en populasjon vil være tilnærmet lik gjennomsnittsverdien populasjonen hvis størrelsen på utvalgene er tilstrekkelig stort.Fordelingen til tilfeldige utvalg fra en populasjon vil være tilnærmet normalfordelt uavhengig av fordelingen på populasjonen. Dette innebærer selv om populasjonen er langt fra normalfordelt vil et tilstrekkelig stort utvalg vise seg å være tilnærmet normalfordelt.La oss se på dette gjennom et eksempel der vi starter med en bimodal fordeling (altså langt fra normalfordeling). Vi generer et datasett og plotter et Q-Q diagram (mer om dette et annet sted notatet, men per nå trenger vi bare vite dette er en effektiv måte å sjekke om en variabel er normalfordelt eller ikke).\nFigure 11.1: Ikke-normal fordeling og Q-Q plott\nSom sagt har vi gjort rede Q-Q plott et annet sted boka, så kan vi nøye oss med å slå fast denne variabelen definitivt ikke er normalfordelt.\nDet framkommer også tydelig når vi plotter et histogram:\nFigure 11.2: Histogram bimodal fordeling\nUt fra denne populasjonen tar vi 100 utvalg med 30 hvert utvalg. Fordelingen ser da slik ut:\nFigure 1.1: Histogram 100 utvalg fra bimodal fordeling\nVi kan allerede nå ane en tilnørming mot normalfordeling, og hvert fall en endret form enn populasjonen viste. Vi tar nå 1000 utvalg med 30 hvert utvalg.\nFigure 1.2: Histogram 1000 utvalg fra bimodal fordeling\nDet er videre åpenbart dette begynner å se mer og mer ut som en normalfordeling. Til slutt øker vi til 10000 utvalg av 30.\nFigure 1.3: Histogram 10000 utvalg fra bimodal fordeling\nDet vi kan se bekrefter hva Central Limit Theorem sier vi bør forvente. Vi kan starte med en hvilken som helst fordeling (kontinuerlig eller diskret) som har et definert gjennomsnitt og definert varians (og dermed definert standardavvik) og ta tilfeldige utvalg fra denne fordelingen – og vi vil få en tilnærmet normalfordelt fordeling. det virkelige liv har vi ofte populasjonsfordelinger som har alt annet enn normalfordeling. Likevel kan vi ta tilfeldige utvalg og få en tilnærmet normalfordelt frekvensplott (av f.eks. gjennomsnittsverdier). Størrelsen på utvalget og antallet ganger vi tar utvalg vil påvirke -> jo større utvalg og jo flere utvalg, jo nærmere normalfordeling vil frekvensplottet være.En interessant illustrasjon av CLT ligger .","code":""},{"path":"vedlegg-c---chebyshevs-teorem.html","id":"vedlegg-c---chebyshevs-teorem","chapter":"Vedlegg C - Chebyshevs teorem","heading":"Vedlegg C - Chebyshevs teorem","text":"Dette vedlegget er stor grad bygget på Hartmann, Krois, Waske (2018a).Vi diskuterte noe detalj hvordan vi kan bruke normalfordelingen til å si noe om hvordan verdier et datasett kan antas å falle innenfor en gitt avstand fra gjennomsnittet (Hartmann, Krois, Waske 2018b):\nFigure 11.1: Normalfordeling med standardavvik\nVi kan ut fra normalfordeingen si at68 % av observajsonene vil ligge innenfor ett standardavvik fra gjennomsnittsverdien95 % av observasjonene vil ligge innenfor standradavvik fra gjennomsnittsverdien99.7 % av observasjonene vil ligge innenfor tre standardaavik fra gjennomsnittsverdienDette kalles ofte den empiriske regelen (“empirical rule”), og gjelder kun normalfordelte data. Chebyshevs teorem gjelder imidlertid alle fordelinger. Normalfordelingen gir oss datapunkter med en viss sannsynlighet ligger innenfor en viss avstand fra gjennomsnittsverdien. Det samme sier Chebyshevs teorem om datafordelinger som ikke er normalfordelte: bare en gitt mengde datapunkter kan ligge mer enn en gitt avstand fra gjennomsnittsverdien.Teoremet uttrykkes slik (Hartmann, Krois, Waske 2018b):ethvert nummer k større enn 1 vil minst \\(1-1/\\)k\\(^2\\) av dataverdiene ligge innenfor k standardavvik fra gjennomsnittet.Teoremet kan generisk kan framstilles slik:\nFigure 11.2: Chebyshevs teorem\nethvert numerisk datasett gjelder:Minst ¾ av datapunktene ligger innenfor standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm2s\\) et utvalg og \\(\\overline{x}\\pm2\\sigma\\) populasjoner.Minst 8/9 av datapunktene ligger innenfor tre standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm3s\\) et utvalg og \\(\\overline{x}\\pm3\\sigma\\) populasjoner.Minst \\(1-1/\\)k\\(^2\\) av datapunktene ligger mellom k standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm\\)k\\(s\\) et utvalg og \\(\\overline{x}\\pm\\)k\\(\\sigma\\) populasjoner.Ut fra tabellen ser vi dersom vi velger scroller til k = 2 vil 75 % av verdiene ligge innenfor (altså 75 % innenfor 2 standardavvik).Vi kan også vise en grafisk framstilling av Chebyshevs teorem med fokus på prosenter (y-aksen) mot k (x-aksen).\nFigure 1.2: Chebyshevs teorem - prosent\nNår vi vet minst 75% av distribusjonen ligger innenfor \\(\\overline{x}\\pm2s\\) vet vi også maksimalt 25% ligger utenfor. Likeledes \\(\\overline{x}\\pm3s\\) vil maksimalt 11,11 % av distribusjonen ligge utenfor. Så mens reglene normalfordeling kun gjelder normalfordelte eller tilnærmet-normalfordelte datasett, er Chebyshevs teorem et faktum som gjelder alle datadistribusjoner og som beskriver minimumsandelen av observasjoner/datapunkter som ligger innenfor hhv +/- 1, 2 og 3 standardavvik fra gjennomsnittet.","code":""},{"path":"vedlegg-x---session-info.html","id":"vedlegg-x---session-info","chapter":"Vedlegg X - Session Info","heading":"Vedlegg X - Session Info","text":"","code":"#> R version 4.1.2 (2021-11-01)\n#> Platform: x86_64-w64-mingw32/x64 (64-bit)\n#> Running under: Windows 10 x64 (build 19044)\n#> \n#> Locale:\n#>   LC_COLLATE=Norwegian Bokmål_Norway.1252 \n#>   LC_CTYPE=Norwegian Bokmål_Norway.1252   \n#>   LC_MONETARY=Norwegian Bokmål_Norway.1252\n#>   LC_NUMERIC=C                            \n#>   LC_TIME=Norwegian Bokmål_Norway.1252    \n#> \n#> Package version:\n#>   base64enc_0.1.3 bookdown_0.26   brio_1.1.3     \n#>   bslib_0.3.1     cachem_1.0.6    cli_3.2.0      \n#>   compiler_4.1.2  desc_1.4.1      digest_0.6.29  \n#>   downlit_0.4.0   evaluate_0.15   fansi_1.0.3    \n#>   fastmap_1.1.0   fs_1.5.2        glue_1.6.2     \n#>   graphics_4.1.2  grDevices_4.1.2 highr_0.9      \n#>   htmltools_0.5.2 jquerylib_0.1.4 jsonlite_1.8.0 \n#>   knitr_1.39      magrittr_2.0.3  memoise_2.0.1  \n#>   methods_4.1.2   R6_2.5.1        rappdirs_0.3.3 \n#>   rlang_1.0.2     rmarkdown_2.14  rprojroot_2.0.3\n#>   rstudioapi_0.13 sass_0.4.1      stats_4.1.2    \n#>   stringi_1.7.6   stringr_1.4.0   tinytex_0.39   \n#>   tools_4.1.2     utils_4.1.2     vctrs_0.4.1    \n#>   xfun_0.30       xml2_1.3.3      yaml_2.3.5"}]
