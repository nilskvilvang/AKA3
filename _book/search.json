[{"path":"index.html","id":"introduksjon","chapter":"Introduksjon","heading":"Introduksjon","text":"Dette notatet gir en introduksjon til anvendt kvantitativ analyse rettet mot samfunnsvitenskapene.Innholdet er utvikling og oppdateres jevnlig om enn noe uregelmessig. Tilbakemeldinger og innspill bes gitt til Nils Kvilvang.Bilde: Gjengitt uten kreditering et stort antall steder, men Gleitman, Gross, Reisberg (2011), s.28, fig. 1.8 er illustrasjonen signert Chase","code":""},{"path":"index.html","id":"versjoner","chapter":"Introduksjon","heading":"Versjoner","text":"","code":""},{"path":"hvorfor-dette-notatet.html","id":"hvorfor-dette-notatet","chapter":"Kapittel 1 Hvorfor dette notatet?","heading":"Kapittel 1 Hvorfor dette notatet?","text":"ability take data, able understand , process , extract values , visualize , communicate - ’s going hugely important skill next decades… (Varian (2009), Chief Economist Google og Professor Emeritus University California, Berkeley)Dette notatet oppsummerer ulike forelesningsnotater og undervisningsopplegg anvendt kvantitativ analyse. Hensikten er å forsøke å formidle sentrale konsepter og teknikker anvendt kvantitativ analyse, samt gjennom kodeblokkene med R-kode gi “oppskrift” på prosedyre slik leseren kan reprodusere eksempler. Jeg håper det kan tjene som et oppslagsverk og kanskje også som litt inspirasjon?Det finnes mange veldig gode bøker som tar seg introduksjon til og/eller videregående emner kvantitativ analyse. Min gode kollega Christer Thranes bøker er eksempler på det. Det finnes også bøker som hvert fall jeg tenker ikke er spesielt gode (betydningen pedagogiske) formidlingen (dette er selvsagt veldig subjektivt). Likvel - er det virkelig behov enda flere, og er mitt bidrag nødvendigvis bedre/mer pedagogisk? Igjen - det får bli en subjektiv vurdering av leseren. Jeg har hvert fall gjort et ærlig forsøk på å formidle dette stoffet på en måte jeg håper er forståelig etter å ha pløyd gjennom et utall bøker og andre ressurser. Jeg skal heller ikke legge skjul på jeg ikke gjør anspråk på å revolusjonere noe som helst. Tvert mot - jeg velger lett å bruke det jeg oppfatter som gode eksempler ulike temaer fra andre. Sånn sett er mitt bidrag mer å samle gode gjennomganger og eksempler, vise leseren til hvor dette kommer fra slik man kan gå til originalkilden om man ønsker, og formidle dette på et samlet sted, norsk språkdrakt, på en forhåpentligvis forståelig måte.Samtidig er det åpenbart slik det kan være krøkkete formuleringer, dårlige forklaringer, ulne beskrivelser, dårlige eksempler, lite av noe, mye av annet osv. Alle innspill og tilbakemeldinger mottas med stor takk (helt sant!).","code":""},{"path":"hvorfor-dette-notatet.html","id":"programvare","chapter":"Kapittel 1 Hvorfor dette notatet?","heading":"Programvare","text":"Notatet er skrevet med bruk av R (R Core Team 2021) og RStudio (RStudio Team 2022). R er en velkjent programvare innenfor statistikk, dataanalyse, datamodellering osv. R har noen store fordeler; det er gratis, det kjører på “alle” operativsystemer, og det har et svært stort bruker- og utviklermiljø som hovedsak deler alt gratis. Det er også enkelt å finne løsninger på det meste gjennom veiledninger og brukerforum på nett. Selve R er et programmeringsspråk og utviklermiljø statistikk som gir en kjernefunksjonalitet innenfor datahåndtering, kalkulasjoner, dataanalyse, datamodellering, grafisk framstilling av data o.l.R kommer med 14 basis “pakker”. Det store potensialet ligger imidlertid brukerne av R utvikler tilleggspakker som man bruker R, det finnes pr. april 2022 19000 ulike pakker som bygger på kjernen R. Alle pakkene tilbyr ulike tilrettelagte løsninger ulike problemer/analyser.Den største ulempen med R er brukergrensesnittet er veldig ulikt hva vi kjenner fra Microsoft Office-typen brukergrensesnitt, så det vil ta litt tid å bli kjent med programmet. tillegg er brukergrensesnittet kodebasert og ikke menybasert, og kan (dessverre) virke avskrekkende. Likevel, det er et utrolig kraftig verktøy hvis man tar seg tid til å lære seg det grunnleggende.Brukere av notatet vil kanskje bruke programvare som enten SPSS eller Stata. Konsepter og eksempler notatet er som sagt laget og kjørt R. Ambisjonen er også å gi dere en oppskrift framgangsmåte SPSS og Stata tillegg til kode som viser R undervegs å få fram tilsvarende analyser på samme eksempler som notatet. Dette kommer på plass gradvis.tillegg er det på sin plass å nevne alternativer til store og dyre programvarepakker som SPSS og Stata som mer og mer framstår som reelle og gode alternativer. av disse, jamovi og JASP, har sterkt voksende bruk (også akademia) rundt om verden. Dette er grafisk tiltalende og funksjonsrike statistikkpakker som kjører med R bakgrunnen (alle analyser jamovi og JASP bruker R), og som også kan inkludere R kode direkte. Det gjør man kan utnytte alle pakkene skrevet R direkte de grafiske brukergrensesnitt. tillegg kan man (varierende grad mellom de ) hente ut R-kode fra analyser man gjør gjennom det grafiske brukergrensesnittet - noe som kan gjøre en overgang/introduksjon til R lettere om man vil den veien.Leseren står selvsagt fritt til å hoppe elegant alle verktøy som ikke er interessante. Det er klare fordeler og ulemper med alle, men forhåpentligvis vil de fleste finne et verktøy de kan bruke utvalget vi har tatt med.Der det er naturlig er kodingen inkludert slik eksempler skal kunne replikeres av leseren, men vi går ikke inn på R utover dette. Kodingen er gjengitt fortløpende der analysen er gjort. Dette gjør teksten kanskje kan fremstå som mer fragmentert, men samtidig håper vi analysene blir reproduserbare leseren. Derfor ligger også datasettene nedlastbare der det er nødvendig slik leseren selv kan se både data og kode vi har brukt. Vi har brukt R/RStudio og rmarkdown (Allaire et al. 2022) – en såkalt pakke til R – produksjonen av dette notatet. R baserer seg som sagt på bruk av ulike “pakker” som er utviklet av forskjellige utviklere og som er fritt tilgjengelig. Mange av disse har også datasett inkludert slik det er enkelt å replikere eksempler. Så langt det er mulig har vi somsagt basert oss på det vi viser som eksempler skal være replikerbare leseren.En ting man lærer seg ved å begynne å bruke R er det finnes mange måter å gjøre samme ting på. Flere pakker tilbyr samme analyser som kanskje også “Base R” også gjør, med litt forskjellig output (eller hvert fall design på output). Det er ganske sikkert (helt sikkert kan jeg vel si) erfarne R-brukere/programmerere vil finne tungvinte måter å gjøre ting på dette notatet. Vi har ikke lagt vekt på å være ekstremt konsistente - kanskje har vi steder brukt ulike måter å få fram samme ting på. Selv om dette kanskje kan forvirre en leser håper vi samtidig det kan illustrere mulighetene - og hovedpoenget, mener vi, er å få fram gode analyser som kan hjelpe oss med å svare på en problemstillinge og gi oss innsikt og forståelse av det vi ønsker å skjønne mer av. Siden dette ikke er en lærebok R, men anvendt kvantitativ analyse, har vi latt det være utgangspunktet.Vi har lite fokus på matematikk og formler den forstand vi mange steder ikke utleder dybden forklaring på ulike formler. Vi tror det er fullt mulig å ha en praktisk forståelse og anvendt bruk av kvantitative analyser uten å ha dyptgående kjennskap til de matematiske eller statistiske forklaringene. Likevel, det er heller ikke slik vi absolutt skal unngå dette. Vi har derfor inkludert noe bakgrunnskunnskap å skape en ramme rundt kjernen notatet der vi tenker det kan være interessant de som ønsker å fordype seg noe mer. Mange vil kanskje velge å hoppe lett det mer tekniske og matematiske. Det er på mange måter helt greit, men samtidig tilbyr vi også noe mer redegjørelse ulike konsepter.Vi skrev litt lenger opp denne innledningen dette ikke er en lærebok R. Det er riktig, men med en viss modifikasjon. Vi tror R er et godt alternativ (vi har jo valgt å bruke det selv…). de som likevel ønsker litt inputs på R og RStudio har vi inkludert et kapittel tidlig der vi tar endel grunnleggende ting som kan gjøre oppstarten med R litt mer myk og anvendelsen av dette notatet enklere. Man kan godt hoppe dette kapittelet, ignorere kode undervegs, og forhåpentligvis likevel tilnærme seg stoffet.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"grunnleggende-begreper-og-sammenhenger","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"Kapittel 2 Grunnleggende begreper og sammenhenger","text":"R-pakker brukt dette kapittelet:Vi skal dette kapittelet gå gjennom en rekke begreper og forhold som vi vil komme tilbake til gjennom flere ulike analyser senere, større eller mindre grad, men de er alle det vi vil kalle grunnleggende begreper vi bør ha en grad av kjennskap til.","code":"\npacman::p_load(xfun, flextable, tidyverse, officer, readxl, knitr, kableExtra, writexl, car, readxl, effects, writexl, ggpubr, tidyverse, gridExtra, nortest, knitr, kableExtra, tseries, normtest, flextable, magrittr, ISLR, olsrr, lmtest, rnorsk, qwraps2, sjPlot, sjmisc, sjlabelled, xtable, Hmisc, gt, gtsummary, sjPlot, modelsummary, table1, jtools, interactions, outliers, EnvStats, qqplotr, summarytools, caret, ggfortify)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"populasjon-og-utvalg","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.1 Populasjon og utvalg","text":"Når vi gjør undersøkelser om «et eller annet» kan vi veldig ofte ikke samle inn informasjon (data) fra alle. Om man gjør en meningsmåling før et valg å anslå utfallet av valget kan man naturligvis ikke spørre alle stemmeberettigede hele landet (+ alle stemmeberettigede som ikke er landet akkurat når man gjennomfører meningsmålingen). Det er praktisk umulig. Alle stemmeberettigede kalles denne sammenhengen populasjonen. Populasjonen er altså begrepet vi bruker på hele gruppen/den totale mengden av objekter vi ønsker å undersøke. en meningsmåling tar man derfor et utvalg fra populasjonen, spør dem, og antar man kan la resultatene fra utvalget snakke /representere hele populasjonen. Men – man kan selvsagt gjøre undersøkelser på hele populasjoner om det er praktisk mulig, det avhenger bare av hva man definerer som populasjonen.Populasjon og utvalg\nPopulasjon og utvalg.\nfiguren har vi illustrert dette. Populasjonen består av et antall (kanskje ukjent) antall individer (N). Gjennomsnittsverdien populasjonen (kalles – µ) en egenskap, som eksempel høyde, er dermed også ukjent. Derfor tar vi et utvalg individer fra populasjonen, måler dem, og kan regne ut gjennomsnittsverdien (kalles x strek, eller «x bar» på engelsk) utvalget. Så lar vi \\(\\overline{x}\\) være et estimat µ, og antar gjennomsnittet utvalget er representativt gjennomsnittet populasjonen.Det er viktig å huske på \\(\\overline{x}\\) er nettopp et estimat. Kanskje treffer vi bra, kanskje treffer vi dårlig. Hvordan vi velger ut utvalget vil derfor være viktig. kvantitativ metode opererer vi som regel med det som kalles sannsynlighetsutvalg (motsetning til strategisk utvalg som ofte brukes kvalitativ metode). Sannsynlighetsutvalg innebærer alle enhetene populasjonen har en gitt sannsynlighet å bli trukket ut utvalget. Det gjør vi innenfor visse feilmarginer kan anta det vi finner utvalget gjelder populasjonen.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"utvelgelse-fra-populasjonen","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.1.1 Utvelgelse fra populasjonen","text":"Ofte deler man måten man foretar sannsynlighetsutvalg inn fire metoder (se eksempel Grønmo (2021)):Enkel tilfeldig utvelgelse. Enhetene trekkes ut helt tilfeldig en og en fra populasjonen («random sampling»). Tilfeldig vil innebære ethvert medlem populasjonen har lik sjanse til å bli trukket ut og hvert objekt trekkes ut uavhengig av hverandre. Også dette er praksis umulig å få til perfekt, så ethvert utvalg vil trolig ha en eller annen form skjevhet («bias»).Enkel tilfeldig utvelgelse. Enhetene trekkes ut helt tilfeldig en og en fra populasjonen («random sampling»). Tilfeldig vil innebære ethvert medlem populasjonen har lik sjanse til å bli trukket ut og hvert objekt trekkes ut uavhengig av hverandre. Også dette er praksis umulig å få til perfekt, så ethvert utvalg vil trolig ha en eller annen form skjevhet («bias»).Systematisk utvelgelse. «Den første enheten utvalget trekkes tilfeldig blant de n første (eksempel de 100 første) enhetene universet. Deretter trekkes systematisk hver n’te enhet universet til utvalget. Hvis den første tilfeldig utvalgte enheten er nummer 83 universet, vil de neste enhetene utvalget være universets enheter nummer 183, 283, 383 og så videre» (Grønmo 2021).Systematisk utvelgelse. «Den første enheten utvalget trekkes tilfeldig blant de n første (eksempel de 100 første) enhetene universet. Deretter trekkes systematisk hver n’te enhet universet til utvalget. Hvis den første tilfeldig utvalgte enheten er nummer 83 universet, vil de neste enhetene utvalget være universets enheter nummer 183, 283, 383 og så videre» (Grønmo 2021).Stratifisert utvelgelse. Man deler først inn populasjonen kategorier (eller strata) før man deretter foretar et tilfeldig eller systematisk utvalg. Kategoriene kan eksempel være kjønn, alder ellerliknende).Stratifisert utvelgelse. Man deler først inn populasjonen kategorier (eller strata) før man deretter foretar et tilfeldig eller systematisk utvalg. Kategoriene kan eksempel være kjønn, alder ellerliknende).Populasjonen deles inn klynger basert på fysisk eller geografisk nærhet mellom enhetene. Deretter foretas et tilfeldig eller systematisk utvalg.Populasjonen deles inn klynger basert på fysisk eller geografisk nærhet mellom enhetene. Deretter foretas et tilfeldig eller systematisk utvalg.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"enheter-variabler-og-verdier","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.2 Enheter, variabler og verdier","text":"","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"enhet","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.2.1 Enhet","text":"En enhet er det vi forsøker å si noe om. eksempel kan et individ være en enhet. Et individ kan vi kalle en enhet på mikronivå. En organisasjon eller en gruppe individer kan også utgjøre en enhet. Dette nivået kaller vi mesonivå. tillegg kan vi ha enheter på makronivå – dette kan være samfunnsgrupper (eksempel klasser, etnisitet og religion).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"variabel","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.2.2 Variabel","text":"Egenskapene ved enheten vi ønsker å si noe om. eksempel egenskaper ved et individ.Ofte vil vi snakke om uavhengig og avhengig variabel. En uavhengig variabel (kan også kalles årsaksvariabel) er en variabel som ikke er påvirket av det vi forsøker å si noe om, men tvert imot påvirker det vi forsøker å si noe om (den avhengige variabelen).En avhengig variabel (også kalt virkningsvariabel) er en variabel som påvirkes av andre variabler (andre uavhengige variabler). Dvs. verdien på den avhengige variabelen er avhengig av verdien på en eller flere uavhengige variabler.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"verdi","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.2.3 Verdi","text":"Hvordan egenskapen måles, hvordan egenskapene ser ut.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"målenivå","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.3 Målenivå","text":"Vi måler altså verdien på variabler. Men variabler er forskjellige, det vil si vi kan måle de på ulik måte ut fra hva de representerer. eksempel måler vi alder år, temperatur grader, inntekt kroner, kjønn og utdanning ulike kategorier. De ulike måle- eller kategoriseringsmåtene gjør oss stand til å gjøre ulike ting med målingene/kategoriene. Vi kan vise dette en tabell:NivåDataForklaringOperasjonerEksempelKategoriskBinær/dikotomKun muligheter/kategorierTelle frekvenser.Til stede/ikke til stede, død/levende, ja/nei, på/avKategoriskNominellMer enn kategorier som er gjensidig utelukkende. Tallverdi er en 'merkelapp' som ikke sier noe om egenskaper.Telle frekvenser.Nasjonalitet, politiske partier, yrke, studieretningKategoriskOrdinalKategorier som kan rangeres/ordnes, men der avstanden mellom kategoriene er betydningsløs.Arrangere rekkefølge.Likertskalaer (sterkt uenig-sterkt enig), UtdanningsnivåKontinuerligeIntervallKan rangeres og man kan si noe kvantitativt om avstanden mellom verdier. Fast avstand mellom måleverdier – lik avstand på måleskalaen representerer lik avstand fenomenet som måles. Har et kunstig nullpunktAddere, subtrahere og regne ut gjennomsnittTemperatur: 10°C er dobbelt så mye som 5°C, men man kan ikke si 10°C er dobbelt så varmt som 5°C.KontinuerligeSkala/ratio/ forholdstallKan rangere, måle avstand og beregne forholdstall mellom verdier. Har et faktisk nullpunktRegne ut ratio/forholdstall og prosenterInntekt: 200000 er dobbelt så mye som 100000, og 200000 er dobbelt så stor inntekt som 100000. Samtidig er 400000 dobbelt så høy inntekt som 200000.Vi skal imidlertid merke oss et viktig poeng. Såkalte responsskalaer og Likertskalaer (Likert 1932) som brukes mye spørreundersøkelser (typiske 5 eller 7 svaralternativer langs en skala der man velger en verdi) er formelt på ordinalnivå. vil vi eksempel respondentene svare på en skala fra 1-7, der 7 er «Helt enig» og 1 er «Helt uenig» en påstand. Dette gir data som ikke er på intervallnivå (S. S. Stevens 1966). Vi kan umulig si med sikkerhet forskjellen mellom «Helt uenig» og «Uenig» er lik forskjellen mellom «Enig» og «Helt enig». Data på ordinalnivå kan man strengt tatt ikke regne ut gjennomsnitt på. Det er imidlertid svært vanlig å behandle denne typen data som intervalldata, og det finnes gode argumenter litteraturen å gjøre dette – vi går litt mer dybden det påfølgende delkapittelet. Du kan leve lenge uten å måtte gå dybden på dette, men vi velger likevel å behandle dette litt mer inngående det påfølgende delkapittelet slik dette er drøftet - så om du trenger argumenter kan du finne det der. Foreløpig nøyer vi oss med å fastslå vi kan behandle denne typen data som intervalldata. en ofte sitert bok sier Tabachnik Fidell (2007):distinction continuous discrete variables always clear. add enough digits digital clock, instance, becomes practical purposes continuous measuring device, whereas time measured analog device can also read discrete categories hours half hours. fact, continuous measurement may rendered discrete (dichotomous) loss information, specifying cutoffs continuous scale.følge Kahler et al. (2008) kan vi ikke bruke parametriske tester på slik data. Noen statistikkbøker Pallant (2010) slår ganske enkelt fast dataene skal være på minimum intervallnivå å tilfredsstille forutsetningene parametriske tester, men diskuterer ikke dette nærmere.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"forutsetninger-om-intervalldata","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.3.1 Forutsetninger om intervalldata","text":"Latente variabler (vi kommer sterkt tilbake til begrepet latente variabler såvel faktoranalyse som SEM-analyse senere kapitler, men kort fortalt er dette variabler vi ikke kan observere eller måle direkte, men som må tilnærmes gjennom andre variabler) måles ofte gjennom skalaer som måler respondentens holdninger eller oppfatninger (“semantic differnatial scales - Jamieson (2004)), jfr. Likert (1932). Slike skalaer, der respondentene velger et av flere svaralternativ som står forhold til hverandre – f.eks.”Svært uenig” - “Uenig” - “Nøytral” - “Enig” - “Svært enig” - anses produserer data på ordinalnivå ifølge kjente klassifiseringer. En ofte sitert klassifisering er S. S. Stevens (1946). Det er umulig å fastslå forskjellen mellom “Sterkt uenig” og “Uenig” er nøyaktig den samme som forskjellen mellom “Enig” og “Sterkt enig”.Stevens’ taksonomi av målenivå er grunnlaget “representational theory”. Michell (1986) påpeker “numbers used measurements represents empirical relations objects” (s.398). Dette innebærer vitenskapelige konklusjoner bør være uforanderlige ift skalaen som er brukt (konklusjoner skal ikke endres med ulike skalaer) (Marcus-Roberts Roberts 1987). Parametriske tester, denne tradisjonen, krever mulighet lineære transformasjoner, noe ordinale data ikke muliggjør (N. H. Anderson 1961). Konsekvensen er ordinale data ikke tilfredsstiller forutsetningene parametriske tester, og derfor ikke kan gjøres (L. Cohen, Manion, Morrison 2000). Dette medfører en konservativ tilnærming til hvilke tester man kan gjøre med ulike typer data som følger klassisk “measurement theory” (Michell 1986).motsetning til den konservative tilnærmingen finnes det en mer liberal tradisjon som hevder tilhengerne av den konservative tilnærmingen blander sammen “measurement theory” og “statistical theory”, og dermed misforstår når det er mulig/hensiktsmessig å kjøre parametriske tester (Gaito 1959, 1960, 1980). Savage (1957) og Gaito (1980) hevder den forbindelsen det ikke finnes noen matematisk grunn til å begrense statistiske prosedyrer til de som involverer aritmetiske operasjoner av kontinuerlige data av de observerte størrelsene. N. H. Anderson (1961) poengterer en statistisk test ikke kan være bevisst den empiriske betydningen/det empiriske innholdet av tallene man putter inn testen, mens Baker, Hardyck, Petrinovich (1966) påpeker en statistisk test svarer på spørsmålet den er ment å svare på uavhengig av om målingene er sterke eller svake – typen statistisk test er dermed uavhengig av det empiriske betydningen av dataene per se (Michell 1986). Tilhengere av denne tradisjonen mener derfor valget av type statistisk test utelukkende bør handle om statistiske vurderinger som “nothing scale type” (N. H. Anderson 1961, s.309). Det sentrale valget av type statistisk test bør heller være vurdering av dataenes distribusjon, utvalgsstørrelse, uavhengighet, bias, robusthet, kontekst og empirisk meningsfullhet (Carifio Perla 2007; Carifio Perla 2008; Hand 1996; Knapp 1990; Muthen Kaplan 1992; Pell 2005). Det er også empirisk vist skalaen på målingene liten grad påvirker variansbaserte statistiske tester Kempthorne (1955). Det er også vist parametriske tester er robuste de fleste forhold (Gardner 1975; Glass, Peckham, Sanders 1972; Norman 2010).Det finnes med andre ord gode begrunnelser både en konservativ og en liberal tilnærming til hvorvidt man kan kjøre parametriske tester på ordinale data som data fra spørreundersøkelser som bruker Likert skalaer. Det er imidlertid ingen praktisk grunn til å anta man ikke kan gjøre det – det finnes gode teoretiske og empiriske argumenter det kan gjøres, men man bør vurdere andre aspekter ved dataene også som nevnt overfor.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"modeller-og-modellering","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.4 Modeller og modellering","text":"En regresjonsanalyse lager en modell, en kjikvadrattest er en modell, og som vi kommer tilbake til delkapittelet er gjennomsnittsverdi en modell osv. Begrepet er selvsagt helt sentralt anvendt kvantitativ analyse, og kanskje spesielt maskinlæring. Derfor noen ord om modeller . En god ressurs, spesielt R brukere, er boka “Tidy Modeling R” (Kuhn Silge 2022). Innledningsvis boka beskriver Kuhn Silge (2022) modeller slik:Models mathematical tools can describe system capture relationships data given . Models can used various purposes, including predicting future events, determining difference several groups, aiding map-based visualization, discovering novel patterns data investigated, . utility model hinges ability reductive, reduce complex relationships simpler terms. primary influences data can captured mathematically useful way, relationship can expressed equation.oss er dette kjernen modeller og modellering: Ta noe komplekst, kanskje uoversiktlig, og kanskje vanskelig å tolke (eller med andre ord: slik virkeligheten som regel er) og forsøk å framstille det på en måte som gjør det lettere å forstå og lettere å bruke til å si noe om både hvordan og hvorfor ting er som de er og hvordan de vil bli framover. Tilgangen til data som kan hjelpe oss dette har eksplodert, jfr tall fra Statista (n.d.) (se Wikipedia (2021)).Volume data/information created, captured, copied, consumed worldwide 2010 2025 (zettabytes)Heldigvis oss har vi datamaskiner og programvare som gjør vi faktisk kan utnytte denne enorme datamengden. Samtidig er kanskje den største utfordringen framover å lage modeller som kan si oss noe “fornuftig” og nyttig. Med enorme datamengder og prosesseringskraft kommer også behovet (fortsatt) ha fagkunnskap/domenekunnskap - modellene skal må gi oss noe praktisk - hvert fall rammen av “anvendt kvantitativ analyse”. Modellene må hvile på teori og kunnskap om fenomenene som undersøkes, hvis ikke kan store datamengder og prosesseringskraft bli like mye til bry som til nytte.Det er vanlig å dele modeller inn tre hovedkategorier (jfr f.eks. Kuhn Silge (2022)):Beskrivende modeller (“descriptive models”). Disse modellenes formål er å beskrive og illustrere gitte karakteristika en datamengde.Inferensielle modeller/Slutningsmodeller (“inferential models”). Der beskrivende modeller skal - vel - beskrive, formålet med slutningsmodeller å undersøke spesifikke hypoteser og/eller virke som beslutningsgrunnlag (kanskje er ok ord er “beslutningsmodeller”?). Vi kan f.eks. være interessert å undersøke om en ny covid-19 vaksine er mer effektiv enn eksisterende vaksiner. Hypotesen vår vil være den er det, mens det vi vil kalle nullhypotesen er den ikke er det. Så bruker vi data å se om nullhypotesen vår kan forkastes (mer om hypotesetesting på mange stedere senere boka). Vi bruker altså en modell å se på dataene og treffe en beslutning om effektene av den nye vaksinen. Slutningsmodeller sier derfor noe om det vi “vet” (data fra forsøkene rundt den nye vaksinen) og gir oss en eller annen sikkerhet vår konklusjon.Inferensielle modeller/Slutningsmodeller (“inferential models”). Der beskrivende modeller skal - vel - beskrive, formålet med slutningsmodeller å undersøke spesifikke hypoteser og/eller virke som beslutningsgrunnlag (kanskje er ok ord er “beslutningsmodeller”?). Vi kan f.eks. være interessert å undersøke om en ny covid-19 vaksine er mer effektiv enn eksisterende vaksiner. Hypotesen vår vil være den er det, mens det vi vil kalle nullhypotesen er den ikke er det. Så bruker vi data å se om nullhypotesen vår kan forkastes (mer om hypotesetesting på mange stedere senere boka). Vi bruker altså en modell å se på dataene og treffe en beslutning om effektene av den nye vaksinen. Slutningsmodeller sier derfor noe om det vi “vet” (data fra forsøkene rundt den nye vaksinen) og gir oss en eller annen sikkerhet vår konklusjon.Prediktive modeller. bruker vi data å lage modeller som skal kunne si oss noe om hvilke verdier tilsvarende data med en eller annen form sikkerhet vil være framtiden. Et apotek kan bruke salgsdata en gitt medisin (og tilhørende data som ikke er rene salgstall, men som kan være med på å forklare salgstallene) til å predikere hvor store mengder av den gitte medisin de skal bestille før sommersesongen. Dette gjøres som regel ved å “trene opp” en modell på en del av de foreliggende salgstallene og deretter teste modellen på en annen del av de samme salgstallene. På den måten kan man anslå hvor god modellen er (= hvor stor feilmargin vil prediksjonene gjennomsnitt ha = hvor nære observerte verdier er de predikerte verdiene). En modell kan enten underestimere eller overestimere antallet bokser av den respektive medisinen man bestiller. Prediktive modeller komemr vi sterkt tilbake til kapittelet om maskinlæring.Prediktive modeller. bruker vi data å lage modeller som skal kunne si oss noe om hvilke verdier tilsvarende data med en eller annen form sikkerhet vil være framtiden. Et apotek kan bruke salgsdata en gitt medisin (og tilhørende data som ikke er rene salgstall, men som kan være med på å forklare salgstallene) til å predikere hvor store mengder av den gitte medisin de skal bestille før sommersesongen. Dette gjøres som regel ved å “trene opp” en modell på en del av de foreliggende salgstallene og deretter teste modellen på en annen del av de samme salgstallene. På den måten kan man anslå hvor god modellen er (= hvor stor feilmargin vil prediksjonene gjennomsnitt ha = hvor nære observerte verdier er de predikerte verdiene). En modell kan enten underestimere eller overestimere antallet bokser av den respektive medisinen man bestiller. Prediktive modeller komemr vi sterkt tilbake til kapittelet om maskinlæring.Om en modell er beskrivende, inferensiell eller prediktiv kommer mange sammenhenger på hvordan den brukes. En regresjonsanalyse kan f.eks. være alle tre (jfr. Kuhn Silge (2022)) - men ikke alle modeller er slik.","code":"#> `geom_smooth()` using formula 'y ~ x'"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"gjennomsnitt-som-modell","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.4.1 Gjennomsnitt som modell","text":"En mer utførlig forklaring kan finnes f.eks. Miles Shevlin (2001).Hvis vi tenker oss vi har en gruppe på 100 personer vi ikke kjenner, men der vi vet gjennomsnittshøyden er 175 cm – hvor høy vil du gjette en tilfeldig person den gruppa er? kjenner vi kun gjennomsnittshøyden – hva vi kan kalle en parameter. Hvis vi ikke kjenner noen andre karakteristika, vil vår beste antakelse være 175 cm. Dette er gjennomsnittshøyden, og det er rimelig å anta vi vil treffe nærmest hvis vi sier 175 cm hver gang vi blir spurt om hvor høy vi tror en tilfeldig person den gruppa er. Vi kan selvsagt gjette 182 cm første gang, 168 cm andre gang, 171 cm tredje gang osv. og treffe 100%, men det vil være ren flaks. Første person kan være 163 cm, andre person 190 cm, tredje person 182 cm osv. Hver gang vil vi så fall bomme grovt. Gjennomsnittsverdien blir vår “modell”. En modell er en representasjon av virkeligheten (Miles Shevlin 2001). Modellen vil aldri være perfekt – hadde den vært prefekt hadde vi ikke hatt en modell av virkeligheten, men et duplikat av virkeligheten – det vil alltid være feil ved modellen en eller annen grad.vårt eksempel er modellen gjennomsnittshøyden, og vi vet selv om vår beste gjetning når vi blir spurt om høyden på en tilfeldig person er 175 cm vil vi oppleve av vi kun treffer noen få (og kanskje ingen) tilfeller. Modellen vår vil imidlertid søke å minimere feilene vi får slik vi treffer best mulig. Det er mer sannsynlig en tilfeldig person er nærheten av 175 cm enn 195 cm. Vi kan si:\\(Virkeligheten = Modell + Feil\\)Imidlertid vil vi ofte ikke kjenne “virkeligheten” - vi kjenner ikke populasjonsgjennomsnittet. Vi har som regel data om populasjonen fra et tilfeldig utvalg gjort av populasjonen. Vi vil derfor heller uttrykke:\\(Data = Modell + Feil\\)Data (altså en observert verdi – vårt tilfelle en høyde) = \\(x\\). Gjennomsnittsverdi = \\(\\overline{x}\\). Det er vanlig å benevne feiltermen som \\(e\\). Vi kan derfor første observerte høydeverdi uttrykke dette matematisk som:\\(x_1 = x - e_1\\)Eller på en generell form:\\(x_i = \\overline{x} - e_i\\)Feil en modell vil ofte betegnes residual (fra engelsk: residual = rest/gjenværende). En residual er altså verdien vi sitter igjen med når vi trekker gjennomsnittsverdien fra den observerte verdien. Hvis den observerte høyden er 179 cm og gjennomsnittsverdien er 175 cm er residualen 3 cm. Dette kan vi uttrykke slik:\\(Feil = Data - Modell\\)Eller:\\(e = x - \\overline{x}\\)Residualbegrepet kommer vi sterkt tilbake til når vi skal ta oss regresjonsanalyse.Vi kommer til å snakke mye om modeller. Vi kommer også til å snakke mye om “hvor god er modellen” - eller med andre ord: har vi klart å lage en modell som representerer “virkeligheten” på en god måte. Vi skal imidlertid huske på “models wrong, useful” (Box 1976). En statistisk modell vil aldri klare å representere den komplekse virkeligheten - vi må alltid forsøke å finne måter å måle og representere enkelte deler av virkeligeheten og lage modeller som kan fortelle oss noe fornuftig og nyttig om denne virkeligheten. Men selv om modellen alltid er feil og imperfekt kan den fortsatt være nyttig.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordeling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.5 Normalfordeling","text":"Når vi snakker om distribusjonen av et datasett tenker vi på hvordan dataene vi har samlet inn fordeler seg forhold til hverandre etter gitte egenskaper. Vi kan eksempel ha målt høyden på 100 mennesker.Dataene kan vi sette inn et histogram å se på fordelingen.\nFigure 2.1: Høydefordeling 100 tilfeldige menn, genererte data\nHvis vi tar et utvalg på 100 andre personer kan fordelingen se slik ut:\nFigure 2.2: Høydefordeling 100 andre tilfeldige menn, genererte data\nHver gang vi måler høyden på 100 tilfeldig utvalgte menn vil fordelingen se ulik ut siden de er observerte fordelinger et utvalg av populasjonen (alle) «norske menn». Hvis vi imidlertid økte antallet utvalget vi målte til 1000 eller 10000 vil vi med større sikkerhet kunne si vi faktisk viser populasjonens fordeling (mulighetene vi tilfeldigvis måler 10000 veldig lave eller veldig høye menn er svært liten). Vi kan derfor, gitt visse forutsetninger om utvalget, si noe om hele populasjonen ut fra utvalget.Hittil har vi snakket om observerte fordelinger – altså hva vi har målt, observert, samlet inn osv. Ut fra dette kan vi si vi kan ha visse forventninger til hvordan fordelingen av ulike populasjoner vil se ut, og vi kan snakke om teoretiske fordelinger – eller sannsynlighetsfordelinger med andre ord. Hvor sannsynlig er det en tilfeldig x-verdi dukker opp dataene?høyde kan vi ha visse forventninger til hvilke sannsynligheter det er en tilfeldig person har en gitt høyde, eller hvor mange prosent av den mannlige befolkningen som har en høyde innenfor et gitt intervall. Det vil si fordelingen har en viss form med visse karakteristika. Vi forventer flest observasjoner befinner seg nærheten av gjennomsnittet, og vi vil se færre og færre observasjoner jo lenger unna gjennomsnittet vi beveger oss. Vi forventer å finne flere norske menn 20 år på rundt 180 cm enn 160 cm eller 210 cm. fordelingen av høydedata vil vi si dette er data som er normalfordelte.En normalfordeling er en sannsynlighetsfunksjon der flesteparten av verdiene fra funksjonen samler seg om en sentral tendens, og der tettheten (hyppigheten, eller “density” på engelsk) av verdier avtar jevnt jo lenger unna den sentrale tendensen man kommer. Grafisk framstilt får fordelingskurven en klokkeform, og normalfordeling omtales også som “bell shaped”. Overraskende mange fenomener viser seg å være nærme en normalfordeling, og den er derfor en helt sentral teoretisk sannsynlighetsfordeling mange sammenhenger kvantitativ metode. Vi bruker dermed normalfordelingen som en modell observerte data.Vi skal ikke bry oss om det matematisk uttrykket sannsynlighetstetthetsfunksjonen. Hvis vi derimot genererer et tenkt datasett etter standard normalfordelingsfunksjon vil det kunne se slik ut:\nFigure 2.3: Genererte standard normalfordelte data\nkan vi legge på en forventningskurve – en teoretisk kurve som viser en standard normalfordeling:\nFigure 2.4: Genererte standard normalfordelte data med normalfordelingskurve\nVi kan ta bort det genererte datasettet og sitte igjen med bare forventningskurven:\nFigure 2.5: Normalfordelingskurve\nDet den standardiserte normalfordelingskurven (også kjent som Gausskurven eller også Bellkurven – “Klokkekurven” - fordi den har en klokkeform) – kan brukes til er å si noe om spredningen på forventede verdier – eller hvor langt fra gjennomsnittsverdien man kan forvente å finne de enkelte verdiene.Før vi ser nærmere på egenskaper ved normalfordelingskurven kan det være nødvendig å gå litt inn på begrepene varians og standardavvik som mål på spredningen datasett. Disse begrepene, spesielt standardavvik, vil være helt sentrale videre arbeid med temaet.","code":"\nset.seed(30)\nx <- rnorm(100, 179, 16)\ny <- rep(\"Mann\", each = 100)\nhoyde <- bind_cols( y, x)\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\ncolnames(hoyde) <- c(\"Kjonn\", \"Hoyde\")\np <- ggplot(hoyde, aes(x = Hoyde)) + \n  geom_histogram(color = \"black\", fill = \"lightblue\", bins = 10) + \n    theme_bw() + \n    ylab(\"Antall\")\np\n\ndata1 <- sample(165:175, 50, replace = TRUE)\ndata2 <- sample(170:180, 30, replace = TRUE)\ndata3 <- sample(180:185, 15, replace = TRUE)\ndata4 <- sample(185:190, 5, replace = TRUE)\n\ndata <- as_tibble(c(data1, data2, data3, data4))\ny <- rep(\"Mann\",each=100)\nhoyde2 <- bind_cols( y, data)\ncolnames(hoyde2) <- c(\"Kjonn\", \"Hoyde\")\n\np2 <- ggplot(hoyde2, aes(x = Hoyde)) + \n  geom_histogram(color = \"black\", fill = \"lightblue\", bins = 10) +     theme_bw() + \n    ylab(\"Antall\")\np2\n\nset.seed(321)\n\ndata2 <- as_tibble(rnorm(10000, mean = 0, sd = 1))\ny <- rep(\"y\",each=10000)\nnormalfordeling <- bind_cols( y, data2)\ncolnames(normalfordeling) <- c(\"f(x)\", \"x\")\n\np3 <- ggplot(normalfordeling, aes(x = x)) + \n  geom_histogram(color=\"black\", fill=\"lightblue\", bins = 20) + \n    theme_bw() + \n    ylab(\"f(x)\")\np3\np4 <- ggplot(normalfordeling, aes(x)) + \n    geom_histogram(aes(y = ..density..), fill='lightblue', col='black') + \n    stat_function(fun = dnorm, \n                  args = list(mean=mean(normalfordeling$x), \n                              sd=sd(normalfordeling$x))) + \n    theme_bw()\np4\n\nggplot(data.frame(x = c(-4, 4)), aes(x)) +\n  geom_function(fun = dnorm, colour = \"darkblue\", size = 1.5) +\n  theme_classic() +\n  scale_y_continuous(limits = c(0, 0.5), \n                     breaks = seq(0, 0.5, by = 0.1))"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"varians-og-standardavvik","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.6 Varians og standardavvik","text":"Variansen en variabel representerer det gjennomsnittlige avviket fra gjennomsnittsverdien (Field, Miles, Field 2012a) og er et mål på spredningen dataene (som navnet antyder: hvor mye dataene variere ut fra den sentrale tendensen). vises et eksempel basert på Field (2009b).La oss anta vi har spurt 5 studenter på høgskolen hvor mange kjæledyr de har. Svarene kan settes opp en enkel tabell. gjennomsnitt har de 2,6 kjæledyr. Vi ønsker imidlertid å se hvor mye avviket er den enkelte fra snittet (siden vi har regnet ut snittet kan vi se på gjennomsnittsverdien som en modell på forholdet mellom studenter og antall kjæledyr). Vi registrerer svarene vi fikk et skjema:StudentnrAntallAvvikAvvik_kvadrert11-1.62.5622-0.60.36330.40.16430.40.16541.41.96Snitt2.6Sum05.20Når vi regner ut avviket (sum deviances) summerer vi avvikene. Siden denne er 0 skulle det innebære det totalt sett “modellen” ikke er avvik mellom modellen og våre virkelige observasjoner. Problemet er det er både positive og negative avvik som nuller hverandre ut. Man må derfor kvadrere avvikene å omgå problemet med fortegn. Imidlertid får vi et nytt problem. La oss anta vi stedet 5 studenter har spurt 500. Da får vi et svært høyt kvadrert avvik fra snitt. Altså – vi må ta høyde antallet observasjoner. Vi deler derfor sum kvadrert avvik fra snitt på antall observasjoner (5,20/5). MEN: vi må foreta et litt teknisk og komplisert tillegg utregningen. Vi må dele på antall observasjoner MINUS 1 (som er antallet frihetsgrader – degrees freedom). Dette vil ikke bli nærmere forklart , men de som ønsker å lese mer om frihetsgrader kan prøve noen andre kilder, f.eks. Walker (1940), Good (1973) eller Pandey Bright (2008). Vi ender altså opp med regnestykket 5,20/(5-1) = 1,3. Dette er variansen. Variansen er altså det gjennomsnittlige avviket mellom gjennomsnittsverdien av de observerte dataene og verdiene til de enkelte observasjonene.Som regel snakker vi imidlertid om standardavviket. Dette finner vi ved å ta kvadratroten av variansen (som vi jo har funnet ved å kvadrere avvikene å unngå fortegnsproblemer). Vi får da vårt tilfelle et standardavvik på 1,14. Variansen og standardavviket forteller oss altså noe om spredningen dataene. Liten varians betyr spredningen er liten (om vi har gjennomført en spørreundersøkelse betyr det respondentene har svart ganske likt). Stor varians betyr stor spredning (respondentene har svart ganske ulikt).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordeling-standardavvik-og-forventninger","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7 Normalfordeling, standardavvik og forventninger","text":"Vi kan nå se nærmere på normalfordelingen.\nFigure 2.6: Normalfordeling med 1 standardavvik\nEtt standardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 68 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan vise det samme 2 og 3 standardavvik:\nFigure 2.7: Normalfordeling med 2 standardavvik\nstandardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 95 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan finne arealet mellom x=-2 og x=2, som er 0.95449971.\nFigure 2.8: Normalfordeling med 3 standardavvik\nTre standardavvik “og ” 0 (= det skraverte området grafen ) innebærer et normalfordelt datasett vil 99.7 % av tilfeldig valgte x-verdier befinner seg dette intervallet. Vi kan finne arealet mellom x=-3 og x=3, som er 0.99730022. Dette utgjør et kjernepunkt statistisk prosesskontroll som vi vil komme mye tilbake til.Oppsummert kan vi framstille normalfodeling og standardavvik slik (Hartmann, Krois, Waske 2018b):\nFigure 2.9: Normalfordeling med standardavvik\nSom nevnt er mange fenomener hverdagen normalfordelte, eller nærme nok normalfordeling til vi kan bruke normalfordeling som teoretisk modell observerte data 3. Det finnes imidlertid mange tilfeller der vi ikke kan bruke normalfordelingen. Hvis dataene er sterkt asymmetriske vil ikke reglene normalfordeling som vi har skissert ovenfor gjelde 4.","code":"\nx <- seq(-4, 4, length = 200)\ny <- dnorm(x)\nplot(x, y, \n     type = \"l\", \n     lty = 1, \n     lwd = 2, \n     col = \"red\", \n     xlab = \"x\",\n     ylab = \"f(x)\")\nx <- seq(-1, 1, length = 100)\ny <- dnorm(x)\npolygon(c(-1, x, 1),c(0, y, 0),col = \"lightblue\")\nabline(v = -1, col = \"green\", lwd = 2)\ntext(1.3, 0.38, \"1 SD\", col = \"black\")\ntext(-1.35, 0.38, \"-1 SD\", col = \"black\")\nabline(v = 1, col = \"green\", lwd = 2)\ntext(0, 0.2, \"68 %\", col = \"black\")\nx <- seq(-4,4, length = 200)\ny <- dnorm(x)\nplot(x, y, \n     type = \"l\", \n     lty = 1, \n     lwd = 2, \n     col = \"red\", \n     xlab=\"x\",\n     ylab=\"f(x)\")\nx <- seq(-2, 2, length = 200)\ny <- dnorm(x)\npolygon(c(-2, x, 2),c(0, y, 0),col = \"lightblue\")\nabline(v = -2, col = \"green\", lwd = 2)\ntext(2.3, 0.38, \"2 SD\", col = \"black\")\ntext(-2.35, 0.38, \"-2 SD\", col = \"black\")\nabline(v = 2, col = \"green\", lwd = 2)\ntext(0, 0.2, \"95 %\", col = \"black\")\nx <- seq(-4, 4, length = 200)\ny <- dnorm(x)\nplot(x, y, \n     type = \"l\",\n     lwd = 2,\n     col = \"red\", \n     xlab = \"x\",\n     ylab=\"f(x)\")\nx <- seq(-3, 3, length = 200)\ny <- dnorm(x)\npolygon(c(-3, x, 3),c(0, y, 0), col = \"lightblue\")\nabline(v = -3, col = \"green\", lwd = 2)\ntext(3.3, 0.38, \"3 SD\", col = \"black\")\ntext(-3.35, 0.38, \"-3 SD\", col = \"black\")\nabline(v = 3, col =\"green\", lwd = 2)\ntext(0, 0.2, \"99.7 %\", col = \"black\")\ny.norm <- rnorm(n = 100000, mean = 0, sd = 1)\nh <- hist(y.norm, breaks = 100, plot = F)\ncuts <- cut(h$breaks, c(-Inf, -3, -2, -1, 1, 2, 3, Inf), right = F)\nplot(h, \n     col = rep(c(\"white\", \"4\", \"3\", \"2\", \"3\", \"4\", \"white\"))[cuts], \n     main = 'Normalfordeling', \n     xlab = '', \n     freq = F, \n     ylim = c(0,0.6))\nlwd = 3\n# Horisontale linjer\nlines(x = c(2,-2), y = c(0.48,0.48), type = \"l\", col=3, lwd = lwd)\nlines(x = c(3,-3), y = c(0.55,0.55), type = \"l\", col=4, lwd = lwd)\nlines(x = c(1,-1), y = c(0.41,0.41), type = \"l\", col=2, lwd = lwd)\n# Vertikale linjer\nlines(x = c(1,1), y = c(0,0.41), type = \"l\", col=2, lwd = lwd)\nlines(x = c(-1,-1), y = c(0,0.41), type = \"l\", col=2, lwd = lwd)\nlines(x = c(2,2), y = c(0,0.48), type = \"l\", col=3, lwd = lwd)\nlines(x = c(-2,-2), y = c(0,0.48), type = \"l\", col=3, lwd = lwd)\nlines(x = c(3,3), y = c(0,0.55), type = \"l\", col=4, lwd = lwd)\nlines(x = c(-3,-3), y = c(0,0.55), type = \"l\", col=4, lwd = lwd)\n# tekst\ntext(0, 0.44, \"68%\", cex = 1.5, col=2)\ntext(0, 0.51, \"95%\", cex = 1.5, col=3)\ntext(0, 0.58, \"99.7%\", cex = 1.5, col=4)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"er-dataene-dine-normalfordelte","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1 Er dataene dine normalfordelte?","text":"Vi vil se senere mange tester har en forutsetning om dataene er normalfordelte. Ulike analyser vi gjør statistisk har forutsetninger/bygger på antalkelser om hvordan dataene er fordelt (det gjelder forsåvidt alle statistiske analyser vi gjør). Dvs. mange sammenhenger må de være “tilnærmet” normalfordelte. Mange tester er såkalt “robuste”, altså de tåler avvik fra en nærmest perfekt normalfordeling uten resultatene av testen nødvendigvis blir uåpålitelige. Det er rimelig å si det er tildels stor uenighet om hvor alvorlig avvik fra normalfordelingens teoretiske forventning man kan være likevel å bruke ulike statistiske analyser. Vi skal også være klar utregning av såkalte testverdier (som Shapiro-Wilk eller Anderson-Darling) også bygger på visse forutsetninger.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"histogram","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.1 Histogram","text":"\nFigure 2.10: Høydefordeling 100 tilfeldige menn, genererte data\n","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"qq-plott","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2 QQ-plott","text":"Q-Q plottet (“quantile-quantile plot”) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet (se video forklaring på utregning) innebærer å se distribusjoner mot hverandre – empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om “normal Q-Q plott - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). eksempelet vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med.viser vi typiske mønstre histogram og “tilhørende” qq-plott som kan være til hjelp tolkning av dataene dine. Dette er genererte tall og ikke tallene fra eksempelet :","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"normalfordelt","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.1 Normalfordelt","text":"Download QQ_norm.xlsx\nFigure 2.11: Q-Q plott normalfordeling\nVi ser dette Q-Q plottet viser oss vi kan være ganske sikre på dette datasettet er normalfordelt (noe som gir meninig siden vi har brukt R til å lage et normalfordelt datasett).","code":"\nset.seed(89)\nqqnorm <- as_tibble(rnorm(10000, mean=90, sd=5))\nggqqplot(qqnorm$value) +\n    ggtitle(\"Normal Q-Q plott\") +\n    labs(x = \"Teoretisk forventning\", y = \"Data\")"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"skjevhet-høyre","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.2 Skjevhet høyre","text":"Download QQ_norm_rs.xlsx\nFigure 2.12: Q-Q plott - fordeling skjevhet høyre\net datasett med høyreskjevhet vil ofte Q-Q plottet vise en bananform med bunnen/midten av bananen ned mot høyre hjørne og endene pekende oppover/utover fra den rette linjen.","code":"\n# Lage datasett med right skew\nset.seed(90)\nN <- 5000\nqqrightskew <- as_tibble(rnbinom(N, 10, .1))\nqqrighthist <- ggplot(qqrightskew, aes(x=value)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqrightskew_plott <- ggqqplot(qqrightskew$value) + \n    ggtitle(\"Normal Q-Q plott - skjevhet høyre\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqrighthist, qqrightskew_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"skjevhet-venstre","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.3 Skjevhet venstre","text":"Download QQ_norm_ls.xlsx\nFigure 2.13: Q-Q plott - fordeling skjevhet venstre\ndette datasettet har vi generert en kraftig skjevhet til venstre. Q-Q plottet får da en omvendt bananform forhold til høyre skjevhet, altså en topp på midten og ender som svinger nedover ift den rette linja.","code":"\n# Lage datasett med left skew\nset.seed(91)\nN=5000\nqqleftskew <- as_tibble(rbeta(N,5,1,ncp=0))\n# Plotte histogram og Q-Q plott\nqqlefthist <- ggplot(qqleftskew, aes(x=value)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqleftskew_plott <- ggqqplot(qqleftskew$value) + \n    ggtitle(\"Normal Q-Q plott - skjevhet venstre\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqlefthist, qqleftskew_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"tunge-haler","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.4 Tunge haler","text":"Download QQ_ht.xlsx\nFigure 2.14: Q-Q plott - ‘heavy-tail’\n“Heavy-tailed” (fete/tunge haler) har større sannsynlighet ekstreme verdier vil forekomme). Fordelinger med tunge haler vil ofte følge en slags S-form, men den er ofte mer “liggende” enn S-formen til fordeling med lette haler. Den starter med å vokse raskere enn normalfordelingen og ender med å vokse saktere.","code":"\nset.seed(14)\nN=100\nqqcauchy <- as_tibble(rcauchy(N, scale = 5)) \nqqcauchyhist <- ggplot(qqcauchy, aes(x=value)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqcauchy_plott <- ggqqplot(qqcauchy$value) + \n    ggtitle(\"Normal Q-Q plott - tung hale\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqcauchyhist, qqcauchy_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"lette-haler","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.5 Lette haler","text":"Download QQ_lt.xlsx\nFigure 2.15: Q-Q plott - ‘light-tail’\n“Light-tailed” (lette haler) har liten sannsynlighet ekstreme verdier og utvalg tenderer til å ikke fravike gjennomsnittet med mye. Q-Q plottet en fordeling med lette haler har ofte en S-form. Dataene vokser saktere enn normalfordelingen starten før den følger vekstraten til normalfordelingen. Mot slutten vokser den raskere enn normalfordelingen. Derfor bøyer den av fra normalfordelingen.","code":"\nset.seed(81)\nqqlt <- as_tibble(runif(n = 1000, min = -1, max = 1))\nqqlthist <- ggplot(qqlt, aes(x=value)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqlt_plott <- ggqqplot(qqlt$value) + \n    ggtitle(\"Normal Q-Q plott - lett hale\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqlthist, qqlt_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"bimodalitet","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.6 Bimodalitet","text":"Download QQ_bimod.xlsx\nFigure 2.16: Q-Q plott - bimodal\nDen bimodiale fordelingen viser ofte et brudd eller et distinkt knekkpunkt rundt krysning av den rette linja, med en del av linja på hver side av den rette linja.","code":"\nset.seed(10) \nmode1 <- rnorm(50,2,1)\nmode1 <- mode1[mode1 > 0] \nmode2 <- rnorm(50,6,1)\nmode2 <- mode2[mode2 > 0] \nqqbimod <- as_tibble(sort(c(mode1,mode2)))\nqqbimodhist <- ggplot(qqbimod, aes(x=value)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqbimod_plott <- ggqqplot(qqbimod$value) + \n    ggtitle(\"Normal Q-Q plott - bimodial\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqbimodhist, qqbimod_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"multimodalitet","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.2.7 Multimodalitet","text":"Download QQ_multimod.xlsx\nFigure 2.17: Q-Q plott - multimodal\nMultimodale fordelinger vil som regel vise flere brudd.","code":"\nqqmultimod <- as_tibble(read_xlsx(\"Multimodal.xlsx\"))\nqqmultimodhist <- ggplot(qqmultimod, aes(x=Verdi)) + \n    geom_histogram(color=\"black\", \n                   fill=\"lightblue\") +\n    theme_bw()\nqqmultimod_plott <- ggqqplot(qqmultimod$Verdi) + \n    ggtitle(\"Normal Q-Q plott - multimodial\") + \n    labs(x = \"Teoretisk forventning\", y = \"Data\")\ngrid.arrange(qqmultimodhist, qqmultimod_plott, ncol=2)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistiske-tester-for-vurdering-av-dataenes-distribusjon","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.3 Statistiske tester for vurdering av dataenes distribusjon","text":"Vi har nå sett på noen typiske eksempler på mønstre Q-Q plott. Det kan imidlertid være vanskelig å bedømme fordelinger som ligger nære normalfordelingen, men likevel ikke perfekt oppå (du vil trolig aldri se en perfekt match med mindre du har generert et normalfordelt datasett med mange datapunkter). Vi kan supplere Q-Q plottene med visse statistiske tester (men husk: disse statistiske testene har sine egne forutsetninger og er heller ikke uten utfordringer).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"anderson-darling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.3.1 Anderson-Darling","text":"Anderson-Darlings test er en test å se om et datasett kommer fra en gitt fordeling, f.eks. normalfordelingen (T. W. Anderson Darling 1952; T. W. Anderson Darling 1954). Testen setter opp hypoteser:\\(H_0\\): Dataene følger normalfordelingen\\(H_1\\): Dataene følger ikke normalfordelingenDownload Anderson-Darling_raw.xlsxSiden vi vet nullhypotesen er datasettet har en normalfordeling vil vi forkaste nullhypotesen dersom vi har en signifikant p-verdi (grensen hva som er signifikant bestemmer vi forsåvidt selv, men vanlige verdier er 0.01, 0.05 og 0.1). Altså - dette tilfellet har vi en p-verdi=0.04. Vi forkaster derfor nullhypotesen og aksepterer \\(H_1\\) som sier dataene er trolig ikke er normalfordelte (med andre ord: p-verdien må være større enn signifikansverdien vi skal si dataene trolig er normalfordelte) - en huskeregel: “p low, null must go” (: “low” = terskelverdien vi har satt, ofte 0.05).Generisk ser dette slik ut (Hartmann, Krois, Waske 2018a):Nyllhypoteser - aksept/forkastsonerDet er verdt å merke seg Anderson-Darling testen egentlig ikke forteller deg dataene dine er normalfordelte, men det er usannsynlig de ikke er det om testen viser det. Dette synes kanskje som samme sak, men er realiteten en viktig erkjennelse – en tørr gressplen er et bevis det ikke har regnet, men en våt gressplen er ikke bevis det har regnet. En våt gressplen kan skyldes andre ting enn regn. Altså – en signifikant p-verdi på testen gjør vi forkaster \\(H_0\\) og antar fordelingen er ikke-normal. En ikke-signifikant p-verdi på gjør vi med f.eks. 95% konfidens kan si vi ikke har funnet avvik fra normalfordelingen.Det finnes flere andre statistiske tester som kan kjøres å teste normalitet, f.eks. Kolmogorov-Smirnov, Shapiro-Wilks og Cramer Von-Mises test. Anderson-Darling er en modifisering/videreutvikling av Kolmogorov-Smirnov og anses ofte som en bedre test av de . Andre kilder (se f.eks. Razali Wah (2011)) finner Shapiro-Wilks presterer best 10 000 simuleringer på ulike distribusjoner.Tolkning Kolmogorov-Smirnov: Hvis p-verdien er valgte signifikansnivå (f.eks. 0.05) skal vi anta datasettet ikke er normalfordelt. vil testen peke på datasettet ikke er normalfordelt.Tolkning av Shapiro-Wilks og Cramer-von Mieses test er lik som Kolmogorov-Smirnov.Som et siste eksempel på en statistisk test normalitet kan vi bruke Jarque-Bera test. Denne skiller seg litt ut fra de andre ved den spesifikt ser på skjevhet og kurtosis datasettet opp mot hva en normalfordeling vil ha. å gjøre lykken komplett finnes det versjoner av testen:Tolkningen er lik som før - hvis p-verdien er mindre enn valgte signifikansnivå peker det mot datasettet ikke er normalfordelt. , motsetning til de øvrige testene, er p-verdien større enn signifikansnivået (0,05) så det peker mot datasettet er normalfordelt.Dette er altså ikke så enkelt. Det finnes mange statistiske tester, som kan gi motsatte indikasjoner på om et datasett er normalfordelt eller ikke siden de ser på dataene fra “ulik vinkel” (fokuserer på ulike aspekter ved dataene). Vårt råd blir: Start alltid med Q-Q plott. Velg evt en teststatistikk, men vær klar alle teststatistikker bygger på forutsetninger eller tester ulike sider av distribusjonen. Det vi også kan huske på er henhold til sentralgrenseteoremet (“Central Limit Theorem”) vil populasjonens fordeling være av mindre interesse dersom utvalgsstørrelsen er stor nok. Hva er stor nok? De fleste kilder peker mot 30 er “stort nok”.","code":"\naddata <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\nad.test(addata$Values)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  addata$Values\n#> A = 0.74573, p-value = 0.04046\naddata5 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\nks.test(addata5, \"pnorm\")\n#> \n#>  Exact one-sample Kolmogorov-Smirnov test\n#> \n#> data:  addata5\n#> D = 0.88493, p-value = 0.0000000000000171\n#> alternative hypothesis: two-sided\nshapiro.test(addata5$Values)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  addata5$Values\n#> W = 0.87521, p-value = 0.04027\ncvm.test(addata$Values)\n#> \n#>  Cramer-von Mises normality test\n#> \n#> data:  addata$Values\n#> W = 0.12634, p-value = 0.04326\naddata6 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\njarque.bera.test(addata6$Values)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  addata6$Values\n#> X-squared = 2.1953, df = 2, p-value = 0.3337\najb.norm.test(addata6$Values, nrepl=2000)\n#> \n#>  Adjusted Jarque-Bera test for normality\n#> \n#> data:  addata6$Values\n#> AJB = 3.1014, p-value = 0.131"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"boxplott","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.4 Boxplott","text":"\nFigure 2.18: Boksplott variablene Adverts og Sales\nVi ser vi får først ut en liste uteliggerne identifisert ved id/case/observasjonsnummer (43, 87 og 184) variabelen Adverts. Dette vises også som tre små sirkler boxplottet.Et boxplott forteller oss mye om dataenes distribusjon.Selve boksen representerer 50 % av observasjonene/casene, det vil si nedre kant representerer første kvartil (= 25.prosentil) og øvre kant tredje kvartil (= 75.prosentil).Den tykkere horisontale streken boksen viser medianverdien (= andre kvartil = 50.prosentil)Dersom en observasjon ligger utenfor en terkselverdi (jfr figur ) vises dette med en liten sirkel. Dette defineres som uteliggere. Å identifisere uteliggere kan være viktig mange statistiske tester.Galarnyk (2018) illustrerer boxplott slik:Illustrasjon av boksplott","code":"\npar(mfrow=(c(1,2)))\nBoxplot(Field_OLS_data$Adverts, \n        id = list(n=Inf), \n        ylab = \"\", \n        main = \"Adverts\", \n        col = \"Blue\")\n#> [1]  43  87 184\nBoxplot(Field_OLS_data$Sales, \n        id = list(n=Inf), \n        ylab = \"\", \n        main = \"Sales\", \n        col = \"Green\")"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"scatterplott","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.7.1.5 Scatterplott","text":"\nFigure 2.19: Eksempel scatterplott\nEt scatterplott viser oss på en god visuell måte hvordan de variablene forholder seg til hverandre (vi plotter hver enkelt observasjon gjennom verdiene de har på de variablene). Mønsteret kan derfor si oss mye om sammenhengen mellom de .En god måte å fremstille et scatterplott på R (gjennom pakken car) er denne:\nFigure 2.20: Eksempel scatterplott fra pakken ‘car’\nkombinerer vi scatterplott og boxplott. Den rette blå linja er en minste kvadratssums regresjonslinje (OLS). Den stiplede blå linja bruker en ikke-parametrisk tilnærming. tillegg får vi visualisert de fire mest ekstreme tilfellene (lengst vekk fra gjennomsnitt).","code":"\nggplot(Field_OLS_data, aes(x = Adverts, y = Sales)) +\n  geom_point(colour = \"red\") +\n  theme_bw()\nscatterplot(Adverts ~ Sales, \n            data = Field_OLS_data, \n            id = list(n=4))#> [1]   1  87 169 184"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"binomialfordeling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.8 Binomialfordeling","text":"En distribusjon hvor det kun er mulige utfall av en hendelse kalles en binomial fordeling. Et myntkast er en slik hendelse (gitt vi ser bort fra den fysiske muligheten mynten kan lande stående på høykant). Levende eller død kan også være et eksempel på dette. Det ene utfallet utelukker det andre, men de er uavhengige fordi resultatet ett myntkast ikke påvirker resultatet neste myntkast. Alle myntkastene må derimot være identiske, det vil si sannsynligheten det ene eller det andre resultatet er lik hver gang forsøket eller myntkastet gjennomføres. Hvis vi har lik sannsynlighet, kan en tilfeldig generert binomial distribusjon se slik ut:\nFigure 2.21: Binomialfordeling med lik sannsynlighet\ndiagrammet vises en sannsynlighetsfordeling en binomial fordeling der utfallene suksess/fiasko har lik sannsynlighet. Hvis vi gjennomfører en aktivitet med disse karakteristika 20 ganger kan vi bruke sannsynlighetsfordelingen til å skape en forventning om sannsynligheten antall suksesser/fiaskoer. Hver gang vi gjennomfører aktiviteten blir det enten suksess eller fiasko. Hvis vi har 50% sjanse suksess eller feil hver gang vi gjennomfører aktiviteten er sannsynligheten suksess lik som sannsynligheten fiasko. Vi kan da forvente det er størst sannsynlighet vi 10 av 20 tilfeller får suksess. Det er liten sannsynlighet vi enten får suksess 0 eller 20 av 20 ganger vi gjør aktiviteten.Det er imidlertid verdt å merke seg de utfallene ikke trenger å ha lik sannsynlighet. Da vil den binomiale distribusjonen se annerledes ut:\nFigure 2.22: Binomialfordeling med ulik sannsynlighet\nhar vi bare 20% sannsynlighet suksess, og fordelingen av sannsynligheter vil se annerledes ut. Med 20% sannsynlighet suksess er det veldig liten sannsynlighet vi vil få 10 eller flere suksesser hvis vi gjør forsøket 20 ganger. Det er størst sannsynlighet å få 4 suksesser.Et terningkast (med en vanlig terning med 6 sider) – som ikke er tuklet med – har lik sannsynlighet å lande på hhv 1,2,3,4,5 og 6. Det vil si det er 1/6 sannsynlighet 1, 1/6 sannsynlighet 2 osv. Hvis vi kaster denne terningen 10 ganger kan resultatet se slik ut:\nFigure 2.23: 10 terningkast\nVi ser vi ikke fikk noen 2’ere og 5’ere. Dette kan vi forvente når vi bare har 10 terningkast. Hvis vi imidlertid kaster terningen 100 ganger vil det være svært liten sannsynlighet å ikke få «treff» på alle 6 verdiene på terningen, og vi burde kunne forvente vi får en ganske jevn fordeling på alle 6 verdiene. Nedenfor vises resultatet av 100 terningkast.\nFigure 2.24: 100 terningkast\nVi ser vi har en relativt jevn fordeling. Noe ulikhet er det selvsagt, noe vi vil forvente fra en tilfeldig prosess. Hvis vi gjennomførte 1000 eller 10000 terningkast vil fordelingen bli nærmere og nærmere den teoretisk forventede fordelingen. Vi kan burde, teoretisk, forvente 100 treff på hver mulighet hvis vi kaster terningen 600 ganger, men vi vil sjelden se akkurat 100 treff på hver slik vi ser hvis vi kjører tre runder med 600 terningkast:Runde 1:Runde 2:Runde 3:Selv om vi kjører 6 000 000 terningkast og vil forvente 1 000 000 treff på hver av terningens sider vil vi ikke få en perfekt fordeling iht teoretisk forventning, men resultatet vil være svært nærme og er nærme nok til vi kan bruke sannsynlighetsfordelingen til å lage forventninger om utfall:6 000 000 terningkast:Hvis vi setter resultatet fra 6 000 000 terningkast inn et histogram ser vi resultatet er svært nærme hva vi teoretisk vil forvente:\nFigure 2.25: 6 000 000 terningkast\n","code":"\nsuksess <- 0:20\nplot(suksess, \n     dbinom(suksess, \n            size = 20, \n            prob = .5),\n     type = 'h',\n     main = \"Binomial distribusjon (n = 20, p = 0.5)\",\n     ylab = \"Sannsynlighet\",\n     xlab = \"Suksess\",\n     lwd = 10)\nplot(suksess,\n     dbinom(suksess, \n            size = 20,\n            prob = .2),\n     type = 'h',\n     main = \"Binomial distribusjon (n = 20, p = 0.2)\",\n     ylab = \"Sannsynlighet\",\n     xlab = \"Suksess\",\n     lwd = 10)\nset.seed(32)\nterning10 <- sample(1:6, 10, replace = TRUE)\nstripchart(terning10, \n           method = \"stack\", \n           offset = .5, \n           at = 0, \n           pch = 19,\n           col = \"steelblue\", \n           main = \"10 terningkast\", \n           xlab = \"Verdi på terning\", \n           ylab = \"Antall\")\nset.seed(33)\nterning10 <- sample(1:6, 100, replace = TRUE)\nstripchart(terning10, \n           method = \"stack\", \n           offset = .5, \n           at = 0, \n           pch = 19, \n           col = \"steelblue\", \n           main = \"100 terningkast\", \n           xlab = \"Verdi på terning\", \n           ylab = \"Antall\")\nset.seed(43)\nterning_runde1 <- sample(1:6, 600, replace = TRUE)\ntable(terning_runde1)\n#> terning_runde1\n#>   1   2   3   4   5   6 \n#>  93 102 102 102 108  93#> terning_runde2\n#>   1   2   3   4   5   6 \n#> 101 102  94  91 105 107#> terning_runde3\n#>   1   2   3   4   5   6 \n#> 104 113  92  98  95  98#> minterning\n#>       1       2       3       4       5       6 \n#> 1000492  998250 1000216 1000832 1001422  998788"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"poissonfordeling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.9 Poissonfordeling","text":"Poissonfordelinger finnes situasjoner der hendelser skjer vilkårlig tid (og rom) hvor vi er interessert kun antallet hendelser et gitt tidsintervall. Vi kan f.eks. være interessert hvor mange supporthenvendelser vi får løpet av en time, antallet feilmedisineringer per uke, hvor mange besøk avdelingen får per dag o.l. Andre eksempler kan være antall trafikkulykker langs en angitt veistrekning, antall elgpåkjørlser på en togstrekning, eller antall av en gitt art fugler et definert område et definert tidsrom. En hendelse må være uavhengig tidsmessig av andre hendelser (det er altså ikke økt sannsynlighet en hendelse vil skje fordi en tilsvarende hendelse akkurat har skjedd), sannsynligheten en hendelse et kort perspektiv er lik sannsynligheten et lengre perspektiv, og ettersom et tidsintervall blir kortere og kortere vil sannsynligheten hendelsen gå mot null.Poissonfordeling uttrykker sannsynligheten et gitt antall hendelser inntreffer et gitt tidsintervall (eller et gitt geografisk domene) og vi kjenner gjennomsnittlig hvor ofte hendelsen inntreffer. Denne sannsynligheten uttrykkes som en lambdaverdi (\\(\\lambda\\)).Eksempelet er hentet fra Soage (2020):\nFigure 2.26: Poissonfordelinger\nUt fra hvilken \\(\\lambda\\)-verdi vi setter kan vi si noe om sannsynligheten et antall hendelser inntreffer.Ugarte, Militino, Arnholt (2016) eksemplifiserer Poissonfordeling ved å vise til det gjennomsnitt skåres 2,5 mål en VM-kamp fotball. Denne situasjonen tilfredsstiller forutsetningene å bruke Possionfordeling.Vi kan grafisk framstille sannsynlighetsfordeingen slik:\nFigure 2.27: Poissonfordeling mål VM-kamp fotball\nR kan vi også enkelt regne ut den nøyaktige sannsynligheten x antall mål gitt forutsetningen om det snitt skåres 2.5 mål pr kamp. Vi kan bruke sannsynlighetsfordelingen til å regne ut sannsynligheten et gitt antall mål, f.eks.:Sannsynligheten 0 mål = 0.082085Sannsynligheten 1 mål = 0.2052125Sannsynligheten 2 mål = 0.2565156Sannsynligheten 3 mål = 0.213763Sannsynligheten 4 mål = 0.1336019eller f.eks. sannsynligheten det skåres mellom 1 og 3 mål = 0.6754911.","code":"\n# Grid of X-axis values\nx <- 0:50\n# lambda: 5\nlambda <- 5\nplot(dpois(x, lambda), \n     type = \"h\", \n     lwd = 2,\n     main = \"Poisson sannsynlighetsfordeling\",\n     ylab = \"P(X = x)\", \n     xlab = \"Antall hendelser\")\n# lambda: 10\nlambda <- 10\nlines(dpois(x, lambda), \n      type = \"h\", \n      lwd = 2, \n      col = rgb(1, 0, 0, 0.7))\n# lambda: 20\nlambda <- 20\nlines(dpois(x, lambda), \n      type = \"h\", \n      lwd = 2, \n      col = rgb(0, 1, 0, 0.7))\n# Legend\nlegend(\"topright\", \n       legend = c(\"5\", \"10\", \"20\"),\n       title = expression(lambda), title.adj = 0.75,\n       lty = 1, \n       col = 1:3, \n       lwd = 2, \n       box.lty = 0)\nmaal <- 0:10\nplot(maal, dpois(maal, lambda=2.5),\n     type='h',\n     main='Poissonfordeling (lambda = 2.5)',\n     ylab='Sannsynlighet',\n     xlab ='# Mål',\n     lwd=3)"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"geometrisk-fordeling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.10 Geometrisk fordeling","text":"En geometrisk fordeling er en diskret fordeling der man teller antall hendelser/forsøk inntil et gitt resultat forekommer. Resultatet er suksess eller feil, altså hvor mange ganger man har en hendelse før man får en suksess eller feil (avhengig av hva man måler). Et eksempel er hvor mange ganger man må kaste terninger å få 11 sum. Man kaster da terninger til første gang man får 11 (= suksess). En geometrisk distribusjon kan se slik ut (p = 0,4):\nFigure 2.28: Geometrisk fordeling\nstatistisk prosesskontroll er denne typen fordeling til stede når man f.eks. teller antall dager mellom sjeldne hendelser. Man teller antall dager før man f.eks. får et alvorlig avvik på en medisinering, en operasjon e.l. geometrisk fordeling er sannsynligheten et gitt utfall uavhengig av om det har skjedd før. Man kan bruke geometrisk fordeling f.eks. til å estimere hvor mange dager man normalt vil forvente det går mellom en sjelden hendelse. Hvis man gjennom erfaringstall vet sannsynligheten en sjelden hendelse er p = 0.035 vil man forvente det går 1/0.035 \\(\\approx\\) 29 dager mellom hver hendelse. Geometrisk distribusjon kan hjelpe oss en statistisk prosesskontroll å finne normal/unormal variasjon ved sjeldne hendelser.Det kan være verdt å merke seg binomial og geometrisk fordeling skiller seg fra hverandre ved geometrisk fordeling har et ukjent antall hendelser (man fortsetter til man får første suksess/feil), mens binomial fordeling har et gitt antall hendelser. Som vi skal se senere eksempler derfor geometrisk fordeling viktig når vi håndterer sjeldne hendelser, fordi vi ikke kjenner hvor mange dager det f.eks. går før vi får første suksess/feil.","code":"\nx_dgeom <- seq(1, 20, by = 1)\ny_dgeom <- dgeom(x_dgeom, prob = 0.4) \nplot(y_dgeom,\n    type = \"l\",     \n    main = \"Geometrisk fordeling for p = 0.4\",\n    ylab = \"f(x)\",\n    xlab = \"x\")"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"eksponensiell-fordeling","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.11 Eksponensiell fordeling","text":"En tilfeldig kontinuerlig variabel kan sies å være analog til den geometriske distribusjonen, men kontinuerlige data. Den eksponensielle distribusjonen brukes ofte å modellere tid mellom hendelser. statistisk prosesskontroll vil vi typisk bruke denne distribusjonen hvis vi måler tid mellom sjeldne hendelser. Hvis vi f.eks. måler tiden mellom uventet dødsfall som følge av en type rutineoperasjon på et sykehus vil den ha en eksponensiell distribusjon hvis sannsynligheten hendelsen inntreffer innenfor t gitt tidsintervall er omtrentlig proporsjonal med lengde på tidsintervallet (Taboga 2017). Eksponensielle fordelinger har samme grunnform, men kan ha ulik bratthet avhengig av den såkalte lamdaverdien (= en parameter raten av hendelser). Lambdaverdi er en parameter hvor ofte hendelsene forventes å skje.\nFigure 2.29: Eksponensiell fordeling\n","code":"\neksford <- seq(0, 20, length.out=1000)\ndat1 <- data.frame(x = eksford, fx = dexp(eksford, rate = 0.2)) %>%\n  add_column(ID = 1:1000) %>%\n  relocate(3)\ndat2 <- data.frame(x = eksford, fx = dexp(eksford, rate = 1)) %>%\n  add_column(ID = 1:1000) %>%\n  relocate(3)\ndat3 <- data.frame(x = eksford, fx = dexp(eksford, rate = 1.5)) %>%\n  add_column(ID = 1:1000) %>%\n  relocate(3)\ndat4 <- data.frame(x = eksford, fx = dexp(eksford, rate = 2)) %>%\n  add_column(ID = 1:1000) %>%\n  relocate(3)\ndat1plot <- ggplot(dat1, aes(x=x, y=fx)) + \n    geom_line() + \n    ggtitle(expression( ~ lambda ~ \" = 0.2\"))\ndat2plot <- ggplot(dat2, aes(x=x, y=fx)) + \n    geom_line() + \n    ggtitle(expression( ~ lambda ~ \" = 1.0\"))\ndat3plot <- ggplot(dat3, aes(x=x, y=fx)) + \n    geom_line() + \n    ggtitle(expression( ~ lambda ~ \" = 1.5\"))\ndat4plot <- ggplot(dat4, aes(x=x, y=fx)) + \n    geom_line() + \n    ggtitle(expression( ~ lambda ~ \" = 2.0\"))\nggarrange(dat1plot, dat2plot, dat3plot, dat4plot + \n              rremove(\"x.text\"), ncol = 2, nrow = 2,  widths = c(1, 1))"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"nullhypotese","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.12 Nullhypotese","text":"Vi kommer mye tilbake til hypotesetesting (ulike former), men dette danner grunnlaget å forstå hvorfor vi tester en nullhypotese og forkaster den hvis vi får et signifikant resultat. Vi ønsker å teste hypotesen om M = 53 [51,55] er en god estimator μ. stedet å teste alle muligheter μ vil ligge intervallet, tester vi stedet en presist formulert og testbar nullhypotese om μ ikke vil ligge intervallet [51,55]. Hvis vi får et signifikant resultat på nullhypotesetesten kan vi si sannsynligheten μ vil ligge utenfor [51,55] er svært liten (avhengig av konfidensnivå), og vi derfor har styrket hypotesen om M=53 [51,55] er en god estimator μ. Vi setter med andre ord opp en stråmann: vi vil egentlig teste om våre estimatorer populasjonen er sannsynlige innenfor et konfidensintervall, men tester stedet sannsynligheten de ikke er det håp om å forkaste stråmannen.Nullhypotesen formuleres som regel som en presis og testbar hypotese om ingen forbindelse eller forskjell mellom gitte variabler. Nullhypotesen kan imidlertid være «hva som helst», den forstand det er like gyldig å formulere en nullhypotese som ikke inneholder null betydningen tallet null eller ingen forskjell e.l. Poenget er den må formuleres slik den evt kan forkastes hvis den ikke støttes (“null” kommer, har jeg blitt fortalt, fra det engelske “nullify” - altså “gjøre ugyldig, annullere, oppheve”).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistisk-styrke-statistical-power---og-type-i-og-ii-feil","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.13 Statistisk styrke – “Statistical Power” - og type I og II feil","text":"mange sammenhenger anvendt statistikk leser man om statistisk styrke (“power”). Enkelt forklart er statistisk styrke sannsynligheten en statistisk test vil identifisere en effekt hvis den er der. hypotesetesting referer statistisk styrke til sannsynligheten å få et statistisk signifikant resultat som fører til vi forkaster nullhypotesen når den alternative hypotesen er sann. Når den statistiske styrken øker synker sannsynligheten vi ikke forkaster en feilaktig nullhypotese (type II feil). Alternativt kan man si når den statistiske styrken testen øker, øker sannsynligheten vi korrekt godtar en sann alternativ hypotese.Vi kan uttrykke dette slik:\\(Statistisk\\ styrke = 1 - \\beta\\)En ofte sitert og brukt vurdering rundt nivået på statistisk styrke (som altså er et tall mellom 0 og 1) er J. Cohen (1988). Cohen foreslår 0,8 som et nivå på statistisk styrke som god avveining mellom sannsynligheten type og type II feil. Type feil forekommer når man feilaktig forkaster \\(H_0\\) når den er sann, mens type II feil innebærer å feilaktig beholde \\(H_0\\)når den er usann (eller: vi konkluderer med det ikke er noen effekt når det faktisk er en) (Mayr et al. 2007). Vi kan oppsummere dette slik:Type 1 og type 2 feil #1Vi kan med andre ord treffe riktig konklusjon av de fire mulighetene, men også feil av de fire mulighetene. å huske forskjellen på type og type II feil pleier jeg å huske:Seeing something (type ) – det vi også kallen en falsk positivNot seeing something (type II) – det vi også kaller en falsk negativP. Ellis (2010) illustrerer dette slik:Type 1 og type 2 feil #2Cohen postulerer de fleste forskere vil anse type som langt verre enn type II, faktisk 4 ganger så ille. Dersom man velger \\(\\alpha=0.05\\) (95% konfindensnivå) må da \\(\\beta = 0.05*4=0.2\\). Vi får da:\\(Power = 1 - \\beta\\)\n\\(Power = 1 . 0.2 = 0.8\\)En annen måte å si dette på er med statistisk styrke = 0,8 har man 80 % sjanse å detektere en effekt hvis det virkelig er en effekt. Lav statistisk styrke fører altså til ikke-signifikante resultater. Et ikke-signifikant resultat betyr et uavklart resultat: Det kan være en effekt der og det kan hende det ikke er et resultat der. Et ikke-signifikant resultat betyr IKKE det ikke kan være en effekt. Derimot vil et ikke-signifikant resultat oftest føre til man tolker resultatet som det ikke er noen effekt, og det vil være en type II feil hvis det faktisk er en effekt der som vi ikke ser pga lav statistisk styrke.Vi skal ikke gå nærmere inn på type og type II feil . design av undersøkelser og analyser kan man gjøre valg som reduserer sannsynligheten å gjøre en av feilene, men de typene feil henger sammen så hvis man reduserer sannsynligheten den ene øker man samtidig sannsynligheten den andre feilen (og motsatt). må den som foretar undersøkelsen ta noen valg ut fra situasjonen og hvilken feil som vil være mest alvorlig å gjøre, men det er vanlig å regne type feil som mer alvorlig enn type II (vitenskapsteoretisk sett). Grunnen til dette er man anser det som verre å gå glipp av noe som har en faktisk effekt, enn å hevde noe har en effekt når det ikke har det (men eksempel innen medisinsk forskning kan dette være stikk motsatt – det vil eksempel kunne være svært uheldig om man feilaktig konkluderer med en ny medisin eller behandling ikke har negative bivirkninger hvis den faktisk har det).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"statistisk-styrke---litt-mer","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.14 Statistisk styrke - litt mer","text":"Statistisk styrke kan enten brukes priori eller post hoc – før eller etter.\nå ta det siste først (post hoc). Dette innebærer vi kalkulerer statistisk styrke etter undersøkelsen og analysene er gjort (eller som regel ser på hva f.eks. SPSS forteller oss). Det finnes sterke advarsler mot å gjøre dette (Cumming 2012). Hoenig Heisey (2001) anser dette som fundamentalt feil. Det er likevel rimelig å si dette er vanlig. Man skal hvert fall være klar informasjonen vi får ut av post hoc statistisk styrketester er begrenset og, hevdes det, brukes til dels villedende.Imidlertid er “alle” enige om priori kan statistisk styrke være en viktig del av design av en undersøkelse. Mer spesifikt kan vi bruke “power calculations” å regne ut hvor stort utvalg vi trenger å tilfredsstille et gitt konfidensnivå og antall variabler.å gjennomføre en priori estimering av hvor stor N vi trenger en undersøkelse trenger vi å vite:Hvilken type test vi skal gjennomføre: dette kan gi ulik informasjon man trenger estimering, men uansett trenger man 2-4:Forventet effektstørrelse (f.eks. Cohens d)Ønsket statistisk styrkeSignifikansnivåRetningslinjer effektstørrelse J. Cohen (1988) gir:\nTable 2.1: Effektstørrelser, modifisert fra Cohen (1988)\nEt praktisk hjelpemiddel priori vurderinger rundt design av studier - f.eks. å finne ut hvor stort utvalg (hvor stor N) man bør ha ut fra kriteriene 1-4 ovenfor er programmet G*Power (Faul et al. 2007, 2009) som kan lastes ned .Et eksempel: Vi planlegger å gjennomføre en undersøkelse der vi skal kjøre en multippel lineær regresjonsanalyse. G*Power legger vi følgende verdier: Effect size = 0,15; α = 0,05; Power = 0,8; Number predictors (antall uavhengige variabler) = 3G*Power vil kunne gi oss et plott der vi kan vurdere utvalgsstørrelse:Eksempel på utvalgsstørrelse og statistisk styrke fra GPowerDette plottet kan vi bruke planlegging av en undersøkelse. Det viser oss nødvendig N (y-aksen) en gitt statistisk styrke med den valgte effektstørrelsen. Vi kan visuelt se hvordan en endring statistisk styrke vil gi utslag nødvendig N. Som vi skal komme tilbake til andre steder notatet er planlegging av en studie viktig slik vi får tilstrekkelig stort utvalg forhold til hva vi ønsker å undersøke (ut fra parametrene ovenfor), men samtidig vi ikke “overdriver” utvalgsstørrelsen. Dette kan også få uønskede konsekvenser (som vi kommer tilbake til allerede neste delkapittel).","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"effektstørrelse-og-litt-om-p","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.15 Effektstørrelse (og litt om “p”)","text":"Det er, som vi nå ser, en direkte sammenheng mellom effektstørrelse og statistisk styrke. Jo mindre effekt, jo større statistisk styrke må man ha å oppdage den. P. D. Ellis (2012) peker på effektstørrelse, spesielt samfunnsvitenskapene, det store er svært små (og mye mindre enn man forventer), og peker på - noe som kanskje burde være åpenbart - Effekter eksisterer den virkelige verden. På samme måte som vi bruker utvalgsgjennomsnittet \\(\\overline{x}\\) som estimat på populasjonsgjennomsnittet \\(\\mu\\), er effektstørrelser vi kalkulerer utvalg estimater på populasjonseffekter. Samtidig er det klart effektstørrelse har en vesentlig informasjonsverdi tillegg til p verdi. Ikke bare kan vi si om det er en signifikant effekt, men vi kan si noe om denne effekten er liten eller stor. Uten en formening om effekten er liten eller stor (ikke bare om den er statistisk liten eller stor, men også om den er liten eller stor praksis) er informasjonsverdien av å vite det er en statistisk signifikant effekt begrenset. Likeledes, et ikke-signifikant resultat innebærer ikke det ikke kan være en effekt (det kan godt være en effekt, men vi har ikke hatt nok statistisk styrke til å oppdage den).\ner det på sin plass med noen (flere) ord om effekt og signifikans. P. D. Ellis (2012) illustrerer sammenhengen med denne likningen:\\(Statistisk\\ signifikans = Effekstørrelse * Utvalgsstørrelse\\)Sammenhenger: Jo større effektstørrelse, jo lavere p verdi (ved uendret utvalg). Ergo: En lav p verdi kan indikere en stor effekt. Men, en lav p verdi kan også skyldes et stort utvalg (og en liten effekt). Det motsatte gjelder selvsagt også. En høy p verdi kan skyldes en lav effekt. Eller et lite utvalg. Eller en kombinasjon. Det er med andre ord umulig å si noe om praktisk eller substansiell signifikans ut fra en p verdi og en statistisk signifikans (P. D. Ellis 2012).Det finnes et stort antall mål effektstørrelser. De kan det store deles inn “familier” (P. D. Ellis 2012):Effektmål som måler forskjeller mellom grupper – d familien. Eksempler: Cohens d, Hedges’ g.Effektmål som måler assosiasjon/forbindelse (hvor sterk er denne forbindelsen mellom x og y) – r familien. Eksempler: Pearsons r, Spearmans rho (\\(\\rho\\)) og Eta squared (\\(\\eta^2\\)).J. Cohen (1988) er en ofte referert kilde terskelverdier vurdering av effektstørrelse (gjengitt fra P. D. Ellis 2012, s.44, tabell 5):\nTable 2.2: Effektstørrelser, modifisert fra Cohen (1988)\nLenhard Lenhard (2016) modifiserer J. Cohen (1988) og gir følgende retningslinjer:\nTable 2.3: Effektstørrelser, modifisert fra Lenhard & Lenhard (2017)\nDet må sies hva som er en liten, middels eller stor effekt er svært kontekstavhengig. Som P. D. Ellis (2012), s.46, sier:proper way view Cohen’s thresholds interpretation tool last resort. might refer basis drawing meaning results. fact used – given raison d’être beyond Cohen’s study teenage girls – speaks volumes inherent difficulties assessing substantive significance results.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"standardisering---transformasjon-av-data-z-skåre","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.16 Standardisering - transformasjon av data (z-skåre)","text":"et tidligere delkapittel har vi vist data kan ha ulike målenivå. Målenivå/skala kan derfor skape utfordringer oss. Variabler er målt på ulike nivåer og ulike skalaer. dette kan skape utfordringer oss. Hvis vi har resultater fra ulike studier kan vi kun sammenlikne resultatene meningsfullt hvis de er gjennomført med samme skalaer.dette delkapittelet bruker vi et eksempel fra Miles Shevlin (2001). Vi kan eksempel ha en studie som undersøker eksamensresultater (avhengig variabel) ut fra antall fagbøker lest (uavhengig variabel) og finner en økning eksamensresultater på x prosentpoeng per leste bok. En annen studie har samme variabler, men presenterer resultatet som en gitt økning på en til F karakterskala per leste bok. Hvordan kan vi sammenlikne disse studiene? Siden vi ikke kan tvinge alle til å måle på nøyaktig samme måte og presentere funnene på nøyaktig samme måte (det ville jo unektelig ha gjort en god del mye lettere…) kan vi standardisere målene som er gjort, det vil si vi transformerer distribusjonen til å ha en gjennomsnittsverdi på 0 og et standardavvik på 1, og benevnes ofte som z-score.Formelen å finne z-scores er:\\(z_i = \\frac{verdien\\ av\\ x\\ på\\ målepkt\\ - gjennomsnittverdien\\ \\ x}{standardavviket}\\)som kan uttrykkes:\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\) populasjoneneller som:\\(z_i = \\frac{x_i - \\overline{x}}{s}\\) utvalget.En z-score er altså det antallet standardavvik en verdi x er fra gjennomsnittet. En z-score verdien xi på 0,45 betyr en standardisert distribusjon ligger observasjonen 0,45 standardavvik fra gjennomsnittet.La oss anta vi har normalfordelte data en variabel x. Gjennomsnittsverdien x er gitt som \\(\\mu\\) og standardavviket som \\(\\sigma\\).grafen til er dette illustrert.Normalfordeling #1Den blå linjen viser en normalfordelt datamengde som har en gjennomsnittsverdi \\(\\mu\\) og standardavviket \\(\\sigma\\). dette eksempelet er dataene høyde norske kvinner 2012 målt på sesjon5. medisinske kretser regnes ofte et standardavvik på høydefordeling som 6 cm6. Tallene eksempelet reflekterer dette. Fordelingskurven kvinners høyde 2012 viser altså en gjennomsnittshøyde på 167 cm (egentlig 167,1 cm) og et standardavvik på 6 cm.Vi antar høyden målt på kvinner på sesjon er representative populasjonen norske kvinner, og kan si eksempel 68 % av norske kvinner er mellom 161 cm og 173 cm høye. 95 % av norske kvinner har en høyde på mellom 156 cm og 179 cm. På bakgrunn av dette kan vi gjøre sannsynlighetsberegninger gjennom å bruke standardverdien z. Vi kan eksempel være interessert å vite hva sannsynligheten er en tilfeldig norsk kvinne er 175 cm.Normalfordeling #2I dette tilfellet er det det rødskraverte området av distribusjonen vi er interessert , formulert slik:\\(p=x > 175\\)Dette kan vi omformulere:\\(p=\\frac{x-\\mu}{\\sigma}\\ som\\ gir\\ \\frac{175 - \\mu}{\\sigma}\\)Fra før vet vi :\\(z=\\frac{x-\\mu}{\\sigma}\\)Vi kan dermed uttrykke :\\(p=z>\\frac{175-167}{6}\\ som\\ gir\\ p=z>1.33\\)Med andre ord kan vi si sannsynligheten x er større enn 175 er den samme som sannsynligheten z > 1,33. Vi kan illustrere dette med figuren , der fordelingen er standardisert med gjennomsnitt 0 og standardavvik 1. Området vi er interessert er det rødskraverte som ligger til høyre z = 1,33.Z-scoreSiden vi vet en standardisert normalfordeling har \\(\\sigma=1\\) betyr det vi kan se visuelt det området vi er interessert , sannsynligheten en tilfeldig norsk kvinne er høyere enn 175 cm, ligger utenfor 1 standardavvik.Vi kan bruke en tabell standard normalfordeling (f.eks. .Tabell standard normalfordelingMerk: På toppen av tabellen står det «Table Values Represent AREA LEFT Z score». Vi er jo interessert området til høyre, så vi kan da si:\\(p=1-0.90824 = 0.09176\\)Vi kan derfor si det er cirka 9,2 % sannsynlighet en norsk kvinne er 175 cm. Hadde vi vært interessert sannsynligheten en tilfeldig norsk kvinne er lavere enn 175 cm kunne vi lest det rett ut av tabellen som 0,90824, altså cirka 91 % sannsynlighet.Tilsvarende kan vi finne sannsynligheten en tilfeldig norsk kvinne er mellom 165 cm og 175 cm.Normalfordeling #3Vi kan uttrykke dette som\\(p(165 < x < 175)\\)Ved å bruke samme framgangsmåte kommer vi fram til:\\(p(\\frac{165-167}{6} < z < \\frac{175-167}{6})\\)som gir:\\(p(-0.33 < z < 1.33)\\)Vi bruker samme tabell og finner verdiene 0,37070 og 0,90824, det vil si sannsynligheten en tilfeldig norsk kvinne er mellom 165 cm og 175 cm er (0,90824 – 0,37070 = 0,53754), det vil si cirka 54 %.Hvis vi ser på dette tallet forhold til hva vi ville forvente ut fra en normalfordeling ser vi vi forventer 68 % av verdiene en normalfordeling ligger innenfor intervallet +/- 1 standardavvik. Det vil si vi forventer 68 % av et utvalg tilfeldige norske kvinner vil ligge høydeintervallet 161 cm til 173 cm. Siden vi har sett på intervallet 165 cm til 175 cm) har vi et mindre intervall noe som gjør vi vil forvente en noe lavere sannsynlighet en tilfeldig kvinne vil ligge vårt intervall forhold til sannsynligheten å ligge intervallet +/- 1 standardavvik. Vår utregning virker derfor rimelig forhold til hva vi kunne forvente.Heldigvis vil alle statistikkprogrammer regne ut dette raskt. det første eksempelet:det andre eksempelet:","code":"\npopgjsnitt <- 167\nsd <- 6\n    \npnorm(175, popgjsnitt, sd, lower.tail = FALSE)\n#> [1] 0.09121122\npnorm(175, popgjsnitt, sd) - pnorm(165, popgjsnitt, sd)\n#> [1] 0.5393474"},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"standardisering---transformasjon-av-data-z-skåre---del-2","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.16.1 Standardisering - transformasjon av data (z-skåre) - del 2","text":"Så langt har vi brukt standardisering på en “enkel” måte – altså med en variabel. Som nevnt tidligere kan vi ha spesielt nytte av standardisering dersom vi befinner oss en situasjon der vi har datainnsamling av ulike variabler som bruker ulike måleskalaer. Standardisering (som også omtales som normalisering) defineres som en prosess som transformerer data av ulike typer/måleskalaer til en uniform/felles skala slik de kan sammenliknes. Hvis vi eksempel har alder (målt år) og inntekt (målt kroner) vil vi gjennom å standardisere gi begge fordelingene en gjennomsnittsverdi på 0 og et standardavvik på 1. En z-score alder på 0,45 vil innebære 0,45 standardavvik fra gjennomsnittet av alder. Det samme vil en z-score på 0,45 inntekt.Ofte kan man være interessert å lage komposittvariabler av et antall variabler. La oss eksempel si du sitter med data på høyde (cm), vekt (kg) og lengde på øre (mm). Du har altså tre mål på hvert objekt en studie, men alle tre er målt på forskjellige skalaer. Det er standardisering kommer inn bildet dersom man ønsker å lage en variabel som heter “Kroppstype”. Vi standardiserer hver skåre hver observasjon og får nye z-skåre variabler de tre opprinnelige variablene. Vi har dermed fått tre z-skåre variabler som deler de har gjennomsnitt på 0 og standardavvik på 1.Høyde er målt cm, vekt kg og ørelengde mm. Nå kan vi “sammenlikne” de tre variablene på en meningsfull måte fordi alle de tre nye variablene har gjennomsnittsverdi på 0 og standardavvik på 1. Z-scoren gir altså avstand fra gjennomsnittet den enkelte observasjon uavhengig av hvilken skala målingene er gjennomført på. Jeg ønsker nå å lage en komposittvariabel - en ny variabel der jeg bruker de 3 målene en variabel jeg kan kalle kroppstype (vi “later som” høyde, vekt og ørelengde kan si noe meningsfullt om kroppstype). Siden vi har ulike måleskalaer kan vi bruke standardisering.Et annet praktisk anvendelsesområde av standardiserte verdier er tolkning av ulike variablers betydning en regresjonsanalyse. Dette vil vi vise kapittelet der vi gjennomgår regresjonsanalyse.","code":""},{"path":"grunnleggende-begreper-og-sammenhenger.html","id":"variabelbegreper-og-modeller","chapter":"Kapittel 2 Grunnleggende begreper og sammenhenger","heading":"2.17 Variabelbegreper og modeller","text":"Man vil komme ulike termer/begreper/“ord” datavariabler. gir vi en kort definisjon på begreper som vil forekomme senere:Obervert variabel (“observed variabel”): en variabel som finnes/eksisterer dataene “seg selv”, motsetning til:Latent variabel (“latent variabel”): en variabel som er konstruert (som ikke finnes målet/uttrykt direkte dataene selv, men f.eks. som blir konstruert av observerte variabler).Uavhengig variabel: En variabel av interesse som påvirker den annen variabel vi er interessert . Andre begreper: prediktor, årsaksvariabel, forklaringsvariabel.Avhengig variabel: En variabel som påvirkes (“er avhengig av…”) av andre, uavhengige variabler. Andre begreper: virkningsvariabel.Eksogen variabel: En uavhengig variabel (observert eller latent) som forklarer en endogen variabel. Disse variablene blir bestemt utenfor modellen vi bruker - de er oppgitt eller målt, og er “input” modellen. Ofte brukt begrep f.eks. økonomiske modeller.Endogen variabel: En avhengig variabel (observert eller latent) som har en kausal sti (“path”) som forklaring (“som leder fram til den endogene variabelen”). Dette er variabler som bestemmes av/modellen. Vi kan se på dette som “output” av modellen.Indikator: En observert variabel en “measurement model” (kan både være eksogen og endogen)Faktor: En latent variabel definert av indikatorer (kan være både eksogen og endogen)Lading (“loading”): En sti mellom indikator og faktorMålemodell (“measurement model”): En modell som ser på forholdet mellom observerte variabler og latente variablerStrukturell modell (“structural model”): En modell som ser på det kausale forholdet mellom eksogene og endogene variabler (som begge kan være både observerte og latente)","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"samvariasjon-og-korrelasjon","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"Kapittel 3 Samvariasjon og korrelasjon","text":"R-pakker brukt dette kapittelet:","code":"\npacman::p_load(flextable, tidyverse, officer, readxl, knitr, kableExtra, writexl, car, corrplot)"},{"path":"samvariasjon-og-korrelasjon.html","id":"samvariasjonkovarians","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.1 Samvariasjon/kovarians","text":"Samvariasjon – noe varierer sammen (omtales også som kovarians/“covariance”) – er et mål på en felles variasjon mellom (minst) variabler. Samvariasjonen kan være enten negativ eller positiv. Hvis vi har data på variablene x og y kan vi undersøke om verdiene x og y varierer tilfeldig eller om det er en sammenheng mellom de . Ved positiv samvariasjon vil en økning eller nedgang en variabel korrespondere med en økning eller nedgang den andre. Samvariasjon innebærer det ikke er tilfeldig hvordan variabler varierer forhold til hverandre – det er et gjensidig forhold mellom variablene.forrige kapittel var begrepet varians et tema. Et alternativt begrep samvariasjon er kovarians (fra engelsk «covariance»). Kovariansen er den enkleste måten å se om variabler varierer sammen – altså om endring den ene variabelen følges av en endring den andre. Eller, med andre ord, dersom en variabel avviker fra gjennomsnittet forventer vi den andre også gjør det (enten samme eller motsatt retning hvis vi tror de kovarierer). Vi ønsker derfor å finne ut hvor stor kovariansen er.\nTable 3.1: Kovarians\nVi regner altså ut snittet og avviket fra snittet begge variablene (antall husdyr og antall familien). Så multipliserer vi avvikene pr rad – og får tverrproduktavviket pr observasjon/respondent. Til slutt summerer vi tverrproduktavvikene. Så regner vi ut kovariansen:\\(\\frac{6.8}{n-1}=\\frac{6.8}{(5-1)}=1.7\\)dette enkle eksempelet kan vi altså si antall husdyr samvarierer positivt med antall medlemmer familien. Det er imidlertid ikke så lett å tolke hva kovariansen betyr da det ikke er et standardisert mål. Vi kan altså ikke sammenlikne kovarianser mellom ulike undersøkelser på en objektiv måte (Field 2009a). Det fører oss på begrepet korrelasjon.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"korrelasjon-og-korrelasjonskoeffisient","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.2 Korrelasjon og korrelasjonskoeffisient","text":"å kunne si noe mer fornuftig om samvariasjonen må derfor normalisere kovariansen gjennom å dele på variablenes standardavvik – noe som gir oss korrelasjonen mellom variablene (Løvås 2013). Den standardiserte kovariansen er med andre ord korrelasjonskoeffisienten. vårt tilfelle ser det da slik ut:Vi har allerede funnet kovariansen gjennom å multiplisere avvikene de variablene med hverandre (6.8). Vi gjør det samme med standardavvikene. Standardavviket antall husdyr har vi tidligere regnet vi ut til å være 1.14. antall medlemmer familien blir standardavviket:\\(\\sqrt{\\frac{(-2.4)^2 + (-0.4)^2 + (1.6)^2 + (1.6)^2}{(5-1)}} = \\sqrt{\\frac{5.76 + 0.16 + 0.16 + 2.56 + 2.56}{4}}=1.67\\)Neste skritt blir å multiplisere standardavvikene:\\(1.14 * 1.67 = 1.923\\)Til slutt tar vi kovariansen og deler på standardavvikproduktet:\\(\\frac{1.7}{1.923}=0.88\\)Resultatet 0,88 er det som betegnes Pearsons korrelasjonskoeffisient (\\(r\\)). oss betyr det vårt lille eksempel antall familiemedlemmer er positivt korrelert med antall husdyr – jo flere familien, jo flere husdyr. Korrelasjonskoeffsienten er et mål på hvor sterk korrelasjonen er og hvilken retning korrelasjonen går (om den er positiv eller negativ).","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"tolkning-av-korrelasjon-og-korrelasjonskoeffisenter","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.3 Tolkning av korrelasjon og korrelasjonskoeffisenter","text":"Korrelasjonskoeffisienten har alltid en verdi mellom -1 og 1, der 0 betyr ingen korrelasjon. Det er vanlig å bruke følgende retningslinjer vurdering av koeffisienten:Nær x/- 1: Perfekt eller tilnærmet perfekt korrelasjonMellom +/- 0.5 og +/- 1: Sterk korrelasjonMellom +/- 0.3 og +/- 0.49: Moderat korrelasjonUnder +/- 0.29: Liten korrelasjonHinkle, Wiersma, Jurs (2003) opererer med en noe mer finmasket inndeling:\nTable 3.2: Korrelasjonskoeffisient - Pearsons r. Modifisert fra Hinkle et al. (2003)\nOfte ønsker vi (og anbefaler) å ikke bare se på verdien av korrelasjonskoeffisienten, men også se på en grafisk framstilling av datane - gjerne omtalt som et spredningsplott (“scatter plot”).Eksempler på korrelasjonerI eksempelet og illustrasjonene har vi vist Pearsons korrelasjonskoeffisient \\(r\\). Det finnes ulike korrelasjonskoeffisienter ulike typer datasett. Pearsons korrelasjonskoeffisient brukes som regel på datasett der vi gjennomfører såkalte parametriske tester. andre tester, ikke-parametriske, kan Spearmans rho (\\(\\rho\\)) - ofte brukt ordinale variabler (men kan også brukes på intervall/ratiodata) med parrede data (“matched pair”) - og Kendalls tau (\\(\\tau\\)) - ofte brukt “nominelle variabler med rangert data (”ranked data”) - være gode alternativer. Vi går ikke nærmere inn på disse .Vi skal merke oss en korrelasjonskoeffisient på 0 ikke betyr det ikke er noen korrelasjon. Som vi kan se av bildet (Frøslie 2022) kan korrelasjonskoeffisienten være 0 og variablene like vel være avhengige (delt iht Creative Commons: CC SA 3.0), jfr illustrasjonen .Som sagt, et viktig poeng er vi skal være veldig forsiktige med å tolke en korrelasjonskoeffisient uten å ha en grafisk framstilling av hvordan datapunktene fordeler seg (hvordan distribusjonen av datapunkter ser ut). Dette fordi en gitt korrelasjonskoeffisient kan representere et uendelig antall mønstre mellom variabler. Vanhove (2018) viser dette:Eksempler på lik korrelasjon (r = 0.5)Alle 16 eksemplene viser altså mønstre av korrelasjon mellom variabler som alle har korrelasjonskoeffisienten r=0,5.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"spuriøs-korrelasjon","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.4 Spuriøs korrelasjon","text":"Det er viktig å huske på en korrelasjon mellom variabler (x og y) ikke er det samme som å si det er en årsakssammenheng (kausalitet). Selv om variabler korrelerer perfekt betyr ikke det nødvendigvis den ene er årsak til den andre. Det kan være en såkalt spuriøs sammenheng. mange tilfeller er det åpenbart det er urimelig å anta det skal være en sammenheng mellom variablene.Korrelasjon betyr altså ikke automatisk kausalitet. Dette er utrolig viktig å huske – og en kilde til mye feilinformasjon/feiltolkninger. La oss ta eksempler fra Tyler Vigen.Spuriøs korrelasjon - filmer med Nicolas Cage og dødsfall som følge av drukning svømmebassengSpuriøs korrelasjon - konsum av ost og dødsfall gjennom innvikling eget sengetøyI det første eksempelet ovenfor er det altså en sterk korrelasjon mellom filmer Nicholas Cage medvirker og antall mennesker som druknet etter å ha falt et svømmebasseng. Med mindre man tenker seg Cage er så dårlig skuespiller det får folk til å hoppe frivillig ut et svømmebasseng å drukne seg virker denne sammenhengen ganske søkt.det andre eksempelet virker sammenhengen like sprø. det skal være en reell og nesten perfekt sammenheng mellom antall mennesker som kveles av deres eget sengetøy og osteforbruket per capita er ekstremt lite troverdig.Det synes åpenbart selv om det er en klar korrelasjon mellom variablene de eksemplene virker det fullstendig meningsløst å tro de faktisk henger sammen. Dette er det vi kaller spuriøs korrelasjon.","code":""},{"path":"samvariasjon-og-korrelasjon.html","id":"eksempel-på-bivariat-korrelasjonsanalyse","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.5 Eksempel på bivariat korrelasjonsanalyse","text":"tittelen på delkapittelet har vi brukt begrepet bivariat korrelasjonsanalyse. Det innebærer vi ser på korrelasjonen mellom variabler. neste delkapittel skal vi vise et enkelt eksempel på korrelasjonsanalyse av flere variabler.Dette eksempelet er hentet fra Pallant (2010). Datasettet kan lastes ned fra nett .Vi har modifisert datasettet til å kun inneholde de variablene vi skal bruke og fjernet observasjoner med manglende verdier:Download Pallantmod.xlsxDatasettet inneholder opprinnelig flere variabler, men det jeg skal se på nå er om oppfattelse av stress (“tpstress”) korrelerer med variabelen “tpcoiss” (“Total perceived control internal states”). Jeg ønsker altså å se på sammenhengen mellom oppfattet stress og hvordan man oppfatter man har “indre kontroll”.resultatet kan vi se -0.5805759. Siden vi har et negativt fortegn betyr det vi har en negativ korrelasjon mellom variablene – altså en høy verdi på den ene variabelen er forbundet med en lav verdi på den andre. Vi ser korrelasjonen er signifikant (9.4088925^{-40}). Vi kan også si noe om styrken på korrelasjonen (uavhengig av fortegn på koeffisienten).Lenger opp har vi gjengitt forslag til tokning av størrelsen på korrelasjonskoeffisienten. Etter disse har vi en sterk negativ korrelasjon mellom disse variablene ut fra Cohens inndeling, men en moderat korrelasjon ut fra Hinkle, Wiersma, Jurs (2003).Det er vanligvis ikke nødvendig å vise resultater av en korrelasjonsanalyse dersom det ikke er mer enn variabler. eksempelet kan man rapportere (vi har ikke sjekket forutsetninger, men eksempelets skyld antar vi vi ikke har funnet problemer der):Forholdet mellom oppfattelse av indre kontroll (målt med PCOISS) og oppfattelse av stress (målt med Perceived Stress Scale) ble undersøkt med Pearsons korrelasjonskoeffisient. Innledende analyser avdekket ingen brudd på forutsetningene om normalitet, linearitet og homoskedastisitet. Basert på resultatene av studien er oppfattelse av indre kontroll sterkt negativt korrelert med oppfattelse av stress, r = -0.5805759, n = rnrow(Pallantmod), p = 9.4088925^{-40}.","code":"\nkorr <- cor.test(Pallantmod$Pallantmod.tpstress, Pallantmod$Pallantmod.tpcoiss)\nkorr\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  Pallantmod$Pallantmod.tpstress and Pallantmod$Pallantmod.tpcoiss\n#> t = -14.683, df = 424, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.6402679 -0.5139141\n#> sample estimates:\n#>        cor \n#> -0.5805759\n\nplot(Pallantmod$Pallantmod.tpcoiss, Pallantmod$Pallantmod.tpstress, main = 'Spredningsplott ', xlab = 'tpcoiss', ylab = 'tpstress')\n\nkorrplott <- ggplot(Pallantmod, aes(x=Pallantmod.tpcoiss, y=Pallantmod.tpstress)) + \n    geom_point(col = \"red\") +\n    theme_bw() +\n    labs(caption = \"Scatterplott for Pallantmod.tpcoiss og Pallantmod.tpstress\")\nkorrplott"},{"path":"samvariasjon-og-korrelasjon.html","id":"multivariat-korrelasjonsanalyse","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.6 Multivariat korrelasjonsanalyse","text":"Eksempelet er hentet fra Coghlan (2010) (Creative Commons Attribution 3.0 License). Datasettet inneholder data på mengde av 13 ulike kjemikalier vin fra en region Italia fra tre ulike produsenter.Vi kan først lage en korrelasjonsmatrise der vi kun tar oss de første fem kjemikaliene (kolonne 2 til 5 datasettet):Og et sammensatt plott:\nFigure 2.5: Korrelasjonsplott 5 kjemikalier\nspredningsplottet kan vi se korrelasjonen mellom V2 og V3 ser mest “spredd” ut. Dette kan vi også se matrisen . Ut fra bare plottet er det ikke så lett å f.eks. se hvor korrelasjonen er størst, men matrisen ser vi det selvsagt enkelt (V4 og V5). Så kombinasjonen av en korrelasjonsmatrise og et spredningsplott vil være et godt hjelpemiddel å se på korrelasjon.","code":"\nvinkorrelasjon <- as.matrix(cor(wine[2:5]))\nupper <- round(vinkorrelasjon, 2)\nupper[upper.tri(vinkorrelasjon)] <- \"\"\nupper <- as.data.frame(upper)\n# upper\nkable(upper)\nscatterplotMatrix(wine[2:5])"},{"path":"samvariasjon-og-korrelasjon.html","id":"kausalitet-årsakssammenheng","chapter":"Kapittel 3 Samvariasjon og korrelasjon","heading":"3.7 Kausalitet (årsakssammenheng)","text":"Kausalitet – fra latin “causa” -> engelsk “cause” - betyr altså årsak eller grunn. Kausalitet innebærer altså det må være en årsakssammenheng mellom hendelser/fenomener.Vi kan tydeligst (enklest) se kausalitet naturvitenskapene, der vi snakker om et deterministisk årsaksforhold (det er ikke begrenset til naturvitenskapene selvsagt, men naturvitenskapene sysler mye med lovmessigheter vi ikke like lett kan se/påvise eksempel samfunnsvitenskapene). En deterministisk årsakssammenheng innebærer et fenomen B alltid har samme årsak (fenomen ), og fenomen alltid fører til fenomen B. Det er således et tidsperspektiv involvert (rekkefølge av hendelser/fenomen).samfunnsvitenskapene har man gjerne en litt annerledes tilnærming til kausalitet fordi det er vanskelig å vise/se de lovmessige og deterministiske årsaksforholdene. sier man et fenomen () er årsak til fenomen B, dersom det er slik (med en viss sannsynlighet) enten fører til eller øker sannsynligheten B. Dette faller inn en stokastisk årsakssammenheng (Dahlum Grønmo 2021). samfunnsvitenskapene kan man mindre grad kontrollere alle ting som påvirker et fenomen B, så man kan også snakke om tendenser – det er en tendensiell forståelse av kausalitet.et naturvitenskapelig eksperiment kan vi ofte isolere fenomenene vi undersøker fra annen påvirkning (selv om dette naturligvis på ingen måte er gjeldende naturvitenskapelig forskning). Vi kan kontrollere hva som påvirker hva, tid og rom. Et kontrollert eksperiment er derfor en slags gullstandard forskning når man skal si noe om kausale forhold.samfunnsvitenskapene er dette ofte praksis umulig. Det vil være mange mulige påvirkninger på et fenomen, og det kan være vanskelig å si noe sikkert om 1) har vi tenkt på alle ting som kan påvirke, 2) klarer vi å ta hensyn til denne usikkerheten våre analyser og 3) vet vi egentlig hva som påvirker hva (kan det være sånn vår antakelse om påvirker B faktisk kan være motsatt)? Moralen er nok: Vi skal være veldig varsomme med å dra bastante slutninger om kausalitet så lenge vi ikke gjennomfører et kontrollert eksperiment der vi kontrollerer omgivelsene og variablene. Det finnes imidlertid (selvsagt) metoder også samfunnsvitenskapene som gjør vi kan snakke om kausalitet. Disse kommer vi (litt) tilbake til undervegs senere kapitler.","code":""},{"path":"univariat-analyse.html","id":"univariat-analyse","chapter":"Kapittel 4 Univariat analyse","heading":"Kapittel 4 Univariat analyse","text":"R-pakker brukt dette kapittelet:Univariat analyse handler om analyse av en enkelt variabel - dvs. vi kan godt gjøre analysen på flere variabler samtidig, men vi ser kun på karakteristika ved den enkelte variabel, ikke variabler sammenheng eller forhold til hverandre). hovedsak gjør vi dette på tre måter:Deskriptiv statistikk (“Summary statistics”)FrekvenstabellerDiagrammer / plott","code":"\npacman::p_load(summarytools, tidyverse)"},{"path":"univariat-analyse.html","id":"deskriptiv-statistikk","chapter":"Kapittel 4 Univariat analyse","heading":"4.1 Deskriptiv statistikk","text":"Deskriptiv statistikk kan vi gjerne gruppere hovedgrupper:Tendens (“location measures”)Spredning (“dispersion measuresd”)Typiske karakteristika vi kan være interessert å se på er gjennomsnitt, median, maksimums- og minimumsverdier (“range”), kvartiler / interkvartil avstand (“IQR”), varians / standardavvik, manglende verdier (“missing values” / “NAs”), skjevhet og kurtosis.Vi fram igjen datasettet vi brukte kapittel 2 der vi hadde genererte høydedata 100 tilfeldige menn.Et statistikkprogrgam vil lett gi oss en rekke utregninger som sier noe deskriptivt (beskrivende) om dataene. resultatet ovenfor ser vi typiske ting som gjennomsnittsverdi, medianverdi, minimums- og maksimumsverdi, kvartiler (mer om det boksplott), men også litt mer ukjente forhold som skjevhet og kurtosis. Skjevhet var vi en god del inne på når vi snakket om normalfordelingen kapittel 1. Kurtosis handler om hvor “tunge eller lette haler” en datafordeling har (ofte omtales kurtosis som “hvor spiss fordelignen er”, men det er egentlig unøyaktig).","code":"\n\nset.seed(30)\nhoyde100 <- rnorm(100, 179, 16)\ndescr(hoyde100)\n#> Descriptive Statistics  \n#> hoyde100  \n#> N: 100  \n#> \n#>                     hoyde100\n#> ----------------- ----------\n#>              Mean     177.79\n#>           Std.Dev      16.93\n#>               Min     132.05\n#>                Q1     166.99\n#>            Median     177.84\n#>                Q3     188.10\n#>               Max     220.57\n#>               MAD      15.73\n#>               IQR      20.99\n#>                CV       0.10\n#>          Skewness       0.15\n#>       SE.Skewness       0.24\n#>          Kurtosis      -0.20\n#>           N.Valid     100.00\n#>         Pct.Valid     100.00"},{"path":"univariat-analyse.html","id":"frekvenstabell","chapter":"Kapittel 4 Univariat analyse","heading":"4.2 Frekvenstabell","text":"Det gir ikke særlig mening å lage en frekvenstabell de genererte høydedataene siden alle verdiene er unike (altså får vi frekvens 1 på alle 100 observasjonene). La oss derfor lage et enkelt eksempel:En frekvenstabell teller enkelt opp hvor mange forekomster vi har av ulike verdier, grupper e.l. Vi kan se tabellen verdien fire forekommer tre ganger. Et statistikkprogram gir oss også (som regel autmatisk, evt. vi må om det) hvor mange prosent det respektive antall forekomster utgjør av totalt antall forekomster (f.eks. utgjør de tre forekomstene av verdien fire 20% av totalt antall forekomster). Videre har vi ofte kumulativ prosent - som er en summering av prosentandelene fra første verdi til siste (summen blir åpenbart alltid 100). verdien fire ser vi kumulativt 46.67%. Altså utgjør forekomstene av verdiene 1, 2, 3.5 og 4 tilsammen 46.67% av alle forekomster.","code":"\nx <- c(1, 1, 2, 3.5, 4, 4, 4, 5, 5, 6.5, 7, 7.4, 8, 13, 14.2)\nsummarytools::freq(x)\n#> Frequencies  \n#> x  \n#> Type: Numeric  \n#> \n#>               Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#> ----------- ------ --------- -------------- --------- --------------\n#>           1      2     13.33          13.33     13.33          13.33\n#>           2      1      6.67          20.00      6.67          20.00\n#>         3.5      1      6.67          26.67      6.67          26.67\n#>           4      3     20.00          46.67     20.00          46.67\n#>           5      2     13.33          60.00     13.33          60.00\n#>         6.5      1      6.67          66.67      6.67          66.67\n#>           7      1      6.67          73.33      6.67          73.33\n#>         7.4      1      6.67          80.00      6.67          80.00\n#>           8      1      6.67          86.67      6.67          86.67\n#>          13      1      6.67          93.33      6.67          93.33\n#>        14.2      1      6.67         100.00      6.67         100.00\n#>        <NA>      0                               0.00         100.00\n#>       Total     15    100.00         100.00    100.00         100.00"},{"path":"univariat-analyse.html","id":"diagrammer-plott","chapter":"Kapittel 4 Univariat analyse","heading":"4.3 Diagrammer / plott","text":"Det er en rekke diagrammer / plott vi kan lage som gir oss informasjon om en enkelt variabel.","code":""},{"path":"univariat-analyse.html","id":"stolpediagram-bar-chart","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.1 Stolpediagram (“bar chart”)","text":"\nFigure 4.1: Stolpediagram ensfarget\n\nFigure 4.2: Stolpediagram flerfarget\nEn alternativ måte å framstille det samme som et stolpediagram:\nFigure 2.1: Alternativ til stolpediagram\n","code":"\nliksomdata <- tibble(\n  Karakter = c(\"A\", \"B\" ,\"C\" ,\"D\" ,\"E\", \"F\"),  \n  Antall = c(3, 8, 18, 16, 13, 4)\n  )\nstolpe <- ggplot(liksomdata, aes(x=Karakter, y=Antall)) + \n  geom_bar(fill = \"#0073C2FF\", stat = \"identity\") + \n  theme_bw()\nstolpe\nstolpe2 <- ggplot(liksomdata, aes(x=Karakter, y=Antall, fill=Karakter)) + \n  geom_bar(stat = \"identity\") + \n  theme_bw()\nstolpe2\nggplot(liksomdata, aes(Karakter, Antall)) +\n  geom_linerange(\n    aes(x = Karakter, ymin = 0, ymax = Antall), \n    color = \"lightgray\", size = 1.5\n    )+\n  geom_point(aes(color = Karakter), size = 2)+\n  ggpubr::color_palette(\"jco\")+\n  theme_bw()"},{"path":"univariat-analyse.html","id":"kakediagram-pie-chart","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.2 Kakediagram (“pie chart”)","text":"\nFigure 2.2: Kakediagram\n","code":"\nggplot(liksomdata, aes(x = \"\", y = Antall, fill = Karakter)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(label = Antall),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer() +\n  theme_void()"},{"path":"univariat-analyse.html","id":"histogram-1","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.3 Histogram:","text":"\nFigure 2.3: Histogram med 10 søyler\nHistogrammet viser søyler av kontinuerlige data gruppert etter en gitt bredde på søylene - antall søyler definerer vi selv ut fra hvor stor del av bredden av observasjoner vi “putter inn ” søyla. histogrammet har vi delt dataene inn åtte deler (søylene har alltid lik bredde). Vi kan også dele inn annerledes.\nFigure 2.4: Histogram med 4søyler\nVi kan også være interessert å vise histogram en variabel - høyde - grupper (f.eks. kjønn).\nFigure 2.5: Histogram høyde grupper\n","code":"\nset.seed(35)\nx <- rnorm(100, 179, 16)\ny <- rep(\"Mann\", each = 100)\nhoyde <- bind_cols( y, x)\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\ncolnames(hoyde) <- c(\"Kjonn\", \"Hoyde\")\nh <- ggplot(hoyde, aes(x = Hoyde)) + \n  geom_histogram(color = \"black\", fill = \"lightblue\", bins = 10) + \n    theme_bw() + \n    ylab(\"Antall\")\nh\nh2 <- ggplot(hoyde, aes(x = Hoyde)) + \n  geom_histogram(color = \"black\", fill = \"lightblue\", bins = 4) + \n    theme_bw() + \n    ylab(\"Antall\")\nh2\nx2 <- as_tibble(rnorm(100, 172, 17))\ny2 <- rep(\"Kvinne\", each = 100)\nhoyde2 <- bind_cols( y2, x2)\n#> New names:\n#> • `` -> `...1`\ncolnames(hoyde2) <- c(\"Kjonn\", \"Hoyde\")\nhoydefelles <- rbind(hoyde, hoyde2)\nggplot(hoydefelles, aes(x = Hoyde)) +\n    geom_histogram(aes(color = Kjonn, fill = Kjonn),\n                         alpha = 0.4, position = \"identity\") +\n  scale_color_manual(values = c(\"steelblue\", \"orange\"))+\n  scale_fill_manual(values = c(\"steelblue\", \"orange\")) +\n  theme_bw()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"univariat-analyse.html","id":"frekvenspolygon","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.4 Frekvenspolygon","text":"Dette er svært likt histogram, men bruker linjer stedet stolper.\nFigure 4.3: Frekvenspolygon\nOg grupper:\nFigure 2.6: Frekvenspolygon grupper\n","code":"\nggplot(hoyde, aes(x = Hoyde)) +\n    geom_area( stat = \"bin\", bins = 30,\n               color = \"black\", fill = \"lightblue\") +\n    theme_bw() +\n    ylab(\"Antall\")\nggplot(hoydefelles, aes(x = Hoyde)) +\n    geom_freqpoly( aes(color = Kjonn, linetype = Kjonn),\n                   bins = 30, size = 1.5) +\n  scale_color_manual(values = c(\"steelblue\", \"orange\")) +\n  theme_bw() +\n  ylab(\"Antall\")"},{"path":"univariat-analyse.html","id":"density","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.5 “Density”:","text":"\nFigure 2.7: Density kurve med gjennomsnittsverdi\nEt densityplott viser distribusjonen til en numerisk variabel. Det er på en måte en “smooth” versjon av histogrammet. Densityplott kan være et alternativ når man skal se på dataene og hvilken distribusjon den har. Det kan være enklere å se om distribusjonen f.eks. er bimodal eller unimodal (en eller topper). Det kan også være en god visualisering av en variabel med grupper.\nFigure 2.8: Density kurve med gjennomsnittsverdi grupper\n","code":"\nggplot(hoyde, aes(x = Hoyde)) +\n    geom_density(fill = \"lightblue\") + \n    theme_bw() + \n    geom_vline(aes(xintercept = mean(Hoyde)), \n             linetype = \"dashed\", size = 0.6,\n             color = \"#FC4E07\")\nsnitt <- hoydefelles %>% \n  group_by(Kjonn) %>%\n  summarise(grp.mean = mean(Hoyde))\nggplot(hoydefelles, aes(x = Hoyde)) +\n    geom_density(aes(fill = Kjonn), alpha = 0.4) +\n      geom_vline(aes(xintercept = grp.mean, color = Kjonn),\n             data = snitt, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"steelblue\", \"red\"))+\n  scale_fill_manual(values = c(\"lightblue\", \"orange\")) +\n  theme_bw()"},{"path":"univariat-analyse.html","id":"boksplott","chapter":"Kapittel 4 Univariat analyse","heading":"4.3.6 Boksplott","text":"\nFigure 2.9: Boxplott\nBoxplott ble ganske grundig gjennomgått kapittel 2.7.1.4.Vi kommer tilbake flere steder til mer utførlig tolkning av ulike diagrammer ulike kapitler der de brukes en kontekst.","code":"\nggplot(hoydefelles, aes(x = Kjonn, y = Hoyde, fill = Kjonn)) +\n    geom_boxplot() +\n    theme_bw()"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"kjikvadrattest---analyse-av-kategoriske-data","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","text":"R-pakker brukt dette kapittelet:Dette kapittelet tar seg kjikvadrattester. Dersom vi har kategoriske variabler - jamfør tidligere kapittel der vi tok oss målenivå - er kjikvadrattester en god måte å gjøre bivariat analyse på (dersom vi har metriske/kontinuerlige variabler bruker vi t-test som vi kommer tilbake til et senere kapittel).Vi skal vise tre tilfeller, det vil si tre måter å gjør kjikvadrattester:Vi har kategoriske variabler og ønsker å se om det er en sammenheng mellom dem - (“Test association”).Vi har en kategorisk variabel og ønsker å se om den representerer en kjent populasjon eller forventning (“Goodness fit”).Vi har en kategorisk variabel en gruppe mennesker / en gruppe observasjoner målt på f.eks. ulike tidspunkt eller ulike forhold (“Paired samples - McNemars test”).","code":"\npacman::p_load(tidyverse, readxl, psych, kableExtra)"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"test-of-association---sammenheng-mellom-to-kategorisek-variabler","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.1 Test of association - sammenheng mellom to kategorisek variabler","text":"Vi bruker et generert datasett:Download Modul 5_Krysstabell_Fargeblind2.xlsxDette datasettet består av 1626 observasjoner av gutter og jenter der vi også har registrert fargeblindhet. Vi ønsker å se om det er en sammenheng mellom disse variablene - altså om det er en statistisk signifikant sammenheng.Vi kan sette opp en krysstabell (“contigency table”/“crosstab”).kjikvadrattester setter vi derfor opp en hypotese om en eller annen sammenheng mellom de variablene vi ønsker å undersøke. dette eksempelet blir vår hypotese det er en sammenheng. Teknisk sett vil testen faktisk undersøke det motsatte - altså det ikke er noen sammenheng. Testen vil vise oss om vi kan forkaste denne hypotsene - ingen sammenheng. Om vi kan det er vi styrket troen på det faktisk er en sammenheng. Viser testen oss vi ikke kan forkaste hypotesen ingen sammenheng er vi svekket troen på det er en sammenheng.Vi setter da opp det som kalles en nullhypotses og en alternativ hypotses (og det er altså nullhypotsesen vi tester):\\(H_0 = Ingen\\ sammenheng\\ mellom\\ kjønn\\ og\\ fargeblindhet\\)\\(H_a = Sammenheng\\ mellom\\ kjønn\\ og\\ fargeblindhet\\)R gir oss som default test med en såkalt “Yates continuity correction” som gjør verdien kan virke annerledes enn programvare som ikke gjør denne korreksjonen. Yateskorreksjonen tar utgangspunkt det er en forutsetning om normalfordeling dataene, noe som ikke kan forutsettes binære data som vi har . Dette skal ikke være et stort problem større utvalg (mer enn 5-10 forventede hver celle krysstabellen) (SAGE 2019).vårt konkrete tilfelle ser vi med korreksjon er p-verdien 0.0528378. Hvis vi kjører testen uten korreksjonen får vi:er altså p-verdien 0.0353085. dette eksempelet er altså forskjellen mellom de ikke ubetydelig, da uten korreksjon er 0.05 og med korreksjon er 0.05. En tommelfingerreglel tolkning av p-verdien er:p low, null must go. p high, null flies.Med andre ord: Er p-verdien 0.05 (“low”) vil vi forkaste nullhypotesen, er p-verdien 0.05 (“high”) vil vi ikke forkaste nullhypotesen.tilegg til p-verdien er vi spesielt interessert kjikvadratverdien (\\(\\chi^2\\)). Kjikvadratverdien sammenlikner vi med kritisk verdi. Kritisk verdi kan vi finne tabeller på nett (f.eks. ). Hvis vi går inn tabellen lenka fører til og ser på 0.95 - 1 frihetsgrad (1 df) finner vi 3.841. Statistikkprogrammer vil gi oss antall frihetsgrader (df), men vi kan også regne ut dette en krysstabell slik:\\(df = (rader - 1) * (kolonner - 1) = (2-1)*(2-1)=1\\)Mange programmer vil uansett lett gi deg den kritiske verdien:Hvis kjikvadratverdien er større eller lik den kritiske verdien er resultatene våre statistisk signifikante. Den kritiske verdien dette eksempelet er 3.8414588, mens kjikvadratverdien er hhv. 0.0528378 og 0.0353085.Vi kan altså bruke både p-verdien og kjikvadratverdien til å vurdere om vi skal forkaste nullhypotesen eller ikke. Hvis vi tar fram igjen krysstabellen vår:Vi vet dersom antall forventede hver celle er minst 5, og gjerne 10, er det greit å kjøre test uten Yates korreksjon. så fall indikerer det vi kan forkaste nullhypotesen (“null must go”), og vår hypotese om det er en statistisk singifikant forskjell mellom kjønnene når det gjelder fargeblindhet er styrket.En mulighet er også å kjøre en såkalt “Fisher’s exact test” dersom de forventede celletallene er 5. Det er ikke tilfelle , men vi viser likevel prosedyren. Først vil vi sjekke hva forventede celletall er:Forventede verdier er regnet ut matematisk ved:\\(\\frac{radsum * kolonnesum}{totalsum}\\)Vi har krysstabellen:vårt tilfelle gutt - fargeblind:\\(\\frac{802*36}{1626}=17.75\\)som er samme verdi (heldigivs…) som den R regner ut oss lenger opp.Dersom vi får forventede 5 kan vi altså kjøre en kjikvadrattest med korreksjon, eller en Fishers eksakt test.Tolkningen er den samme som kjikvadrattesten.En siste vurdering vi kan gjøre er å se på den såkalte phi-koeffisienten.Phi-koeffisienten kan tolkes som en korrelasjonskoeffisient (Pearson r).\nTable 5.1: Phi-koeffisient - uavhengig av fortegn\n\nTable 5.1: Phi-koeffisient - uavhengig av fortegn\n, altså ingen eller neglisjerbar sammenheng.Oppsummert vårt eksempel: Når vi undersøker forventede verdier hver celle ser vi det minste forventede tallet er 17.7564576. Det er godt et minimum på 5 og en anbefalt grense på 10. Derfor kan vi gjennomføre en kjikvadrattest uten Yates korreksjon. vårt tilfelle forkaster vi altså nullhypotesen. Selv om vi finner en statistisk sammengeng mellom kjønn og fargeblindhet er denne liten eller neglisjerbar.","code":"\nkji1 <- read_excel(\"Modul 5_Krysstabell_Fargeblind2.xlsx\")\ndim(kji1)\n#> [1] 1626    2\ntable(kji1$Kjonn)\n#> \n#>  Gutt Jente \n#>   802   824\ntable(kji1$Fargeblind)\n#> \n#>   Ja  Nei \n#>   36 1590\nkrysstab <- addmargins(table(kji1$Kjonn, kji1$Fargeblind),c(1,2))\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626\nkjitest1 <- chisq.test(kji1$Kjonn, kji1$Fargeblind)\nkjitest1\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  kji1$Kjonn and kji1$Fargeblind\n#> X-squared = 3.749, df = 1, p-value = 0.05284\nkjitest2 <- chisq.test(kji1$Kjonn, kji1$Fargeblind, correct = FALSE)\nkjitest2\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  kji1$Kjonn and kji1$Fargeblind\n#> X-squared = 4.4302, df = 1, p-value = 0.03531\nkritiskverdi <- qchisq(p=.05, df=1, lower.tail=FALSE)\nkritiskverdi\n#> [1] 3.841459\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626\nforventet <- as_tibble(kjitest1$expected)\nforventet\n#> # A tibble: 2 × 2\n#>      Ja   Nei\n#>   <dbl> <dbl>\n#> 1  17.8  784.\n#> 2  18.2  806.\nmin(forventet)\n#> [1] 17.75646\nmin(forventet)\n#> [1] 17.75646\nkrysstab\n#>        \n#>           Ja  Nei  Sum\n#>   Gutt    24  778  802\n#>   Jente   12  812  824\n#>   Sum     36 1590 1626\ntest <- fisher.test(table(kji1$Kjonn, kji1$Fargeblind))\ntest\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  table(kji1$Kjonn, kji1$Fargeblind)\n#> p-value = 0.04246\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  0.9953805 4.6127889\n#> sample estimates:\n#> odds ratio \n#>   2.086476\nkrysstab2 <- table(kji1$Kjonn, kji1$Fargeblind) \nphikoeff <- phi(krysstab2, digits = 3)"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"utregning-av-kjikvadratverdi","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.1.1 Utregning av kjikvadratverdi","text":"Når vi vet faktisk tall og forventet tall, og har krysstabell med rad- og kolonnesummer, kan vi regne ut kjikvadratverdien manuelt:Siste steg er å summere \\(2.19 + 2.13 + 0.05 + 0.05 = 4.42\\).\nDifferansen mellom \\(4.43\\) og \\(4.42\\) skydes avrundinger.","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"goodness-of-fit---sammenheng-mellom-kategorisk-variabel-og-kjentforventet-datafordeling","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.2 Goodness of fit - sammenheng mellom kategorisk variabel og kjent/forventet datafordeling","text":"Kjikvadrattest brukes også på en annen måte. Vi kan bruke den å teste om en variabel kommer fra en spesifikk datadistribusjon, eller med andre ord om de empiriske (innsamlede/observerte) dataene stemmer overens med en teoretisk datadistribusjon (som eksempel normalfordelingen). Det vil si vi kan vurdere om et utvalg er representativt en populasjon.Som den første måten å bruke kjikvadrattest tester vi en hypotese. Hypotesen sier det er en signifikant forskjell mellom verdiene på de empiriske/observerte dataene og de forventede/teoretiske verdiene. Nullhypotesen blir da det ikke er signifikant forskjell.La oss lage et eksempel. En produsent av et skrapelodd hevder det er null gevinst 80% av loddene, en liten gevinst 15% av loddene, en litt større gevinst 4% av loddene og en stor gevinst 1% av loddene.Download lotteri.csv\nTable 5.2: Lovet fordeling av gevinster\nVi trekker ut 100 tilfeldige lodd og finner: 85 uten gevinst, 10 med liten gevinst, 3 med en middels gevinst og 2 med stor gevinst.\nTable 5.3: Faktisk fordeling av gevinster\nStemmer de resultatene vi fikk det tilfeldige utvalget på 100 med det produsentene lover?Stegene videre analyse blir:Regne ut frihetsgraderRegne ut forventede verdierRegne ut kjikvadratverdien","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"frihetsgrader","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.2.1 Frihetsgrader","text":"\\(df = k - 1 = 4 - 1 = 3\\)\n(k = antall nivåer den kategoriske variabelen)","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"forventede-verdier","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.2.2 Forventede verdier","text":"Disse har vi forsåvidt allerede tabellen - de produsenten har lovet er det forventede. Utregningen blir:\\(F_1 = n * p_1 = 100 * 0.80 = 80\\)\n\\(F_2 = n * p_2 = 100 * 0.15 = 15\\)\nosv.","code":""},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"kjikvadratverdi","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.2.3 Kjikvadratverdi","text":"\\(\\chi^2=\\sum\\left(\\frac{(O_i - E_i)^2}{E_i}\\right)\\)der O = Observert og E = Forventet (“Expected”)\\(\\chi^2=\\frac{(85-80)^2}{80}+\\frac{(10-15)^2}{15}+\\frac{(3-4)^2}{4}+\\frac{(2-1)^2}{1} = 0.31 + 1.67 + 0.25 + 1 = 3.23\\)Vi kan sammenlikne denne med kritisk verdi:Hvis kjikvadratverdien er større eller lik den kritiske verdien er resultatene våre statistisk signifikante. Den kritiske verdien dette eksempelet er 7.81, mens kjikvadratverdien er 3.23. Vi vil derfor anta resultatene ikke er statistisk signifikante. Dette viser seg også p-verdien. vil vi vise en nettressurs å regne ut p-verdien. Den finner du .Vi kan også kjøre testen R:P-verdien er sannsynligheten kjikvadratverdien med 3 frihetsgrader er høyere enn 3.23. Huskeregel: «p low, null must go». er ikke p lav (ikke 0.05). Det vil si nullhypotesen ikke kan forkastes. Med andre ord – vi kan konkludere med det er sannsynlig utvalget representerer populasjonen – eller dette tilfellet: basert på dette tilfeldige utvalget har produsentene produsert lodd iht det som er lovet fordi vi ikke kan forkaste nullhypotesen om det ikke er forskjell mellom utvalget vi fikk og det vi ville forvente.","code":"\nkritiskverdi2 <- qchisq(p=.05, df=3, lower.tail=FALSE)\nkritiskverdi2\n#> [1] 7.814728\nobserved <- c(85, 10, 3, 2) \nexpected <- c(.8, .15, .04, .01)\n\nchisq.test(x=observed, p=expected)\n#> Warning in chisq.test(x = observed, p = expected): Chi-\n#> squared approximation may be incorrect\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 3.2292, df = 3, p-value = 0.3576"},{"path":"kjikvadrattest---analyse-av-kategoriske-data.html","id":"paired-samples-mcnemars-test","chapter":"Kapittel 5 Kjikvadrattest - Analyse av kategoriske data","heading":"5.3 Paired samples (McNemars test)","text":"Download Modul 5_McNemar.xlsxMcNemars test brukes når vi ikke har uavhengige variabler, men derimot har tilfeller der vi har “paired data”/“repreated measures” av kategoriske data. Et eksempel på dette er en pasientgruppe som er testet ved anledninger. tabellen har vi laget data som viser et tenkt forsøk med en ny type behandling en gruppe mennesker. De er spurt om de har smerter før og etter de er gitt en behandling.Det vi jo vil undersøke er om man kan si behandlingen har en effekt.Vi kan tilnærme oss dette først litt teoretisk ved å sette opp samme tabell uten tall:Teoretisk blikk på McNemars testI denne testen er rutene med og d ikke interessante, de svarte det samme før og etter behandling. Hvis behandlingen ikke har noen effekt bør antallet respondenter som går fra ja til nei, og fra nei til ja, være likt. Derfor er vi interessert cellene b og c, dette er respondentene som beveget seg fra ja til nei eller motsatt. Hvis behandlingen har en effekt kan vi forvente antallet som går fra ja til nei vil være større, og hvis behandlingen har en negativ effekt vil vi forvente motsatt (flere fra ja til nei enn andre veg).Vi kan derfor sette opp følgende hypoteser:\\(H_0: p_b = p_c\\)\\(H_1: p_b \\neq p_c\\)der p er sannsynligheten tallet cellene med b og c. Nullhypotesen, altså den vi vil teste kjikvadrattesten, går altså på det er like mange som går fra ja til nei, som fra nei til ja. Den alternative hypotesen sier det er en forskjell.Når vi kjører testen vil vi altså forkaste \\(H_0\\) dersom p-verdien er terskelverdien (“p low, null must go”. Hvis vi forkaster nullhypotesen betyr det vi mener hypotesen om behandlingen har en effekt er styrket.Vi kan regne ut testverdien manuelt ganske enkelt ut fra krysstabellen:\\(\\chi^2=\\frac{(b-c)^2}{b+c}=\\frac{(75-785)^2}{75+785}=586.163\\)Testverdien kan sammenliknes med kritisk verdi.Vi kan se både på kjikvadratveri mot kritiske verdi og p-verdi vi har gode grunner til å forkaste nullhypotesen. Det kan derfor se ut til behandlingen virker.","code":"\nkrysstab2 <- addmargins(table(mcnemar$Før, mcnemar$Etter),c(1,2))\nkrysstab2\n#>               \n#>                Ikke_smerter Smerter  Sum\n#>   Ikke_smerter          215      75  290\n#>   Smerter               785     380 1165\n#>   Sum                  1000     455 1455\nkrysstab2\n#>               \n#>                Ikke_smerter Smerter  Sum\n#>   Ikke_smerter          215      75  290\n#>   Smerter               785     380 1165\n#>   Sum                  1000     455 1455\nqchisq(p=.05, df=1, lower.tail=FALSE)\n#> [1] 3.841459\nmcnemar3 <- matrix(c(215, 785, 75, 380), \n                   nrow = 2,\n                   dimnames = list(\"Før behandling\" = c(\"Nei\", \"Ja\"),\n                                   \"Etter behandling\" = c(\"Nei\", \"Ja\")))\nmcnemar.test(mcnemar3, correct=FALSE) \n#> \n#>  McNemar's Chi-squared test\n#> \n#> data:  mcnemar3\n#> McNemar's chi-squared = 586.16, df = 1, p-value <\n#> 0.00000000000000022"},{"path":"t-tester.html","id":"t-tester","chapter":"Kapittel 6 T-tester","heading":"Kapittel 6 T-tester","text":"R-pakker brukt dette kapittelet:En t-test brukes når man vil sammenlikne gjennomsnittsverdier og utvalget er relativt lite. Vi kan se oss tre tilfeller:Vi sammenlikner en gruppe mot en kjent gjennomsnittsstørelse og tester om gruppas gjennomsnitt er signifikant forskjellig fra det kjente gjennomsnittet («One sample t-test»).\nVi sammenlikner uavhengige gruppers gjennomsnitt å se om det er signifikant forskjell på gjennomsnittene en variabel («Independent samples t-test»)\nVi sammenlikner samme gruppe på ulike tidspunkt – observasjonene er altså ikke uavhengige av hverandre («Paired samples t-test»)\nEn t-test handler altså om å undersøke om det er signifikant forskjell på gjennomsnittsverdiene sett med data. Vi setter derfor opp en nullhypotese som sier det ikke er forskjell:\\(H_0: \\mu_1 = \\mu_2\\)Hvis vi denne nullhypotesen ikke kan forkastes (vi konkluderer med gjennomsnittene er like) betyr det eksempel en gruppe som har fått «ekte» medisin ikke skiller seg fra en gruppe som har fått placebo. Hvis vi derimot forkaster nullhypotesen vil vi konkludere med det er signifikant forskjell mellom de gruppene (på en eller annen verdi vi måler). T-testen tester denne nullhypotesen – det ikke er forskjell.en t-test får vi en testverdi. Dersom p-verdien denne testen er mindre enn 0.05 (gitt vi bruker \\(\\alpha = 0.05\\)) forkaster vi nullhypotesen (p low, null must go»), og vi vil anta det er signifikant forskjell mellom gruppene (og denne forskjellen ikke skyldes tilfeldigheter). Er p høyere enn valgt α vil vi beholde nullhypotesen.","code":"\npacman::p_load(tidyverse, readxl, summarytools, ggpubr, nortest, tseries)"},{"path":"t-tester.html","id":"students-t-test","chapter":"Kapittel 6 T-tester","heading":"6.1 Students t-test","text":"T-test, eller “Student’s t-test” som den ofte omtales som, baserer seg på en såkalt t-fordeling. En t-fordeling er ganske lik en normaldistribusjon, men har tyngre haler. Fordelingen vil variere med antall frihetsgrader, men likere og likere en normaldistribusjon ettersom utvalgsstørrelsen øker:\nFigure 6.1: t-fordeling med ulike frihetsgrader\nSå hvorfor behovet en t-distribusjon? William Sealy Gosset, .k.. “Student”, fant ut hvis man ikke er helt sikker på hva standardavviket er må man bruke et estimat på standardavviket som gjør fordelingen endrer seg litt fra normalfordelingen. Det vi omtaler som t-test er en test av en statistisk hypotese som baserer seg på Students t-distribusjon.","code":"\ncurve(dt(x, df = 2), from = -4, to = 4, col = \"blue\", ylim = c(0, 0.41))\ncurve(dt(x, df = 5), from =-4, to = 4, col = \"brown\", add = TRUE)\ncurve(dt(x, df=20), from = -4, to = 4, col = \"black\", add = TRUE)\ncurve(dnorm, -4, 4, col = \"red\", add = TRUE)\nlegend(-4, .3, legend = c(\"df=2\", \"df=10\", \"df=20\", \"Normal\"),\n       col = c(\"blue\", \"brown\", \"black\", \"red\"), lty = 1, cex = 1.2)"},{"path":"t-tester.html","id":"one-sample-t-test","chapter":"Kapittel 6 T-tester","heading":"6.2 One sample t-test","text":"La oss anta vi har en gruppe på 20 studenter som gjennomfører et nettbasert kurs anvendt kvantitativ analyse basert på bruk av en pakke med digitale læringsressurser som legger opp til mange selvøvelser. Vi tester denne gruppa opp mot en gjennomsnittsskåre på en test på 67.5 alle andre studenter (skåre 0-100) som har gjennomført samme kurs tidligere der man ikke har hatt samme tilgang til digitale øvingsoppgaver. Skårer denne testgruppa signifikant bedre enn resten av studentene?Download t-test_onesample.csvVi regner ut teststatstikken (t) slik:\\(t=\\frac{\\overline{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\)der:\\(t = t-verdi\\)\\(\\overline{x} = observert\\ gjennomsnitt\\)\\(\\mu = teoretisk/forventet\\ gjennomsnitt\\)\\(s = standardavviket\\ \\ utvalget/observerte\\)\\(n=utvalgsstørrelse/antall\\ observerte\\)Vi henter nødvendige verdier fra datasettet:Dette gir da:\\(t=\\frac{72.3-67.5}{\\frac{9.52}{\\sqrt{20}}}=\\frac{4.8}{2.129}=2.255\\)Vi sammenlikner t-verdien 2.255 med kritisk verdi, f.eks. . Vi finner verdien 1.729. Hvis t-verdien er større enn kritisk verdi: forkast nullhypotesen. forkaster vi nullhypotsesen fordi 2.255 er større enn 1.729. Vår alternative hypotese om det er signifikant forskjell er styrket.R bruker vi:Vi ser også t-testverdien er større enn kritisk verdi 2.2547129 > 1.7291328). tillegg ser vi p-verdien er 0.0361452 (“p low, null must go”).","code":"\ndescr(ttestonesample$Score)\n#> Descriptive Statistics  \n#> ttestonesample$Score  \n#> N: 20  \n#> \n#>                      Score\n#> ----------------- --------\n#>              Mean    72.30\n#>           Std.Dev     9.52\n#>               Min    50.00\n#>                Q1    66.00\n#>            Median    75.00\n#>                Q3    79.00\n#>               Max    89.00\n#>               MAD     9.64\n#>               IQR    13.00\n#>                CV     0.13\n#>          Skewness    -0.45\n#>       SE.Skewness     0.51\n#>          Kurtosis    -0.50\n#>           N.Valid    20.00\n#>         Pct.Valid   100.00\nttest <- t.test(ttestonesample$Score, mu = 67.5, alternative = \"two.sided\")\nttest\n#> \n#>  One Sample t-test\n#> \n#> data:  ttestonesample$Score\n#> t = 2.2547, df = 19, p-value = 0.03615\n#> alternative hypothesis: true mean is not equal to 67.5\n#> 95 percent confidence interval:\n#>  67.84422 76.75578\n#> sample estimates:\n#> mean of x \n#>      72.3\nttestqt <- qt(0.05, 19, lower.tail=FALSE)\nttestqt\n#> [1] 1.729133"},{"path":"t-tester.html","id":"sjekk-av-forutsetninger-for-one-sample-t-test","chapter":"Kapittel 6 T-tester","heading":"6.2.1 Sjekk av forutsetninger for one sample t-test","text":"Tilfeldig utvalg fra en definert/gitt populasjonVariabelen må være kontinuerligPopulasjonen er normalfordeltVi ser spesielt på nr 3. Det finnes flere måter å se på normalitetsforutsetningen, både grafisk og formelle statistiske tester. Vi skal vise en formell test - Shapiro-Wilks som ofte brukes. Andre eksempler er Kolmogorov-Smirnov og Anderson-Darling. Razali Wah (2011) finner en sammenlinende studie Shapiro-WIlks fungerer bra.Vi sammenlikner testverdien med 0,05 (gitt vi bruker 0,05 som signifikansnivå). Dersom testverdien er 0,05 indikerer det dataene er normalfordelte. Hvis testeverdien er 0,05 indikerer det dataene avviker fra normalfordelingen. Dette er ikke tilfelle (0.5855703).","code":"\nshapirotest <- shapiro.test(ttestonesample$Score)\nshapirotest\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ttestonesample$Score\n#> W = 0.96205, p-value = 0.5856"},{"path":"t-tester.html","id":"paired-samples-t-test","chapter":"Kapittel 6 T-tester","heading":"6.3 Paired samples t-test","text":"Paired samples t-test brukes når gruppene/målingene ikke er uavhengige av hverandre. tilfeller hvor vi eksempel har en gruppe som er testet ganger på ulike tidspunkt er ikke observasjonene uavhengige av hverandre.Paired samples t-test og one sample t-test er på en måte “samme sak” - altså, der en one sample t-test sammenlikner gjennomsnittet fra et utvalg med en kjent størrelse kalkulerer en paired samples t-test forskjellen mellom de gjennomsnittene deretter å gjennomføre en one sample t-test på forskjellen.La oss anta vi har en gruppe på 15 studenter som vi har testet ganger. Imellom testene har de gjennomført en aktivitet som skal trigge hukommelsen.Download t-test_paired.csvVi kan regne ut testverdien slik:\\(t=\\frac{\\sum{d}}{\\frac{n(\\sum{d^2})-(\\sum{d}^2)}{(n-1)}}\\)der\\(t = testverdi\\)\\(d=differansen\\ innad\\ \\ hvert\\ par\\)\\(n=antall\\ \\ utvalget\\)Gjennom en manuell utregning finner vi \\(t=0.02\\)Vi gjennomfører en t-test R:ser vi p er 0,05. Dvs vi kan ikke forkaste nullhypotesen. Vi kan derfor ikke si det er en signifikant forskjell mellom de gruppene.\nFigure 2.7: Pre-Post sammenlikning\nAv grafen og tabellen ser vi spredningen er mindre etter aktiviteten (se f.eks. på standardavviket tabellen).Vi kan se fra ulike tester ovenfor forutsetningen om normalfordeling ser ut til å være innfridd. Normalt trenger vi ikke gjøre alle tre testene, men vi har tatt de med å vise relevant kode om man skulle ha behov den ene eller den andre - de tester normalitet fra ulike vinkler, og svært mange tilfeller vil man se Shapiro-Wilks brukt. Robusthetstester viser også Shapiro-Wilks viser bra robusthet og egenskaper sammenliknet med alternativer (Razali Wah 2011).","code":"\npairedsamples <- read.csv(\"t-test_paired.csv\")\ndescr(pairedsamples)\n#> Descriptive Statistics  \n#> pairedsamples  \n#> N: 15  \n#> \n#>                       Post      Pre\n#> ----------------- -------- --------\n#>              Mean    76.19    76.21\n#>           Std.Dev     3.69     4.87\n#>               Min    70.76    67.82\n#>                Q1    72.78    70.81\n#>            Median    77.34    77.60\n#>                Q3    78.68    79.57\n#>               Max    83.02    83.36\n#>               MAD     3.47     5.02\n#>               IQR     5.50     7.25\n#>                CV     0.05     0.06\n#>          Skewness    -0.09    -0.39\n#>       SE.Skewness     0.58     0.58\n#>          Kurtosis    -1.18    -1.31\n#>           N.Valid    15.00    15.00\n#>         Pct.Valid   100.00   100.00\nt.test(pairedsamples$Pre, pairedsamples$Post, paired = TRUE, alternative = \"two.sided\")\n#> \n#>  Paired t-test\n#> \n#> data:  pairedsamples$Pre and pairedsamples$Post\n#> t = 0.020764, df = 14, p-value = 0.9837\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.934057  2.991423\n#> sample estimates:\n#> mean difference \n#>      0.02868286\nggpaired(pairedsamples, cond1 = \"Pre\", cond2 = \"Post\",\n    fill = \"condition\")\ndescr(pairedsamples)\n#> Descriptive Statistics  \n#> pairedsamples  \n#> N: 15  \n#> \n#>                       Post      Pre\n#> ----------------- -------- --------\n#>              Mean    76.19    76.21\n#>           Std.Dev     3.69     4.87\n#>               Min    70.76    67.82\n#>                Q1    72.78    70.81\n#>            Median    77.34    77.60\n#>                Q3    78.68    79.57\n#>               Max    83.02    83.36\n#>               MAD     3.47     5.02\n#>               IQR     5.50     7.25\n#>                CV     0.05     0.06\n#>          Skewness    -0.09    -0.39\n#>       SE.Skewness     0.58     0.58\n#>          Kurtosis    -1.18    -1.31\n#>           N.Valid    15.00    15.00\n#>         Pct.Valid   100.00   100.00\nforskjell <- pairedsamples$Post - pairedsamples$Pre\nshapiro.test(forskjell)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  forskjell\n#> W = 0.94997, p-value = 0.5239\nad.test(forskjell)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  forskjell\n#> A = 0.44249, p-value = 0.2483\njarque.bera.test(forskjell)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  forskjell\n#> X-squared = 0.26188, df = 2, p-value = 0.8773"},{"path":"t-tester.html","id":"independent-samples-t-test","chapter":"Kapittel 6 T-tester","heading":"6.4 Independent samples t-test","text":"En vanlig situasjon er vi har grupper som skal sammenliknes. Hvis gruppene er uavhengige av hverandre (motsetning til paired samples t-test), kan vi bruke independent samples t-test.eksempelet ønsker vi å sammenlikne grupper studenter: en gruppe har ren nettundervisning, mens den andre gruppa har hybrid undervisning (nett og fysisk). Vi har eksamensresultater begge gruppene. Kan vi ut fra eksamensresultatene si om gruppene er signifikant forskjellige fra hverandre?Vi ser datasettet består av 200 observasjoner av studenter som enten har gjennomført ren nettundervisning eller en hybrid undervisning (nett og fysisk undervisning).Vi kan regne ut testverdien:\\(t=\\frac{\\overline{x}_A - \\overline{x}_B}{\\sqrt{\\biggl({\\frac{(\\sum ^2-\\frac{(\\sum )^2}{n_A})+(\\sum B^2-\\frac{(\\sum B)^2}{n_B})}{n_A+n_B-2}\\biggr)}*\\biggl(\\frac{1}{n_A}+\\frac{1}{n_B}\\biggr)}}\\)der:\\(= Variabel 1 - \\ vårt\\ tilfelle\\ \"nett\"\\)\\(B = Variabel 2 - \\ vårt\\ tilfelle\\ \"hybrid\"\\)\\((\\sum )^2 = Summen\\ av\\ 'ene\\ kvadrert\\)\\((\\sum B)^2 = Summen\\ av\\ B'ene\\ kvadrert\\)\\(A_2 = Enkeltverdien\\ \\ kvadrert\\ (hver\\ enkelt\\ verdi)\\)\\(B_2 = Enkeltverdien\\ B\\ kvadrert\\ (hver\\ enkelt\\ verdi)\\)\\(\\sum ^2 = Summen\\ av\\ de\\ kvadrerte\\ 'ene\\)\\(\\sum B^2 = Summen\\ av\\ de\\ kvadrerte\\ B'ene\\)\\(\\overline{x}_A = Gjennomsnitt\\ \\ variabel\\ \\)\\(\\overline{x}_B = Gjennomsnitt\\ \\ variabel\\ B\\)\\(n_A = antall\\ \\ variabelen\\ \\)\\(n_B = antall\\ \\ variabelen\\ B\\)Vi kan legge merke til t-testen R velger Welch t-test stedet Student t-test. Forskjellen ligger Welch t-test ikke forutsetter lik varians, mens Student t-test gjør dette.dette eksempelet har vi ingen forskjell mellom Welch og Student t-test (10.979841vs. 10.979841). Noen programmer vil automatisk gi deg den ene eller den andre, alternativt begge . Forskjellen på Student’s t og Welch’s t er altså førstnevnte forutsetter begge gruppene har likt standardavvik (“assumption equal variances”). virkeligheten er dette ofte ikke tilfelle (det er liten grunn til å tro gruppene har likt standardavvik hvis de ikke har lik gjennomsnittsverdi). slike tilfeller er Welch’s t en mer robust test.Ut fra testen kan vi si det er en statistisk signifikant forskjell mellom de gruppene. Hvis vi ser på resultatet fra testen ser vi hybridgruppa har høyere snitt enn nettgruppa. Dette forteller oss hybridgruppa skårer signifikant høyere enn nettgruppa.","code":"\nindependent <- read.csv(\"t-test_independent.csv\")\nHeadTail <- function(independent){rbind(head(independent),tail(independent))}\nHeadTail(independent)\n#>     Studentnr   Type    Score\n#> 1           1   Nett 77.36150\n#> 2           2   Nett 79.39069\n#> 3           3   Nett 71.88117\n#> 4           4   Nett 80.00219\n#> 5           5   Nett 81.99973\n#> 6           6   Nett 73.35320\n#> 195       195 Hybrid 93.46116\n#> 196       196 Hybrid 83.60819\n#> 197       197 Hybrid 79.41945\n#> 198       198 Hybrid 78.83346\n#> 199       199 Hybrid 94.84745\n#> 200       200 Hybrid 86.55348\nttestind <- t.test(Score ~ Type, independent)\nttestind\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  Score by Type\n#> t = 10.98, df = 179.14, p-value < 0.00000000000000022\n#> alternative hypothesis: true difference in means between group Hybrid and group Nett is not equal to 0\n#> 95 percent confidence interval:\n#>   7.215675 10.377521\n#> sample estimates:\n#> mean in group Hybrid   mean in group Nett \n#>             84.58566             75.78906\nttestind2 <- t.test(Score ~ Type, independent, var.equal=TRUE)\nttestind2\n#> \n#>  Two Sample t-test\n#> \n#> data:  Score by Type\n#> t = 10.98, df = 198, p-value < 0.00000000000000022\n#> alternative hypothesis: true difference in means between group Hybrid and group Nett is not equal to 0\n#> 95 percent confidence interval:\n#>   7.216698 10.376497\n#> sample estimates:\n#> mean in group Hybrid   mean in group Nett \n#>             84.58566             75.78906\nttestind$estimate\n#> mean in group Hybrid   mean in group Nett \n#>             84.58566             75.78906"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"variansanalyse---anova-analysis-of-variance","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","text":"R-pakker brukt dette kapittelet:kapittel 2 så vi på korrelasjon mellom enkeltvariabler (bivariat korrelasjon) ) og sammenlikninger mellom grupper eller en gruppe på tidspunkter (kjikvadrattester og t-tester). Regresjonsanalyser, som vi kommer itlbake til neste kapittel, kan også ses på som analyser av forholdet mellom enkeltvariabler.Variansanalyse - heretter ANOVA - er en samlebetegnelse på flere statistiske metoder der man tester likheter mellom eller flere utvalg. Har man grupper vil ANOVA og en t-test gi samme resultat (hypotesen \\(H_0: \\mu_1 = \\mu_2\\) mot \\(H_A: \\mu_1 \\neq \\mu_2\\)). Man kan faktisk (prinsippet) gjennomføre t-tester x antall kombinasjoner av y antall grupper, men risikoen type-feil øker sammenliknet med en ANOVA test på samme data.en ANOVA snakker man om elementer som utgjør den totale variansen: varians innad gruppen og varians mellom gruppene. Det er derfor vanlig å dele ANOVA inn hovedgrupper: enveis (=en faktor) og toveis (= faktorer) ANOVA. Enveis analyser ser kun på en egenskap som varierer mellom gruppene, mens toveis inkluderer egenskaper som kan variere mellom enhetene gruppene. Toveis ANOVA gir derfor innsikt både hovedeffekter og interaksjonseffekter.Det ANOVA innbærer - helt grunnleggende - er å teste om variansen mellom gruppene er større enn variansen innad gruppene.Vi skal dette kapittelet se på:Enveis mellom grupper ANOVA (“One-way -groups”)Enveis avhengig ANOVA (“One-way repreated measures”)Toveis mellom grupper ANOVA (“Two-way groups”)Blandet design ANOVA (“Mixed -within”)","code":"\npacman::p_load(readxl, summarytools, ggpubr, tidyverse, writexl, car, lsr, haven, rstatix, ez, effectsize, sjstats, effsize, apaTables, performance, afex)"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"enveis-mellom-grupper-anova-one-way-between-groups-anova","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"7.1 Enveis mellom grupper ANOVA (“One-way between-groups ANOVA”)","text":"Enveis ANOVA (one-way ANOVA) er analyser der vi har en uavhengig variabel som er målt på/har flere nivåer. Hvis vi f.eks. vil undersøke effekt av ulike opplæringstiltak selgere (den uavhengige variabelen er opplæringsmetode) som kan bestå av tre grupper (metoder opplæring): e-læring/egenlæring, gruppeopplæring og observasjon/mentorering av erfaren selger) vil den avhengige variabelen kunne være ukentlig salg (kroner, enheter e.l.).\nAnalysen sammenlikner variansen mellom gruppene (metodene opplæring) med variansen internt hver gruppe. Teststatistikken kalles F (F ratio):\\(F = \\frac{Varians\\ mellom\\ gruppene}{Varians\\ innad\\ \\ gruppen}\\)En høy F-verdi vil innebære det er høyere varians mellom gruppene enn internt gruppene. En signifikant F-verdi betyr vi kan forkaste hypotesen (\\(H_0\\))) om det gjennomsnittene er like.dette eksempelet skal vi bruke et datasett fra Pallant (2010) og trekker ut de variablene vi trenger eksempelet. Vi gjør også om variabelen “agegp3” til faktor (dette er nødvendig R, men andre programmer “fikser” dette selv).Download Pallant_Survey_ANOVA1.xlsxVi skal bruke en variabel datasettet som deler respondentene inn tre aldersgrupper (“agegp3”) og en variabel som måler total optimisme (“toptim”) der respondentene skårer på en skala fra 6 til 30 (30 er høyeste nivået av optimisme).Hypotesen er altså:\\(H_0: \\mu_1 = \\mu_2,\\ dvs.\\ gjennomsnittet\\ til\\ de\\ ulike\\ gruppene\\ er\\ like\\)\\(H_A: \\mu_1 \\neq \\mu_2\\ dvs.\\ gjennomsnittet\\ til\\ de\\ ulike\\ gruppene\\ er\\ ulike\\)Vi kan først se på datasettet:Vi kan se gjennomsnittene er forskjellige de tre gruppene, men vi vet ikke om denne forskjellen er statistisk signifikant.Vi kan også se på dette grafisk:\nFigure 4.2: Boxplot tre variabler\nOm vi skal anta noe ut fra grafen vil det kunne være gruppe 1 og 2 ikke er signifikant ulike, mens gruppe 3 kanskje skiller seg statistisk signifikant ut.Siden p-verdien er 0.0101328 kan vi konkludere med det er signifikante forskjell et sted (mellom eller flere grupper) variabelen (agegp3). Men vi kan ikke ut fra dette si hvor – altså hvilken gruppe som er signifikant forskjellig fra de andre).Vi bør sjekke forutsetningen om homogenitet variansen. Homogen (lik) varians har vi når standardavvikene ulike grupper er omtrent like.Bartletts test brukes dersom vi har normalfordelte data. Vi kan derfor sjekke dette:Vi kan teste normalfordeling:Siden testverdien er 0.0000007 (< 0.05) må vi anta dataene er signifikant forskjellig fra normalfordelingen. Dette betyr vi bør bruke Levenes og/eller Fligner-Killeen.En annen test av homogenitet varians som framholdes som robust avvik fra normalfordeling er altså “Fligner-Killeen test”:Tolkningen av alle tre testene homogenitet varians er den samme: Hvis p < 0.05 er variansen ikke lik. Det fremheves imidlertid dersom gruppene er tilnærmet like store er ikke denne forutsetningen kritisk (eller sågar nødvendig) - ANOVA (og t-tester) er generelt robuste forhold til brudd på forutsetningen om homogen varians dersom gruppene er relativt like (Solutions 2013). vårt eksempel viser f.eks. Levenes test (p = 0.49) vi har homogenitet variansen.Et alternativ er å gjøre en såkalt Welch enveis test. Welch test forutsetter ikke homogen varians:Vi kan deretter se nærmere på hvilke grupper som er statistisk signifikant forskjellige:Vi ser av tabellen gruppene 1 og 3 er signifikant forskjellige. De andre parene - 1-2 og 2-3 ikke har signifikante forskjeller.\nDette kan også visualiseres:\nFigure 2.8: Plott av TukeyHSD\nGruppene 1-2 og 2-3 har konfiendsintervall som inneholder 0, mens 1-3 ikke har det (konfidensintervallene inkluderer verdien 0).Til slutt kan vi være interessert å vurdere hvor stor effektstørrelsen (eta squared = \\(\\eta^2\\)).J. Cohen (1988) angir følgende forslag på grenseverdier tolkning av \\(\\eta^2\\):Liten effekt: 0.01Middels effekt: 0.06Stor effekt: 0.14I vårt eksempel er det altså en statistisk signifikant, men liten forskjell (noe vi sannsynligvis fikk en mistanke om plottet lenger opp der vi antok det kanskje kunne være en forskjell gruppe 3), noe vi også fikk en indikasjon på tabellen med forskjellene gjennomsnittsverdier gruppene. Dette er ikke uvanlig - vi har 435 observasjoner, og store utvalg kan selv små forskjeller gi statistisk signifikans og vi bør tolke resultatene med det øye. en tolkning bør vi også vurdere hvilken praktisk forskjell det er mellom gruppene selv om vi har funnet en statistisk signifikant forskjell.","code":"\ngroup_by(optimisme, agegp3) %>%\n  summarise(\n    count = n(),\n    mean = mean(toptim, na.rm = TRUE),\n    sd = sd(toptim, na.rm = TRUE)\n  )\n#> # A tibble: 3 × 4\n#>   agegp3 count  mean    sd\n#>   <fct>  <int> <dbl> <dbl>\n#> 1 1        149  21.4  4.55\n#> 2 2        153  22.1  4.15\n#> 3 3        137  23.0  4.49\nggboxplot(optimisme, x = \"agegp3\", y = \"toptim\", \n          color = \"agegp3\", palette = c(\"steelblue\", \"red\", \"darkgreen\"),\n          order = c(\"1\", \"2\", \"3\"),\n          ylab = \"Total optimisme\", xlab = \"Aldersgruppe\")\nresultataov1 <- aov(toptim ~ agegp3, data = optimisme)\nsummary(resultataov1)\n#>              Df Sum Sq Mean Sq F value Pr(>F)  \n#> agegp3        2    179   89.53   4.641 0.0101 *\n#> Residuals   432   8334   19.29                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 4 observations deleted due to missingness\nbartlett.test(toptim ~ agegp3, data = optimisme)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  toptim by agegp3\n#> Bartlett's K-squared = 1.4561, df = 2, p-value =\n#> 0.4828\nshapirotest <- shapiro.test(optimisme$toptim)\nshapirotest\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  optimisme$toptim\n#> W = 0.97471, p-value = 0.0000007366\nleveneoptimisme <- leveneTest(toptim ~ agegp3, data = optimisme)\nleveneoptimisme\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>        Df F value Pr(>F)\n#> group   2  0.7147 0.4899\n#>       432\nflignertest <- fligner.test(toptim ~ agegp3, data = optimisme)\nflignertest\n#> \n#>  Fligner-Killeen test of homogeneity of variances\n#> \n#> data:  toptim by agegp3\n#> Fligner-Killeen:med chi-squared = 1.5588, df = 2,\n#> p-value = 0.4587\nwelchoptimisme <- oneway.test(toptim ~ agegp3, data = optimisme)\nwelchoptimisme\n#> \n#>  One-way analysis of means (not assuming equal\n#>  variances)\n#> \n#> data:  toptim and agegp3\n#> F = 4.38, num df = 2.00, denom df = 284.51, p-value =\n#> 0.01338\nTukeyHSD(resultataov1)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = toptim ~ agegp3, data = optimisme)\n#> \n#> $agegp3\n#>          diff        lwr      upr     p adj\n#> 2-1 0.7440309 -0.4489781 1.937040 0.3080109\n#> 3-1 1.5950113  0.3636472 2.826376 0.0069296\n#> 3-2 0.8509804 -0.3687705 2.070731 0.2296685\nplot(TukeyHSD(resultataov1, conf.level=.95), las = 2)\nlsr::etaSquared(resultataov1)\n#>            eta.sq eta.sq.part\n#> agegp3 0.02103477  0.02103477"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"enveis-avhengig-anova-one-way-repreated-measures","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"7.2 Enveis avhengig ANOVA (“One-way repreated measures”)","text":"stedet å se på ulike grupper mot en avhengig variabel målt på et tidspunkt kan vi være interessert å se på en gruppe målt på ulike tidspunkt («repeated»). Hvis du har sett gjennom kapittelet om t-tester vil du se paralleller til designet.Vi bruker også et eksempel fra Pallant (2010) som vi først modifiserer.Enveis avhengig ANOVA (“One-way repreated measures”)R arrangerer vi dataene “long” format og lager faktor:Modifisert datasett ligger :\nDownload experim.xlsxSå kan vi se på gjennomsnitt og standardavvik:Og det samme grafisk:\nFigure 7.1: Boxplott tre variabler\nVi sjekker forutsetning om normalfordeling:Disse testene peker mot vi har normalfordelte data.Det kan også være verdt å sjekke uteliggere siden disse kan ha stor påvirkning på analysen senere:Visuelt får vi samme indikasjon:\nFigure 2.12: Sjekk av uteliggere grafisk\nNår vi har “repeated measures” ANOVA erstatter vi testen homogenitet varians med den såkalte “Mauchly’s test sphericity” (Field 2009a). Sphericity referer til likhet variansen forskjellene nivåene/tidspunktene målingene er tatt. Sagt på en annen måte; Hvis du har tre måletidspunkter (som dette tilfellet) må variansen forskjellen mellom tid 1 og 2 være tilnærmet lik variansen forskjellen mellom tid 2 og 3.å vise bruk av en annen pakke R kjører vi analysen med pakken “ez”, som automatisk gir oss “vanlig” ANOVA, tester sferisitet (Mauchly’s test) og - dersom denne er signifikant - videre gir oss resultater korrigert (“Spericity Corrections”):Siden vi ser p = 0 (< 0.05) Mauchly’s test bruker vi nederste del av resultatet ovenfor. Det er tester som vises der som begge gjør korreksjoner på antall frihetsgrader gitt forutsetningen om sferisitet ikke er oppfyllt. Vi går ikke inn på hvordan disse korreksjonene gjøres, ut F verdiene (= forholdet mellom systematisk og usystematisk varians) vil være like alle tre, men signifikansverdien vil endres ettersom det er den kritiske verdien gjennom frihetsgradene som endres ved de korreksjonene).Greenhouse-Geisser (GG) som gir \\(\\epsilon\\) (epsilon) - som gir en verdi mellom 0 og 1 på hvor langt de foreliggende dataene er fra det ideelle/optimale. vårt tilfelle er epsilon = 0.71 (se “GGe” resultatet ovenfor). Karadimitriou (n.d.) angir man bør bruke Huynh-Feldt dersom \\(\\epsilon > 0.75\\) eller om man har små utvalg (10). kan vi derfor se på verdien p[GG] som er < 0.005. Dette indikerer det er en signifikant effekt/forskjell et sted mellom måletidspunktene.Greenhouse-Geisser (GG) som gir \\(\\epsilon\\) (epsilon) - som gir en verdi mellom 0 og 1 på hvor langt de foreliggende dataene er fra det ideelle/optimale. vårt tilfelle er epsilon = 0.71 (se “GGe” resultatet ovenfor). Karadimitriou (n.d.) angir man bør bruke Huynh-Feldt dersom \\(\\epsilon > 0.75\\) eller om man har små utvalg (10). kan vi derfor se på verdien p[GG] som er < 0.005. Dette indikerer det er en signifikant effekt/forskjell et sted mellom måletidspunktene.Huynh-Feldt (HF) - som brukes/tolkes på lik måte som Greenhouse-Geisser dersom betingelsene det er tilstede (som nevnt forrige punkt.)Huynh-Feldt (HF) - som brukes/tolkes på lik måte som Greenhouse-Geisser dersom betingelsene det er tilstede (som nevnt forrige punkt.)Vi kan oppsummere dette på APA-format:Vi ønsker da å se på de parvise sammenlikningene å se hvor vi har signifikante forskjeller:tabellen vises ikke den mest interessante kolonnen “p.adj”, men verdiene er:Vi kan se det er statistisk signifikant forskjell mellom alle tre parene: confid1 - confid2 = 0.003, confid1 - confid3 = 0.000 og confid2 - confid3 (0.000) (p er selvagt ikke 0, men 0.000 angir f.eks. siste par 0.0000001).Til slutt ser vi på effektstørrelsen (\\(\\eta^2\\)). Denne ble gitt automatisk da vi brukte pakken “ez” som “ges” (“ges” = Generalized Eta-Squared). Verdien “ges” tabellen = 0.1777185). Vi kan også hente verdien direkte (funksjonen “eta-squared” klarer ikke å hente fra ezANOVA, men som sagt ligger det som “ges” output fra ezANOVA):Vi kan gjenta fra J. Cohen (1988) som grenseverdier tolkning av \\(\\eta^2\\):Liten effekt: 0.01Middels effekt: 0.06Stor effekt: 0.14’Når det gjelder effektstørrelse anbefaler Field (2009a) \\(\\omega^2\\) (“omega squared”) som det beste målet effektstørrelse denne typen analyser (jfr. Kirk 1996). \\(\\omega^2\\) regnes ofte som mindre systematisk skjev (“biased”) enn \\(\\eta^2\\), spesielt mindre utvalg (Ben-Shachar, Lüdecke, Makowski 2020).\\(\\omega^2\\) vil ha en verdi på \\(\\pm 1\\). 0 indikerer ingen effekt. Et fiffig hjelpemiddel finnes pakken “effectsize” (Ben-Shachar, Lüdecke, Makowski 2020), som - basert på hvilken verdi vi får - indikerer effektstørrelse en lang rekke effektmål.Litt avhengig av hvilke retningslinjer vi velger å følge (J. Cohen (1988) eller Field, Miles, Field (2012b)) har vi en middels til stor effekt.Vi kan si det er en signifikant økning testscore mellom tidspunkt 1 og tidspunkt 2, og tispunkt 2 til tidspunkt 3 (økning fordi gjennomsnittet har økt), og denne effekten er middels til stor samlet sett. Vi kan derfor si “intervensjonen” mellom tid 1 og tid 2 har hatt en signifikant effekt på gjennomsnittlig selvtillit ift statistikk, og tiden mellom tid 2 og tid 3 tilsvarende har ført til en signifikant høyere selvtillit. Så både intervensjonen og tidseffekten er signifikante. Dette gjelder hele modellen. Det kan selvsagt være interessant å se effekten fra tid 1 til tid 2, og videre tid 2 til tid 3.Vi ser Cohens d tid 1 - tid 2 er 0.5228587 og tid 2 til tid 3 er 0.5862199. Dette gir følgende tolkning (Ben-Shachar, Lüdecke, Makowski 2020), jfr. Gignac Szodorai (2016):Vi kan med andre ord si intervensjonen gir en moderat/middels effekt, og tidsperioden fra intervensjon til siste måling også gir en moderat/middels effekt. Når det gjelder effektstørrelser har vi altså nå mål på effekten hele modellen og de ulike “stegene” separat.En hendig funksjon pakken “sjstats” gir en samlet oppsummering (inneholder verdier vi ikke har vært innom tillegg):","code":"\nselvtillit2 <- selvtillit %>% \n    select(6, 9, 12)\nselvtillit2$ID <- c(1:30)\nselvtillit2 <- selvtillit2[, c(4, 1, 2, 3)]\nselvtillit3 <- selvtillit2 %>%\n  gather(key = \"time\", value = \"score\", confid1, confid2, confid3) %>%\n  convert_as_factor(ID)\n#> Warning: attributes are not identical across measure variables;\n#> they will be dropped\nselvtillit3 %>%\n  group_by(time) %>%\n  get_summary_stats(score, type = \"mean_sd\")\n#> # A tibble: 3 × 5\n#>   time    variable     n  mean    sd\n#>   <chr>   <chr>    <dbl> <dbl> <dbl>\n#> 1 confid1 score       30  19    5.37\n#> 2 confid2 score       30  21.9  5.59\n#> 3 confid3 score       30  25.0  5.20\nggboxplot(\n  selvtillit3, \n  x = \"time\", \n  y = \"score\",\n  color = \"time\", \n  palette = \"jco\"\n  )\nselvtillit3 %>%\n  group_by(time) %>%\n  shapiro_test(score)\n#> # A tibble: 3 × 4\n#>   time    variable statistic      p\n#>   <chr>   <chr>        <dbl>  <dbl>\n#> 1 confid1 score        0.936 0.0700\n#> 2 confid2 score        0.965 0.419 \n#> 3 confid3 score        0.959 0.294\nselvtillit3 %>%\n  group_by(time) %>%\n  identify_outliers(score)\n#> [1] time       ID         score      is.outlier is.extreme\n#> <0 rows> (or 0-length row.names)\nggqqplot(selvtillit3, \"score\", facet.by = \"time\")\nresultataov4 <- ezANOVA(data =selvtillit3, dv = score, wid = ID, within = time, type=3)\n#> Warning: Converting \"time\" to factor for ANOVA.\nres <- resultataov4$ANOVA\nresultataov4\n#> $ANOVA\n#>   Effect DFn DFd        F                  p p<.05\n#> 2   time   2  58 33.18623 0.0000000002468359     *\n#>         ges\n#> 2 0.1777185\n#> \n#> $`Mauchly's Test for Sphericity`\n#>   Effect         W            p p<.05\n#> 2   time 0.5923923 0.0006554377     *\n#> \n#> $`Sphericity Corrections`\n#>   Effect       GGe            p[GG] p[GG]<.05       HFe\n#> 2   time 0.7104252 0.00000005729919         * 0.7365258\n#>              p[HF] p[HF]<.05\n#> 2 0.00000003501919         *#> \n#> \n#> ANOVA results\n#>  \n#> \n#>  Predictor df_num df_den Epsilon     F    p ges\n#>       time   1.42  41.20    0.71 33.19 .000 .18\n#> \n#> Note. df_num indicates degrees of freedom numerator. df_den indicates degrees of freedom denominator. \n#> Epsilon indicates Greenhouse-Geisser multiplier for degrees of freedom, \n#> p-values and degrees of freedom in the table incorporate this correction.\n#> ges indicates generalized eta-squared.\n#> \npadj <- selvtillit3 %>%\n  pairwise_t_test(\n    score ~ time, paired = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npadj\n#> # A tibble: 3 × 10\n#>   .y.   group1  group2     n1    n2 statistic    df        p\n#> * <chr> <chr>   <chr>   <int> <int>     <dbl> <dbl>    <dbl>\n#> 1 score confid1 confid2    30    30     -3.30    29  3   e-3\n#> 2 score confid1 confid3    30    30     -7.25    29  5.58e-8\n#> 3 score confid2 confid3    30    30     -7.08    29  8.72e-8\n#> # … with 2 more variables: p.adj <dbl>, p.adj.signif <chr>\npadj$p.adj\n#> [1] 0.008000000 0.000000167 0.000000262\nresultataov5 <- aov(score ~ time, data = selvtillit3)\neta_squared(resultataov5)\n#> For one-way between subjects designs, partial eta squared is equivalent to eta squared.\n#> Returning eta squared.\n#> # Effect Size for ANOVA\n#> \n#> Parameter | Eta2 |       95% CI\n#> -------------------------------\n#> time      | 0.18 | [0.06, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\nomega_squared(resultataov5)\n#> For one-way between subjects designs, partial omega squared is equivalent to omega squared.\n#> Returning omega squared.\n#> # Effect Size for ANOVA\n#> \n#> Parameter | Omega2 |       95% CI\n#> ---------------------------------\n#> time      |   0.16 | [0.05, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\ninterpret_omega_squared(0.16, rules = \"field2013\")\n#> [1] \"large\"\n#> (Rules: field2013)\ninterpret_omega_squared(0.16, rules = \"cohen1992\")\n#> [1] \"medium\"\n#> (Rules: cohen1992)\ncohen1 <- cohen.d(selvtillit2$confid2, selvtillit2$confid1)\ncohen2 <- cohen.d(selvtillit2$confid3, selvtillit2$confid2)\ncohen1\n#> \n#> Cohen's d\n#> \n#> d estimate: 0.5228587 (medium)\n#> 95 percent confidence interval:\n#>        lower        upper \n#> -0.002739233  1.048456706\ncohen2\n#> \n#> Cohen's d\n#> \n#> d estimate: 0.5862199 (medium)\n#> 95 percent confidence interval:\n#>      lower      upper \n#> 0.05839449 1.11404532\ninterpret_cohens_d(0.52, rules = \"gignac2016\")\n#> [1] \"moderate\"\n#> (Rules: gignac2016)\ninterpret_cohens_d(0.57, rules = \"gignac2016\")\n#> [1] \"moderate\"\n#> (Rules: gignac2016)\nsjstats::anova_stats(resultataov5)\n#> term      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n#> --------------------------------------------------------------------------------------------------------------------------------------------\n#> time      |  2 |  546.467 | 273.233 |     9.402 |  < .001 | 0.178 |         0.178 |   0.157 |           0.157 |     0.159 |    0.465 | 0.979\n#> Residuals | 87 | 2528.433 |  29.062 |           |         |       |               |         |                 |           |          |"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"toveis-mellom-grupper-anova-two-way-between-groups","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"7.3 Toveis mellom grupper ANOVA (“Two-way between groups”)","text":"ANOVA-analysene vi til nå har gjort har inneholdt en uavhengig variabel. Det vi skal se på nå er tilfeller der vi har uavhengige variabler og flere grupper de ulike gruppene. Vi kan da se på både individuelle effekter og «sameffekter» av de uavhengige på den avhengige variabelen, f.eks. kan vi ønske å se på både kjønn og alder som uahvengige variabler opp mot optimisme som den avhengige variabelen.Toveis mellom grupper ANOVA (“Two-way groups”)Vi skal fortsette å bruke datasettet fra Pallant (2010).Download Pallant_Survey_ANOVA2.xlsxVi kan se på dataene grafisk:\nFigure 7.2: Grafisk framstilling tre variabler til toveis ANOVA\nDet første vi kan legge merke til er “sex*agegp3”, altså interaksjonseffekten mellom de uavhengige variablene, ikke er signifikant. Dette betyr det ikke er noen signifikant forskjell effekten av alder mellom menn og kvinner (vi finner ikke en signifikant interaksjonseffekt). Hadde vi derimot fått en signifikant forskjell ville det vært vanskelig å tolke den totale effekten. Siden vi ikke hadde en signifikant effekt kan vi se på totaleffekten.Vi ser da på radene «agegp3» og «sex». Vi kan se det er en signifikant effekt av alder, men ikke kjønn. Kvinner og menn skårer ikke signifikant forskjellig, men alder har en signifikant betydning.Videre ser vi på effektstørrelsen. alder, som var signifikant, finner vi en liten effekt (Partial Eta Squared = 0.018 (kolonnen “ges”), hvilket er en middels effekt - jfr. tidligere grenseverdier gitt av J. Cohen (1988).Forutsetning om homogenitet varians:er Levene’s test ikke signifikant. Det innebærer forutsetningen ikke er brutt da vi har lik varians den avhengige variabelen alle gruppene (homogenitet variansen).","code":"\nggpubr::ggboxplot(optimisme2, x = \"agegp3\", y = \"toptim\", color = \"sex\", palette = c(\"#00AFBB\", \"#E7B800\"))\nresultataov6 <- optimisme2 %>%\n    anova_test(toptim ~ sex + agegp3 + sex*agegp3, type = \"III\")\n#> Coefficient covariances computed by hccm()\nresultataov6\n#> ANOVA Table (type tests)\n#> \n#>       Effect DFn DFd     F     p p<.05     ges\n#> 1        sex   1 429 0.296 0.586       0.00069\n#> 2     agegp3   2 429 3.911 0.021     * 0.01800\n#> 3 sex:agegp3   2 429 1.444 0.237       0.00700\nleveneoptimisme2 <- leveneTest(toptim ~ sex*agegp3, center = mean, data = optimisme2)\nleveneoptimisme2\n#> Levene's Test for Homogeneity of Variance (center = mean)\n#>        Df F value Pr(>F)\n#> group   5   1.083  0.369\n#>       429"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"blandet-design-anova-mixed-between-within","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"7.4 Blandet design ANOVA (“Mixed between-within”)","text":"siste delkapittel skal vi kombinere tidligere analyser. enkelte situasjoner ønsker vi å se på nivået på et fenomen hos ulike grupper på ulike måletidspunkter. Vi har da en uavhengig variabel («subjects») som er dikotom (type påvirkning) og en uavhengig variabel («within subjects») som er verdier på ulike måletidspunkter (f.eks. før og etter en påvirkning; tid 1, tid 2 og tid 3).Blandet design ANOVA (“Mixed -within”)fost1, fost2 og fost3 er målinger på ulike tidspunkter på en skala som måler respondentens selvrapporterte frykt statistikk. Gruppe 1 er gruppen som ble utsatt et program som skulle heve deres matematikkunnskaper, mens gruppe 2 er gruppen som fikk et tilsvarende program som var rettet mot å heve deres selvtillit statistikkoppgaver.Modifisert datasett ligger :\nDownload experim3.xlsxI R må vi før analysen konvertere dataene til såkalt “long” format slik hver rad representerer en observasjon (og ikke en person som det gjør nå “wide” format). Dette er gjort den nedlastbare fila ovenfor.Vi har modifisert den opprinnelige datafila slik det ser slik ut (første 6 rader):Forutsetningen om sferisitet er brutt:Vi bruker funksjonen “aov_ez” fra pakken “afex” å kunne vise sjekk av sferisitet (ikke tilgjengelig på denne måten gjennom funksjonen “ezANOVA” som vi bruker ). Enkelte funksjoner enkelte pakker er avhengig av input på en viss måte å fungere. “check_sphericity” vil ikke funke med pakken “ezANOVA”, og “apa.ezANOVA” vil ikke funke med pakken “afex”. Både funksjonene “aov_ez” og “ezANOVA” gjør denne sjekken og korrigerer hvis nødvendig automatisk (slik det også fremgår begge funksjonenes respektive resultattabeller).\nFigure 7.3: ANOVA-plott\nDet vi kan se av resultattabellen ANOVA er først group*fost (interaksjonseffekten “type intervensjon”) ikke er signifikant. Som tidligere nevnt – dersom vi hadde hatt en signifikant interaksjonseffekt ville det vært vanskelig å tolke totaleffektene siden effekten av en variabel er påvirket av den andre. Vi kan se på hovedeffekten.Variabelen “group” er heller ikke signifikant. Derimot er “fost” signifikant (p < 0.001) med en \\(\\eta^2\\) = `r round(effektstorrelse[2,2], 2). Dette indikerer en middels effekt (J. Cohen 1988). Dette støttes av utregning av \\(\\omega^2\\):Til slutt effektvurderingen kan vi se på Cohens d fost1-fost2 og fost2-fost3:Vi ser Cohens d tid 1 - tid 2 er -0.52 og tid 2 til tid 3 er -0.40. Dette gir følgende tolkning (Ben-Shachar, Lüdecke, Makowski 2020), jfr. Gignac Szodorai (2016):Vi kan dermed si fost1 - fost2 har en middels effekt, mens fost2 - fost3 har en liten effekt. Begge gruppene fikk altså en signifikant lavere selvrapportert frykt statistikk, både totalt sett og fra tidspunkt til tidspunkt måling, men det var ingen signifikant forskjell mellom gruppene (hvilket av de programmene de fulgte).","code":"\nstatfrykt <- read_excel(\"experim3.xlsx\")\nhead(statfryktlong)\n#> # A tibble: 6 × 4\n#>      id group fost  score\n#>   <dbl> <fct> <fct> <dbl>\n#> 1     1 1     fost1    32\n#> 2     1 1     fost2    28\n#> 3     1 1     fost3    23\n#> 4     2 2     fost1    37\n#> 5     2 2     fost2    36\n#> 6     2 2     fost3    34\nstatfryktaov <- aov_ez(\n  id = \"id\",\n  dv = \"score\",\n  data = statfryktlong,\n  between = \"group\",\n  within = \"fost\")\n#> Contrasts set to contr.sum for the following variables: group\nstatfryktaov\n#> Anova Table (Type 3 tests)\n#> \n#> Response: score\n#>       Effect          df   MSE         F  ges p.value\n#> 1      group       1, 28 83.21      0.06 .002    .810\n#> 2       fost 1.21, 33.89  6.98 43.29 *** .125   <.001\n#> 3 group:fost 1.21, 33.89  6.98      2.30 .008    .134\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n#> \n#> Sphericity correction method: GG\ncheck_sphericity(statfryktaov)\n#> Warning: Sphericity violated for: \n#>  - fost (p < .001)\n#>  - group:fost (p < .001).\napatabell <- ezANOVA(\n    statfryktlong, \n    dv = .(score),\n    wid = .(id),\n    within = .(fost),\n    between = .(group)\n)\n#> Warning: Converting \"id\" to factor for ANOVA.\napatabell <- apa.ezANOVA.table(apatabell, correction = \"GG\", table.title = \"\")\napatabell\n#> \n#> \n#> ANOVA results\n#>  \n#> \n#>     Predictor df_num df_den Epsilon     F    p ges\n#>         group   1.00  28.00          0.06 .810 .00\n#>          fost   1.21  33.89    0.61 43.29 .000 .12\n#>  group x fost   1.21  33.89    0.61  2.30 .134 .01\n#> \n#> Note. df_num indicates degrees of freedom numerator. df_den indicates degrees of freedom denominator. \n#> Epsilon indicates Greenhouse-Geisser multiplier for degrees of freedom, \n#> p-values and degrees of freedom in the table incorporate this correction.\n#> ges indicates generalized eta-squared.\n#> \nafex_plot(statfryktaov, \n          x = \"fost\", \n          trace = \"group\",\n          dodge = 0.4,\n          mapping = c(\"shape\", \"color\")) +\n    theme_bw() +\n    theme(legend.position = \"bottom\")\neffektstorrelse <- omega_squared(statfryktaov)\neffectsize::interpret_omega_squared(0.12, rules = \"field2013\")\n#> [1] \"medium\"\n#> (Rules: field2013)\ncohen3 <- cohen.d(statfrykt$fost2, statfrykt$fost1)\ncohen4 <- cohen.d(statfrykt$fost3, statfrykt$fost2)\ncohen3\n#> \n#> Cohen's d\n#> \n#> d estimate: -0.5172345 (medium)\n#> 95 percent confidence interval:\n#>       lower       upper \n#> -1.04264663  0.00817762\ncohen4\n#> \n#> Cohen's d\n#> \n#> d estimate: -0.4047634 (small)\n#> 95 percent confidence interval:\n#>      lower      upper \n#> -0.9268700  0.1173433\ninterpret_cohens_d(-0.52, rules = \"gignac2016\")\n#> [1] \"moderate\"\n#> (Rules: gignac2016)\ninterpret_cohens_d(-0.40, rules = \"gignac2016\")\n#> [1] \"small\"\n#> (Rules: gignac2016)"},{"path":"variansanalyse---anova-analysis-of-variance.html","id":"eksport-til-word-etter-apa-standard","chapter":"Kapittel 7 Variansanalyse - ANOVA (“Analysis of Variance”)","heading":"7.4.1 Eksport til word etter APA-standard","text":"En nyttig pakke dersom man skal eksportere resultattabell iht. f.eks. APA-standard er “apaTables” (Stanley Spence 2018). Nedenfor vises først tabellen slik den fremstår R, deretter bilde av hvordan det ser ut det eksporterte worddokumentet.(merk: Tallene er noe avvikende fra vårt eksempel og kan skyldes ulike måter å foreta korreksjoner på, men er tatt med å vise muligheten. Pakken apaTables kan gi APA-tabeller en rekke ulike analyser).","code":"\napa.aov.table(lm(score ~ group*fost, statfryktlong),filename = \"Example1.doc\",table.number = 1)\n#> \n#> \n#> Table 1 \n#> \n#> ANOVA results using score as the dependent variable\n#>  \n#> \n#>     Predictor       SS df       MS      F    p partial_eta2\n#>   (Intercept) 23840.27  1 23840.27 780.23 .000             \n#>         group     2.70  1     2.70   0.09 .767          .00\n#>          fost   109.20  2    54.60   1.79 .174          .04\n#>  group x fost    19.47  2     9.73   0.32 .728          .01\n#>         Error  2566.67 84    30.56                         \n#>  CI_90_partial_eta2\n#>                    \n#>          [.00, .04]\n#>          [.00, .11]\n#>          [.00, .04]\n#>                    \n#> \n#> Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"variansanalyse---manova-multivariate-analysis-of-variance","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","text":"R-pakker brukt dette kapittelet:","code":"\npacman::p_load(readxl, writexl, tidyverse, summarytools, gridExtra, lsr, rstatix, mvnormalTest, heplots, plyr, xtable, broom, ggpubr, car, GGally)"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"enveis-manova","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.1 Enveis MANOVA","text":"Forskjellen mellom de analysene vi har gjort til nå og MANOVA er vi kan ønske å se på effekter av ulike påvirkninger på flere enn en avhengig variabel. MANOVA er således en ANOVA med eller flere kontinuerlige avhengige variabler, der flere avhengige variabler testes statistiske forskjeller gjennom en “grouping variable”.Vi kan se oss vi forventer en påvirkning har effekt på flere avhengige variabler. På en måte er MANOVA en analyse som kjører gjentatte ANOVAer (fordi vi har mer enn en avhengig variabel), men som samtidig korrigerer risikoen type 1 feil (vi tror det er en signifikant forskjell mellom grupper uten det er det). Hvis vi gjennomfører gjentatte ANOVAer øker sannsynligheten vi får signifikante resultater (kort fortalt fordi sjansen tilfeldig signifikant resultat øker med antall ganger vi kjører testen (Mittelhammer, Judge, Miller 2000) og får type 1 feil. Det skal sies man kan gjennomføre flere ANOVAer, men et tiltak å redusere type 1 feil kan være å sette en strengere alfaverdi. En relativt enkel korrigering er å ta den vanlige alfaverdien og dele på antall avhengige variabler (dvs antall ganger du vil kjøre ANOVA) (Pallant 2010). Dette kalles en Bonferroni justering:\\(Justert\\ \\alpha = \\frac{0.05}{Antall\\ variabler}, f.eks:\\ Justert\\ \\alpha=\\frac{0.05}{3}=0.17\\)Nedsiden ved å korrigere slik er man øker sjansen type II feil (sjansene type 1 og type 2 feil er dessverre omvendt korrelerte). Det finnes alternativer, men vi går ikke inn disse (se f.eks. Frane 2015).Det vi ønsker å undersøke er:Download Pallant_Survey_MANOVA.xlsxVi kan se på variablene gruppert etter kjønn:Grafisk ser de avhengige variablene slik ut:Gruppert på en litt annen måte kan vi også framstille dataene grafisk slik:Vi kan se ut fra de grafiske framstillingene (og tabellen lenger opp) variabelen “tposaff” ser gjennomsnittsverdien og spredningen ut til å være veldig lik mellom kjønnene, mens begge de andre ser ut til å ha høyere gjennomsnittsverdi kvinner. Vi vet imidlertid ikke om dette er statistisk signifikant eller sannsynligvis tilfeldig.Vi kan så lage en MANOVA der vi tester følgende:\\(H_0: De\\ multivariate\\ vektorene\\ av\\ gjennomsnittsverdier\\ \\ \\ eller\\ flere\\ grupperer\\ like\\)\\(H_A: Minst\\ en\\ er\\ ulik\\)Pillais testverdi er statistisk signifikant [Pillais Trace = 0.0244, F(3, 428) = 3.569, p < 0.05]. Det er grunn til å tro det er forskjeller mellom gruppene. Vi finner med andre ord en signifikant forskjell mellom menn og kvinner (men kan ikke ut fra dette si noe om hvor forskjellene er).\\(\\eta^2 = 0.02\\) viser imidlertid effekten er liten.Vi kan se kun en av variablene - betegnet “Response 3” (= “tpstress” siden den er lagt inn som tredje variabel vår modell) - er statistisk signifikant (p < 0.05). Vi omtalte imidlertid behovet å foreta en justering av \\(\\alpha\\) til 0.17 gjennom en Bonferronikorreksjon (det finnes andre måter å korrigere , men det går vi ikke videre inn på ). Med justering blir også “Response 1” (“tnegaff”) statistisk signifikant.Vi kan gå tilbake til de signifikante variablene:Ut fra våre funn kan vi si kvinner er statistisk signifikant mer stresset enn menn denne undersøkelsen på 2 av 3 variabler, men forskjellen er liten.","code":"\nmanova1 %>%\n  group_by(Kjonn) %>%\n  rstatix::get_summary_stats(tnegaff, tposaff, tpstress, type = \"mean_sd\")\n#> # A tibble: 6 × 5\n#>   Kjonn  variable     n  mean    sd\n#>   <fct>  <chr>    <dbl> <dbl> <dbl>\n#> 1 Mann   tnegaff    184  18.7  6.90\n#> 2 Mann   tposaff    184  33.6  6.98\n#> 3 Mann   tpstress   184  25.8  5.41\n#> 4 Kvinne tnegaff    248  20.0  7.18\n#> 5 Kvinne tposaff    248  33.7  7.44\n#> 6 Kvinne tpstress   248  27.4  6.08\np1 <- ggplot(manova1, aes(x = Kjonn, y = tnegaff, fill = Kjonn)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) + theme(legend.position=\"top\")\np2 <- ggplot(manova1, aes(x = Kjonn, y = tposaff, fill = Kjonn)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) + theme(legend.position=\"top\")\np3 <- ggplot(manova1, aes(x = Kjonn, y = tpstress, fill = Kjonn)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) + theme(legend.position=\"top\")\ngridExtra::grid.arrange(p1, p2, p3, ncol=3)\nggpubr::ggboxplot(\n  manova1, x = \"Kjonn\", y = c(\"tnegaff\", \"tposaff\", \"tpstress\"), \n  merge = TRUE, palette = \"jco\", xlab = \"Verdi\")\navhengige <- cbind(manova1$tnegaff, manova1$tposaff, manova1$tpstress)\nmanovares1 <- manova(avhengige ~ Kjonn, data = manova1)\nsummary(manovares1)\n#>            Df   Pillai approx F num Df den Df  Pr(>F)  \n#> Kjonn       1 0.024406    3.569      3    428 0.01418 *\n#> Residuals 430                                          \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\neffectsize::eta_squared(manovares1)\n#> # Effect Size for ANOVA (Type I)\n#> \n#> Parameter | Eta2 (partial) |       95% CI\n#> -----------------------------------------\n#> Kjonn     |           0.02 | [0.00, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\nsummary.aov(manovares1)\n#>  Response 1 :\n#>              Df  Sum Sq Mean Sq F value Pr(>F)  \n#> Kjonn         1   172.3 172.348  3.4563 0.0637 .\n#> Residuals   430 21442.1  49.865                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>  Response 2 :\n#>              Df  Sum Sq Mean Sq F value Pr(>F)\n#> Kjonn         1     0.4   0.440  0.0084 0.9272\n#> Residuals   430 22596.2  52.549               \n#> \n#>  Response 3 :\n#>              Df  Sum Sq Mean Sq F value   Pr(>F)   \n#> Kjonn         1   281.1 281.099  8.3423 0.004069 **\n#> Residuals   430 14489.1  33.696                    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmanova1 %>% group_by(Kjonn) %>%  dplyr::summarise(n = n(), mean = mean(tnegaff), sd = sd(tnegaff))\n#> # A tibble: 2 × 4\n#>   Kjonn      n  mean    sd\n#>   <fct>  <int> <dbl> <dbl>\n#> 1 Mann     184  18.7  6.90\n#> 2 Kvinne   248  20.0  7.18\nmanova1 %>% group_by(Kjonn) %>%  dplyr::summarise(n = n(), mean = mean(tpstress), sd = sd(tpstress)) \n#> # A tibble: 2 × 4\n#>   Kjonn      n  mean    sd\n#>   <fct>  <int> <dbl> <dbl>\n#> 1 Mann     184  25.8  5.41\n#> 2 Kvinne   248  27.4  6.08"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"sjekk-av-forutsetninger","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2 Sjekk av forutsetninger","text":"","code":""},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"teoretisk-begrunnede-variabler","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.1 Teoretisk begrunnede variabler","text":"Dette er kanskje en litt «merkelig» forutsetning, og strengt tatt er det vel ikke en forutsetning, men heller en forsiktighetsregel mot å misbruke MANOVA. MANOVA kan synes som en type analyse der man kan slenge inn en rekke variabler og får et statistikkprogram til å kjøre gjentatte ANOVAer. Som Field, Miles, Field (2012b) presisierer må det være en grunn til å kjøre en MANOVA. Så kan man selvsat si dette er - vel, selvsagt - men det er likevel slik dette kanskje ikke er - vel, selvsagt - så derfor nevnes det . Dette poenget er selvsagt den forstand det bør gjelde alle analyser vi gjør.","code":""},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"utvalgsstørrelse","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.2 Utvalgsstørrelse","text":"Dette er godt “nødvendig” utvalgsstørrelse. Litteraturen gir ingen entydige retningslinjer hva som er “stort nok”, men en tommelfingerregel som gjengis flere steder er:\\(n\\ \\ hver\\ celle\\ > antall\\ avhengige\\ variabler\\)Wilson Van Voorhis Morgan (2007) angir ANOVA hver celle bør ha 30. Dette er uansett ingen utfordring vårt eksempel.","code":"\nmanova1 %>%\n  group_by(Kjonn) %>%\n  dplyr::summarise(N = n())\n#> # A tibble: 2 × 2\n#>   Kjonn      N\n#>   <fct>  <int>\n#> 1 Mann     184\n#> 2 Kvinne   248"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"univariat-normalitet","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.3 Univariat normalitet","text":"Vi ser vi har gjentatte problemer med signifikante verdier på shapiros test, noe som er problematisk ift. normalitet. Vi kan også sjekke skjevhet og kurtosis:Dette ser også ut til å bekrefte brudd på forutsetning om univariat normalitet.Siden vi har store utvalg (432) vil vi imidlertid kunne vise til sentralgrenseteoremet (se vedlegg B) og anta vi har multivariat normalitet (Bedre 2018 angir n > 20 hver kombinasjon av variabler), og gjerne se på dataene gjennom QQ-plott:Vi ser spesielt “tnegaff” viser avvik fra normalitet.","code":"\nmanova1 %>%\n  group_by(Kjonn) %>%\n  rstatix::shapiro_test(tnegaff, tposaff, tpstress) %>%\n  arrange(variable)\n#> # A tibble: 6 × 4\n#>   Kjonn  variable statistic             p\n#>   <fct>  <chr>        <dbl>         <dbl>\n#> 1 Mann   tnegaff      0.903 0.00000000130\n#> 2 Kvinne tnegaff      0.937 0.00000000848\n#> 3 Mann   tposaff      0.977 0.00395      \n#> 4 Kvinne tposaff      0.979 0.000827     \n#> 5 Mann   tpstress     0.987 0.0959       \n#> 6 Kvinne tpstress     0.992 0.179\noptions(scipen = 999)\nmvnormalTest::mardia(manova1[, c(\"tnegaff\", \"tposaff\", \"tpstress\")])$mv.test\n#>           Test Statistic p-value Result\n#> 1     Skewness   96.5931       0     NO\n#> 2     Kurtosis    3.5846  0.0003     NO\n#> 3 MV Normality      <NA>    <NA>     NO\ndiagram1 <- ggpubr::ggqqplot(manova1, \"tnegaff\", facet.by = \"Kjonn\", title = \"tnegaff\")\ndiagram2 <- ggpubr::ggqqplot(manova1, \"tposaff\", facet.by = \"Kjonn\", title = \"tposaff\")\ndiagram3 <- ggpubr::ggqqplot(manova1, \"tpstress\", facet.by = \"Kjonn\", title = \"tpstress\")\ndiagram1\ndiagram2\ndiagram3"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"multivariat-normalitet","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.4 Multivariat normalitet","text":"Vi ser ut til å ha en utfordring også med multivariat normalitet (men jfr. pkt. om sentralgrenseteoremet og utvalgsstørrelse ovenfor).","code":"\nmanova1 %>%\n  select(tnegaff, tposaff, tpstress) %>%\n  rstatix::mshapiro_test()\n#> # A tibble: 1 × 2\n#>   statistic  p.value\n#>       <dbl>    <dbl>\n#> 1     0.984 0.000128"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"homogenitet-i-varians-kovariansmatrisene","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.5 Homogenitet i varians-kovariansmatrisene","text":"Siden p > 0.05 kan vi si varians-kovariansmatrisene er like alle kombinasjoner av de avhengige variablene gitt av de ulike gruppene den uavhengige variabelen.","code":"\nboxM(Y = manova1[, c(\"tnegaff\", \"tposaff\", \"tpstress\")], group = manova1$Kjonn)\n#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  manova1[, c(\"tnegaff\", \"tposaff\", \"tpstress\")]\n#> Chi-Sq (approx.) = 6.8875, df = 6, p-value = 0.3314"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"univariate-uteliggere","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.6 Univariate uteliggere","text":"sjekker vi uteliggere med ekstrem påvirkning de enkelte avhengige variablene:Vi observerer ingen uteliggere med ekstrem påvirkning. Pakken “rstatix” definerer uteliggere som observasjoner som ligger \\(Q3 + 1.5 * IQR\\) eller \\(Q1 - 1.5*IQR\\) der\\(Q1\\ og\\ Q3\\ er\\ første\\ og\\ tredje\\ kvartil\\)\\(IQR = Interquartile\\ range\\ (IQR = Q3-Q1)\\)Uteliggere med ekstrem påvirkning er definert som observasjoner \\(Q3 + 3 * IQR\\) eller \\(Q1 - 3*IQR\\).","code":"\nmanova1 %>%\n    group_by(Kjonn) %>%\n    rstatix::identify_outliers(tnegaff)\n#> # A tibble: 14 × 6\n#>    Kjonn  tnegaff tposaff tpstress is.outlier is.extreme\n#>    <fct>    <dbl>   <dbl>    <dbl> <lgl>      <lgl>     \n#>  1 Mann        35      35       22 TRUE       FALSE     \n#>  2 Mann        36      49       31 TRUE       FALSE     \n#>  3 Mann        38      12       39 TRUE       FALSE     \n#>  4 Mann        39      20       46 TRUE       FALSE     \n#>  5 Mann        37      25       30 TRUE       FALSE     \n#>  6 Mann        37      36       31 TRUE       FALSE     \n#>  7 Mann        37      33       33 TRUE       FALSE     \n#>  8 Mann        36      22       36 TRUE       FALSE     \n#>  9 Kvinne      39      49       29 TRUE       FALSE     \n#> 10 Kvinne      39      16       42 TRUE       FALSE     \n#> 11 Kvinne      39      19       40 TRUE       FALSE     \n#> 12 Kvinne      39      27       33 TRUE       FALSE     \n#> 13 Kvinne      39      31       34 TRUE       FALSE     \n#> 14 Kvinne      39      31       37 TRUE       FALSE\nmanova1 %>%\n    group_by(Kjonn) %>%\n    rstatix::identify_outliers(tposaff)\n#> # A tibble: 5 × 6\n#>   Kjonn  tnegaff tposaff tpstress is.outlier is.extreme\n#>   <fct>    <dbl>   <dbl>    <dbl> <lgl>      <lgl>     \n#> 1 Mann        11      12       27 TRUE       FALSE     \n#> 2 Mann        38      12       39 TRUE       FALSE     \n#> 3 Mann        20      11       34 TRUE       FALSE     \n#> 4 Kvinne      37      11       39 TRUE       FALSE     \n#> 5 Kvinne      34      11       43 TRUE       FALSE\nmanova1 %>%\n    group_by(Kjonn) %>%\n    rstatix::identify_outliers(tpstress)\n#> # A tibble: 10 × 6\n#>    Kjonn  tnegaff tposaff tpstress is.outlier is.extreme\n#>    <fct>    <dbl>   <dbl>    <dbl> <lgl>      <lgl>     \n#>  1 Mann        39      20       46 TRUE       FALSE     \n#>  2 Kvinne      34      11       43 TRUE       FALSE     \n#>  3 Kvinne      39      16       42 TRUE       FALSE     \n#>  4 Kvinne      24      23       42 TRUE       FALSE     \n#>  5 Kvinne      36      22       44 TRUE       FALSE     \n#>  6 Kvinne      35      17       42 TRUE       FALSE     \n#>  7 Kvinne      10      44       12 TRUE       FALSE     \n#>  8 Kvinne      10      41       12 TRUE       FALSE     \n#>  9 Kvinne      12      45       13 TRUE       FALSE     \n#> 10 Kvinne      10      40       13 TRUE       FALSE"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"multivariate-uteliggere","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.7 Multivariate uteliggere","text":"Multivariate uteliggere er observasjoner som har en uvanlig kombinasjon av verdier ift. de avhengige variablene. En vanlig måte å sjekke multivariate uteliggere er gjennom den såkalte “Mahalanobis distance”. Mahalanobis’ avstand er et mål på en observasjons avstand til et tenkt senterpunkt (“centroid”) et multivariat rom der gjennomsnittet alle variablene møtes. Hvis vi tenker oss alle observasjonene alle de avhengige variabene plottet en sky - og vi har et senterpunkt denne skyen - er Mahalanobis’ avstand avstanden fra en observajson til senterpunktet skyen. Multivariate uteliggere utgjøres av eventuelle observasjoner som har uvanlige kombinasjoner av skårer, f.eks. veldig høyt på en variabel og veldig lavt på en annen.Vi ser vi har en multivariat uteligger - observasjon nr. 185. Vi kan se om MANOVA endrer seg mye om denne tas vekk:Sammenliknet med verdier fra den opprinnelige modellen har vi praksis ingen endring ved å ta bort den ene definerte multivariate uteliggeren. Den kan derfor ikke ha nevneverdig innflytelse på modellen.","code":"\nmvuteliggere <- rstatix::mahalanobis_distance(data = manova1[, c(\"tnegaff\", \"tposaff\", \"tpstress\")])$is.outlier\nwhich(mvuteliggere, arr.ind = TRUE)\n#> [1] 185\nmanova2 <- manova1[-c(185), ]\ndim(manova2)\n#> [1] 431   4\navhengige2 <- cbind(manova2$tnegaff, manova2$tposaff, manova2$tpstress)\nmanovares2 <- manova(avhengige2 ~ Kjonn, data = manova2)\nsummary(manovares2)\n#>            Df   Pillai approx F num Df den Df  Pr(>F)  \n#> Kjonn       1 0.023713   3.4571      3    427 0.01649 *\n#> Residuals 429                                          \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\neffectsize::eta_squared(manovares2)\n#> # Effect Size for ANOVA (Type I)\n#> \n#> Parameter | Eta2 (partial) |       95% CI\n#> -----------------------------------------\n#> Kjonn     |           0.02 | [0.00, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00]."},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"linearitet","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.8 Linearitet","text":"De parvise sammenhengene mellom de avhengige variablene bør være lineære hver av gruppene den uavhengige variabelen.Det vi kan se er det muligens er en utfordring med variabelen “tnegaff”. Vi så også sjekk av normalitet tnegaff var problematisk.","code":"\nlinearitet <- manova1 %>%\n  select(tnegaff, tposaff, tpstress, Kjonn) %>%\n  group_by(Kjonn) %>%\n  doo(~ggpairs(.) + theme_bw(), result = \"plots\")\nlinearitet$plots\n#> [[1]]#> \n#> [[2]]"},{"path":"variansanalyse---manova-multivariate-analysis-of-variance.html","id":"multikolinearitet","chapter":"Kapittel 8 Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)","heading":"8.2.9 Multikolinearitet","text":"Tabachnik Fidell (2007) forslår r=0.90 som en øvre grense korrelasjon mellom variabler, mens Pallant (2010) opererer med 0.8 som en grense bekymring. Ut fra dette ser det ikke ut til vi har noen bekymringer dette eksempelet.","code":"\nmanova3 <- manova1[ ,c(\"tnegaff\", \"tposaff\", \"tpstress\")]\nmanovacor <- round(cor(manova3),2)\nupper <- manovacor\nupper[upper.tri(manovacor)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>          tnegaff tposaff tpstress\n#> tnegaff        1                 \n#> tposaff     -0.3       1         \n#> tpstress    0.67   -0.44        1"},{"path":"regresjonsanalyse---ols.html","id":"regresjonsanalyse---ols","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"Kapittel 9 Regresjonsanalyse - OLS","text":"R-pakker brukt dette kapittelet:","code":"\npacman::p_load(car, readxl, effects, writexl, ggpubr, tidyverse, gridExtra, nortest, knitr, kableExtra, tseries, normtest, flextable, magrittr, ISLR, olsrr, lmtest, rnorsk, qwraps2, sjPlot, sjmisc, sjlabelled, xtable, Hmisc, gt, gtsummary, sjPlot, modelsummary, table1, jtools, interactions, outliers, EnvStats, qqplotr, summarytools, caret, gridExtra)"},{"path":"regresjonsanalyse---ols.html","id":"innledning","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.1 Innledning","text":"Regresjonsanalyse er et særtilfelle av variansanalyse, og er følge Mehmetoglu Mittner (2020) muligens den mest brukte analysemetoden dataanalyse, eller arbeidshesten forskning på økonomiske og sosiale forhold (Thrane 2019). Det er først og fremst metodens er fleksibilitet som en hovedgrunn til dette.En regresjonsanalyse er en statistisk analyse som undersøker sammenhengen mellom en kontinuerlig avhengig variabel og en eller flere kontinuerlige og/eller kategoriske uavhengige variabler. Selv om korrelasjon kan være veldig hjelpsomt å forstå vil en regresjonsanalyse søke å ta vår forståelse av sammenhengen litt videre, til eksempel å forsøke å predikere nivået en avhengig variabel ut fra nivået på den/de uavhengige variabler. Hvis vi lykkes med dette vil vi kunne klare å si noe om forventet verdi på et fenomen vi er interessert ut fra kjente verdier på andre variabler. La oss anta vi har variabler som beliggenhet (avstand fra sentrum), areal, etasje, solforhold, antall rom, antall bad, standard på bad og liknende en leilighet kan vi bruke disse uavhengige variablene til å predikere en salgssum denne boligen (som en avhengig variabel). Vi lager da en modell dette forholdet - dette tilfellet en regresjonsmodell. Vi går dermed fra å spørre om det er en sammenheng til å spørre hvilken sammenheng det er.La oss forsøke å illustrere prinsippet med regresjonsanalyse gjennom et såkalt Venndiagram.Den gule sirkelen illustrerer det forholdet vi er interessert å “finne ut noe om”. Den representerer det vi kaller den avhengige variabelen - fordi det vi ønsker å finne ut er avhengig av andre forhold (andre variabler). Vi kan si den gule sirkelen viser variasjonen drivstofforbruket til alle biler vi har med undersøkelsen vår, og vi betegner denne variabelen \\(Y\\). Biler har ulikt drivstofforbruk, så vi har altså en variasjon drivstofforbruket mellom bilene. Den blå sirkelen viser variasjonen motorstørrelse (vi kaller denne variabelen \\(x_1\\)). Ulike biler har ulik motorstørrelse, og vi tenker større motor betyr mer drivstofforbruk enn mindre motor. Den grønne sirkelen representerer en variabel vi har kalt kjørestil (\\(x_2\\)).Vi har en hypotese om vi kan predikere (forutsi) drifstofforbruket til en gitt bil ut fra motorstørrelse og kjørestil. Så det vi ønsker å se på er hvor mye av korrelasjonen mellom drivstofforbruk og motorstørrelse skyldes faktisk motorstørrelse, og hvor mye skyldes kjørestil. Vi tenker også kjørestil og drivstofforbruk er korrelert (det er naturlig å tenke seg personer med en aggresiv kjørestil har biler med større motorer - det er altså en korrelasjon mellom kjørestil og motorstørrelse). Vi ser dette figuren . Korrelasjonen mellom drivstofforbruk og motorstørrelse er gitt områdene merket 1 og 2. Korrelasjonen mellom drivstofforbruk og kjørestil er gitt områdene 2 og 3. Korrelasjonen mellom kjørestil og motorstørrelse er gitt 2 og 4.Området 2 viser den delte variasjonen mellom drivstofforbruk, motorstørrelse og kjørestil. Det vil innebære vi kan bruke regresjonsanalsye til å isolere ut område 1 ved å se på motorstørrelsens totale korrelasjon med drivstofforbruk og trekke fra den delen av den totale korrelasjonen som deles med kjørestil (område 2). Da finner vi motorstørrelsens (\\(x_1\\)’s) unike bidrag.Det samme kan vi gjøre kjørestil, der det unike bidraget utgjøres av område 3. Vi kan selvsagt ha flere prediktorer (uavhengige variabler) - noe vi veldig ofte vil ha. Det vi gjør er prinsippet det samme: vi tar bort biter av korrelasjonen mellom motorstørrelse og drivstofforbruk som skyldes samvariasjon med andre variabler slik vi får isolert den delen av korrelasjonen som utelukkende skyldes motorstørelse. Man kan tenke seg en ny variabel med rød sirkel. Igjen - regresjonsanalysen forsøker å isolere den unike delen korrelasjonen mellom motorstørrelse og drivstofforbruk (og det samme de andre variablene: den unike delen). Når vi klarer å isolere den unike korrelasjonen kan vi også si vi har isolert den unike kausale effekten motorstørrelse har på drivstofforbruket (gitt vi har inkludert alle relevant uavhengige variabler modellen, noe vi praksis sjelden vil klare).Den videre innledningen til regresjonsanalyse tar utgangspunkt eksempelet Løvås (2013) (boka kom ut 4. utgave 2018). Illustrasjonene som er brukt er hentet fra bokas nettressurser. Løvås’ bok «Statistikk universiteter og høgskoler» kan anbefales som introduksjonsbok til statistikk på universitets- og høgskolenivået. En annen bok som fungerer fint til dette formålet er Jan Ubøes “Statistikk økonomifag” (vi har brukt 4. utgave, 2014 - 5. utgave kom 2015) (Ubøe 2014).","code":""},{"path":"regresjonsanalyse---ols.html","id":"teori","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.2 Teori","text":"Eksempelet Løvås (2013) dreier seg om sammenhengen mellom motorstørrelse og drivstofforbruk. Vi kan måle motorstørrelse hestekrefter (hk) og drivstofforbruk liter/mil.å vise sammenhengen kan vi sette opp ligningen \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) der\\(Y=drivstofforbruket\\)\\(\\alpha=konstantleddet\\) (krysningspunktet på y-aksen, altså Y-verdien om x er 0)\\(\\beta=linjens\\:stigningstall\\) (dersom x øker med 1, øker y med \\(\\beta\\)\\(e=forstyrrelsen\\) (vi antar det er flere ting som forstyrrer forholdet mellom motorstørrelse og drivstofforbruk - drivstofforbruket er ikke bare avhengig av motorstørrelse). Vi skal snakke mye om residualer regresjonsanalyse - residualer er dette restleddet/feilleddet/forstyrrelsen. En av forutsetningene regresjonsanalyse er knyttet til fordelingen av disse residualene, men det kommer vi tilbake til.Dersom vi ikke hadde hatt et feilledd kunne vi framstilt denne ligningen slik:Løvås (2013), s.287, Fig. 7.6: Den vanlige formelen en rett linjeNår vi plotter inn et antall observasjoner av motorstørrelse og drivstofforbruk kan det se slik ut:Løvås (2013), s.288, Fig. 7.8: Er det en sammenheng mellom motorstørrelse og bensinforbruk?Det vi en lineær regresjonsanalyse gjør er å finne den rette linja som best passer til disse observasjonene. Vi ønsker altså å finne en rett linje som best «beskriver» observasjonene. Tenk deg vi trekker den rette linja som samlet sett ligger nærmest punktene og deretter tar bort punktene. Det vi sitter igjen med er regresjonslinja. Denne linja gir oss da “tilgang til” alle punkter som ligger på linja som en modell på sammenhengen mellom de variablene. Selv om vi bare hadde noen observasjoner på gitte punkter på x-aksen har vi gjennom regresjonslinja fått tilgang til alle tenkelige punkter på x-linja og kan anta et drivstofforbruk ut fra det (ved å gå opp fra x-aksen, finne skjæringspunktet med regresjonslinja, og deretter gå inn på y-aksen og lese av drivstofforbruket). Den prediksjonen vi da gjør er vår beste gjetning på hvor stort drivstofforbruket vil være en gitt motorstørrelse. Dette vil selvsagt være en kvalifisert gjetning - nettopp fordi det er en modell. Og alle modeller er feil, men noen modeller er nyttige likevel.eksempelet kan vi eksempel tenke oss mulige linjer:Løvås (2013), s.289, Fig. 7.9: Avvik mellom observasjonspunktene og vilkårlige linjerBegge linjene er forsøk på å lage en rett linje som har kortest mulig avvik. Vi kan deretter legge sammen de absolutte vertikale avstandene (de stiplede linjene) fra observasjonspunktene ned til den rette linja. prinsippet er da den rette linja som medfører minst samlet avstand fra observasjonspunktene den rette linja som best representerer observasjonspunktene, og vi kan si vi har laget en modell sammenhengen mellom motorstørrelse og drivstofforbruk. Siden vi har en sammenhengende rett linje har vi også mulighet til å mene noe om drivstofforbruk på motorstørrelser vi ikke har målt/har observasjoner på. Vi har med andre ord en modell å predikere drivstofforbruk ut fra motorstørrelse. Uavhengige variabler regresjonsanalyser kalles også ofte prediktorer, fordi vi bruker de til å predikere en verdi den avhengige variabelen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"minste-kvadratsum-ordinary-least-squares---ols","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.2.1 Minste kvadratsum (Ordinary Least Squares - OLS)","text":"Imidlertid er absoluttverdier matematisk problematiske (Løvås 2013). Pre-datamaskiner ble det derfor utviklet en alternativ måte som kalles «minste kvadraters metode» - derav begrepet OLS («Ordinary Least Squares»). Det finnes andre måter å tilnærme seg dette, men dette kurset går vi kun inn på OLS-regresjon. Hvis vi fortsetter eksempelet kan vi tenke oss en mengde forslag på ulike linjer som forsøker å beskrive sammenhengen mellom de variablene:Løvås (2013), s.289, Fig. 7.10: Illustrasjon av minste kvadraters metodeMan regner deretter ut kvadratene som dannes av hvert punkt og avstanden til den rette linja. Den linja som har den laveste kvadratsummen («least squares») er den linja som best representerer datapunktene og som derfor er den beste lineære modellen av forholdet mellom variablene. Regresjonslinja er således en modell. Som Thrane (2019) beskriver: den diagonale linja oppsummerer den typiske trenden det statistiske forholdet mellom de variablene - en linje vi kjenner som regresjonslinja.\nHvis vi har et stort antall datapunkter er dette selvsagt en omfattende prosess å gjøre manuelt. Det statistikkprogrammer gjør oss er å regne ut kvadratsummen et stort antall mulige linjer og deretter fortelle oss hvilken som har lavest kvadratsum.Hvis vi tenker tilbake til formelen modellen vår: \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) kan vi nå fylle ut med verdier fra eksempelet.Det vi egentlig har gjort når vi finner den rette linja som gir minste kvadratsum er å identifisere \\(\\alpha\\) (skjæringspunktet på y-aksen) og \\(\\beta\\) (stigningstallet). Statistikkprogrammer vil gi oss verdiene på dette. Vi går ikke inn på en manuell utregning , men bruker de verdiene Løvås (2013) viser (\\(\\alpha=0.211\\) og \\(\\beta=0.00576\\)). Vår modell ser da slik ut: \\(Y_i=0,211\\:+0,00576x_i\\).Som sagt har vi ønsket å lage en modell som predikerer drivstofforbruk ut fra motorstørrelse – eller sagt på en annen måte: hvilket drivstofforbruk kan vi forvente med en motor på 100 hk? Vi får da: \\[Y_i=0,211\\:+0,00576x_i=0,211\\:+\\:0,00576\\times100\\:=\\:0,787\\]Dette blir vårt “best guess”, vår antakelse (vår prediksjon av verdien på y-aksen som er drivstofforbruket ut fra verdien på x-aksen som er motorstørrelse), om forventet drivstofforbruk en motor med 100 hk basert på den modellen vi har laget om sammenhengen mellom motorstørrelse og drivstofforbruk (som er basert på de observasjonene vi har).Vi kan naturligvis umiddelbart tenke drivstofforbruket er avhengig av mange andre faktorer enn motorstørrelse, eksempel bilens design (luftmotstand), vekt, rullemotstand, temperatur, type motor og så videre. Dette belyser så vidt et sentralt problem når vi ønsker å lage modeller prediksjon: Virkeligheten er utrolig sammensatt, mange relevante variabler er vanskelig å måle, og man ønsker en modell som er enkel nok til å kunne brukes og sammensatt nok til å gi relevante prediksjoner. Tenk eksempel bare på «klimamodellene» som brukes å analysere og predikere temperatur, issmelting, global oppvarming og liknende. Det er klart de fleste tilfeller trenger vi flere prediktorer enn en – og regresjonssammenheng snakker vi da om multippel regresjonsanalyse. Vi kan ha som en tommelfingerregel vi skal ha med så mange variabler modellen har praktisk verdi, men likevel så få som mulig.","code":""},{"path":"regresjonsanalyse---ols.html","id":"r2-r-squared","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.2.1.1 R^2 (“R squared)","text":"Som vi har sett regnes kvadratsummene ut avstandene fra datapunktene til en tenkt linje som “går gjennom” dataene. Den linja som gir minst samlet sum av kvadratsummer er vår best modell og det vi vil kalle regresjonslinja. Dette gjør oss stand til å kalkulere en såkalt \\(R^2\\). har vi forsøkt å illustrere hvordan dette skjer:Utregning av \\(R^2\\)Til venstre ser vi illustrert avstanden fra det enkelte datapunkt til regresjonslinja. Dette gir oss til sammen \\(SS_{res}\\) - som vi kaller variabelen (y). Gjennomsnittet er som vi kommer tilbake til et par andre steder også en modell sammenhengen mellom den avhengige og den uavhengige variabelen. Summen av kvadratsummene gjennomsnittsmodellen er \\(SS_{tot\\)} - som vi kaller “total sum squares”. Ut fra disse størrelsene ser vi den nederste likningen figuren hvordan \\(R^2\\) framkommer.\\(R^2\\) kommer vi tilbake til mange ganger når vi snakker om regresjonsanalyser.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konfidensintervall","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.2.2 Konfidensintervall","text":"Avslutningsvis denne introduksjonen til regresjonsanalyse kan vi se kort på begrepet konfidensintervall (se eksempel Løvås (2013) eller Hinkle, Wiersma, Jurs (2003)). de som ønsker å fordype seg effektstørrelser og konfidensintervaller anbefales Cumming Calin-Jageman (2017) “Introduction new statistics: Estimation, open science, & beyond”.Man kan si estimatet på stigningstallet \\(\\beta\\) er det viktigste resultatet en regresjonsanalyse fordi dette sier noe om hvor sterk sammenhengen mellom de variablene er. Løvås (2013) illustrerer dette slik:Løvås (2013), s.287, Fig. 7.6: Et 95% konfidensintervall forventet bensinforbrukDe røde stiplede linjene utgjør konfidensgrensene 95 % konfidensintervall. grafen har vi kun 5 observasjoner, noe som selvsagt er lite. Vi kan gå litt dypere inn hvordan konfidensgrensene framkommer en regresjonsanalyse.La oss anta vi har et datasett der vi har plottet korrelasjonen mellom en prediktor (den uavhengige variabelen) på x-aksen og en avhengig varaibel på y-aksen (se graf ). Den røde prikken markerer verdien x=8. Verdien på y-aksen (10,458) er vår prediksjon (vår buest guess) på hva verdien den avhengige variabelen vil være ved den observerte verdien x=8.Eksempel på prediksjonI punktet x=8 har vi en hel populasjon av mulige normalfordelte verdier. Vårt beste estimat av gjennomsnittsverdien denne populasjonen er 10,458. Dette er et punktestimat. Det tilhørende intervallestimatet er vårt konfidensintervall. Vi må tenke på konfidensintervallet som en vertikal linje. Så stedet å tenke punkt- og intervallestimat slikPunkt- og intervallestimat #1Kan vi tenke det slik:Punkt- og intervallestimat #2Overført til vårt eksempel får vi:Punkt- og intervallestimat sammen med prediksjonDen røde streken er vårt 95 % konfidensintervall punktestimatet. Hvis vi legger på 95 % konfidensintervaller på alle punktestimatene (alle punktene som utgjør regresjonslinja) kan vi lage de stiplede linjene som toucher endepunktene på alle konfidensintervallene. Disse stiplede linjene utgjør da konfidensgrensene regresjonslinja.Vi ser konfidensgrensene er lett buede mot hverandre med minst avstand mellom dem “på midten”. Vi skal kort se på hvorfor det er slik. Vi har nå lagt på et nytt kryss grafen . Dette krysset markerer punktet der gjennomsnittene av X og Y krysser. Regresjonslinja må gå gjennom dette punktet, slik alle alternative regresjonslinjer må pivotere rundt dette punktet. Dette medfører det er litt større usikkerhet rundt punktestimatenes konfidensintervaller endene forhold til midten. Konfidensintervallene hvert enkelt punkt blir derfor litt lenger jo lenger ut fra krysningspunket vi går, og resultatet blir en form buet linje.Prediksjon og konfidensintervaller","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-i-analyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.2.3 Steg i analyse","text":"Vi anbefaler en analyse går gjennom disse stegene:Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterVi skal det følgende gå gjennom disse stegene en regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"enkel-lineær-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3 Enkel, lineær regresjonsanalyse","text":"Dette eksempelet bygger på Field (2009a). Hvis vi ønsker å se på hvilken grad vi kan predikere salgstall gjennom hvor mye vi bruker på reklame før lansering kan vi gjøre en lineær regresjonsanalyse med salg som avhengig variabel og reklame (adverts) som uavhengig variabel.Du kan laste ned datasettet ulike formater :Download Field_datasett_OLS.xlsxDownload Field_datasett_OLS.savDownload Field_datasett_OLS.dtaDatasettet kan også finnes herVi skal nå gå gjennom våre anbefalte steg analysen, og starter med en analyse av dataene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-1-analyse-av-dataene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.1 Steg 1: Analyse av dataene","text":"“soon collected data, compute statistics, look data. Data screening data snooping. opportunity discard data change values favor hypotheses. However, assess hypotheses without examining data, risk publishing nonsense” (Wilkinson Task Force Statistical Inference 1999).Vi ser på datasettet og noen nøkkeltall datasettet.Descriptive Statistics\nField_OLS_data\nN: 200Vi kan se datasettet består av 200 obervasjoner av 4 variabler. Hver av observasjonene er en CD:Adverts: Dette er summen brukt på reklame før lanseringsdatoSales: Dette er salgtall per ukeAirplay: Antall ganger et spor fra CDen ble spilt på radio uka før lanseringsdatoImage: En rating på hvor attraktiv gruppen/artisten (positivt image)Ofte er det imidlertid mer hensiktsmessig å se på dataene grafisk en utforskende hensikt (Tukey 1977).","code":"\nField_OLS_data <- read_excel(\"Field_datasett_OLS.xlsx\")\nsummarytools::descr(Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"histogram-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.1.1 Histogram","text":"Det kan se ut som salgstallene er rimelig normalfordelte, mens reklamevariabelen er klart skjev.","code":"\nannotations <- data.frame(\n  x = c(round(min(Field_OLS_data$Adverts), 2), round(mean(Field_OLS_data$Adverts), 2), round(max(Field_OLS_data$Adverts), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n  \nplott1 <- ggplot(Field_OLS_data, aes(Adverts)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Adverts)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\n\nannotations2 <- data.frame(\n  x = c(round(min(Field_OLS_data$Sales), 2), round(mean(Field_OLS_data$Sales), 2), round(max(Field_OLS_data$Sales), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n\nplott2 <- ggplot(Field_OLS_data, aes(Sales)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Sales)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations2, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\ngrid.arrange(plott1, plott2, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"quantile-quantile-plott-qq","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.1.2 Quantile-Quantile plott (qq)","text":"Q-Q plottet (“quantile-quantile plot”) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet innebærer å se distribusjoner mot hverandre – empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om “normal Q-Q plott” - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). eksempelet vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med.Som vi fikk indikert gjennom histogrammene er salgsvariabelen rimelig normalfordelt, mens reklamevariabelen viser avvik fra normalfordelingen - dette tilfellet (ut fra histogram og qq-plott vil vi si den er høyreskjev).","code":"\npar(mfrow=(c(1,2)))\nqqSales2 <- car::qqPlot(Field_OLS_data$Adverts)\nqqAdverts2 <- car::qqPlot(Field_OLS_data$Sales)"},{"path":"regresjonsanalyse---ols.html","id":"steg-2-evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.2 Steg 2: Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Dette er ikke relevant en enkel lineær regresjonsanalyse. Når vi skal gjøre en multippel regresjonsanalyse - altså vi har eller flere uavhengige variabler (prediktorer) vil analysen av dataene våre - og hvilken rekkefølge vi legger de uavhengige variablene inn regresjonsmodellen (mer om det eksempelet multippel regresjon) kunne informeres av analysen vi gjør forkant. Derfor viser vi dette nå selv om det altså ikke er relevant enkel regresjonsanalyse.Vi lager en korrelasjonstabell de tre uavhengige og den avhengige variabelen (Sales, Adverts, Airplay, Image):p < .0001**** , p < .001*** , p < .01**, p < .05*standard multippel regresjonsanalyse legger vi alle de uavhengige variablene inn samtidig. Dersom vi skal gjøre en stegvis regresjonsanalyse vil vi legge de uavhengige variablene inn en og en ut fra statistiske kriterier - som hvor stor korrelasjonen er. Den uavhengige variabelen med størst korrelasjon legges inn først og så videre. det eksempelet vi har ser vi Sales korrelerer høyest med Airplay, og nesten like mye med Adverts. Korrelasjonen med Image er noe lavere.","code":"\ntab_corr(Field_OLS_data, triangle = \"lower\")"},{"path":"regresjonsanalyse---ols.html","id":"steg-3-lage-modell-og-kjøre-regresjonsanalysen","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.3 Steg 3: Lage modell (og kjøre regresjonsanalysen)","text":"Vi ønsker å se om Adverts kan predikere Sales. Grafisk kan vi vise dette:Platesalg predikert av reklame - modell","code":"\nFieldOLS_reg <- ols_regress(Sales ~ Adverts, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"steg-4-analyse-av-resultatene-diagnostikk","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.4 Steg 4: Analyse av resultatene (diagnostikk)","text":"","code":"\nFieldOLS_reg\n#>                          Model Summary                           \n#> ----------------------------------------------------------------\n#> R                       0.578       RMSE                 65.991 \n#> R-Squared               0.335       Coef. Var            34.157 \n#> Adj. R-Squared          0.331       MSE                4354.870 \n#> Pred R-Squared          0.323       MAE                  50.869 \n#> ----------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                  ANOVA                                   \n#> ------------------------------------------------------------------------\n#>                    Sum of                                               \n#>                   Squares         DF    Mean Square      F         Sig. \n#> ------------------------------------------------------------------------\n#> Regression     433687.833          1     433687.833    99.587    0.0000 \n#> Residual       862264.167        198       4354.870                     \n#> Total         1295952.000        199                                    \n#> ------------------------------------------------------------------------\n#> \n#>                                     Parameter Estimates                                     \n#> -------------------------------------------------------------------------------------------\n#>       model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n#> -------------------------------------------------------------------------------------------\n#> (Intercept)    134.140         7.537                 17.799    0.000    119.278    149.002 \n#>     Adverts      0.096         0.010        0.578     9.979    0.000      0.077      0.115 \n#> -------------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-mye-forklarer-modellen-vår","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.4.1 Hvor mye forklarer modellen vår?","text":"Det første vi kan se på er \\(R^{2}\\) som forteller oss hvor stor del av den totale variansen modellen forklarer. dette tilfellet er \\(R^{2} = 0.3346\\). Det innebærer modellen vår kan forklare 33.46 % av den totale variansen. Det betyr reklame forklarer («accounts ») 33.5 % av variansen salget. Det er med andre ord mange andre faktorer som kan forklare hvorfor noen plater selger bedre enn andre, men reklame kan forklare drøye 33 % av den totale variansen. Dette kan vi også se er statistisk signifikant p < .001.Endel programmer vil også gi en R verdi. Siden vi kun har en uavhengig variabel (en prediktor) vil verdien R utgjøre den bivariate korrelasjonen (korrelasjonskoeffisienten mellom de variablene - vi ser dette er samme verdi som tabellen korrelasjonskoeffisienter lenger opp).Adjusted \\(R^{2}\\) er en “modifisert versjon” av \\(R^{2}\\) der det legges inn en korreksjon antall prediktorer modellen. Motivasjonen dette er det å legge til flere prodeiktorer alltid vil øke \\(R^{2}\\) verdien (Navarro Foxcroft 2019). Navarro Foxcroft (2019) påpeker imidlertid man ikke kan tolke adjusted \\(R^{2}\\) like rett fram som \\(R^{2}\\), og anbefaler man bruker \\(R^{2}\\). Det vi også kan si er dersom verdiene på henholdsvis \\(R^{2}\\) og adjusted \\(R^{2}\\) er nærme hverandre (eller like) indikerer dette en god kryssvaliditet modellen, noe som kan gjøre oss sikrere generalisering av funnene våre.","code":""},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-og-regresjonslikning","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.4.2 Modellens koeffisienter og regresjonslikning","text":"Koeffisientene vil fortelle oss mer multippel regresjonsanalyse, men gir oss noen interessante opplysninger også . introduksjonen snakket vi om punktet der regresjonslinja skjærer y-aksen (konstanten). Fra analysen ser vi (Intercept) = 134.14 og Adverts = 0.096. 134.14 er punktet på y-aksen regresjonslinja “begynner” (der x = 0). Altså, ettersom x-aksen angir verdier hva vi bruker på reklame er Intercept estimatet antallet plater vi kan forvente å selge dersom vi bruker 0 kroner på reklame. Estimatet på «Adverts» på 0,096 er stigningstallet regresjonslinja – hvis prediktoren (reklame) stiger med 1 enhet stiger salget med 0,096 plater. Vi kan da lage følgende likning: \\[ Sales=134,14\\:+\\:0,096\\left(Adverts\\right) \\]","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.4.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi ønsker å ha en formening om hvor god modellen er («goodness fit»). Altså, er regresjonsmodellen vår bedre enn en modell der vi ikke vet noe om forholdet mellom reklame og platesalg? Vi kan bruke gjennomsnitt av salgstallene som en modell ingen sammenheng mellom reklame og platesalg, og deretter sammenlikne regresjonsmodellen med gjennomsnittsmodellen. Sammenlikningen mellom modellene skjer gjennom å se på forskjellene mellom de observerte målingene (salgstall) og verdier predikert de ulike modellene. Dersom regresjonsmodellen signifikant predikerer bedre er det en bedre modell enn alternativet.Analysen vår har gitt oss F verdien 99.59 med p < .001. Vi ser verdien er statistisk signifikant. F verdien er et mål på forbedring prediksjonen sett opp mot unøyaktigheter modellen (alle modeller er unøyaktige (eller “feil”)).\nVi kan sjekke F-verdien opp mot antall frihetsgrader (df) gjennom tabeller som ofte finnes statistikkbøker, eller bruke onlineressurser som herVi ser av resultatene fra analysen antall df teller er 1 og antall df nevner er 198. Hvis vi leser av tabellen ser vi df 1/df 200 er kritisk verdi 3,89 α = 0,05 og 11,15 α = 0,001. Vår F er med andre ord langt kritisk verdi. Vi kan derfor si vår regresjonsmodell gir en signifikant bedre prediksjon av platesalg enn alternativet. Reklame er med andre ord en god prediktor platesalg.Helt nøyaktig kan vi regne ut kritisk verdi (som vi har gjort R) df 1 og df 198:Ut fra dette kan vi si det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.Til slutt vil vi se på konfidensintervallet:","code":"\nqf(p=.05, df1=1, df2=198, lower.tail=FALSE)\n#> [1] 3.888853\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\nConfint(FieldOLS_reg)\n#>                 Estimate        2.5 %      97.5 %\n#> (Intercept) 134.13993781 119.27768082 149.0021948\n#> Adverts       0.09612449   0.07712929   0.1151197"},{"path":"regresjonsanalyse---ols.html","id":"steg-5-sjekk-av-forutsetningene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5 Steg 5: Sjekk av forutsetningene","text":"Dette er et tema som behandles og framstilles på noe ulike måter litteraturen. Det som er klart er brudd på forutsetningene kan gjøre modellen vår mer usikker opp til et punkt hvor regresjonsanalyse ikke bør gjennomføres. Noen av forutsetningene er empirisk testbare (vi kan få ut en eller annen form analyse av et statistikkprogram som SPSS, Stata, R og så videre) mens noen er ikke empirisk testbare (det vil si vi må bruke egen vurdering). Vi skal dette delkapittelet gå gjennom forutsetningene lineær regresjon. Selv om noen ikke er aktuelle enkel regresjon tar vi med alle forutsetningene oversiktens skyld.Ved regresjonsanalyse gjør vi en rekke sjekker av datamaterialet vi har å avgjøre om regresjonsanalyse er en egnet teknikk og hvorvidt vi mener vi kan generalisere funnene. Dersom forutsetningene brytes gjør det vi kan sette spørsmålstegn ved hvor nærme regresjonskoeffisienten er populasjonskoeffisienten – eller med andre ord: Hvis regresjonskoeffisienten er helt forventningsrett («unbiased», dvs 0) så vil regresjonskoeffisienten være lik populasjonskoeffisienten («estimatet er lik virkeligheten»). Nå vil det praksis aldri være tilfelle, men ved å sette visse forutsetninger kan vi fastslå om våre data egner seg regresjonsanalyse og hvor sikre vi føler oss funnene kan generaliseres. Det er verdt å merke seg hva Field et. al (2012b, s.298) skriver:“’s worth remembering can perfectly good model data (outliers, influential cases, etc.) can use model draw conclusions sample, even assumptions violated. However, ’s much interesting generalize regression model assumptions become important. violated generalize findings beyond sample.”Med andre ord: Vi kan ha brudd på forutsetningene og likevel si noe meningsfullt om vårt utvalg/våre data, men resultatene våre blir mer usikre og vi skal være veldig forsiktige med å kreve generaliserbarhet dersom vi har brudd på forutsetningene. Så alt håp er ikke ute med brudd på forutsetningene, men vi skal behandle konklusjonene våre deretter.Regresjonsforutsetninger behandles ulikt av ulke kilder, og får ulik plass diskusjoner om regresjon. Vi har undersøkt en rekke kilder å framstille dette (blant annet Green (1991), Berry (1993), Miles Shevlin (2001), Hinkle, Wiersma, Jurs (2003), Tabachnik Fidell (2007), Eikemo Clausen (2007), Hair Jr. et al. (2010), Lomax Hahs-Vaughn (2012)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.1 Kausalitet","text":"Forutsetningen om kausalitet hviler fagunnskap og teoretiske vurderinger. Det sier seg så vidt selv vi ikke er interesserte å ha med irrelevante variabler modellen. Med irrelevant menes variabler som korrelerer med den avhengige variabelen, men hvor korrelasjonen er ikke har noe med årsakssammenheng å gjøre (sammenhenger medfører ikke seg selv kausalitet som vi har vært inne på en tidligere modul).Kausalitet er dermed en forutsetning. Den uavhengige variabelen må variere korrelert med de avhengige, det er en kausal sammenheng (hvis ikke det er kausalitet har den/de uavhengige variablene ingen effekt på den avhengige – det kan like gjerne være motsatt). Når vi velger en avhengig variabel og en eller flere uavhengige variabler har vi også gjort en antakelse om kausalitet og retning på kausaliteten, og forutsatt denne er tilstede - basert på teoretisk kunnskap om det vi undersøker. Men det er viktig å understreke verken korrelasjon eller regresjon indikerer kausalitet.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil våre data. Thoresen (2003) viser eksempel til en studie av MacMahon et al. (1990) der de fant en 60% sterkere sammneheng mellom blodtrykk og hjerte-karsykdommer en stor metastudie når de korrigerte skjevhet estimatene de tidligere studiene (som var inkludert metastudien).Vi skal også være oppmerksom på utfordringer dersom feil den ene variabelen korrelerer med feil en annen variabel. “Dersom målt eksponering og målt helseutfall er rammet av avhengige feil, blir resultatet oftest en falskt forhøyet sammenheng mellom de . Slik resultatskjevhet er sannsynligvis ikke uvanlig tverrsnittsstudier, hvor data om eksponering og utfall skaffes til veie gjennom spørreskjema” (Kristensen 2005).","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.3 Relevante og irrelevante variabler","text":"Alle relevante uavhengige variabler må være inkludert modellen, og alle irrelevante uvhengige variabler er fjernet/er ikke med modellen. Man kan si en hovedgrunn til vi veldig ofte kjører mulitippel regresjonsanalyse stedet bivariat regresjonsanalyse er å unngå vi ikke inkluderer relevante variabler (såkalt “omitted variable bias”) (Thrane 2019). Imidlertid, som Thrane (2019) påpeker, er dette praksis umulig, så det vi tilstreber er å inkludere de mest relevante variablene. Dette faller igjen tilbake på teoretiske betraktninger og faglig kjennskap til området man holder på med. Du skal hvert fall kunne begrunne valget av hvilke uavhengige variabler som er inkludert og hvilke som kanskje kunne tenkes å være inkludert, men som du har valgt å ikke inkludere.Teoretisk sett skal vi også forsikre oss om ikke-relevante variabler ikke er inkludert modellen. Igjen er dette delvis umulig og delvis forvirrende/unøyaktig. Det er delvis umulig fordi vi vanskelig kan vite eksakt hvilke potensielle variabler som er relevante og ikke. Det er delvis forvirrende/unøyaktig fordi det kan være viktig å identifisere variabler som ikke har noen effekt - dette kan være viktig policyrevisjon/-utforming (jfr Thrane (2019)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Forholdstallet mellom respondenter/caser/observasjoner og uavhengig variabler er av stor betydning dersom man skal gjennomføre en multippel regresjon. Dette gjelder spesielt ved skjevdistribusjon av den avhengige variabelen, effektstørrelsen er forventet liten eller man kan mistenke vesentlige målefeil (Tabachnik Fidell 2007).Det finnes ulike anbefalte normer vurdering av forholdstallet (vi tar utgangspunkt standard multippel regresjonsanalyse - stegvis multippel regresjonsanalyse kan gi andre vurderinger rundt forholdstallet). viser vi noen eksempler på hvordan man kan vurdere dette:\nTable 9.1: Forholdstall\nTable 9.1: ForholdstallKildeAntallMarks (1966, Harris (2013)Minimum 200 uansettSchmidt (1971)15-1 til 25-1Nunally (1978)2-3 IV = minst 100, 9-10 IV = 300-400Stevens (1996)15 pr IVGreen (1991)N≥50+8m (m=antall uavhengige variabler) ved «medium-sized relationship IVs DV, α=.05 β=.20Miles & Shevlin (2001)Som Green (2001). Utvalgsstørrelse avhenger av størrelse på effekt og statistisk styrke (se Cohen, 1988 effektstørrelser). Stor effekt: 80 respondenter er alltid nok opptil 20 IVs. Middels effekt: 200 respondenter vil alltid være nok opptil 20 IVs, 100 er nok opptil 6 eller færre IVs. Lav effekt: Minst 600.Det er også verdt å merke seg det ikke er ønskelig med mange respondenter, da et svært stort antall respondenter vil gi statistisk signifikans nesten enhver multippel korrelasjon – “statistical practical reasons, , one wants measure smallest number cases decent chance revealing relationship specified size” (Tabachnik Fidell 2007, s.123). Dette har med andre ord mye å si hvordan man planlegger en studie. Miles Shevlin (2001), som angitt siste rad tabellen , ser på sammenhengen mellom effektstørrelse, statistisk styrke og utvalgsstørrelse. Field (2009a, s.223, figur 7.10) har modifisert grafisk denne sammenhengen:Sammenheng antall prediktorer og nødvendig utvalgsstørrelse gitt ønsket statistisk styrke","code":""},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Denne forutsetningen gjelder om man har minst uavhengige variabler - altså multippel regresjon. Med dette menes vi må forvente variasjonen den avhengige variabelen er en funksjon av sum av endringer de uavhengige variablene. Vi kan også uttrykke dette som effekten av den uavhengige variabelen \\(x_1\\) på den avhengige variabelen \\(y\\) er uavhengig av eventuelle andre uavhengige variablers effekt på \\(y\\). Forutsetningen om additivitet betyr altså det ikke er interaksjon mellom eller flere uavhengige variabler.\nMed andre ord, hvis effekten av \\(x_1\\) på \\(y\\) er avhengig av hvordan \\(x_2...x_n\\) påvirker \\(y\\) brytes forutsetningen om additivitet. praksis er det imidlertid ikke uvanlig denne forutsetningen brytes en eller annen grad.La oss vise dette med et eksempel fra Thrane (2019) (se figur 6.1). Dersom vi har data kvinners og menns inntekt relatert til antall års utdannelse vil vi kunne se en regresjonsanalyse antall års utdanning predikerer inntekt. Forutsetningen om additivitet sier da antall års utdanning har lik effekt på inntekt menn og kvinner. Slik er det imidlertid ikke. Generisk kan det framstilles som grafen :Vi ser linjene har ulikt stigningstall. Kvinner starter menn, men har et høyere stigningstall. Det vil si kvinner har større effekt av et (ekstra) års utdanning enn menn. Da er forutsetningen om additivitet brutt. Som Thrane (2019) påpeker: “Additivity thus means parallel regression lines”.Vi kan altså sjekke dette ved å kjøre regresjonsanalyser. En annen måte, som vi ikke går inn på , er å lage en interaksjonsvariabel som vi dernest bruker modellen. En interaksjonsvariabel er et produkt av (minst) uavhengige variabler.La oss se på et annet eksempel å vise interaksjonsvariabel og -effekt basert på et eksempel fra Thomas (2017).Datasettet inneholder ti variabler tillegg til det vi vil bruke som avhengig variabel: Sales. Vi vil altså se om vi kan bruke de ni variablene til å predikere salg.Denne modellen kan altså forklare 87 % av variansen Sales.\nVi kan imidlertid mistenke det kan være en interaksjon mellom variablene Income og Population - jo større befolkning, jo større inntekt tilgjengelig.Forskjellen mellom modellene ligger altså den nederste koeffisienten (Income:Population) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Vi ser en klar interaksjon ved linjene krysser (jfr. Thranes merknad om forutsetningen om additivitet gir parallelle linjer).R har vi et hjelpemiddel pakken interactions:Tolkningen av plottet fra pakken interactions er den samme: Parallelle linjer indikerer fravær av interaksjoneseffekt, mens ikke-parallelle linjer indikerer tilstedeværelse av interaksjoneseffekt.Vi ser tillegg interaksjonseffekten (Income:Population) er statistisk signifikant.","code":"\ndata(Carseats)\nsummarytools::descr(Carseats, stats = \"common\")\n#> Non-numerical variable(s) ignored: ShelveLoc, Urban, US\n#> Descriptive Statistics  \n#> Carseats  \n#> N: 400  \n#> \n#>                   Advertising      Age   CompPrice   Education   Income   Population    Price    Sales\n#> --------------- ------------- -------- ----------- ----------- -------- ------------ -------- --------\n#>            Mean          6.64    53.32      124.97       13.90    68.66       264.84   115.80     7.50\n#>         Std.Dev          6.65    16.20       15.33        2.62    27.99       147.38    23.68     2.82\n#>             Min          0.00    25.00       77.00       10.00    21.00        10.00    24.00     0.00\n#>          Median          5.00    54.50      125.00       14.00    69.00       272.00   117.00     7.49\n#>             Max         29.00    80.00      175.00       18.00   120.00       509.00   191.00    16.27\n#>         N.Valid        400.00   400.00      400.00      400.00   400.00       400.00   400.00   400.00\n#>       Pct.Valid        100.00   100.00      100.00      100.00   100.00       100.00   100.00   100.00\nsaleslm <- ols_regress(Sales~. ,data = Carseats)\nsaleslm\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.935       RMSE                1.019 \n#> R-Squared               0.873       Coef. Var          13.592 \n#> Adj. R-Squared          0.870       MSE                 1.038 \n#> Pred R-Squared          0.865       MAE                 0.804 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                 Sum of                                                \n#>                Squares         DF    Mean Square       F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression    2779.441         11        252.676    243.372    0.0000 \n#> Residual       402.834        388          1.038                      \n#> Total         3182.275        399                                     \n#> ----------------------------------------------------------------------\n#> \n#>                                      Parameter Estimates                                      \n#> ---------------------------------------------------------------------------------------------\n#>           model      Beta    Std. Error    Std. Beta       t        Sig      lower     upper \n#> ---------------------------------------------------------------------------------------------\n#>     (Intercept)     5.661         0.603                   9.380    0.000     4.474     6.847 \n#>       CompPrice     0.093         0.004        0.504     22.378    0.000     0.085     0.101 \n#>          Income     0.016         0.002        0.157      8.565    0.000     0.012     0.019 \n#>     Advertising     0.123         0.011        0.290     11.066    0.000     0.101     0.145 \n#>      Population     0.000         0.000        0.011      0.561    0.575    -0.001     0.001 \n#>           Price    -0.095         0.003       -0.799    -35.700    0.000    -0.101    -0.090 \n#>   ShelveLocGood     4.850         0.153        0.703     31.678    0.000     4.549     5.151 \n#> ShelveLocMedium     1.957         0.126        0.345     15.516    0.000     1.709     2.205 \n#>             Age    -0.046         0.003       -0.264    -14.472    0.000    -0.052    -0.040 \n#>       Education    -0.021         0.020       -0.020     -1.070    0.285    -0.060     0.018 \n#>        UrbanYes     0.123         0.113        0.020      1.088    0.277    -0.099     0.345 \n#>           USYes    -0.184         0.150       -0.031     -1.229    0.220    -0.479     0.111 \n#> ---------------------------------------------------------------------------------------------\nsaleslm2 <- lm(Sales~. ,Carseats)\nsaleslm1 <- lm(Sales~.+Population*Income, Carseats)\ncompareCoefs(saleslm2, saleslm1)\n#> Calls:\n#> 1: lm(formula = Sales ~ ., data = Carseats)\n#> 2: lm(formula = Sales ~ . + Population * Income, data = \n#>   Carseats)\n#> \n#>                     Model 1   Model 2\n#> (Intercept)           5.661     6.195\n#> SE                    0.603     0.644\n#>                                      \n#> CompPrice           0.09282   0.09262\n#> SE                  0.00415   0.00413\n#>                                      \n#> Income              0.01580   0.00797\n#> SE                  0.00185   0.00387\n#>                                      \n#> Advertising          0.1231    0.1237\n#> SE                   0.0111    0.0111\n#>                                      \n#> Population         0.000208 -0.001811\n#> SE                 0.000370  0.000952\n#>                                      \n#> Price              -0.09536  -0.09511\n#> SE                  0.00267   0.00266\n#>                                      \n#> ShelveLocGood         4.850     4.859\n#> SE                    0.153     0.152\n#>                                      \n#> ShelveLocMedium       1.957     1.964\n#> SE                    0.126     0.125\n#>                                      \n#> Age                -0.04605  -0.04566\n#> SE                  0.00318   0.00317\n#>                                      \n#> Education           -0.0211   -0.0216\n#> SE                   0.0197    0.0196\n#>                                      \n#> UrbanYes              0.123     0.133\n#> SE                    0.113     0.112\n#>                                      \n#> USYes                -0.184    -0.216\n#> SE                    0.150     0.150\n#>                                      \n#> Income:Population            2.88e-05\n#> SE                           1.25e-05\n#> \nggplot(data=Carseats, aes(x=Income, y=Sales, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(Population,Sales), method=lm, se=F,color=\"black\")+xlab(\"Income and Population\")+labs(\n        title=\"Inntekt i blått - Befolkning i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\ninteract_plot(saleslm1, pred = Population, modx = Income)\nsumm(saleslm1)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.6 Linearitet","text":"Vi tar, som navnet “linær regresjonsanalyse” ganske klart indikerer, utgangspunkt forholdet mellom den/de uavhengige varaibelen(e) og den avhengige variabelen kan beskrives som en lineær funksjon (se eksempel Ringdal (2007)). Sammenhengen mellom variablene må ikke være perfekt lineær, men må hvert fall være tilnærmet lineær.Vi har allerede sett en grafisk framstilling punktet analyse av dataene som lar oss visuelt vurdere denne forutsetningen:Den stiplede linjen som buer ca midt det skraverte området gjør ingen forutsetninger, men plotter bare dataene (ofte kalt “scatterplot smoother”). Vi kan vurdere om denne er nærme eller langt fra en rett linje. grafen ligger det både en stiplet buet linje og en rett linje (regresjonslinje). vårt tilfelle vil vi raskt konkludere med forholdet mellom de variablene er tilnærmet lineært.","code":"\nscatterplot(Adverts ~ Sales, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.7 Residualene skal være normalfordelte","text":"Forutsetningen er residualene modellen er tilfeldige, normalfordelte variabler med gjennomsnittsverdi 0 (Field, Miles, Field 2012b), hvilket innebærer forskjellen mellom modellen og de observerte dataene er 0 eller nær 0 de fleste tilfeller (og ulikhet skyldes tilfeldigheter). Poenget er dersom regresjonsmodellen er god skal det være omtrent like stor sannsynlighet den underestimerer som den overestimerer. Er den det vil fordelingen være tilnærmet symmetrisk (perfekt normalfordeling vil praksis ikke inntreffe).En tilnærming til å se på denne forutsetningen er å se på et histogram residualene. Når vi lagrer regresjonsmodellen som et objekt (R) kan vi se hvilke parametere som lagres modellen:Residualene lagres altså modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\nnames(FieldOLS_reg)\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"      \n#>  [4] \"rank\"          \"fitted.values\" \"assign\"       \n#>  [7] \"qr\"            \"df.residual\"   \"xlevels\"      \n#> [10] \"call\"          \"terms\"         \"model\"\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\n\nhist(FieldOLS_reg$residuals)\n\nFieldOLSresid <- FieldOLS_reg$residuals\n\nplott3 <- ggplot(data = FieldOLS_reg, aes(FieldOLS_reg$residuals)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") +\n    theme_classic() +\n    labs(title = 'Histogram av residualer', x = 'Residualer', y = 'Antall')\n\nplott3\nols_plot_resid_qq(FieldOLS_reg)\nols_test_normality(FieldOLS_reg)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9899         0.1757 \n#> Kolmogorov-Smirnov        0.0634         0.3970 \n#> Cramer-von Mises          16.055         0.0000 \n#> Anderson-Darling          0.4298         0.3057 \n#> -----------------------------------------------\nols_plot_resid_fit(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.8 Fravær av multikolinearitet","text":"Multikolinearitet innebærer det er korrelasjon mellom de uavhengige varaiblene (multippel regresjon). Det kan ofte forekomme vi har en viss korrelasjon, men hvis korrelasjonen blir stor blir det vanskelig å skille mellom effekten den enkelte uavhengige variabel har på den avhengige variabelen.Multikolinearitet inntreffer dersom vi har sterk korrelasjon mellom eller flere av de uavhengige variablene. Multikollinearitet handler altså om det innbyrdes forholdet mellom de uavhengige variablene. Hvis disse er høyt korrelerte har vi multikollinearitet, altså det kan være (tilnærmet) perfekt linearitet mellom uavhengige variabler (Berry 1993) hvilket innebærer muligheten ingen av korrelasjonskoeffisientene er signifikante pga størrelsen på standardfeil. En perfekt kolinearitet har koeffisienten 1.Berry (1993) angir angir \\(r=.9\\) gir en dobling av standardfeil regresjonskoeffisienten, og selv \\(0.5\\) og \\(0.6\\) kan gi utfordringer tolkningen av regresjonskoeffisientene. Field (2009a) anser \\(0.8\\) til \\(0.9\\) som høy korrelasjon.Samtidig sier Pallant (2010) det (naturligvis) bør være en viss korrelasjon mellom de uavhengige og den avhengige variabelen, og hevder de bør være på \\(0.3\\), men samtidig bivariat korrelasjon mellom de uavhengige variablene ikke bør være \\(0.7\\).Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de tre uavhengige variablene korrelerer mellom \\(0.33\\) og \\(0.60\\) med den avhengige. De bivariate korrelasjonene mellom de uavhengige variablene erpå hhv. \\(0.08\\), \\(0.10\\) og \\(0.18\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Dette er aktuelt multippel regresjonsanalyse, så å vise dette lager vi en slik datasettet til Field der vi inkluderer tre uavhengige variabler:Imidletid finnes det også advarsler mot å se kun på de parvise korrelasjonene. Selv om korrelasjonen mellom f.eks. fem par variabler er lav/0.8/0.9, kan det hende korrelasjonen mellom fire av disse tilsammen forklarer en veldig høy andel av variansen den femte, uten dette fremkommer de parvise korrelasjonene. En måte å supplere vurderingen av multikolinearitet er gjennom VIF (“Variance Inflation Factor”). VIF bruker multippel regresjonsanalyse (se evt kap. om regresjonsanalyse) å regne ut graden av multikolinearitet. Det statistikkprogrammet ditt gjør er å kjøre en og en av variablene som avhengig variabel:\\(x1 <- x2, x3, x4, x5\\)\\(x2 <- x1, x3, x4, x5\\)\\(x3 <- x1, x2, x4, x5\\)\\(x4 <- x1, x2, x3, x5\\)\\(x5 <- x1, x2, x3, x4\\)Hver av disse modellene fremskaffer en \\(R^2\\) (igjen: se kapittelet om regresjonsanalyse mer forklaring om dette). den første modellen ovenfor forklarer \\(R^2\\)-verdien hvor mye av variansen x1 som forklares av x2, x3, x4 og x5 - og tilsvarende de andre modellene. Det innebærer jo høyere \\(R^2\\)-verdi, jo høyere multikolinearitet. VIF-verdien regnes ut slik:\\(VIF_i=\\frac{1}{1-R^2_i}\\)Vi kan også se på Variance Inflation Factor (VIF), som måler hvor mye variansen til en estimert regresjonskoeffisient øker pga. multikollinearitet.VIF kan anta verdier fra 1, som indikerer 0 multikolinearitet. Så hvilken verdi av VIF er så høy vi har problemer med multikolinearitet? Hair Jr. et al. (2010) og Myers (1990) opererer med en akseptabel grense på 10 - altså 10 er det høy multikolinearitet. Andre anbefaler en langt lavere grense. Menard (2002) hevder VIF > 5.0 er bekymringsfullt, mens Johnston, Jones, Manley (2018) sier VIF > 2.5 indikerer bekymringsfull multikolinearitet.er rådene (også) litt ulike. En VIF-verdi på 10 VIF anses f.eks. pakken “olsrr” R (som vi har brukt ) som et tegn på alvorlig multikolinearitet (jfr. Belsley, Kuh, Welsch 1980). Der settes VIF på 4 som en grense der man bør se nærmere på om multikolinearitet kan være et problem. Bowerman O’Connell (1990) tilføyer snittet av VIF de uavhengige variablene ikke bør være vesentlig 1.De Jongh et al. (2015) framholder problemet med multikolinearitet er et mindre problem jo større utvalgsstørrelsen er - deres studie undersøkte utvalgsstørrelser på 100, 250, 500, 750, 1000 og 10000. Fra 500 og oppover var feilene små. Så noen krystallklar anbefaling er ikke lett å finne. Et tips kan være å sjekke om det er noen retningslinjer eller allment anerkjente anbefalinger innenfor ditt fagområde.","code":"\nFieldKorr <- cor(Field_OLS_data, method = \"pearson\")\nround(FieldKorr, 2)\n#>         Adverts Sales Airplay Image\n#> Adverts    1.00  0.58    0.10  0.08\n#> Sales      0.58  1.00    0.60  0.33\n#> Airplay    0.10  0.60    1.00  0.18\n#> Image      0.08  0.33    0.18  1.00\nFieldOLS_mult_reg <- lm(Sales ~ ., data = Field_OLS_data)\nbrief(FieldOLS_mult_reg)\n#>            (Intercept) Adverts Airplay Image\n#> Estimate         -26.6 0.08488   3.367 11.09\n#> Std. Error        17.4 0.00692   0.278  2.44\n#> \n#>  Residual SD = 47.1 on 196 df, R-squared = 0.665\nols_vif_tol(FieldOLS_mult_reg)\n#>   Variables Tolerance      VIF\n#> 1   Adverts 0.9856172 1.014593\n#> 2   Airplay 0.9592287 1.042504\n#> 3     Image 0.9629695 1.038455"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.9 Fravær av heteroskedasisitet","text":"Variansen til residualene de uavhengige variablene skal være lik (Miles Shevlin 2001). Heteroskedasitsitet innebærer residualene ikke har konstant varianse (motsatt: vi ønsker lik varians residualene alle x-verdier og kaller dette homoskedastisitet). Hvis vi har homoskedastisitet predikerer modellen vår likt på alle predikerte verdier av Y, noe vi ønsker.Variansen til residualen kan altså ikke avhenge av de uavhengige variablene, men være lik på alle nivåer av verdier prediktorene. Dersom vi har heteroskedastisitet vil spredningen rundt regresjonslinja variere med X.Forutsetningen om homoskedastisitet er godt illustrert av Miles Shevlin (2001), s.100-101, fig. 4.22, 4.23 og 4.24:Figuren til venstre viser et scatterplot residualer. midten har vi samme fordeling av residualer med et antall normalfordelingskurver. Til høyre ser vi et alternativt scatterplot residualer som bryter med forutsetningen om homoskedastisitet.Løvås (2013) illustrerer det samme slik eksempelet om motorstørrelse og drivstofforbruk:Variasjonen residualene er like stor uansett verdien av x.Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi p-verdien er \\(0.4027\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\nFieldOLS_mult2 <- lm(Sales ~ Adverts + Airplay + Image, data = Field_OLS_data)\nbptest(FieldOLS_mult2, ~ Adverts*Airplay*Image + I(Adverts^2) + I(Airplay^2)+ I(Image^2), data = Field_OLS_data)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  FieldOLS_mult2\n#> BP = 10.44, df = 10, p-value = 0.4027"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.10 Fravær av autokorrelasjon","text":"Dette er typisk et problem tidsserieanalyser eller geografiske analyser (Eikemo Clausen 2007, .124), der “verdien på variabel X enhet N stor grad er bestemt av verdien på variabel X enhet N-1”. Et annet kjent eksempel er fra aksjemarkedet: Hvis en aksje stiger dag er det mer sannsynlig den stiger morgen. Verdien av aksjen morgen avhenger (delvis) av verdien dag. Verdiene dataene autokorrelerer. Grad av autokorrelasjon er således et mål på forholdet mellom en variabels verdi på tidspunkt X og verdien på tidspunkt før X.tilfeldige observasjoner bør ikke ha korrelasjon residualen. Dette kan testes gjennom en Durbin-Watson test (Durbin Watson 1951).\nDurbin-Watson testen er en test på autokorrelasjon residualene, og vil ha en verdi på mellom 0 og 4. Verdien 2 indikerer ingen autokorrelasjon. Verdier mellom 0 og 2 indikerer en positiv autokorrelasjon, mens verdier mellom 2 og 4 indikerer en negativ autokorrelasjon. En konservativ tommelfingerregel sier verdier 1 og 3 er bekymringsfullt (Field, Miles, Field 2012b).viser verdien \\(2.03\\) noe som ikke gir grunn til bekymring.","code":"\ndurbinWatsonTest(FieldOLS_reg)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1     -0.04394305      2.032324   0.796\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene 1 og 169 identifiseres som statistisk signifikante uteliggere","code":"\ncar::qqPlot(FieldOLS_reg, id.method=\"identify\", main=\"Q-Q Plott\")#> [1]   1 169"},{"path":"regresjonsanalyse---ols.html","id":"hampel-filter","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.11.1 Hampel filter","text":"Hampel filter innebærer man ser alle observasjoner som ligger utenfor intervallet \\(median +/- 3 median\\ absolute\\ deviation\\ (MAS)\\).","code":"\nnedregrense <- median(Field_OLS_data$Adverts) - 3*(mad(Field_OLS_data$Adverts, constant = 1))\nnedregrense\n#> [1] -457.7405\novregrense <- median(Field_OLS_data$Adverts) + 3*(mad(Field_OLS_data$Adverts, constant = 1))\novregrense\n#> [1] 1521.572\nuteligger_ind <- which(Field_OLS_data$Adverts < nedregrense |Field_OLS_data$Adverts > ovregrense)\nuteligger_ind\n#>  [1]  11  23  28  43  55  87  88  93 126 175 184"},{"path":"regresjonsanalyse---ols.html","id":"grubbs-test","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.11.2 Grubbs’ test","text":"Grubbs’ test ser om den høyeste verdien variabelen bør regnes som en uteligger (hvis den høyeste ikke er det vil ingen andre heller være det).","code":"\ngrubbstest <- grubbs.test(Field_OLS_data$Adverts)\ngrubbstest\n#> \n#>  Grubbs test for one outlier\n#> \n#> data:  Field_OLS_data$Adverts\n#> G = 3.41281, U = 0.94118, p-value = 0.05396\n#> alternative hypothesis: highest value 2271.86 is an outlier"},{"path":"regresjonsanalyse---ols.html","id":"rosners-test","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.11.3 Rosners test","text":"angir vi det antallet vi tror er uteliggere, f.eks. fra et box plott.","code":"\nrosnerstest <- rosnerTest(Field_OLS_data$Adverts,\n  k = 3\n)\nrosnerstest$all.stats\n#>   i   Mean.i     SD.i    Value Obs.Num    R.i+1 lambda.i+1\n#> 1 0 614.4123 485.6552 2271.860     184 3.412808   3.605525\n#> 2 1 606.0834 472.3432 2000.000      43 2.951068   3.604019\n#> 3 2 599.0434 462.9555 1985.119      87 2.993971   3.602505\n#>   Outlier\n#> 1   FALSE\n#> 2   FALSE\n#> 3   FALSE"},{"path":"regresjonsanalyse---ols.html","id":"influential-cases","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.5.12 Influential cases","text":"Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):Det som grafen kalles “hat values” er et vanlig mål å finne observasjoner/caser som er relativt langt fra senter av prediksjonsrommet og som derfor potensielt har stor innflytelse på OLS-regresjonskoeffisientene (“leverage”) (Fox Weisberg 2019). Huber (1981) anbefaler følgende grenseverdier: verdier \\(0.2\\) er ønskelig, verdier \\(0.5\\) uønskede, og verdier mellom \\(0.2\\) og \\(0.5\\) problematiske.“Hat values” er altså et mål på potensiell innflytelse. Neste mål - DfBetas - er et mål på observasjonens effekt på regresjonskoeffisinenten hver variabel med og uten den innflytelsesrike observasjonen - eller med andre ord: observasjonenes innflytelse på variablene. Belsley, Kuh, Welsch (1980) anbefaler 2 som cut-verdi å indikere innflytelsesrike observasjoner.Pakken legger automatisk inn cutoff-verdi (dette tilfellet 0.14). Formelen utregning av dfbeta cutoff-verdi er \\(\\frac{2}{\\sqrt{n}}\\), der n=antall observasjoner.Vi kan også se på dffit (Welsch Kuh 1977).Cutoff-verdi dette tilfellet er 0.2. Formel utregning er \\(2*\\frac{\\sqrt{(k+1)}}{(n-k-1)}\\), der k = antall prediktorer og n = antall observasjoner.Det siste målet vi ønsker å se på (og trolig den mest brukte) er “Cook’s distance”, som gir et mål på observasjonens totale innflytelse på regresjonsmodellen.grafen vises Cook’s distance - et mål på vektet kvadratsum forskjellene mellom de individuelle elementene til koeffisienten. Sagt på en annen måte: Vi bruker Cook’s distance til å se hvilke observasjoner/caser som kan påvirke modellen vår uforholdsmessig mye (totalt sett). Dersom vi har mange caser med høy verdi på Cook’s distance kan det være en indikasjon på lineær regresjon kanskje ikke er en egnet analyse det foreliggende datasettet.Så hva er høy verdi på Cook’s distance? Kilder som Cook Weisberg (1982) og Tabachnik Fidell (2007) angir verdier 1 er bekymringsfullt. Andre, som Fox (2020), advarer mot en ren numerisk vurdering (og fremhever viktigheten av både grafisk presentasjon og vurdering av hvert enkelt tilfelle). En tilnærming som er anbefalt (se f.eks. Zach (2019b)) er å bruker forholdstallet \\(4/N\\) - vårt tilfelle \\(4/200=0.02\\).La oss hente opp Cook’s distance de største verdiene de enkelte observasjoner:kjenner vi igjen observasjonene 1, 169 og 42 som de med høyest verdi på Cook’s distance, men også casene 10, 55 og 125 har verdier anbefalingen som kommer fra \\(4/n\\). Men vi ser også verdien er relativt lave hvis man tar utgangspunkt 1 som bekymringsfullt. å vise eventuell justering av modellen som følge av uteliggere viser vi likevel framgangsmåte.Vi bør også undersøke “added variable plots” - en regresjon kan observasjonene ha både en individuell og en sammensatt/felles påvirkning.sier Fox Weisberg (2019), s.44 “Points extreme left right plot correspond cases high leverage corresponding coefficients consequenlty potentially influential”.","code":"\ninfluenceIndexPlot(FieldOLS_reg, vars = \"hat\", id = list(n=3))\nols_plot_dfbetas(FieldOLS_reg, print_plot = TRUE)\nols_plot_dffits(FieldOLS_reg)\ninfluenceIndexPlot(FieldOLS_reg, vars = \"Cook\", id = list(n=3))\nmineCDverdier <- cooks.distance(FieldOLS_reg)\nmineCDverdier <- round(mineCDverdier, 3)\nhead(sort(mineCDverdier, decreasing = TRUE), n = 10)\n#>     1   169    42    10    55   125     3   148    86    72 \n#> 0.057 0.051 0.041 0.024 0.024 0.023 0.018 0.018 0.017 0.016\navPlots(FieldOLS_reg, id=list(cex=0.75, n=3, method=\"mahal\"))"},{"path":"regresjonsanalyse---ols.html","id":"steg-6-eventuell-revisjon-av-modell","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.6 Steg 6: Eventuell revisjon av modell","text":"kan vi eksempel se hvordan modellens presterer ved bortfall av visse ekstreme verdier (spesielt innflytelsesrike observasjoner/caser, jfr. diskusjon regresjonsforutsetnigner) eller ved inkludering/eksklusjon av gitte variabler modellen (først og fremst ved multippel regresjonsanalyse).Vi bør vurdere punktene og vurdere om vi ønsker å lage en revidert modell der vi tar ut veldig innflytelsesrike caser/observasjoner. Som tidligere nevnt har vi ikke svært store verdier , men la oss som et eksempel si vi ønsker å se om en modell uten observasjon 169. Vi anbefaler å ta bort en og en observasjon siden (som nevnt) observasjonene/casene har både en individuell og felles påvirkning.Som forventet ser vi ikke de store forskjellene. Vi kan ta bort de andre observasjonene illustrasjonens skyld:Igjen, ikke de store endringene. Vi kan se Intercept (\\(\\beta\\)) går litt ned etter hvert som vi tar bort caser, og betydningen av Adverts går litt opp (men det er marginalt).La oss, eksempelets skyld, manipulere datasettet slik en analyse av Cook’s distance ser slik ut:Hvis vi nå kjører denne modellen opp mot en modell der vi tar bort 11 og 23 får vi:ser vi koeffisienten Adverts stiger fra 0.01 til 0.09, eller en endring på 11.1%.","code":"\nFieldOLS_reg2 <- update(FieldOLS_reg, subset = -169)\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> \n#>             Model 1 Model 2\n#> (Intercept)  134.14  131.76\n#> SE             7.54    7.39\n#>                            \n#> Adverts     0.09612 0.09826\n#> SE          0.00963 0.00942\n#> \nFieldOLS_reg3 <- update(FieldOLS_reg, subset = -c(1, 42, 169, 184))\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2, FieldOLS_reg3)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> 3: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -c(1, 42, 169, 184))\n#> \n#>             Model 1 Model 2 Model 3\n#> (Intercept)  134.14  131.76  126.14\n#> SE             7.54    7.39    7.28\n#>                                    \n#> Adverts     0.09612 0.09826 0.10461\n#> SE          0.00963 0.00942 0.00942\n#> \nField_OLS_data2 <- read_excel(\"Field_datasett_OLS2.xlsx\")\nFieldOLS_man <- lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\ninfluenceIndexPlot(FieldOLS_man, vars = c(\"Cook\"), id = list(n=3))\nFieldOLS_man2 <- update(FieldOLS_man, subset = -c(11, 23))\ncompareCoefs(FieldOLS_man, FieldOLS_man2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data2,\n#>    subset = -c(11, 23))\n#> \n#>             Model 1 Model 2\n#> (Intercept)  183.77  134.14\n#> SE             5.97    7.64\n#>                            \n#> Adverts     0.01207 0.09621\n#> SE          0.00298 0.00999\n#> "},{"path":"regresjonsanalyse---ols.html","id":"steg-7-eventuell-analyse-av-revidert-modell","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.7 Steg 7: Eventuell analyse av revidert modell","text":"vil vi prinsippet bare gjenta samme analyser som ved analyse av den opprinnelige modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-8-konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.8 Steg 8: Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Vi viser rapportering av en enkel lineær regresjonsanalyse etter APA-standard:En enkel lineær regresjon ble gjennomført å predikere salgstall per uke basert på sum brukt på reklame uka før lansering. Vi fant en signifikant regresjonslikning (F(1,198) = 99.59, \\(\\beta\\) = 134.14p < .001) med en \\(R^2\\) på .335, 95% CI [119.28, 149.00].","code":"\ntable1::label(Field_OLS_data$Adverts) <- \"Adverts\"\ntable1::label(Field_OLS_data$Sales) <- \"Sales\"\ntable1::table1(~Adverts + Sales, data = Field_OLS_data)\nFieldOLSkorr <- tab_corr(Field_OLS_data, triangle = \"lower\")\nFieldOLSkorr\ntab_model(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"til-slutt-for-r-brukere","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.3.9 Til slutt for R-brukere…","text":"Mehmetoglu Mittner (2020) har skrevet en veldig god bok om på norsk: “Innføring R statistiske analyser” som vi varmt kan anbefale. Den kommer med en pakke (“rnorsk”) som kan lastes ned gjennom kommandoen devtools::install_github(“ihrke/rnorsk”) (forutsetter pakken “devtools” er på plass, hvis ikke så kjør “package.install(”devtools”)).Forfatterne har laget en samling av regresjonsdiagnostikk som vi viser på Fields data der vi laget en multippel regresjonsmodell:Analysen gir en samlet oversikt et antall parametere og søker å hjelpe til med beslutning om forutsetninger er ok/ikke ok, men vi vil understreke kunnskap om hva som ligger bak de ulike testene og kriteriene er essensielt. Mer informasjon om parameterene finner dere .","code":"\noptions(scipen=999)\nregression.diagnostics(FieldOLS_mult_reg)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 1/11 (9.1 %) checks failed\n#> \n#> \n#> Identified problems: \n#>  functional form\n#> Summary:\n#> # A tibble: 11 × 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske… global   stud…   6.19e+0  0.103   0.05 No Pro…\n#>  2 heteroske… global   Non-…   3.03e-1  0.582   0.05 No Pro…\n#>  3 multicoll… Adverts  Vari…   1.01e+0 NA       5    No Pro…\n#>  4 multicoll… Airplay  Vari…   1.04e+0 NA       5    No Pro…\n#>  5 multicoll… Image    Vari…   1.04e+0 NA       5    No Pro…\n#>  6 normality  global   Shap…   9.95e-1  0.725   0.01 No Pro…\n#>  7 model spe… global   Stat…  -6.99e-5  0.916   0.05 No Pro…\n#>  8 functiona… global   RESE…   3.72e+0  0.0261  0.05 Problem\n#>  9 outliers   global   Cook…   7.08e-2 NA       1    No Pro…\n#> 10 outliers   global   Bonf…   3.16e+0  0.362   0.05 No Pro…\n#> 11 autocorre… global   Durb…   2.70e-3  0.778   0.05 No Pro…\n#> # … with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"standard-multippel-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4 Standard multippel regresjonsanalyse","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-standard-multippel-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.1 Eksempel standard multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på multippel regresjonsanalyse, ved å følge stegene .Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultater","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.2 Analyse av dataene","text":"Vi skal bruke et datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaDatasettet er stort til å vises fram sin helhet, men vi skal bruke uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) mot den avhengige variabelen tpstress (Perceived stress).Vi ønsker altså å se om en gruppe studenters (N = 439) egenrapporterte oppfattelse av sin kontroll eksterne forhold som kan skape stress og deres evne til å kontrollere deres følelser, tanker og fysiske reaksjoner (Pallant 2000). Vi kan derfor se nærmere på variablene.","code":"\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\nPallant_survey2 <- select(Pallant_survey, tmast, tpcoiss, tpstress)\nPallant_survey2 <- na.omit(Pallant_survey2) \nsummarytools::descr(Pallant_survey2, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey2  \n#> N: 426  \n#> \n#>                    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- ----------\n#>            Mean    21.74     60.56      26.75\n#>         Std.Dev     3.97     11.96       5.84\n#>             Min     8.00     20.00      12.00\n#>          Median    22.00     62.00      26.00\n#>             Max    28.00     88.00      46.00\n#>         N.Valid   426.00    426.00     426.00\n#>       Pct.Valid   100.00    100.00     100.00\npar(mfrow=(c(2,2)))\nhisttmast <- with(Pallant_survey2, hist(tmast))\nhisttpcoiss <- with(Pallant_survey2, hist(tpcoiss))\nhisttpstress <- with(Pallant_survey2, hist(tpstress))\nqqtmast <- car::qqPlot(~ tmast, data = Pallant_survey2)\nqqtpcoiss <- car::qqPlot(~ tpcoiss, data = Pallant_survey2)\nqqtpstress <- car::qqPlot(~ tpstress, data = Pallant_survey2)\nboxplot(Pallant_survey2)"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.3 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.4 Lage modell (kjøre regresjonsanalysen)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.5 Analyse av resultatene (diagnostikk)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"antall-prediktoreruavhengige-variabler-overfitting-og-predicted-r-square","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.5.1 Antall prediktorer/uavhengige variabler, overfitting og predicted R-square","text":"multippel regresjonsanalyse har vi mer enn en prediktor/uavhengig variabel. Som regel kan vi ønske å inkludere mer enn en prediktor fordi vi sjeldent klarer å fange nok av variansen en avhengig variabel gjennom en prediktor. Samtidig ønsker vi ikke flere uavhengige variabler enn nødvendig å lage en så enkel modell som mulig som gir oss prediksjoner vi kan bruke. Matematisk er det også slik enhver uavhengig variabel som har en grad av korrelasjon med den avhengige variabelen vil bidra til å øke \\(R^2\\) uten det nødvendigvis gjør modellen “riktigere” (men kan gjøre den vanskeligere å tolke). Dersom vi legger til unødvendige uavhengige variabler risikerer vi det som kalles overfitting - altså modellen blir god på å beskrive tilfeldige feil dataene heller enn å beskrive forholdet mellom variablene. Resultatet er modellen ikke kan generaliseres.Underfit - God fit - OverfitVi kan også se tilbake på regresjonsforutsetningene og vil se et stort antall uavhengige variabler er en invitasjon til multikolinearitet.En måte å sjekke dette er å dele datasettet slik man gjør regresjonsanalysen på en del av datasettet og deretter tester modellen på den andre delen av datasettet. Dette kalles kryssvalidering. En annen måte er å se på “predicted R-square” som innebærer følgende prosedyre (som statistikkprogrammer gjør oss naturligvis):Et datapunkt/observasjon tas ut av datasettetRegresjonslikningen kalkuleresModellens evne til å predikere det datapunktet/observasjonen som ble tatt ut evalueres (altså - hvor nærme datapunktet kommer vår modell sin prediksjon?)Dette gjentas alle datapunktene datasettetTolkningen av dette er ganske grei. Man sammenlikner R-square med predicted R-square. Dersom det er liten forskjell mellom disse verdiene har man trolig liten sannsynlighet du har overfitting av modellen. Dersom forskjellen er stor er det grunn til å tro man kan ha overfitting.å unngå dette er det viktig å tenke på forholdstallene som ble diskutert foregående punkt om regresjonsforutsetninger. Kjennskap til tidligere forskning og resultater vil gi informasjon om og et teoretisk grunnlag hvilke variabler som bør inkluderes modellen.Vi skal illustrere overfitting basert på et eksempel fra Frost (2022) (dette eksempelet har altså ikke noe direkte med analysen vi er inne - Pallant sitt datasett - men er tatt med å illustrere poenget med overfitting).Datasettet inneholder variabler: Hvordan historikere rangerer amerikanske presidenter (Historians.rank) og hvor stor generell støtte presidenter har hatt befolkningen (Approval.High).Vi ser R-squared er \\(0.0068\\) - hvilket vi vil tolke som det praksis ikke er noen sammenheng mellom variablene.Vi kan så bruke en polynomisk likning (har vi brukt \\(x^3\\) å lage regresjonslinjen):Vi har nå en R-squared på \\(0.66\\) denne modellen mot \\(0.0068\\) den lineære modellen. Dette betyr den polynomiske regresjonsmodellen forklarer drøye 66% av variansen den avhengige variabelen. Modellen vår er ut fra dette en god modell våre data.\nImidlertid gir en analyse av Predicted R-square oss et annet bilde av modellen:realiteten forteller både verdien på predicted r-square på 0, og forskjellen mellom R-square (\\(0.664\\)) og predicted R-square \\(0\\), vi har en seriøs overfitting. Vi har med andre ord funnet en modell som beskriver dataene våre veldig godt, men som ikke kan brukes på andre data enn de vi har (vel, den kan jo brukes, men vil ikke kunne gi oss noe av verdi). Vi kan altså ikke predikere noe ut fra modellen.Vi ser \\(R^2 = 0.466\\). Modellen kan altså forklare \\(46.6%\\) av variansen den avhengige variabelen. Vi ser vi får en noe lavere verdi «Adjusted R Squared» forhold til «R Squared». Når vi legger til en uavhengig variabel en regresjonsanalyse er det lite sannsynlig korrelasjonen mellom den nye uavhengige variabelen og den avhengige variabelen vil være nøyaktig 0. Den vil stedet fluktuere rundt 0. Pga. denne tilfeldige fluktuasjonen rundt 0 vil \\(R^2\\) alltid øke litt når man legger til en ny uavhengig variabel. Adjusted \\(R^2\\) søker å kompensere dette å få fram en mer korrekt verdi. Jo større antall uavhengige variabler, jo større forskjell vil man se mellom \\(R^2\\) og Adjusted \\(R^2\\). Det samme vil være tilfelle ved mindre utvalgsstørrelser fordi variasjonen rundt 0 vil være større mindre utvalg.Vi kan også legge merke til Predicted R-Squared er \\(0.456\\) og dermed svært lik R-squared.","code":"\nPresidentRanking <- read.csv(\"PresidentRanking.csv\")\nsummarytools::descr(PresidentRanking)\n#> Descriptive Statistics  \n#> PresidentRanking  \n#> N: 12  \n#> \n#>                     Approval.High   Historians.rank\n#> ----------------- --------------- -----------------\n#>              Mean           78.75             17.00\n#>           Std.Dev            8.01             10.90\n#>               Min           67.00              2.00\n#>                Q1           72.00              8.50\n#>            Median           79.00             15.00\n#>                Q3           85.50             25.00\n#>               Max           90.00             38.00\n#>               MAD           10.38             11.86\n#>               IQR           12.25             15.75\n#>                CV            0.10              0.64\n#>          Skewness           -0.05              0.37\n#>       SE.Skewness            0.64              0.64\n#>          Kurtosis           -1.58             -1.20\n#>           N.Valid           12.00             12.00\n#>         Pct.Valid          100.00            100.00\npresidentlm <- lm(formula = Historians.rank ~ Approval.High, data = PresidentRanking)\nplot(Historians.rank ~ Approval.High, data = PresidentRanking)\nabline(presidentlm, lwd = 2, col = \"red\")\nsumm(presidentlm)\npresidentlm2 <- lm(Historians.rank ~ poly(Approval.High, degree=3), data=PresidentRanking)\nggplot(data=PresidentRanking, aes(Approval.High,Historians.rank)) +\n    geom_point() + \n    geom_smooth(method=\"lm\", formula=y~I(x^3)+I(x^2))\nsumm(presidentlm2)\nols_pred_rsq(presidentlm2)\n#> [1] -0.2162257\nPallantOLS_reg <- lm(tpstress ~ tmast + tpcoiss, data = Pallant_survey2)\nsumm(PallantOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.5.2 Modellens koeffisienter","text":"Vi ser først på tallene “Std.Beta” “Parameter Estimated”. Vi ser tmast bidrar større grad enn tpcoiss (fortegn er denne sammenheng irrelevant). Begge bidrar signifikant.","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.5.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 184.5. Vi kan regne ut (bruke R) til å finne kritiske verdi.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\nqf(p=.05, df1=2, df2=423, lower.tail=FALSE)\n#> [1] 3.017049"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.3 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Det kan se ut som en interaksjonseffekt ved linjene ikke er parallelle. Det kan imidlertid være noe vanskelig å tolke interaksjonseffekt. Man kan f.eks. ha en interaksjonseffekt som ikke er statistisk signifikant. R kan vi bruke pakken jtools som hjelp:Vi ser interaksjonen tmast:tpcoiss ikke er statistisk signifikant.Tolkningen er “som før”: Parallelle linjer indikerer fravær av interaksjonseffekt.","code":"\nPallantOLS_reg2 <- lm(tpstress ~ tmast + tpcoiss, Pallant_survey2)\nPallantOLS_inter <- lm(tpstress ~.+tmast*tpcoiss, Pallant_survey2)\ncompareCoefs(PallantOLS_reg2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ tmast + tpcoiss, data = \n#>   Pallant_survey2)\n#> 2: lm(formula = tpstress ~ . + tmast * tpcoiss, data = \n#>   Pallant_survey2)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     50.83   52.66\n#> SE               1.27    4.34\n#>                              \n#> tmast         -0.6207 -0.7093\n#> SE             0.0614  0.2103\n#>                              \n#> tpcoiss       -0.1747 -0.2071\n#> SE             0.0204  0.0763\n#>                              \n#> tmast:tpcoiss         0.00154\n#> SE                    0.00348\n#> \nggplot(data=Pallant_survey2, aes(x=tmast, y=tpstress, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(tmast,tpcoiss), method=lm, se=F,color=\"black\")+xlab(\"tmast og tpcoiss\")+labs(\n        title=\"tmast i blått - tpcoiss i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\nsumm(PallantOLS_inter)\ninteract_plot(PallantOLS_inter, pred = tmast, modx = tpcoiss)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.6 Linearitet","text":"Vi kan ikke se på samme type scatterplott sjekk av linearitet som vi gjorde enkel OLS (på et todimensjonalt plott). Vi kan imidlertid lage “added variable plots” (Mosteller Tukey 1977).X-aksene representerer en enkelt uavhengig variabel (per graf ovenfor). Y-aksen = den avhengige variabelen. Den blå linjen viser sammenhengen mellom den uavhengige variabelen og den avhengige variabelen når alle andre uavhengige variabler holdes konstant. Jo sterkere lineær sammenheng plottene, jo sterkere er den respektive uavhengige variabelens bidrag modellen.vårt tilfelle vil vi nok konkludere med forutsetningen om linearitet er (nok) oppfylt.","code":"\nols_plot_added_variable(PallantOLS_reg2, print_plot = TRUE)\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.7 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\nhist(PallantOLS_reg2$residuals)\nols_plot_resid_qq(PallantOLS_reg2)\nols_test_normality(PallantOLS_reg2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9911         0.0115 \n#> Kolmogorov-Smirnov        0.0509         0.2197 \n#> Cramer-von Mises         31.6502         0.0000 \n#> Anderson-Darling          1.0978         0.0070 \n#> -----------------------------------------------\nols_plot_resid_fit(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.8 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.58\\) og \\(r=-0.61\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.53\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\", use=\"pairwise.complete.obs\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\nols_vif_tol(PallantOLS_reg2)\n#>   Variables Tolerance      VIF\n#> 1     tmast 0.7220785 1.384891\n#> 2   tpcoiss 0.7220785 1.384891"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.9 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\nbptest(PallantOLS_reg2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  PallantOLS_reg2\n#> BP = 42.208, df = 5, p-value = 0.00000005347"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.10 Fravær av autokorrelasjon","text":"viser verdien \\(1.826\\) noe som ikke gir grunn til bekymring.","code":"\ndurbinWatsonTest(PallantOLS_reg2)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08185218      1.825972   0.068\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:dffit:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\ncar::qqPlot(PallantOLS_reg2, id = list(n=3))#>  22 194 269 \n#>  21 190 263\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"hat\", id = list(n=3))\nols_plot_dfbetas(PallantOLS_reg2, print_plot = TRUE)\nols_plot_dffits(PallantOLS_reg2, print_plot = TRUE)\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"Cook\", id = list(n=3))\nmineCDverdier2 <- cooks.distance(PallantOLS_reg2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22     268      23     191     413     194 \n#> 0.09556 0.05833 0.05434 0.04374 0.03689 0.03601"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.6.12 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\nregression.diagnostics(PallantOLS_reg2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/10 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 10 × 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske… global   stud…   3.95     0.139   0.05 No Pro…\n#>  2 heteroske… global   Non-…   2.91     0.0881  0.05 No Pro…\n#>  3 multicoll… tmast    Vari…   1.38    NA       5    No Pro…\n#>  4 multicoll… tpcoiss  Vari…   1.38    NA       5    No Pro…\n#>  5 normality  global   Shap…   0.991    0.0115  0.01 No Pro…\n#>  6 model spe… global   Stat…   0.00495  0.568   0.05 No Pro…\n#>  7 functiona… global   RESE…   0.419    0.658   0.05 No Pro…\n#>  8 outliers   global   Cook…   0.0956  NA       1    No Pro…\n#>  9 outliers   global   Bonf…   3.56     0.177   0.05 No Pro…\n#> 10 autocorre… global   Durb…   0.0819   0.0760  0.05 No Pro…\n#> # … with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.7 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.8 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.9 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:En multippel lineær regresjonsanalyse ble gjennomført å teste om respondentenes oppfattede nivå av indre kontroll og kontroll på ytre faktorer predikerer totalt nivå av oppfattet stress. Analyser ble gjennomført å sikre det ikke var brudd på forutsetningene en multippel lineær regresjonsanalyse. En statistisk signifikant regresjonsmodell ble funnet (F (2, 423) = 184.5, p < .001), med en \\(R^2\\) på .466. Både nivå av indre kontroll og kontroll på ytre faktorer var signifikante prediktorer.","code":"\ntable1::label(Pallant_survey$tmast) <- \"tmast\"\ntable1::label(Pallant_survey$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey$tpstress) <- \"tpstress\"\ntable1::table1(~tmast + tpcoiss + tpstress, data = Pallant_survey)\nPallantkorr1 <- tab_corr(Pallant_survey2, triangle = \"lower\")\nPallantkorr1\ntab_model(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"modellens-evne-til-å-predikere","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.10 Modellens evne til å predikere","text":"Et siste tema vi skal ta oss multippel regresjonsanalyse inkluderes ofte ikke lærebøkers kapitler om regresjonsanalyse. Det omtales gjerne andre sammenhenger lærebøker som “predictive analytics” eller “predictive modelling”, og hvis man skal lære seg om temaer som maskinlæring vil dette naturlig element der. Selv synes jeg dette er naturlig å omtale regresjonsanalyse også fordi:“Model fit” handler kun om hvor god modellen vår beskriver/passer til dataene vi har dette tilfelletDette sier ikke nødvendigvis så mye om hvor god modellen vår er på andre data. Så hvis vi vil lage en modell basert på våre data som vi også ønsker skal kunne predikere andre data (er vår modell generaliserbar?) trenger vi en tilleggsanalyse.Dette tillegget kaller vi kryssvalidering. Metoden vi skal bruke kalles “k fold cross validation”.","code":""},{"path":"regresjonsanalyse---ols.html","id":"kryssvalidering","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.10.1 Kryssvalidering","text":"Kryssvalidering innebærer vi deler datasettet : en del vi utvikler modeller på, og en del vi tester vår utvalgte modell på. På den måten unngår vi vi bruker hele datasettet på å grave fram (litt sjansepreget) en modell som er ok uten å vite om vi bare har hatt flaks. Hvis vår valgte modell har en god fit med den delen av dataene vi tester på kan vi si noe mer sikkert om vår modell.Grafisk kan vi se oss prosessen slik:Datasettet deles først tilfeldig treningsdata og testdata. Ofte brukes 80/20 eller 70/30 fordeling, men dette kan variere. Treningsdataene deles igjen tilfeldig inn utvalg.Metoden vi ser på kalles “k fold Cross Validation”. k utgjør antall “folds”. Et “fold” er et sett utvalg. Det er ikke noen formell regel hvor mange utvalg man bør ha. Kuhn Johnson (2013) hevder “choice k usually 5 10, formal rule” (s.70).Hvis vi tenker oss vi har tre utvalg (U1-3) treningsdatasettet får vi da:Modell 1: Trent på U1 og U2, validert på U3Modell 2: Trent på U1 og U3, validert på U2Modell 3: Trent på U2 og U3, vlaidert på U1Modellene syntetiseres og til slutt kjøres modellen på testdatasettet. Da utsetter vi modellen data den ikke har “sett” før å se hvor nærme de virkelige verdiene (de observerte/faktiske) testdatene modellen klarer å predikere.","code":""},{"path":"regresjonsanalyse---ols.html","id":"kryssvalidering-av-vår-multiple-regresjonsmodell","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.4.10.2 Kryssvalidering av vår multiple regresjonsmodell","text":"Oppsummeringen av vår kryssvalidering viser:Resultaten vi er spesielt interessert er:RMSE: Root Mean Square Error. Dette er et mål på gjennomsnittlig forskjell mellom prediksjonene gjort av modellen (treningsdelen av datasettet) og de faktiske observasjonene (testdelen av datasettet). Jo lavere RMSE, jo bedre klarer modellen å predikere verdien på ukjente data (testdelen).Rsquared: Er korrelasjonen mellom prediksjonene og observasjoneneMAE: Mean Absolute Error. Dette er et mål på forskjellen mellom prediksjonene og observasjonene absolutte tall. Jo lavere MAE, jo nærmere observasjonene er prediksjonene.skulle vi selvsagt gjerne hatt noen absolutte grenseverdier å forholde seg til RMSE og MAE (Rsquared er litt enklere å forholde seg til som en korrelasjonskoeffisient). Dessverre er det ikke slik vi kan gi slike klare grenseverdier om modellen predikerer bra eller ikke. stedet må vi sammenlikne verdien på hhv. RMSE og MAE med skalaene på variablene. Hvis vi f.eks. predikerer boligpriser en modell vil RMSE gi oss avviket relatert til akkurat boligprisene (motsetning til f.eks. prosent). å fortsette boligpriseksempelet: Hvis vi får en RMSE på 15 000 et datasett der snittprisen på boliger er 15 000 000 vil det absolutt måtte betegnes osm en god (=lav) RMSE. Men samme RMSE på 15 000 et datasett der snittprisen på boliger er 100 000 vil være langt mindre god enn den første. Så man må rett og slett sette seg ned og gjøre en vurdering.","code":"\nset.seed(156)\nPallant_survey <- na.omit(Pallant_survey)\nkontrollspes <- trainControl(method = \"cv\", number = 10, savePredictions = \"final\")\nmodell1 <- train(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey, method = \"lm\", trControl = kontrollspes)\nprint(modell1)\n#> Linear Regression \n#> \n#> 333 samples\n#>   4 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 299, 298, 300, 300, 300, 300, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   4.358655  0.4674404  3.388348\n#> \n#> Tuning parameter 'intercept' was held constant at a\n#>  value of TRUE"},{"path":"regresjonsanalyse---ols.html","id":"hierarkisk-multippel-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5 Hierarkisk multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på hierarksik multippel regresjonsanalyse, ved å følge stegene . Hierarkisk vil dere også kunne se omtalt som blockwise entry fordi data legges inn modellen blokker).Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterEksempelet utvider eksempelet standard multippel regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.1 Analyse av dataene","text":"Vi skal bruke samme datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaI tillegg til de uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) vil vi bruke variablene alder (age) og “social desirability” (tmarlow) som kontrollvariabler.“Social desirability bias” er et kjent fenomen der respondenter har en tendens til å ønske å framstå et bedre lys heller enn et sant/reelt lys (Preiss et al. 2015). Respondenter kan f.eks. ønske å framstå som mer ærlige enn de realiteten er, og vil derfor også svare deretter på spørsmål. tillegg kan man anta alder kan ha påvirkning på opplevelsen av totalt stressnivå.Vi velger å lage et nytt datasett der vi trekker ut de relevante variablene, og fjerne observasjoner med NA (ingen verdi).denne hierarkiske multipple regresjonsanalysen ønsker vi derfor å bruke alder og sosial ønskverdighet som kontrollvariabler. Vi legger første blokk inn de variablene vi ønsker å kontrollere , før vi blokk legger inn de samme uavhengige variablene som forrige eksempel. Hensikten med dette er vi ønsker å fjerne mulige effekter av de “kontrollvariablene”.","code":"\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\nPallant_subset <- Pallant_survey[c(\"age\", \"tmarlow\", \"tmast\", \"tpcoiss\", \"tpstress\")]\nsummarytools::descr(Pallant_subset, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_subset  \n#> N: 439  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.44      5.30    21.76     60.63      26.73\n#>         Std.Dev    13.20      2.04     3.97     11.99       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   439.00    433.00   436.00    430.00     433.00\n#>       Pct.Valid   100.00     98.63    99.32     97.95      98.63\nPallant_survey3 <- select(Pallant_survey, age, tmarlow, tmast, tpcoiss, tpstress)\nPallant_survey3 <- na.omit(Pallant_survey3)\nsummarytools::descr(Pallant_survey3, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey3  \n#> N: 423  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.32      5.30    21.77     60.56      26.74\n#>         Std.Dev    13.09      2.02     3.97     12.00       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   423.00    423.00   423.00    423.00     423.00\n#>       Pct.Valid   100.00    100.00   100.00    100.00     100.00\npar(mfrow=(c(1,2)))\nhistage <- with(Pallant_survey3, hist(age))\nhisttmarlow <- with(Pallant_survey3, hist(tmarlow))"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.2 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt kontrollvariabler og uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.3 Lage modell (kjøre regresjonsanalysen)","text":"lager vi tre modeller: Modell 0 inneholder kun Intercept (som en referansemodell). Modell 1 inneholder kun age og tmarlow, modell 2 inneholder tillegg tmast og tpcoiss.","code":"\nmodell0 <- lm(tpstress ~ 1, data = Pallant_survey3)\nmodell1 <- lm(tpstress ~ age + tmarlow, data = Pallant_survey3)\nmodell2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey3)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.4 Analyse av resultatene (diagnostikk)","text":"Den første modellen er modellen med kun alder og social desirability som forklarer 5,9 % av variansen. Den andre modellen, som består av både kontrollvariablene og de tidligere uavhengige variablene forklarer til sammen 47 %.Vi kan oppsummere modellene:Modell 0: \\(SS_Total\\) = 14450 (ingen prediktorer)Modell 1: \\(SS_Residual\\) = 13598.0, \\(SS_Difference\\) = 852.4, \\(F\\)(2, 420) = 23.26, \\(p\\)<.001 (etter å ha lagt til age og tmarlow)Modell 2: \\(SS_Residual\\) = 7658.8, \\(SS_Difference\\) = 5939.2, \\(F\\)(2, 418) = 162.07, \\(p\\)<.001 (etter å ha lagt til tmast og tpcoiss)Det vi jo ønsket å finne ut av var hvor mye våre uavhengige variabler forklarer etter effektene fra de kontrollvariablene er tatt bort. Dette finner vi «R Square Change» - altså hvor mye \\(R^2\\) endrer seg fra modell 1 til modell 2. Vi har tillegg med hvor mye \\(R^2\\) endrer seg fra modell 0 ti lmodell 1.modell 2 er verdien 0.411. Altså – de uavhengige variablene forklarer 41.1 % av variansen den avhengige variabelen etter vi har kontrollert alder og sosial ønskverdighet. Vi kan så se på de uavhengige variablenes unike bidrag (altså bidrag etter interaksjonseffekter er tatt bort).","code":"\nsumm(modell1)\nsumm(modell2)\nanova(modell0, modell1, modell2)\n#> Analysis of Variance Table\n#> \n#> Model 1: tpstress ~ 1\n#> Model 2: tpstress ~ age + tmarlow\n#> Model 3: tpstress ~ age + tmarlow + tmast + tpcoiss\n#>   Res.Df     RSS Df Sum of Sq      F                Pr(>F)\n#> 1    422 14450.3                                          \n#> 2    420 13598.0  2     852.4  23.26       0.0000000002642\n#> 3    418  7658.8  2    5939.2 162.07 < 0.00000000000000022\n#>      \n#> 1    \n#> 2 ***\n#> 3 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nrsq_diff1 <- summary(modell1)$r.squared - summary(modell0)$r.squared\nrsq_diff2 <- summary(modell2)$r.squared - summary(modell1)$r.squared\nrsq_diff1\n#> [1] 0.05898557\nrsq_diff2\n#> [1] 0.4110042"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.4.1 Modellens koeffisienter","text":"Vi ser på tallene “Std.Beta” “Parameter Estimates”. Vi ser alder og social desirability ikke bidrar signifikant seg selv. Beta verdiene tabellen representerer de unike bidragene hver variabel etter overlappende effekter fra de andre variablene er fjernet. tmast bidrar større grad (beta = -0,631) enn pcoiss (beta = -0,160).","code":"\nmodell2x <- ols_regress(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey)\nmodell2x\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.686       RMSE                4.280 \n#> R-Squared               0.470       Coef. Var          16.011 \n#> Adj. R-Squared          0.465       MSE                18.323 \n#> Pred R-Squared          0.455       MAE                 3.277 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                  Sum of                                               \n#>                 Squares         DF    Mean Square      F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression     6791.514          4       1697.879    92.666    0.0000 \n#> Residual       7658.831        418         18.323                     \n#> Total         14450.345        422                                    \n#> ----------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                    \n#> ----------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n#> ----------------------------------------------------------------------------------------\n#> (Intercept)    51.716         1.371                 37.713    0.000    49.020    54.411 \n#>         age    -0.021         0.017       -0.046    -1.201    0.231    -0.054     0.013 \n#>     tmarlow    -0.147         0.111       -0.051    -1.328    0.185    -0.364     0.071 \n#>       tmast    -0.631         0.063       -0.429    -9.997    0.000    -0.756    -0.507 \n#>     tpcoiss    -0.160         0.022       -0.328    -7.311    0.000    -0.203    -0.117 \n#> ----------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.4.2 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 92.67.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\nqf(p=.05, df1=4, df2=418, lower.tail=FALSE)\n#> [1] 2.393283"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.\n#### Variablene er uten målefeilVi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.2 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.3 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.4 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten (vi gjentar ikke den grafiske framstillingen som er lik som forrige analyse).","code":"\nPallantOLS2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey)\nPallantOLS_inter <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss + tmast*tpcoiss, Pallant_survey)\ncompareCoefs(PallantOLS2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss, data = Pallant_survey)\n#> 2: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss + tmast * tpcoiss, data = Pallant_survey)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     51.72   53.80\n#> SE               1.37    4.38\n#>                              \n#> age           -0.0206 -0.0206\n#> SE             0.0171  0.0171\n#>                              \n#> tmarlow        -0.147  -0.148\n#> SE              0.111   0.111\n#>                              \n#> tmast         -0.6314 -0.7324\n#> SE             0.0632  0.2111\n#>                              \n#> tpcoiss       -0.1600 -0.1969\n#> SE             0.0219  0.0768\n#>                              \n#> tmast:tpcoiss         0.00175\n#> SE                    0.00349\n#> "},{"path":"regresjonsanalyse---ols.html","id":"linearitet-3","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.5 Linearitet","text":"Se forrige regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.6 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik (jfr. )En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\nhist(PallantOLS2$residuals)\nols_plot_resid_qq(PallantOLS2)\nols_test_normality(PallantOLS2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9926         0.0345 \n#> Kolmogorov-Smirnov        0.038          0.5743 \n#> Cramer-von Mises         31.3398         0.0000 \n#> Anderson-Darling          0.8705         0.0255 \n#> -----------------------------------------------\nols_plot_resid_fit(PallantOLS2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.7 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.52\\) og \\(r=-0.58\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.52\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\nols_vif_tol(PallantOLS2)\n#>   Variables Tolerance      VIF\n#> 1       age 0.8636921 1.157820\n#> 2   tmarlow 0.8729179 1.145583\n#> 3     tmast 0.6897936 1.449709\n#> 4   tpcoiss 0.6290247 1.589763"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.8 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\nbptest(PallantOLS2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  PallantOLS2\n#> BP = 40.384, df = 5, p-value = 0.0000001249"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.9 Fravær av autokorrelasjon","text":"viser verdien \\(1.817\\) med p < 0.05 noe som indikerer vi har autokorrelasjon.","code":"\ndurbinWatsonTest(lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey))\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08634585      1.817403   0.034\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.10 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\ncar::qqPlot(PallantOLS2, id = list(n=3))#> [1]  22 194 269\ninfluenceIndexPlot(PallantOLS2, vars = \"hat\", id = list(n=3))\nols_plot_dfbetas(PallantOLS2, print_plot = TRUE)\ninfluenceIndexPlot(PallantOLS2, vars = \"Cook\", id = list(n=3))\nmineCDverdier2 <- cooks.distance(PallantOLS2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22      23     268     389     191     413 \n#> 0.05880 0.04913 0.03618 0.02859 0.02728 0.02530"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.5.11 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\nregression.diagnostics(PallantOLS2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/12 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 12 × 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske… global   stud…   7.01     0.135   0.05 No Pro…\n#>  2 heteroske… global   Non-…   1.86     0.173   0.05 No Pro…\n#>  3 multicoll… age      Vari…   1.16    NA       5    No Pro…\n#>  4 multicoll… tmarlow  Vari…   1.15    NA       5    No Pro…\n#>  5 multicoll… tmast    Vari…   1.45    NA       5    No Pro…\n#>  6 multicoll… tpcoiss  Vari…   1.59    NA       5    No Pro…\n#>  7 normality  global   Shap…   0.993    0.0345  0.01 No Pro…\n#>  8 model spe… global   Stat…   0.00772  0.366   0.05 No Pro…\n#>  9 functiona… global   RESE…   0.897    0.409   0.05 No Pro…\n#> 10 outliers   global   Cook…   0.0588  NA       1    No Pro…\n#> 11 outliers   global   Bonf…   3.54     0.190   0.05 No Pro…\n#> 12 autocorre… global   Durb…   0.0863   0.0680  0.05 No Pro…\n#> # … with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.6 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.7 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-1","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.5.8 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:","code":"\ntable1::label(Pallant_survey3$age) <- \"age\"\ntable1::label(Pallant_survey3$tmarlow) <- \"tmarlow\"\ntable1::label(Pallant_survey3$tmast) <- \"tmast\"\ntable1::label(Pallant_survey3$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey3$tpstress) <- \"tpstress\"\ntable1::table1(~age + tmarlow + tmast + tpcoiss + tpstress, data = Pallant_survey3)\nPallantkorr <- tab_corr(Pallant_survey3, triangle = \"lower\")\nPallantkorr\ntab_model(modell0, modell1, modell2)"},{"path":"regresjonsanalyse---ols.html","id":"stegvis-mutlippel-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6 Stegvis mutlippel regresjonsanalyse","text":"Stegvis regresjon er en iterativ multippel regresjonsanalyse der prosessen gradvis (steg steg = stegvis) setter inn eller tar bort uavhengige variabler som ikke bidrar til forklaring av variansen den avhengige variabelen. Man kan f.eks. ha en situasjon der man har et stort antall uavhengige variabler der problemet er å avgjøre hvor mange og hvilke variabler som skal være med regresjonsanalysen. Det finnes ulike strategier og teknikker å velge de uavhengige variablene som til slutt skal gå inn modellen. Målet er å finne den kombinasjonen av uavhengige variabler blant et større antall variabler som best forklarer variansen den avhengige variabelen.“Stepwise selection” kan foretas på ulike måter: forward, backward og bidirectional (toveis).“Forward Selection” (også omtalt som «Step-») innebærer altså man starter med null variabler. Først settes den variabelen med lavest p-verdi F. hvert steg deretter settes den variabelen som ennå ikke er lagt inn modellen med lavest p-verdi F inn. Når ingen av de gjenværende variablene er signifikante stoppes prosessen. Denne prosedyren kan ha sin nytte en analyse av et stort antall «kandidatvariabler» til en regresjonsmodell. Det som gjøres er:Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.“Backward selection” (også omtalt som “Step-” eller “Backward Elimination”) starter motsatt ende av forward. puttes alle de uavhengige variablene inn først. Deretter tar man hver gang bort variabelen som har høyest p-verdi F inntil man ikke har flere ikke-signifikante variabler igjen modellen.\n“Bidirectional” utvelgelse er en kombinasjon av de foregående. Den starter som forward selection, men hver gang en variabel settes inn blir alle variablene modellen kontrollert å se om p-verdien F har endret seg til en definert terskelverdi som følge av den nye variabelen. Hvis en ikke-signifikant variabel da blir funnet fjernes den. Når ingen variabler tilfredsstiller terskelverdiene enten inkludering eller ekskludering modellen stopper prosessen. Dette krever definerte signifikansnivåer: En å inkludere og en å ekskludere, der terskelverdien å inkludere må være lavere enn å ekskludere (med mindre man ønsker en uendelig loop der man aldri finner en løsning).Det skal bemerkes dette er en rent matematisk/statistisk operasjon. Det ligger ingen teoretiske vurderinger til grunn. Man har selvsagt ingen garanti de uavhengige variablene man ender opp med er fornuftige eller har noen praktisk signifikans. Ethvert resultat fra en stegvis regresjon må derfor vurderes kritisk. Man står også fare såvel overfitting som underfitting (jfr. tidligere punkt om overfitting) (Field 2009a). Det mangler ikke på advarsler om å bruke stegvis regresjon, f.eks. fra Miles Shevlin (2001) som advarer om stegvis regresjonsanalyse “used extreme caution” (s.38). Singer Willett (2003) hevder på sin side “Never let computer select predictors mechanically. computer know research questions literature upon rest. distinguish predictors direct substantive interest whose effects want control”. Man skal hvert fall se på resultatene med et veldig kritisk blikk siden “data analyst knows computer” (Henderson Velleman 1981, s.391).Et alvorlig problem med stegvis regresjonsanalyse er man får en \\(R^2\\)-verdi som har svært unøyaktig høy (Miles Shevlin 2001).Dette skyldes statistikkprogrammet tråler gjennom et stort antall uavhengige variabler på søken etter statistisk signifikante variabler og velger ut alle disse. En andel variabler vil være signifikante av tilfeldighet noe som øker verdien på \\(R^2\\).Forutsetningene er de samme som standard multippel regresjon. Spesielt skal man være observant på uteliggere. En tommelfingerregel som angis er minimum 5 caser per variabel (50 variabler = minimum 250 caser).Vi skal se på et eksempel som er hentet fra  (van den Berg 2018).\nDu kan laste ned datasettet ulike formater :Download magazine_reg.xlsxDownload magazine_reg.savDownload magazine_reg.dta","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-stegvis-multippel-regresjonsanalyse","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.1 Eksempel stegvis multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på stegvis multippel regresjonsanalyse, ved å følge stegene 1-4 .Analyse av dataeneAnalyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.2 Analyse av dataene","text":"","code":"\nmagazine_data <- as.data.frame(read_excel(\"magazine_reg.xlsx\"))\nsummarytools::descr(magazine_data, stats = \"common\")\n#> Non-numerical variable(s) ignored: age\n#> Descriptive Statistics  \n#> magazine_data  \n#> N: 637  \n#> \n#>                     educ    filt1   gender     intnr     mis1     prof     sat1     sat2     sat3\n#> --------------- -------- -------- -------- --------- -------- -------- -------- -------- --------\n#>            Mean     5.11     0.99     1.13   1267.38     0.31     2.31     4.11     4.37     3.77\n#>         Std.Dev     1.13     0.10     0.34    712.79     0.76     0.72     0.88     0.75     0.95\n#>             Min     1.00     0.00     1.00     46.00     0.00     1.00     1.00     2.00     1.00\n#>          Median     5.00     1.00     1.00   1259.00     0.00     2.00     4.00     4.00     4.00\n#>             Max     6.00     1.00     2.00   2497.00     6.00     5.00     6.00     6.00     6.00\n#>         N.Valid   637.00   637.00   637.00    637.00   637.00   637.00   637.00   637.00   637.00\n#>       Pct.Valid   100.00   100.00   100.00    100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                     sat4     sat5     sat6     sat7     sat8     sat9    satov   whours\n#> --------------- -------- -------- -------- -------- -------- -------- -------- --------\n#>            Mean     4.00     4.67     4.41     4.41     3.91     3.97     7.49     4.33\n#>         Std.Dev     1.06     0.59     0.91     0.85     0.92     0.99     0.77     0.65\n#>             Min     1.00     2.00     1.00     1.00     2.00     1.00     5.00     1.00\n#>          Median     4.00     5.00     5.00     5.00     4.00     4.00     8.00     4.00\n#>             Max     6.00     6.00     6.00     5.00     6.00     6.00    10.00     5.00\n#>         N.Valid   637.00   637.00   637.00   637.00   637.00   637.00   637.00   477.00\n#>       Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00    74.88\npar(mfrow=(c(2,3)))\nhisttsat1 <- with(magazine_data, hist(sat1))\nhisttsat2 <- with(magazine_data, hist(sat2))\nhisttsat3 <- with(magazine_data, hist(sat3))\nhisttsat4 <- with(magazine_data, hist(sat4))\nhisttsat5 <- with(magazine_data, hist(sat5))\nhisttsat6 <- with(magazine_data, hist(sat6))\nhisttsat7 <- with(magazine_data, hist(sat7))\nhisttsat8 <- with(magazine_data, hist(sat8))\nhisttsat9 <- with(magazine_data, hist(sat9))"},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.3 Lage modell (kjøre regresjonsanalysen)","text":"Hvilke av de 9 uavhengige variablene (sat1…sat9), som er ulike mål på kunders oppfatning av produktet, bidrar signifikant til å forklare variansen hvordan kundene totalt sett skårer produktet (magasinet) («rate magazine altogether?»). De ulike målene er f.eks. grundighet, objektivitet, lesbarhet og pålitelighet. Spørsmålet er hvilke aspekter (sat1…sat9) har størst innflytelse på kundetilfredsheten?","code":"\nmagazine_intercept <- lm(satov ~ 1, data = magazine_data)\nmagazine_all <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9, data = magazine_data)\nmagazine_forward <- step(magazine_intercept, direction = \"forward\", scope = formula(magazine_all), trace = 0)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---forward","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.4 Analyse av resultatene (diagnostikk) - forward","text":"Første modell er kun intercept. Vi ser på AIC verdiene tabellen . AIC er forkortelse Akaike Information Criterion som er en estimator prediksjonsfeil og brukes som et mål på relativ kvalitet statistiske modeller (hvor godt modellen passer til dataene modellen ble laget av - derfor er AIC egnet til å sammenlikne ulike mulige modeller fra det samme datasettet).skal vi legge merke til fortegnet til AIC verdien er irrelevant - vi ser på absoluttverdien, der lavere AIC verdi er bedre enn høyere AIC verdi.\nNeste steg er alle mulige modeller med en prediktor testes, og prediktoren som produserer den laveste AIC verdien inkluderes modellen. Det er vårt tilfelle sat1. Neste steg er å teste alle modeller/kombinasjoner med prediktorer, hvor den ene er sat1. Den neste prediktoren som inkluderes er da sat3 fordi det er prediktoren som gir lavest AIC av alle mulige kombinasjoner av modeller med prediktorer. Neste steg innebærer å teste alle modeller/kombinasjoner med tre prediktorer, der den første er sat1 og den andre er sat3. Da blir sat5 lagt til, osv.Modellen blir med denne prosedyren:R-brukere kan vi kjøre hele analysen en linje med pakken olsrr:","code":"\nmagazine_forward$anova\n#>     Step Df   Deviance Resid. Df Resid. Dev       AIC\n#> 1        NA         NA       636   381.2308 -325.0134\n#> 2 + sat1 -1 71.7435240       635   309.4872 -455.8202\n#> 3 + sat3 -1 33.8817998       634   275.6054 -527.6782\n#> 4 + sat5 -1 20.9219246       633   254.6835 -575.9685\n#> 5 + sat7 -1  9.7484194       632   244.9351 -598.8295\n#> 6 + sat9 -1  6.5282509       631   238.4069 -614.0379\n#> 7 + sat2 -1  2.2298960       630   236.1770 -618.0240\n#> 8 + sat6 -1  0.8045547       629   235.3724 -618.1977\n#> 9 + sat4 -1  1.0320596       628   234.3403 -618.9969\nmagazine_forward$coefficients\n#> (Intercept)        sat1        sat3        sat5        sat7 \n#>  3.78591494  0.17370968  0.17786494  0.19804309  0.14463696 \n#>        sat9        sat2        sat6        sat4 \n#>  0.10736930  0.08912079 -0.05363069  0.04538282\nols_step_forward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Forward Selection Method \n#> ------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered: \n#> \n#> - sat1 \n#> - sat3 \n#> - sat5 \n#> - sat7 \n#> - sat9 \n#> - sat2 \n#> - sat6 \n#> - sat4 \n#> \n#> No more variables to be added.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#>                          Selection Summary                          \n#> -------------------------------------------------------------------\n#> Variable       AIC       Sum Sq       RSS       R-Sq      Adj. R-Sq \n#> -------------------------------------------------------------------\n#> sat1         1353.907     71.744    309.487    0.18819      0.18691 \n#> sat3         1282.050    105.625    275.605    0.27706      0.27478 \n#> sat5         1233.759    126.547    254.684    0.33194      0.32878 \n#> sat7         1210.898    136.296    244.935    0.35751      0.35345 \n#> sat9         1195.690    142.824    238.407    0.37464      0.36968 \n#> sat2         1191.704    145.054    236.177    0.38049      0.37459 \n#> sat6         1191.530    145.858    235.372    0.38260      0.37573 \n#> sat4         1190.731    146.890    234.340    0.38531      0.37748 \n#> -------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---backward","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.5 Analyse av resultatene (diagnostikk) - backward","text":"","code":"\nols_step_backward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Backward Elimination Method \n#> ---------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Removed: \n#> \n#> - sat8 \n#> \n#> No more variables to be removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                      Backward Elimination Summary                     \n#> --------------------------------------------------------------------\n#> Variable        AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> --------------------------------------------------------------------\n#> Full Model    1191.783    233.992    147.239    0.38622      0.37741 \n#> sat8          1190.731    234.340    146.890    0.38531      0.37748 \n#> --------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---bidirectional","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.6 Analyse av resultatene (diagnostikk) - bidirectional","text":"Vi kan se modellene gir likt resultat alle tre måtene, men det trenger absolutt ikke hende. Vi kan få ulike modeller ved de tre metodene. Husk inkludering/eksludering av variabler skjer rent matematisk.","code":"\nols_step_both_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Stepwise Selection Method \n#> -------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered/Removed: \n#> \n#> - sat1 added \n#> - sat3 added \n#> - sat5 added \n#> - sat7 added \n#> - sat9 added \n#> - sat2 added \n#> - sat6 added \n#> - sat4 added \n#> \n#> No more variables to be added or removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                                 Stepwise Summary                                \n#> ------------------------------------------------------------------------------\n#> Variable     Method       AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> ------------------------------------------------------------------------------\n#> sat1        addition    1353.907    309.487     71.744    0.18819      0.18691 \n#> sat3        addition    1282.050    275.605    105.625    0.27706      0.27478 \n#> sat5        addition    1233.759    254.684    126.547    0.33194      0.32878 \n#> sat7        addition    1210.898    244.935    136.296    0.35751      0.35345 \n#> sat9        addition    1195.690    238.407    142.824    0.37464      0.36968 \n#> sat2        addition    1191.704    236.177    145.054    0.38049      0.37459 \n#> sat6        addition    1191.530    235.372    145.858    0.38260      0.37573 \n#> sat4        addition    1190.731    234.340    146.890    0.38531      0.37748 \n#> ------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-3","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.6.1 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 49.206.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\nqf(p=.05, df1=8, df2=628, lower.tail=FALSE)\n#> [1] 1.953131"},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-2","chapter":"Kapittel 9 Regresjonsanalyse - OLS","heading":"9.6.7 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*","code":"\ntable1::label(magazine_data$sat1) <- \"sat1\"\ntable1::label(magazine_data$sat2) <- \"sat2\"\ntable1::label(magazine_data$sat3) <- \"sat3\"\ntable1::label(magazine_data$sat4) <- \"sat4\"\ntable1::label(magazine_data$sat5) <- \"sat5\"\ntable1::label(magazine_data$sat6) <- \"sat6\"\ntable1::label(magazine_data$sat7) <- \"sat7\"\ntable1::label(magazine_data$sat8) <- \"sat8\"\ntable1::label(magazine_data$sat9) <- \"sat9\"\ntable1::label(magazine_data$satov) <- \"satov\"\ntable1::table1(~sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9 + satov, data = magazine_data)\nmagazine_data2 <- magazine_data[, c(\"sat1\", \"sat2\", \"sat3\", \"sat4\", \"sat5\", \"sat6\", \"sat7\", \"sat9\", \"satov\")]\nmagazinekorr <- tab_corr(magazine_data2, triangle = \"lower\")\nmagazinekorr\nmagazine_finalmod <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat9, data = magazine_data)\ntab_model(magazine_finalmod)"},{"path":"iv-regresjon.html","id":"iv-regresjon","chapter":"Kapittel 10 IV regresjon","heading":"Kapittel 10 IV regresjon","text":"R-pakker brukt dette kapittelet:","code":"\npacman::p_load(haven, tidyverse, readxl, AER, lmtest, jtools, summarytools)"},{"path":"iv-regresjon.html","id":"innledning-1","chapter":"Kapittel 10 IV regresjon","heading":"10.1 Innledning","text":"Multippel OLS-regresjon av tverrsnittsdata kan kalles en tradisjonell empirisk metode å se på sammenhengen mellom en uavhengig variabel og en eller flere avhengige variabler. regresjonen finner vi den gjennomsnittlige endringen en uavhengig variabel - Y - når en avhengig variabel X endres med en enhet, og vi samtidig holder alle andre variabler konstante. Dette (koeffisienten X) er en betinget korrelasjon mellom X og Y, men det er alltid en usikkerhet rundt hvilken grad dette er et godt estimat den kausale sammenhengen. Vi vil sannsynligvis kunne si vi ikke vet om vi har utelatt variabler som påvirker både X og Y. Det kan også være sånn ikke bare påvirker X -> Y, en kanskje også Y -> X. En mulighet “forkludring” av estimatet kausalitet kan også være vi ikke helt forstår kontrollvariablers påvirkning på X, slik deler av X’s effekt på Y forsvinner. Selv om kontrollvariabler kan klargjøre X’s effekt på Y og dermed variasjonen Y kan det tåkelegge den kausale effekten mellom variablene. Vi kjenner til fra kapittelet om OLS-regresjon det alltid er et feilledd regresjonslikningen. Feil, som beskrevet ovenfor, kan føre til en uavhenigig variabel korrelerer med feilleddet og dermed påvirker kausaleffekten. IV-regresjon (Instrumentell Variabel regresjon) kan ses på som et kvasi-eksperiment (se f.eks. Galiani et al. (2017), Andriano Monden (2019) og DiPrete, Burik, Koellinger (2018)).En definisjon av IV er “use additional ‘instrumental’ variables, contained equation interest, estimate unknown parameters equation” (Stock Trebbi 2003, s.179). La oss tenke oss vi har en avhengig variabel Y - inntekt - som påvirkes av sosioøkonomisk status - X. Sosioøkonomisk status påvirker altså inntekten (X -> Y), men samtidig kan inntekt påvirke sosioøkonomisk status (Y -> X). En såkalt instrumentvariabel - Z - som må korrelere sterkt med X, men ikke med Y på andre måter enn gjennom X kan gi oss en omveg rundt problemet med retning på kausaliteten. Å finne en god instrumentvariabel er derimot krevende og vil trolig forutsette svært god kunnskap om tematikken som undersøkes.Oppsummert blir da en IV-regresjon man først gjennomfører en regresjon der X-verdiene predikeres av Z. Deretter gjennomfører man en regresjon med de predikerte X-verdiene mot Y. IV regresjon kalles derfor også ofte “two-stage least squares”.Det hviler forutsetninger dette: Z må være korrelert med X (det såkalte relevanskriteriet) og Z kun påvirker Y gjennom X - altså ingen direkte påvirkning fra Z til Y (det såkalte eksluderingskriteriet).","code":""},{"path":"iv-regresjon.html","id":"et-konstruert-eksempel-for-konseptualisering-av-iv-regresjon","chapter":"Kapittel 10 IV regresjon","heading":"10.2 Et konstruert eksempel for konseptualisering av IV-regresjon","text":"La oss tenke oss vi ønsker å undersøke sammenhengen mellom lengde på utdanning og inntekt. Er det slik et år ekstra utdanning gir deg x mer inntekt, eller x+…? mer inntekt? Dette eksempelet er basert på Masten (2015).La oss videre tenke oss vi har data:Vi ser det er en klar trend disse dataene folk med lengre utdanning har høyere inntekt enn folk med kortere utdanning. Betyr det lengde på utdanning har en kausal effekt på inntekt? Svaret er nei. Fordi lengde på utdanning er ikke tilfeldig (“randomly assigned”) - folk velger hvor lang utdannelse de vil ta. Likevel ser vi jo en klar trend/sammenheng dataene. Så hvorfor gjør vi det?En forklaring kan være denne sammenhengen skyldes en uobservert variabel som “forstyrrer” bildet. La oss videre eksempelet si dette er IQ, og vi har data på dette.Lengde på utdanning ser ut til å henge sammen med IQ.Men også inntekt ser ut til å henge sammen med IQ, uavhengig av hvor mye utdannelse de har. virkeligheten vil vi kanskje ikke se dette - vi har data på lengde på utdannelse og inntekt (vi har generert dataene nettopp å illustrere). Men virkeligheten kan korrelasjonen vi kanskje faktisk ser og som vi viste første diagrammet - korrelasjonen mellom lengde på utdanning og inntekt - altså være drevet av en underliggende, uobservert variabel og IKKE av en kausal sammenheng mellom lengde på utdanning og inntekt.å mitigere dette problemet kommer instrumentet inn bildet. eksempelet fra Masten (2015) vises det til avstand fra hjemadressen til nærmeste studiested (heretter: avstand) kan være et slikt instrument. Vi bygger videre på samme måte.Et instrument må altså kunne påvirke Y gjennom X (og ikke direkte), og være korrelert med X. vårt eksempel betyr det avstand må påvirke inntekt gjennom lengde på utdannelse, og avstand må være korrelert med lengde på utdannelse.Vi må derfor kunne se på X, Y og Z slik:Lav avstand har sammenheng med lengre utdannelse. Forhold som trekkes fram denne sammenhengen kan være større kjennskap til universitetet, lavere kostnad bolig og reise, større påvirkning av eldre studenter nærmiljøet gjennom oppveksten (og sikkert flere) - som sum antyder det kan være en kausal sammenheng mellom avstand og om folk studerer og hvor lenge. Altså en korrelasjon mellom Z og X (som vi sa var en forutsetning).Sammenhengen mellom IQ og avstand er imidlertid tilfeldig. Vi antar det ikke er noen grunn til IQ har sammenheng med avstand. Folk er født “og der” med ulik IQ. Vi ser bort fra det kanskje faktisk kan være en sammenheng… ved universitets ansatte bor nærheten av iuniversitete, og om IQ er arvelig kan man kanskje tenke seg det bor flere unge med høyere IQ nærheten av nuviersitetet enn lenger unna - vi går ikke inn denne “problemstillingen” , men legger til grunn dataene våre viser det ikke er slik. Om det faktisk er sånn det er en sammenheng vil det gjøre det vanskelig å bruke avstand som et instrument.Den siste forutsetningen er avstand ikke kan ha en direkte kausal effekt på inntekt. Dette virker rimelig, da det er vanskelig å se arbeidsgivere har noe forhold til hvor langt unna et unviersitet folk har vokst opp, og avstand dermed skulle kunne ha en kausal effekt på inntekt.Vi kan si instrumentet “avstand” tilfredsstiller forutsetningene, og vi kan se på korrelasjonen mellom avstand og inntekt.Vi ser avstand og inntekt korrelerer. Vi kan dermed si korrelasjonen mellom avstand og inntekt representerer en kausal effekt av lengde på utdanning og inntekt. Vi ser en kausal effekt mellom X og Y gjennom korrelasjonen mellom Z og Y.","code":"#> Descriptive Statistics  \n#> IVregrdata1  \n#> Label: jamovi data set  \n#> N: 161  \n#> \n#>                      Inntekt   Utdanning\n#> --------------- ------------ -----------\n#>            Mean    795578.74       15.20\n#>         Std.Dev    153427.64        1.52\n#>             Min    501846.67       12.02\n#>          Median    801972.01       15.24\n#>             Max   1049757.30       17.89\n#>         N.Valid       161.00      161.00\n#>       Pct.Valid       100.00      100.00#> Descriptive Statistics  \n#> IVregrdata2  \n#> N: 161  \n#> \n#>                       IQ   Utdanning\n#> --------------- -------- -----------\n#>            Mean   119.27       15.20\n#>         Std.Dev    11.25        1.52\n#>             Min    90.01       12.02\n#>          Median   122.55       15.24\n#>             Max   134.95       17.89\n#>         N.Valid   161.00      161.00\n#>       Pct.Valid   100.00      100.00"},{"path":"iv-regresjon.html","id":"eksempel-1-med-data-fra-pakken-aer","chapter":"Kapittel 10 IV regresjon","heading":"10.3 Eksempel 1 med data fra pakken “AER”","text":"datasettet har vi en rekke variabler:Vi skal se på sammenhengen mellom “packs” (= y -> antall sigarettpakker per capita)som avhengig variabel, “price” (= x -> gjennomsnittlig pris pr år inkl skatter og avgifter) som uavhengig variabel, og “taxs” (= z -> gjennomsnittlig skatt pr år) som instrumentvariabel.Forutsetningene er altså:z og x må være korrelerte: pris og skatte-/avgiftsnivå korrelerer virker rimeligz må ikke påvirke y direkte: skatte-/avgiftsnivået ikke seg selv påvirker antall sigarettpakker som selges direkte virker rimelig, men skatte-/avgiftsnivået påvirker antall sigarettpakker som selges gjennom prisnivåt virker også rimelig (dette er en forutsetning som alltid kan diskuteres, men eksempelets skyld antar vi dette).","code":"\ndata(\"CigarettesSW\")\nsummarytools::descr(CigarettesSW, stats = \"common\")\n#> Non-numerical variable(s) ignored: state, year\n#> Descriptive Statistics  \n#> CigarettesSW  \n#> N: 96  \n#> \n#>                      cpi         income    packs    population    price      tax     taxs\n#> --------------- -------- -------------- -------- ------------- -------- -------- --------\n#>            Mean     1.30    99878735.74   109.18    5168866.32   143.45    42.68    48.33\n#>         Std.Dev     0.23   120541138.18    25.87    5442344.66    43.89    16.14    19.33\n#>             Min     1.08     6887097.00    49.27     478447.00    84.97    18.00    21.27\n#>          Median     1.30    61661644.00   110.16    3697471.50   137.72    37.00    41.05\n#>             Max     1.52   771470144.00   197.99   31493524.00   240.85    99.00   112.63\n#>         N.Valid    96.00          96.00    96.00         96.00    96.00    96.00    96.00\n#>       Pct.Valid   100.00         100.00   100.00        100.00   100.00   100.00   100.00\ncor(CigarettesSW$price, CigarettesSW$taxs)\n#> [1] 0.9203278"},{"path":"iv-regresjon.html","id":"modell-1","chapter":"Kapittel 10 IV regresjon","heading":"10.3.1 Modell 1","text":"Vi ser på koeffisientene log(price) er signifikant, og pris påvirker salget (antall sigarettpakker) negativt (negativ koeffisient).","code":"\nmodel1 <- ivreg(log(packs) ~ log(price) | taxs, data = CigarettesSW)\nsummary(model1)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) | taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.597468 -0.105477 -0.008609  0.101021  0.525352 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value\n#> (Intercept)  7.66548    0.34514  22.210\n#> log(price)  -0.60993    0.07004  -8.708\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)     0.000000000000103 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1845 on 94 degrees of freedom\n#> Multiple R-Squared: 0.4324,  Adjusted R-squared: 0.4264 \n#> Wald test: 75.83 on 1 and 94 DF,  p-value: 0.0000000000001025"},{"path":"iv-regresjon.html","id":"modell-2","chapter":"Kapittel 10 IV regresjon","heading":"10.3.2 Modell 2","text":"legger vi inn en eksogen variabel - inntekt - altså en variabel hvis verdi er bestemt utenfor modellen så å legges inn modellen. Pris ser vi på som en endogen variabel, altså en variabel hvis verdi bestemmes modellen. En tilfeldig endogen variabel en modell korrelerer med feilleddet (som vi forsåvidt har beskrevet begynnelsen av dette kapittelet), mens en eksogen variabel ikke er korrelert med feilleddet (hvilket er naturlig siden vi sier verdien er bestemt utenfor modellen).Vi kan legge merke til log(income) ikke er signifikant.","code":"\nmodel2 <- ivreg(log(packs) ~ log(price) + log(income) | log(income) + taxs, data = CigarettesSW)\nsummary(model2)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) + log(income) | log(income) + \n#>     taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.599938 -0.104454 -0.007223  0.101973  0.525855 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value\n#> (Intercept)  7.698702   0.399560  19.268\n#> log(price)  -0.605735   0.075878  -7.983\n#> log(income) -0.003015   0.018990  -0.159\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)      0.00000000000368 ***\n#> log(income)                0.874    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1853 on 93 degrees of freedom\n#> Multiple R-Squared: 0.4335,  Adjusted R-squared: 0.4214 \n#> Wald test: 37.67 on 2 and 93 DF,  p-value: 0.000000000001038"},{"path":"iv-regresjon.html","id":"eksempel-2","chapter":"Kapittel 10 IV regresjon","heading":"10.4 Eksempel 2","text":"Data dette eksempelet er hentet fra Becker Lincoln (2021) (“Mroz.csv”).","code":"\nMrozdata <- read.csv(file = 'Mroz.csv', na.strings = \".\")\nMrozdata <- subset(Mrozdata, is.na(wage) == FALSE) \nhead(Mrozdata)\n#>   inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs\n#> 1    1  1610       1       0  32   12 3.3540    2.65   2708\n#> 2    1  1656       0       2  30   12 1.3889    2.65   2310\n#> 3    1  1980       1       3  35   12 4.5455    4.04   3072\n#> 4    1   456       0       3  34   12 1.0965    3.25   1920\n#> 5    1  1568       1       2  31   14 4.5918    3.60   2000\n#> 6    1  2032       0       0  54   12 4.7421    4.70   1040\n#>   husage huseduc huswage faminc    mtr motheduc fatheduc\n#> 1     34      12  4.0288  16310 0.7215       12        7\n#> 2     30       9  8.4416  21800 0.6615        7        7\n#> 3     40      12  3.5807  21040 0.6915       12        7\n#> 4     53      10  3.5417   7300 0.7815        7        7\n#> 5     32      12 10.0000  27300 0.6215       12       14\n#> 6     57      11  6.7106  19495 0.6915       14        7\n#>   unem city exper  nwifeinc     lwage expersq\n#> 1  5.0    0    14 10.910060 1.2101540     196\n#> 2 11.0    1     5 19.499980 0.3285121      25\n#> 3  5.0    0    15 12.039910 1.5141380     225\n#> 4  5.0    0     6  6.799996 0.0921233      36\n#> 5  9.5    1     7 20.100060 1.5242720      49\n#> 6  7.5    1    33  9.859054 1.5564800    1089"},{"path":"iv-regresjon.html","id":"modell-1-standard-ols","chapter":"Kapittel 10 IV regresjon","heading":"10.4.1 Modell 1: Standard OLS","text":"Vi ønsker å kjøre en regresjon med kvinners lønn (lwage) som avhengig variabel og utdanning (educ) som uavhengig variabel. Eksempelet tar utgangspunkt Wooldridge (2016) (se kap. 15).Vi ser vi får et statistisk signifikant resultat og et års lengre utdanning øker lønn med nesten 11 % (Esitmate = 0.1086). Vi kan imidlertid forvente utdanning korrelerer med mange individuelle karakteristika hos et individ som er viktige den personens lønn, men som ikke fanges opp av utdanningsvariabelen og derfor ikke fanges opp denne modellen. dette tilfellet, og å følge eksempelet Wooldridge (2016) velger vi fars utdanning (fatheduc) som instrument (vi diskuterer ikke instrumentet og forutsetningene ).","code":"\nIVregmod1 <- lm(lwage ~ educ, data = Mrozdata)\nsummary(IVregmod1)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.10256 -0.31473  0.06434  0.40081  2.10029 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept)  -0.1852     0.1852  -1.000             0.318\n#> educ          0.1086     0.0144   7.545 0.000000000000276\n#>                \n#> (Intercept)    \n#> educ        ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.68 on 426 degrees of freedom\n#> Multiple R-squared:  0.1179, Adjusted R-squared:  0.1158 \n#> F-statistic: 56.93 on 1 and 426 DF,  p-value: 0.0000000000002761"},{"path":"iv-regresjon.html","id":"modell-2-iv-regresjon","chapter":"Kapittel 10 IV regresjon","heading":"10.4.2 Modell 2: IV regresjon","text":"Vi kan se “verdien” av et års ekstra utdanning nå kun er 5.9% økning, og signifikant p < 0.1.","code":"\nIVregmod2 <- ivreg(lwage ~ educ | fatheduc, data = Mrozdata)\nprint(summary(IVregmod2))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ | fatheduc, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0870 -0.3393  0.0525  0.4042  2.0677 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.44110    0.44610   0.989   0.3233  \n#> educ         0.05917    0.03514   1.684   0.0929 .\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6894 on 426 degrees of freedom\n#> Multiple R-Squared: 0.09344, Adjusted R-squared: 0.09131 \n#> Wald test: 2.835 on 1 and 426 DF,  p-value: 0.09294"},{"path":"iv-regresjon.html","id":"modell-3-ols-med-flere-prediktorer","chapter":"Kapittel 10 IV regresjon","heading":"10.4.3 Modell 3: OLS med flere prediktorer","text":"Estiemrt koeffisient educ er nå 0.1075 med standardfeil 0.0141","code":"\nIVregmod3 <- lm(lwage ~ educ + age + exper + expersq, data = Mrozdata)\nsummary(IVregmod3)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ + age + exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.08521 -0.30587  0.04946  0.37523  2.37077 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept) -0.5333762  0.2778430  -1.920           0.05557\n#> educ         0.1075228  0.0141745   7.586 0.000000000000212\n#> age          0.0002836  0.0048553   0.058           0.95344\n#> exper        0.0415623  0.0131909   3.151           0.00174\n#> expersq     -0.0008152  0.0003996  -2.040           0.04195\n#>                \n#> (Intercept) .  \n#> educ        ***\n#> age            \n#> exper       ** \n#> expersq     *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6672 on 423 degrees of freedom\n#> Multiple R-squared:  0.1568, Adjusted R-squared:  0.1489 \n#> F-statistic: 19.67 on 4 and 423 DF,  p-value: 0.000000000000007328"},{"path":"iv-regresjon.html","id":"modell-4-iv-regresjon-med-flere-to-instrumenter","chapter":"Kapittel 10 IV regresjon","heading":"10.4.4 Modell 4: IV regresjon med flere (to) instrumenter","text":"Vi legger til både fars og mors utdannelse som instrumenter:fatheduc og motheduc er instrumentene, exper og expersq er eksogene uavhengige variabler.Også ser vi en vesentlig reduksjon estimatet educ fra OLS til IV regresjon; fra 0.1075 med standardfeil 0.0141 til 0.0613 med standardfeil 0.0314. IV estiamtene vil ha større standardfeil enn OLS estimater.","code":"\nIVregmod4 <- ivreg(lwage ~ educ  +exper + expersq | fatheduc + motheduc + exper + expersq, data = Mrozdata)\nprint(summary(IVregmod4))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ + exper + expersq | fatheduc + motheduc + \n#>     exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0986 -0.3196  0.0551  0.3689  2.3493 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  0.0481003  0.4003281   0.120  0.90442   \n#> educ         0.0613966  0.0314367   1.953  0.05147 . \n#> exper        0.0441704  0.0134325   3.288  0.00109 **\n#> expersq     -0.0008990  0.0004017  -2.238  0.02574 * \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6747 on 424 degrees of freedom\n#> Multiple R-Squared: 0.1357,  Adjusted R-squared: 0.1296 \n#> Wald test: 8.141 on 3 and 424 DF,  p-value: 0.00002787"},{"path":"iv-regresjon.html","id":"test-av-relevansen-av-instrumentene","chapter":"Kapittel 10 IV regresjon","heading":"10.4.5 Test av relevansen av instrumentene","text":"Det er selvsagt avgjørende instrumentet/-ene er relevante. Som beskrevet må instrumentene være tilstrekkelig sterkt korrelerte med den/de endogene uavhengige variablene. dette eksempelet (modell 4) må altså fatheduc og motheduc forklare en tilstrekkelig del av variasjonen educ å kunne være relevant instrumenter. Dette kan vi gjøre gjennom en vanlig F-test:F-verdien er 54.943 med p < .001. Vi kan finne kritisk F-verdi:Vi kan klart forkaste nullhypotesen om instrumentene er irrelevante.","code":"\nsteg1 <- lm(educ ~ age + exper + expersq + fatheduc + motheduc, data = Mrozdata)\n\ninstrumentF <- waldtest(steg1, .~. -fatheduc -motheduc)\ninstrumentF\n#> Wald test\n#> \n#> Model 1: educ ~ age + exper + expersq + fatheduc + motheduc\n#> Model 2: educ ~ age + exper + expersq\n#>   Res.Df Df      F                Pr(>F)    \n#> 1    422                                    \n#> 2    424 -2 54.943 < 0.00000000000000022 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nqf(0.05, df1 = 422, df2 = 424)\n#> [1] 0.8520168"},{"path":"polynomisk-regresjon.html","id":"polynomisk-regresjon","chapter":"Kapittel 11 Polynomisk regresjon","heading":"Kapittel 11 Polynomisk regresjon","text":"R-pakker brukt dette kapittelet:Anta vi har lønnsdata fra en bedrift som har regnet ut snittlønn de 10 stillingskategoriene de har av ansatte.Download polydata.xlsxVi kan se på disse dataene og se en klar ikke-lineær trend:Vi kunne kanskje ane dette ut fra dataene (og fordi det selvsagt er et eksempel der vanlig lineær regresjon ikke skal funke så bra…). Det synes hvert fall klart den lineære regresjonslinja sannsynligvis er en dårlig modell lønnsdataene.Fra kapittelet om enkel, lineær regresjon vet vi vi kan uttrykke regresjonslinja slik:\\(y=\\alpha + \\beta x\\)Vi kan videre uttrykke en multippel regresjon slik:\\(y=\\alpha + \\beta_1x_1 + \\beta_2x_2 +\\ ...\\ +\\beta_nx_n\\)En polynomisk regresjonslikning likner, men introduserer vi “n-te ledd”:\\(y=\\alpha + \\beta_1x_1 + \\beta_2x_1^2+\\ ...\\ +\\beta_nx_1^n\\)Selv om dette sikkert er kjent er det greit å illustrere ulike n’te-ledd grafisk:Imidlertid er vi sjeldent interessert negative x-verdier praktisk bruk (matematisk er det selvsagt interessant, men som sagt sjeldent anvendt analyse som vi driver med).Vi skal nedenfor vise hvordan vi kan utvikle polynomisk regresjon, men det er på sin plass å komme med en liten advarsel. Vi kan, som nevnt, prinsippet ha så mange med så mange ledd vi vil, og muligens vil vi få bedre og bedre “fit” til dataene jo flere vi legger til. Det betyr imidlertid ikke modellen blir bedre og bedre til å predikere framtidige verdier. Vi kan få det vi kaller overtilpasning (“overfitting”), dvs modellen vår passer helt utmerket til dataene våre, men modellen ikke vil klare å predikere nye verdier med særlig god treffsikkerhet. Det er alltid en avveining mellom å få en modell som er godt tilpasset dataene, predikativ evne og modellens kompleksitet/enkelhet (alt annet likt foretrekker vi som regel enklere modeller framfor komplekse modeller). Vi skal altså alltid være bevisst på hva vi gjør polynomisk regresjon (som med alt annet selvsagt…).Et annet punkt, som forsåvidt også gjelder andre modeller, er vi skal være klar hvilket område - x-verdier - modellen kan sies å være gyldig. Som vi ser av grafene ovenfor er det tildels store forskjeller hva modellene vil predikere, og valg av modell må ses sammenheng med det området vi har observasjoner og hvilket område vi ønsker å predikere . Vi skal være ekstra observante når vi tenker på å gjøre prediksjoner områder vi ikke har modellert, eller områder vi ikke har hatt data på når vi har modellert.","code":"\npacman::p_load(tidyverse, readxl)\npolydata <- read_excel(\"polydata.xlsx\")\nglimpse(polydata)\n#> Rows: 10\n#> Columns: 2\n#> $ Stillingskategori <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n#> $ Snittlonn         <dbl> 35000, 41000, 48000, 63000, 8800…\nenkelOLS <- lm(Snittlonn ~ Stillingskategori, data = polydata)\nggplot() +\n    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn), col = \"red\") +\n    theme_bw() +\n    xlab(\"Stillingskategori\") +\n    ylab(\"Gjennomsnittslonn\") +\n    labs(caption = \"Ikke-linear forhold mellom stillingskategori og snittlonn\")\nfunksjonsgrafer <- ggplot(data.frame(x = c(-5, 5)), aes(x = x)) + \n            stat_function(fun = function(x){x^4}, color=\"red\", lwd = 1) +        \n            stat_function(fun = function(x){x^3}, color=\"blue\", lwd = 1) + \n            stat_function(fun = function(x){4*x^2}, color=\"orange\", lwd = 1) +\n            stat_function(fun = function(x){10*x}, color=\"purple\", lwd = 1) +\n            annotate(geom=\"text\", label = \"Quartic\", x = -4, y = 200, size = 5) + \n            annotate(geom=\"text\", label = \"Quadratic\", x = -4.6, y = 107, size =5) +\n            annotate(geom=\"text\", label = \"Cubic\", x = -3.8, y = -90, size =5) + \n            annotate(geom=\"text\", label = \"Linear\", x = -4.6, y = -32, size =5) +\n            geom_hline(yintercept=0) +\n            geom_vline(xintercept=0) +\n            ylim(-100, 200)\nfunksjonsgrafer\nfunksjonsgrafer2 <- ggplot(data.frame(x = c(0, 5)), aes(x = x)) + \n            stat_function(fun = function(x){x^4}, color=\"red\", lwd = 1) +        \n            stat_function(fun = function(x){x^3}, color=\"blue\", lwd = 1) + \n            stat_function(fun = function(x){4*x^2}, color=\"orange\", lwd = 1) +\n            stat_function(fun = function(x){10*x}, color=\"purple\", lwd = 1) +\n            annotate(geom=\"text\", label = \"Quartic\", x = 4.2, y = 200, size = 5) + \n            annotate(geom=\"text\", label = \"Quadratic\", x = 4.8, y = 75, size =5) +\n            annotate(geom=\"text\", label = \"Cubic\", x = 4.5, y = 120, size =5) + \n            annotate(geom=\"text\", label = \"Linear\", x = 4.8, y = 40, size =5) +\n            geom_hline(yintercept=0) +\n            geom_vline(xintercept=0) +\n            ylim(-100, 200) +\n            xlim(0, 5)\n\nfunksjonsgrafer2"},{"path":"polynomisk-regresjon.html","id":"modellering-av-andregrads-quadratic-polynomisk-regresjon","chapter":"Kapittel 11 Polynomisk regresjon","heading":"11.1 Modellering av andregrads (“quadratic”) polynomisk regresjon","text":"Vi har altså laget en kvadrert variabel som heter Stillingskategori2. Lager så polynomial modell med ^2 ledd:","code":"\npolydata$Stillingskategori2 <- polydata$Stillingskategori^2\nglimpse(polydata)\n#> Rows: 10\n#> Columns: 3\n#> $ Stillingskategori  <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n#> $ Snittlonn          <dbl> 35000, 41000, 48000, 63000, 880…\n#> $ Stillingskategori2 <dbl> 1, 4, 9, 16, 25, 36, 49, 64, 81…\npoly_reg2 <- lm(Snittlonn ~ ., data = polydata)\nggplot() +\n    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn), col = \"red\") +\n    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg2, newdata = polydata)), col = \"blue\") +\n    ggtitle(\"Polynomial regression ^2\") +\n    xlab(\"Stillingskategori\") +\n    ylab(\"Snittlønn\")"},{"path":"polynomisk-regresjon.html","id":"modellering-av-tredjegrads-cubic-polynomisk-regresjon","chapter":"Kapittel 11 Polynomisk regresjon","heading":"11.2 Modellering av tredjegrads (“cubic”) polynomisk regresjon","text":"","code":"\npolydata$Stillingskategori3 <- polydata$Stillingskategori^3\nglimpse(polydata)\n#> Rows: 10\n#> Columns: 4\n#> $ Stillingskategori  <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n#> $ Snittlonn          <dbl> 35000, 41000, 48000, 63000, 880…\n#> $ Stillingskategori2 <dbl> 1, 4, 9, 16, 25, 36, 49, 64, 81…\n#> $ Stillingskategori3 <dbl> 1, 8, 27, 64, 125, 216, 343, 51…\npolydata$Stillingskategori3 <- polydata$Stillingskategori^3\n\npoly_reg3 <- lm(Snittlonn ~ ., data = polydata)\n\nggplot() +\n    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn), col = \"red\") +\n    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg3, newdata = polydata)), col = \"blue\") +\n    ggtitle(\"Polynomial regression ^3\") +\n    xlab(\"Stillingskategori\") +\n    ylab(\"Snittlønn\")"},{"path":"polynomisk-regresjon.html","id":"modellering-av-fjerdegrads-quartic-polynomisk-regresjon","chapter":"Kapittel 11 Polynomisk regresjon","heading":"11.3 Modellering av fjerdegrads (“quartic”) polynomisk regresjon","text":"","code":"\npolydata$Stillingskategori4 <- polydata$Stillingskategori^4\n\npoly_reg4 <- lm(Snittlonn ~ ., data = polydata)\n\nggplot() +\n    geom_point(aes(x = polydata$Stillingskategori, y = polydata$Snittlonn), col = \"red\") +\n    geom_line(aes(x = polydata$Stillingskategori, y = predict(poly_reg4, newdata = polydata)), col = \"blue\") +\n    ggtitle(\"Polynomial regression ^4\") +\n    xlab(\"Stillingskategori\") +\n    ylab(\"Snittlønn\")"},{"path":"polynomisk-regresjon.html","id":"tolkning","chapter":"Kapittel 11 Polynomisk regresjon","heading":"11.4 Tolkning","text":"Av grafene får vi en indikasjon på hvilken modell som ser ut til å passe våre data best. Vi kan også se på hvordan de ulike modellene predikerer ulikt. La oss f.eks. anta man ønsker å innføre en stillingskategori 5.5. Hva er predikert snittlønn denne kategorien?Vi kan se det er en vesentlig forskjell prediksjonene. Når vi ser på grafen enkel OLS lenger opp ser vi også regresjonslinja ligger godt linja/punktene dataene. Den siste av grafene - polynomial ^4 - ser ut til å treffe ganske bra.","code":"\n# Enkel OLS\npred1 <- predict(enkelOLS, data.frame(Stillingskategori = 5.5))\n# ^4 polynomial\npred2 <- predict(poly_reg4, data.frame(Stillingskategori = 5.5,\n                              Stillingskategori2 = 5.5^2,\n                              Stillingskategori3 = 5.5^3,\n                              Stillingskategori4 = 5.5^4))\npred <- as.data.frame(c(pred1, pred2))\npred\n#>   c(pred1, pred2)\n#> 1          224500\n#> 2          110250"},{"path":"polynomisk-regresjon.html","id":"tilnæmring-gjennom-kryssvalidering---valg-av-antall-grader-i-polynomialfunksjonen","chapter":"Kapittel 11 Polynomisk regresjon","heading":"11.5 Tilnæmring gjennom kryssvalidering - valg av antall grader i polynomialfunksjonen","text":"Dette eksempelet er modifisert fra Zach (2019a).Vi kan tilnærme oss valg av riktig polynomialfunksjon gjennom “Mean Squared Error” (MSE) som forteller oss hvor nærme en regresjonslinje er datapunktene. Jo lavere MSE, jo bedre kan vi anta prediksjonsevnen til modellen er. Vi skal ikke gå dybden på dette, men vise hvordan dette kan se ut eksempelet ovenfor. eksempelet brukes det som kalles “k-fold cross-validation” som vi ikke går nærmere inn på (men som vi forklarte kapittelet om Principal Component Analysis). Utregning av MSE følger av:\\(MSE = \\frac{1}{n}*\\sum(faktisk\\ verdi - predikert\\ verdi)\\)eksempelet har vi generert data som ser på timer studenter har brukt på et emne og respektive resultater.Vi kan se på en enkel OLS på datasettet:Neste steg er å lage 5 modeller (kan selvsagt lage x antall modeller).Modell 2 ser ut til å ha lavest MSE.Vi kan til slutt visualisere valgt modell og se på modellens parametere.","code":"\nset.seed(1)\ndf <- data.frame(timer = runif(50, 5, 15), resultat=50)\ndf$resultat <- df$resultat + df$timer^3/150 + df$timer*runif(50, 1, 2)\nglimpse(df)\n#> Rows: 50\n#> Columns: 2\n#> $ timer    <dbl> 7.655087, 8.721239, 10.728534, 14.082078,…\n#> $ resultat <dbl> 64.30191, 70.65430, 73.66114, 86.14630, 5…\nggplot(df, aes(x = timer, y = resultat)) +\n  geom_point(col = \"red\")\nenkelOLS2 <- lm(resultat ~ timer, data = df)\nggplot() +\n    geom_point(aes(x = df$timer, y = df$resultat), col = \"red\") +\n    geom_line(aes(x = df$timer, y = predict(enkelOLS2, newdata = df)), col = \"blue\") +\n    ggtitle(\"Enkel OLS\") +\n    xlab(\"timer\") +\n    ylab(\"resultat\")\n# Stokker om dataene tilfeldig\ndf.shuffled <- df[sample(nrow(df)),]\n# Setter antall \"folds\" som skal brukes i \"k-fold cross-validation\"\nK <- 10 \n# Definerer antall polynomfunksjoner\ndegree <- 5\n# Lager k antall like store \"folds\"\nfolds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)\n# Lager matrise som skal fylles med MSE for de 5 modellene\nmse = matrix(data=NA,nrow=K,ncol=degree)\n# Gjennomfører \"K-fold cross validation\"\nfor(i in 1:K){\n    # Deler opp i treningsdata og testdata\n    testIndexes <- which(folds==i,arr.ind=TRUE)\n    testData <- df.shuffled[testIndexes, ]\n    trainData <- df.shuffled[-testIndexes, ]\n    # Evaluerer modellene gjennom \"k-fold cv\" \n    for (j in 1:degree){\n        fit.train = lm(resultat ~ poly(timer,j), data=trainData)\n        fit.test = predict(fit.train, newdata=testData)\n        mse[i,j] = mean((fit.test- testData$resultat)^2) \n    }\n}\ncolMeans(mse)\n#> [1]  9.886172  8.589655  9.439514 10.214915 12.745574\nggplot(df, aes(x=timer, y=resultat)) + \n          geom_point(col = \"red\") +\n          stat_smooth(method='lm', formula = y ~ poly(x,2), size = 1) + \n          xlab('timer') +\n          ylab('resultat')\n\nbestemodell <- lm(resultat ~ poly(timer,2, raw=T), data=df)\n\nsummary(bestemodell)\n#> \n#> Call:\n#> lm(formula = resultat ~ poly(timer, 2, raw = T), data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.6589 -2.0770 -0.4599  2.5923  4.5122 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value\n#> (Intercept)              54.00526    5.52855   9.768\n#> poly(timer, 2, raw = T)1 -0.07904    1.15413  -0.068\n#> poly(timer, 2, raw = T)2  0.18596    0.05724   3.249\n#>                          Pr(>|t|)    \n#> (Intercept)              6.78e-13 ***\n#> poly(timer, 2, raw = T)1  0.94569    \n#> poly(timer, 2, raw = T)2  0.00214 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.8 on 47 degrees of freedom\n#> Multiple R-squared:   0.93,  Adjusted R-squared:  0.927 \n#> F-statistic: 312.1 on 2 and 47 DF,  p-value: < 2.2e-16"},{"path":"logistisk-regresjon.html","id":"logistisk-regresjon","chapter":"Kapittel 12 Logistisk regresjon","heading":"Kapittel 12 Logistisk regresjon","text":"R-pakker brukt dette kapittelet:Som vi vil se kapittelet om regresjonsforutsetninger er det en forutsetning den avhengige variabelen er kontinuerlig. Så langt har vi hatt det. Men det kan gjerne tenkes vi sitter med en avhengig variabel som er kategorisk. Logistisk regresjon er regresjonsanalyse der de uavhengige variablene er kontinuerlige eller kategoriske og den avhengige kategorisk. Av den grunn bygger logistisk regresjon på andre prinsipper og teknikker enn «vanlig» regresjon. Der lineær regresjon er en regresjonsalgoritme (bestemmer/predikere en kontonuerlig variabel) er logistisk regresjon en klassifiseringsalgoritme (bestemme/predikere en binær klassifiering - f.eks. sant/usant) dersom analysen kobles til en verdi/terskel beslutninger. (Vyas 2020).Et viktig poeng med logistisk regresjon er det er et viktig element temaer som maskinlæring og kunstige nevrale nettverk (Monroe (2017)). Dette går vi ikke nærmere inn på dette kapittelet.Det er vanlig å snakke om tre typer logistisk regresjon:Binær logistisk regresjon (ofte omtalt kun som “logistisk regresjon”) der vi den avhangige variabelen kun har mulige utfall (f.eks. 0,1 - ja/nei - død/levende o.l.).Binær logistisk regresjon (ofte omtalt kun som “logistisk regresjon”) der vi den avhangige variabelen kun har mulige utfall (f.eks. 0,1 - ja/nei - død/levende o.l.).Ordinal logistisk regresjon (noen steder omtalt som “ordinal regresjon”) er en forlengelse av binær logistisk regresjon. Dette brukes når vi har en avhengig variabel med rangerte (“ordered”) kategorier - f.eks. en svarskala (sterkt uenig - uenig - nøytral - enig - sterkt enig).Ordinal logistisk regresjon (noen steder omtalt som “ordinal regresjon”) er en forlengelse av binær logistisk regresjon. Dette brukes når vi har en avhengig variabel med rangerte (“ordered”) kategorier - f.eks. en svarskala (sterkt uenig - uenig - nøytral - enig - sterkt enig).Multinomial logistisk regresjon der den avhengige variabelen kan ha tre eller flere mulige utfall (f.eks. gruppe 1, gruppe 2, gruppe 3 - klasse , klasse B, klasse C o.l.), men der det ikke er rangerte kategorier forhold til hverandre (som ordinal logistisk regresjon).Multinomial logistisk regresjon der den avhengige variabelen kan ha tre eller flere mulige utfall (f.eks. gruppe 1, gruppe 2, gruppe 3 - klasse , klasse B, klasse C o.l.), men der det ikke er rangerte kategorier forhold til hverandre (som ordinal logistisk regresjon).Selve begrepet “logistisk” har sitt utspring “logit-funksjonen” (der begrepet “logit” sin opprinnelse er en kortform “logistic unit” (Berkson 1944)). Matematikken bak logistisk regresjon er noe mer komplisert enn lineær regresjon. Field (2009a) skriver (s.265) “don’t wish dwell underlying principles logistic regression aren’t necessary understand test (living proof fact)”. Likevel forsøker vi å gi en kort (og ufullstendig) beskrivelse. Et ok sted å lese mer detaljert om dette er f.eks. Miles Shevlin (2001).Fra lineær regresjon kjenner vi den avhengige variabelen er kontinuerlig (med mulig utfall \\(-\\infty, \\infty\\)). en binær avhengig variabel er utfallet en sannsynlighetsverdi mellom \\(0,1\\). Utfordringen er da å transformere \\(0,1\\) til \\(-\\infty, \\infty\\) å kunne kjøre (lineær) regresjon på de transformerte verdiene. Veien til dette går om “log odds”. Enhver sannsynlighet kan konverteres til log odds gjennom å ta logaritmen til odds ratioen (vår gjennomgang lener seg på Glen (2021)). Sannsynlighet, odds og log odds er (litt upresist sagt) samme sak uttrykt på ulik måte:\\(Sannsynlighet = uttrykk\\ \\ hvor\\ trolig\\ det\\ er\\ \\ en\\ hendelse\\ vil\\ inntreffe\\) 7\\(Odds\\ (\"odds\\ \\ success\") =\\frac{sannsynlighet\\ \\ suksess}{sannsynlighet\\ \\ ikke-suksess}= f.eks.\\ \\frac{sannsynlighet\\ \\ regn}{sannsynlighet\\ \\ oppholdsvær}\\)\\(Odds-ratio = \\frac{sannsynlighet\\ \\ regn}{sannsynlighet\\ \\ oppholdsvær}= \\frac{80\\%}{20\\%}=4\\)\\(log-odds=ln(4)\\approx1.386\\)På en mer generell form kan log-odds uttrykkes:\\(log-odds = log\\left[\\frac{p}{p-1}\\right] (=logit(p))\\)der \\(p\\) er sannsynligheten en hendelse.Logistisk regresjon innebærer å konvertere sannsynligheter til log-odds, og deretter tilbake til sannsynligheter (se eksempel litt lenger ned). Logitfunksjonen estimerer sannsynligheter mellom 0 og 1 våre hendelser/data. en binær logistisk regresjon kalukulerer vi dermed den betingede sannsynligheten variabelen (Y) gitt variabelen (X). Dette kan uttrykkes slik:\\(P(Y=1|X), dvs.\\ den\\ betingede\\ sannsynligheten\\ \\ Y=1\\ gitt\\ X\\)eller\\(P(Y=0|X), dvs.\\ den\\ betingede\\ sannsynligheten\\ \\ Y=0\\ gitt\\ X\\)P(Y|X) er estimert som en såkalt sigmoid-funksjon. En sigmoid funksjon\\(p=\\frac{e^{log(odds)}}{1+e^{log(odds)}} = \\frac{1}{e^{-log(odds)}} = \\frac{1}{1+e^{-x}} (= Sigmoid\\ funksjon)\\)Sigmoid-funksjon er (som del av logistisk regresjon) også et sentralt element f.eks. maskinlæring. Vi kan visualisere en sigmoid-funksjon slik:Dette gjør vi kan plotte verdier spennet \\((0,1)\\) som \\(-\\infty, \\infty\\).La oss se på et enkelt eksempel på hvordan logistisk regresjon kan brukes (basert på Starmer (2018b)).Vi vil klassifisere en person som enten smittet eller ikke smittet av en sykdom. Output er enten 1 (= smittet) eller 0 (ikke smittet) - basert på en eller annen test/kriterium. La oss si vi har 10 tilfeller:Målet vårt er altså å finne den linjen som best passer til disse dataene (slik vi gjennom OLS-regresjon forsøker å finne den linja som passer best til dataene lineær regresjon). logistisk regresjon bruker vi imidlertid ikke minste kvadratsums metode (OLS), men det som kalles “Maximum Likelihood” (ML). Grunnen til dette er transformasjonen (gjennom logit funksjonen) gir uendelig store kvadratsummer fordi punktene etter transformasjonen har uendelig verdi.Første steg er å konvertere sannsynlighetene smitte til log(odds) smitte og bruke log(odds) på y-aksen. Vi legger på en “kandidatlinje” til regresjonslinja (“kandidatlinje” som : vi trekker en linje som er ca. riktig visuelt og bruker den videre).Ved å trekke vertikale linjer fra hvert punkt til kandidatlinjen finner vi hvert punkts log(odds) punkt (y-verdien der punktets vertikale linje treffer kandidatlinjen). Deretter brukes sigmoid-funksjonen til å konvertere log(odds) verdiene til hver observasjon til sannsynligheter. Det gir oss y-verdien på linja fra sigmoid-funksjonen den første observasjonen:Som repeteres alle observasjonene. Dette gjør vi kan regne ut samlet sannsynlighet (y-verdien på sigmoid-linja det enkelt punkt er det samme som sannsynligheten y-verdien dette punktet). Dette gjør vi enten ved å multiplisere alle observasjonenes sannsynlighet, eller å addere log(sannsynlighet) alle observasjonene (gir samme resultat). Verdien vi får er altså samlet sannsynlighet dataene gitt kandidatlinja.Deretter roteres kandidatlinja og prosedyren gjentas inntil man får linja med høyest sannsynlighet (= Maximum likelihood). Algoritmen som gjør dette oss roterer kun på en måte som øker sannsynligheten (= tids- og ressursbesparende).Når vi har funnet linja som best forklarer dataen kan vi bruke denne prediksjon. Dersom vi har en gitt test elelr indikasjon på om en ny perosn er smittet kan vi bruke testverdien til å predikere om vi tror vedkommende er smittet eller ikke. Hvilken cut-verdi vi setter er kontekstavhengig, men hvis vi f.eks. setter 0.5 vil en måling på 0.51 gjøre vi klassifiserer vedkommende som smittet, mens en verdi på 0.49 vil klassifisere som ikke-smittet. Man kan selvsagt legge til flere målinger (=flere uavhengige variabler) å fange opp flere forhold som gjør vi kan si noe sikrere om smitte/ikke-smitte. Akkurat dette - muligheten til å bruke sannsynlighet å klassifisere nye tilfeller/observasjoner som kan være både kategoriske og kontinuerlige - gjør metoden særlig egnet teknikker som maskinlæring. Tabachnik Fidell (2007) beskriver logistisk regresjon som relativt fri begrensninger og stand til å analysere en miks av kontinuerlige og kategoriske variabler (“variety complexity data sets can analyzed almost unlimited” - s.441).Imidlertid er det litt mer komplisert å se om en uavhengig variabel bidrar modellen. lineær regresjon er dette relativt enkelt. Det vi gjør logistisk regresjon er å teste om en variabels effekt på prediksjonen er signifikant forskjellig fra 0 (gjennom en såkalt Walds test). Hvis effekten til en variabel er 0 gir den åpenbart ingen hjelp prediksjonen (og er unødvendig modellen).","code":"\npacman::p_load(tidyverse, haven, writexl, aod, pscl, DescTools, reshape2, foreign, nnet, MASS)\nsigmoid <- function(x) {\n   1 / (1 + exp(-x))\n}\nx <- seq(-5, 5, 0.01)\nplot(x, sigmoid(x), col='darkgreen')\nsmitte <- data.frame(x1 = 1:10,\n                   x2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1))\nsmitteplott <- ggplot(smitte, aes(x1, x2)) +\n    geom_point(col = ifelse(1:nrow(smitte) == 1:5, \"red\", \"blue\"))\nsmitteplott <- smitteplott + scale_x_discrete(name =\"Pasient\", \n                    limits=c(\"1\":\"10\")) + ylab(\"Smittet\")\nsmitteplott \nsigmoid <- function(x) {\n   1 / (1 + exp(-x))\n}\nx <- seq(-5, 5, 0.01)\nsigmoidplott <- plot(x, sigmoid(x), col='darkgreen', xaxt='n')\npoints(-4, 0.02, col = \"red\", pch=16)\nsigmoidplott\n#> NULL"},{"path":"logistisk-regresjon.html","id":"binær-logistisk-regresjon","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1 Binær logistisk regresjon","text":"dette eksempelet følger vi Pallant (2010).Download sovn.savSom Pallant beskriver er det vesentlig enklere tolkning av resultatene variablene kodes fornuftig. en dikotom variabler bør 0 brukes på det alternativet som indikerer fravær av det du spør om, mens 1 brukes om tilstedeværelse. F.eks.: Hvis spørsmålet er “Spiller du tennis?” så bør “Nei” kodes 0 og “Ja” kodes 1.","code":"\nsovn <- read_sav(\"sleep4ED.sav\")\nwrite_sav(sovn, \"sovn.sav\")\nhead(sovn)\n#> # A tibble: 6 × 55\n#>      id        sex   age       marital edlevel weight height\n#>   <dbl>  <dbl+lbl> <dbl>     <dbl+lbl> <dbl+l>  <dbl>  <dbl>\n#> 1    83 0 [female]    42 2 [married/d… 2 [sec…     52    162\n#> 2   294 0 [female]    54 2 [married/d… 5 [pos…     65    174\n#> 3   425 1 [male]      NA 2 [married/d… 2 [sec…     89    170\n#> 4    64 0 [female]    41 2 [married/d… 5 [pos…     66    178\n#> 5   536 0 [female]    39 2 [married/d… 5 [pos…     62    160\n#> 6    57 0 [female]    66 2 [married/d… 4 [und…     62    165\n#> # … with 48 more variables: healthrate <dbl+lbl>,\n#> #   fitrate <dbl+lbl>, weightrate <dbl+lbl>,\n#> #   smoke <dbl+lbl>, smokenum <dbl>, alchohol <dbl>,\n#> #   caffeine <dbl>, hourwnit <dbl>, hourwend <dbl>,\n#> #   hourneed <dbl>, trubslep <dbl+lbl>, trubstay <dbl+lbl>,\n#> #   wakenite <dbl+lbl>, niteshft <dbl+lbl>,\n#> #   liteslp <dbl+lbl>, refreshd <dbl+lbl>, …\nsovnlogit <- glm(probsleeprec ~ sex + getsleprec + stayslprec + hourwnit + age, data = sovn, family = \"binomial\")\nsummary(sovnlogit)\n#> \n#> Call:\n#> glm(formula = probsleeprec ~ sex + getsleprec + stayslprec + \n#>     hourwnit + age, family = \"binomial\", data = sovn)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.0186  -0.7329  -0.5076   0.8504   2.0633  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.953457   1.451062   1.346  0.17823    \n#> sex         -0.108266   0.314660  -0.344  0.73079    \n#> getsleprec   0.716079   0.338920   2.113  0.03462 *  \n#> stayslprec   1.984240   0.324843   6.108 1.01e-09 ***\n#> hourwnit    -0.448479   0.165242  -2.714  0.00665 ** \n#> age         -0.006031   0.013711  -0.440  0.66003    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 329.00  on 240  degrees of freedom\n#> Residual deviance: 252.98  on 235  degrees of freedom\n#>   (30 observations deleted due to missingness)\n#> AIC: 264.98\n#> \n#> Number of Fisher Scoring iterations: 4"},{"path":"logistisk-regresjon.html","id":"deviance-residuals","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.1 “Deviance Residuals”","text":"Det første vi kan se på output fra modellen er “Deviance Residuals”. Dette forteller oss noe om modellens “fit”.","code":""},{"path":"logistisk-regresjon.html","id":"koeffisientene","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.2 Koeffisientene","text":"kan vi først se på p-verdiene. Vi ser getsleprec, stayslprec og hourwnit er statistisk signifikante. Med andre ord, de tre variablene som signifikant påvirker søvnproblemer er “Falle søvn”, “Ikke våkne” og “Timer søvn ukedager”.Tolkningen av koeffisientene er ikke like rett fram som lineær regresjon. viser de til endring log(odds) til den avhengige variabelen en enhet økning den respektive uavhengige variabelen:hver enhet økning “Falle søvn” (der 0 er “nei” og 1 er “ja”) øker log(odds) søvnproblemer med 0.72For hver enhet økning “Ikke våkne” (der 0 er “nei” og 1 er “ja”) øker log(odds) søvnproblemer med 1.98For hver enhet økning “Timer søvn hverdager” (timer søvn gjennomsnitt hver ukedag) reduseres log(odds) søvnproblemer med 0.44 (øker med -0.44)","code":"\nsummary(sovnlogit)$coefficients\n#>                 Estimate Std. Error    z value     Pr(>|z|)\n#> (Intercept)  1.953457430 1.45106156  1.3462264 1.782295e-01\n#> sex         -0.108266375 0.31466036 -0.3440738 7.307908e-01\n#> getsleprec   0.716079317 0.33892007  2.1128265 3.461562e-02\n#> stayslprec   1.984240290 0.32484318  6.1083022 1.006965e-09\n#> hourwnit    -0.448478626 0.16524155 -2.7140791 6.646030e-03\n#> age         -0.006031274 0.01371143 -0.4398720 6.600298e-01"},{"path":"logistisk-regresjon.html","id":"anova-på-residualene","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.3 ANOVA på residualene","text":"ser vi på kolonnen “Resid. Dev”. Vi ser på hvor stor nedgang det er hver variabel som legges inn modellen. Vi ser forskjellene er størst de signifikante variablene. Vi har et markert dropp når “getsleprec” legges inn og når “stayslprec” legges inn, og et noe mindre dropp når “hourwnit” inkluderes.","code":"\nanova(sovnlogit, test=\"Chisq\")\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: probsleeprec\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>            Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \n#> NULL                         240     329.00              \n#> sex         1    1.213       239     327.78  0.270801    \n#> getsleprec  1   19.145       238     308.64 1.211e-05 ***\n#> stayslprec  1   47.776       237     260.86 4.779e-12 ***\n#> hourwnit    1    7.692       236     253.17  0.005545 ** \n#> age         1    0.194       235     252.98  0.659761    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"logistisk-regresjon.html","id":"fit-indekser","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.4 Fit indekser","text":"Vi har ingen eksakt ekvivalent til \\(R^2\\) som vi vil ha lineær regresjon, men McFadden \\(\\rho^2\\) indeks kan gi oss verdifull informasjon (McFadden 1974, 1977).McFaddens \\(\\rho^2\\) kalles en \\(pseudo\\ R^2\\). Verdiene på \\(R^2\\) og \\(\\rho^2\\) er imidlertid ikke direkte sammenliknbare.McFadden (1974), s.124, figur 5.5Vi kan se av illustrasjonen forholdet ikke er lineært, og \\(\\rho^2-verdiene\\) er betraktelig lavere enn \\(R^2-verdiene\\) (se feks. T. J. Smith McKenna 2013). McFadden uttrykte selv (McFadden 1979) verdier mellom 0.2 og 0.4 ga uttrykk en utmerket fit. så fall har vi med 0.23 en god modell.T. J. Smith McKenna (2013) finner sin studie av ulike \\(pseudo-R^2\\) indekser “Aldrich-Nelson” ligger nærmest \\(R^2\\). Man skal likevel være observant på \\(pseudo-R^2\\) indekser er en tilnærming til \\(R^2\\) og man uansett skal være forsiktig med å tolke en \\(pseudo-R^2\\) helt likt som en \\(R^2\\) selv om det gir oss en indikasjon på hva modellen forklarer slik \\(R^2\\) gir lineær regresjon.","code":"\nDescTools::PseudoR2(sovnlogit, which = \"all\")\n#>        McFadden     McFaddenAdj        CoxSnell \n#>       0.2310662       0.1945916       0.2705286 \n#>      Nagelkerke   AldrichNelson VeallZimmermann \n#>       0.3632952       0.2397952       0.4154529 \n#>           Efron McKelveyZavoina            Tjur \n#>       0.2876696       0.3452914       0.2899666 \n#>             AIC             BIC          logLik \n#>     264.9760850     285.8848666    -126.4880425 \n#>         logLik0              G2 \n#>    -164.4979666      76.0198483"},{"path":"logistisk-regresjon.html","id":"total-effekt-wald-test","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.5 Total effekt (Wald test)","text":"Vi ser Walds test indikerer den totale effekten er statistisk signifikant.","code":"\noptions(scipen = 999)\naod::wald.test(b = coef(sovnlogit), Sigma = vcov(sovnlogit), Terms = 3:5)\n#> Wald test:\n#> ----------\n#> \n#> Chi-squared test:\n#> X2 = 54.9, df = 3, P(> X2) = 0.0000000000073"},{"path":"logistisk-regresjon.html","id":"forskjeller-mellom-de-signifikante-uavhengige-variablene","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.6 Forskjeller mellom de signifikante uavhengige variablene","text":"Vi kan se av de tre kjikvadrattestene det er statistisk signifikant forskjell mellom varaibelparrene 3-4, 4-5 og 3-5 (som var de tre statistisk signifikante variablene modellen).","code":"\n# Forskjell mellom variabel 3 og 4 i modellen\ndiff1 <- cbind(0, 0, 1, -1, 0, 0)\nwald.test(b = coef(sovnlogit), Sigma = vcov(sovnlogit), L = diff1)\n#> Wald test:\n#> ----------\n#> \n#> Chi-squared test:\n#> X2 = 6.3, df = 1, P(> X2) = 0.012\n# Forskjell mellom variabel 4 og 5 i modellen\ndiff2 <- cbind(0, 0, 0, 1, -1, 0)\nwald.test(b = coef(sovnlogit), Sigma = vcov(sovnlogit), L = diff2)\n#> Wald test:\n#> ----------\n#> \n#> Chi-squared test:\n#> X2 = 42.0, df = 1, P(> X2) = 0.000000000094\n# Forskjell mellom variabel 3 og 5 i modellen\ndiff3 <- cbind(0, 0, 1, 0, -1, 0)\nwald.test(b = coef(sovnlogit), Sigma = vcov(sovnlogit), L = diff3)\n#> Wald test:\n#> ----------\n#> \n#> Chi-squared test:\n#> X2 = 10.8, df = 1, P(> X2) = 0.00099"},{"path":"logistisk-regresjon.html","id":"fra-logodds-til-odds-ratios","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.1.7 Fra log(odds) til odds-ratios","text":"Odds-ratios er enklere å tolke enn log(odds). Derfor ønsker vi å konvertere log(odds) til odds-ratio.Dette kan vi tolke slik:hver enhet økning “Falle søvn” øker oddsene søvnproblemer med en faktor på 2.05.hver enhet økning “Ikke våkne” øker oddsene søvnproblemer med en faktor på 7.28For hver enhet økning “Timer søvn hverdager” reduseres oddsene søvnproblemer med en faktor på 0.64Størst odds å få søvnproblemer får vi altså hvis vi våkner opp løpet av søvnen. Det er imidlertid “gode odds” vi får søvnproblemer også av å ikek få sove, mens - ikke uventet - oddsene å få søvnproblemer synker med lengden (antall timer) på søvnen.","code":"\nexp(cbind(OR = coef(sovnlogit), confint(sovnlogit)))\n#> Waiting for profiling to be done...\n#>                    OR     2.5 %      97.5 %\n#> (Intercept) 7.0530308 0.4184472 126.4735087\n#> sex         0.8973885 0.4831609   1.6657006\n#> getsleprec  2.0463942 1.0542105   3.9997539\n#> stayslprec  7.2735195 3.8980843  13.9805634\n#> hourwnit    0.6385990 0.4573768   0.8762471\n#> age         0.9939869 0.9673924   1.0210516\nexp(coefficients(sovnlogit)[3])\n#> getsleprec \n#>   2.046394\nexp(coefficients(sovnlogit)[4])\n#> stayslprec \n#>    7.27352\nexp(coefficients(sovnlogit)[5])\n#> hourwnit \n#> 0.638599"},{"path":"logistisk-regresjon.html","id":"ordinal-logistisk-regresjon","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.2 Ordinal logistisk regresjon","text":"Ordinal logistisk regresjon er en utvidelse av binær logistisk regresjon når vi har en avhengig variabel som er kategorisk, men der kategoriene er ordnet forhold til hverandre og der analysen bør beholde dette innbyrdes forholdet.Vi bruker et datasett fra UCLA (“Ordinal Logistic Regression | R Data Analysis Examples,” n.d.).Download orddata.xlsxVariabelen “apply” er en rangert kategorisk variabel der studenter har svart på en undersøkelse som kategoriserer dem etter hvor sannsynlig det er de søker høyere akademiske studier (Usannsynlig = 1, Mulig = 2, Sannsynlig = 3). Variabelen “pared” er binær der 0 betyr ingen av foreldrene har gjennomført høyere akademisk utdannelse, og 1 betyr minst en av foreldrene har det. “public” er også binær, der 0 betyr privat institusjon, 1 betyr offentlig. “gps” (“Grade Point Average”) måler gjennomsnittskarakter lavere akademisk utdannelse.Vi kan se på dataene:Visuelt:Lager modellen:Koeffisientene er (log)odds. “pared” (foreldre) vil vi si en enhets økning “pared” (dcs fra 0 til 1 siden det er en binær variabel) vil vi forvente en økning på 1.048 den forvente verdien på “apply” *på (log)odds skalaen. Dette er selvsagt noe kludrete å tolke. Det vil være lettere å tolke odds (odds ratio).kan vi tolke:studenter med foreldre som har høyere utdannelse er oddsene å søke dersom man tilhører kategoriene “likely” eller “somewhat likely” 2.85 ganger høyere enn om man har foreldre som ikke har høyere utdannelse, gitt alle andre variabler holdes konstant.studenter med foreldre som har høyere utdannelse er oddsene å søke dersom man tilhører kategoriene “likely” eller “somewhat likely” 2.85 ganger høyere enn om man har foreldre som ikke har høyere utdannelse, gitt alle andre variabler holdes konstant.studenter som er offentlig skole er oddsene å være kategorien “likely” eller “somewhat likely” 5.7% høyere enn de som går på privat skole (\\((1-0.943)*100\\)).studenter som er offentlig skole er oddsene å være kategorien “likely” eller “somewhat likely” 5.7% høyere enn de som går på privat skole (\\((1-0.943)*100\\)).hver enhet økning “gpa” øker oddsene å være kategoriene “likely” eller “somewhat likely” med 85% (eller øker med en faktor på 1.85).hver enhet økning “gpa” øker oddsene å være kategoriene “likely” eller “somewhat likely” med 85% (eller øker med en faktor på 1.85).Og se p-verdiene.Vi ser variabelen “public” ikke er statistisk signifikant, mens “pared” og “gpa” har p < 0.05.","code":"\norddata <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/ologit.dta\")\nnames(orddata)\n#> [1] \"apply\"  \"pared\"  \"public\" \"gpa\"\ndim(orddata)\n#> [1] 400   4\nlapply(orddata[, c(\"apply\", \"pared\", \"public\")], table)\n#> $apply\n#> \n#>        unlikely somewhat likely     very likely \n#>             220             140              40 \n#> \n#> $pared\n#> \n#>   0   1 \n#> 337  63 \n#> \n#> $public\n#> \n#>   0   1 \n#> 343  57\nftable(xtabs(~ public + apply + pared, data = orddata))\n#>                        pared   0   1\n#> public apply                        \n#> 0      unlikely              175  14\n#>        somewhat likely        98  26\n#>        very likely            20  10\n#> 1      unlikely               25   6\n#>        somewhat likely        12   4\n#>        very likely             7   3\nsummary(orddata$gpa)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.900   2.720   2.990   2.999   3.270   4.000\nround(sd(orddata$gpa),3)\n#> [1] 0.398\nggplot(orddata, aes(x = apply, y = gpa)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(pared ~ public, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\nordlogmod <- polr(apply ~ pared + public + gpa, data = orddata, Hess=TRUE)\nsummary(ordlogmod)\n#> Call:\n#> polr(formula = apply ~ pared + public + gpa, data = orddata, \n#>     Hess = TRUE)\n#> \n#> Coefficients:\n#>           Value Std. Error t value\n#> pared   1.04769     0.2658  3.9418\n#> public -0.05879     0.2979 -0.1974\n#> gpa     0.61594     0.2606  2.3632\n#> \n#> Intercepts:\n#>                             Value   Std. Error t value\n#> unlikely|somewhat likely     2.2039  0.7795     2.8272\n#> somewhat likely|very likely  4.2994  0.8043     5.3453\n#> \n#> Residual Deviance: 717.0249 \n#> AIC: 727.0249\nround(exp(coef(ordlogmod)),3)\n#>  pared public    gpa \n#>  2.851  0.943  1.851\noptions(scipen = 999)\nkoefftabell <- coef(summary(ordlogmod))\nsanns <- pnorm(abs(koefftabell[, \"t value\"]), lower.tail = FALSE) * 2\nkoefftabell <- cbind(koefftabell, \"p value\" = sanns)\nround(koefftabell, 3)\n#>                              Value Std. Error t value\n#> pared                        1.048      0.266   3.942\n#> public                      -0.059      0.298  -0.197\n#> gpa                          0.616      0.261   2.363\n#> unlikely|somewhat likely     2.204      0.780   2.827\n#> somewhat likely|very likely  4.299      0.804   5.345\n#>                             p value\n#> pared                         0.000\n#> public                        0.844\n#> gpa                           0.018\n#> unlikely|somewhat likely      0.005\n#> somewhat likely|very likely   0.000"},{"path":"logistisk-regresjon.html","id":"multinomial-logistisk-regresjon","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.3 Multinomial logistisk regresjon","text":"motsetning til binær logistisk regresjon er multinomial logistisk regresjon klassifisering tilfeller der en kategorisk avhengig variabel kan anta tre eller flere utfall, men der de ikke er rangerte som ordinal logistisk regresjon.Eksempelet er hentet fra “Multinomial Logistic Regression | R Data Analysis Examples” (2014), og går på hvilke valg elever som skal inn på videregående skole (min oversettelse, dataene omtaler “high school students”, men det spiller ikke noen stor betydning eksempelets skyld). Disse elevene velger mellom studieforberedende (“general program”), yrkesfag (“vocational program”) og et akademisk program (“academic program”, ingen klar norsk oversettelse, men som vi kaller “IB” fra International Baccalaureate å skille det fra studieforberedende).kan vi vise hvordan vi laster ned data direkte fra nett R:vårt formål trenger vi den avhengige variabelen “prog”, og de uavhengige variablene “ses” (sosio-økononmisk status) og “write” (score på skrivetest). Vi døper også om variablene til norsk.Download multinommod.xlsxVi kan se litt på dataene:Første skritt å lage modellen er å lage en basline, altså en av kategoriene den avhengige variabelen vi “sammenlikner med”. Vi ser på oddsene å være en kategori (den avhengige variabelen) relativt til oddsene å være baselinekategorien.Vi trenger også å se på p-verdiene koeffisientene gjennom Walds test:Hvis vi først ser på koeffisientene ser vi første linje “Studieforberedende” sammenliknet med baseline “IB”, og på andre linjer “Yrkesfag” sammenliknet med baseline “IB”.Vi ser først på første linje. En enhets økning skrivetesten er forbundet med en nedgang (log)oddsene å være studieforberedende forhold til IB på 0.058. Likeledes - en enhets økning skrivetesten er assosiert med en nedgang (log)oddsene å være yrkesfag forhold til IB på 0.114.(Log)oddsene å være studieforberedende vs. IB synker med 1.163 dersom man går fra lav til høy sosioøkonmisk status.(Log)oddsene å være studieforberedende vs. IB synker med 0.533 dersom man går fra lav til middels sosioøkonmisk status, men forskjellen er ikke signifikant.(Log)oddsene å være yrkesfag vs. IB synker med 0.983 dersom man går fra lav til høy sosioøkonomisk status.(Log)oddsene å være yrkesfag vs. IB øker med 0.291 dersom man går fra lav til middels sosioøkonomisk status, men forskjellen er ikke signifikant.Vi kan også se på den relative risken - oddsene - å være en kategori forhold til å være baselinekateogrien IB:Oddsene en enhets økning på skrivetesten er 0.944 å være studieforberedende vs. IB.Oddsene å endre fra lav til høy sosioøkonomisk status er 0.313 ved å være studieforberedende vs. IB.","code":"\nmultinommod <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\nwith(multinommod, table(ses, program))\n#>          program\n#> ses       Studieforb IB Yrkesfag\n#>   Lav             16 19       12\n#>   Middels         20 44       31\n#>   Høy              9 42        7\nround(with(multinommod, do.call(rbind, tapply(skrivetest, program, function(x) c(M = mean(x), SD = sd(x))))),2)\n#>                M   SD\n#> Studieforb 51.33 9.40\n#> IB         56.26 7.94\n#> Yrkesfag   46.76 9.32\nmultinommod$program2 <- relevel(multinommod$program, ref = \"IB\")\nmodell1 <- nnet::multinom(program2 ~ ses + skrivetest, data = multinommod)\n#> # weights:  15 (8 variable)\n#> initial  value 219.722458 \n#> iter  10 value 179.982880\n#> final  value 179.981726 \n#> converged\nsummary(modell1)\n#> Call:\n#> nnet::multinom(formula = program2 ~ ses + skrivetest, data = multinommod)\n#> \n#> Coefficients:\n#>            (Intercept) sesMiddels     sesHøy skrivetest\n#> Studieforb    2.852198 -0.5332810 -1.1628226 -0.0579287\n#> Yrkesfag      5.218260  0.2913859 -0.9826649 -0.1136037\n#> \n#> Std. Errors:\n#>            (Intercept) sesMiddels    sesHøy skrivetest\n#> Studieforb    1.166441  0.4437323 0.5142196 0.02141097\n#> Yrkesfag      1.163552  0.4763739 0.5955665 0.02221996\n#> \n#> Residual Deviance: 359.9635 \n#> AIC: 375.9635\noptions(scipen = 999)\nzverdier <- summary(modell1)$coefficients/summary(modell1)$standard.errors\npverdier <- (1 - pnorm(abs(zverdier), 0, 1)) * 2\nround(pverdier, 3)\n#>            (Intercept) sesMiddels sesHøy skrivetest\n#> Studieforb       0.014      0.229  0.024      0.007\n#> Yrkesfag         0.000      0.541  0.099      0.000\nround(exp(coef(modell1)),3)\n#>            (Intercept) sesMiddels sesHøy skrivetest\n#> Studieforb      17.326      0.587  0.313      0.944\n#> Yrkesfag       184.613      1.338  0.374      0.893"},{"path":"logistisk-regresjon.html","id":"predikerte-sannsynligheter-i-modellen","chapter":"Kapittel 12 Logistisk regresjon","heading":"12.3.1 Predikerte sannsynligheter i modellen","text":"Vi genererer datasett der vi varierer en variabel mens vi holder de andre konstant (ved å låse den/de som skal holdes konstante til gjennomsnittsverdien).Tabellen inneholder altså de predikerte verdiene nivåene av sosioøkonomisk status (lav, middels, høy) når skrivetest holdes konstant. Modellen predikerer altså f.eks. de som har lav sosioøkonomisk status vil ca. 44% være kateogrien IB, ca. 36% være kateogrien studieforberedende og ca. 20% være kategorien yrkesfag. de med høy sosioøkonomisk status predikerer modellen ca. 70% vil være kateogrien IB, ca. 18% studieforberedende og ca. 12% yrkesfag.Vi kan snu på prediksjonene og se på hva forventet score på skrivetesten hvert nivå av sosioøkonomisk status:hvert nivå av sosioøkonomisk status ser vi predikerte gjennomsnittlige sannsynligheter skrivetesten de ulike kategoriene.Vi kan visualisere det samme (ofte er tolkning lettere visuelt):","code":"\npredsann <- fitted(modell1)\ndses <- data.frame(ses = c(\"Lav\", \"Middels\", \"Høy\"), skrivetest = mean(multinommod$skrivetest))\npredict(modell1, newdata = dses, \"probs\")\n#>          IB Studieforb  Yrkesfag\n#> 1 0.4396845  0.3581917 0.2021238\n#> 2 0.4777488  0.2283353 0.2939159\n#> 3 0.7009007  0.1784939 0.1206054\ndskrivetest <- data.frame(ses = rep(c(\"Lav\", \"Middels\", \"Høy\"), each = 41), skrivetest = rep(c(30:70),\n    3))\npred_skrivetest <- cbind(dskrivetest, predict(modell1, newdata = dskrivetest, type = \"probs\", se = TRUE))\nby(pred_skrivetest[, 3:5], pred_skrivetest$ses, colMeans)\n#> pred_skrivetest$ses: Høy\n#>         IB Studieforb   Yrkesfag \n#>  0.6164315  0.1808037  0.2027648 \n#> --------------------------------------------- \n#> pred_skrivetest$ses: Lav\n#>         IB Studieforb   Yrkesfag \n#>  0.3972977  0.3278174  0.2748849 \n#> --------------------------------------------- \n#> pred_skrivetest$ses: Middels\n#>         IB Studieforb   Yrkesfag \n#>  0.4256198  0.2010864  0.3732938\nlongpred <- melt(pred_skrivetest, id.vars = c(\"ses\", \"skrivetest\"), value.name = \"sannsynlighet\")\nggplot(longpred, aes(x = skrivetest, y = sannsynlighet, colour = ses)) + geom_line() + facet_grid(variable ~\n    ., scales = \"free\")"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","text":"R-pakker brukt dette kapittelet:SEM-analyse er en lineær metodikk som modellerer regresjonslikninger og latente variabler samtidig. En “vanlig” lineær regresjon (og f.eks. CFA) kan ses på som ulike spesielle tilfeller av SEM. Alle modellene er satt et overordnet rammeverk - LISREL (LInear Structural RELations) (Jöreskog 1969, 1970, 1978; Jöreskog, Olsson, Wallentin 2016) (LISREL© er også et program standard og multilevel modellering av strukturelle regresjonsmodeller).en svært nyttig gjennomgang av SEM anbefaler vi Lin (2021b). Lin (2021b) beskriver disse modellene SEM-paraplyen:Enkel lineær regresjonMultippel regresjonMultivariat regresjonStianalyse (“path analysis”)Bekreftende faktoranalyseStrukturell regresjonLineær regresjon behandler forholdet mellom observerte variabler, CFA forholdet mellom latente og observerte variabler, mens strukturell regresjon behandler latent-til-latent variabler. Stianalyse er en form multippel regresjonsanalyse.Det som er særegent SEM er det omfatter både forholdet observert-latente variabler (“measurement model”) og latent-latent variabler (“structural model”).Lin (2021b) illustrerer en typisk SEM-modell slik:Eksempel på SEM-modell fra Lin (2021b)Sirkel = Latent variabelFirkant = Observert variabelTrekant = InterceptRett pil = Sti (startpunkt markerer variabel som predikerer sluttpunkt på pila)Buet pil = Varians eller kovariansEt sentral poeng å huske på er - på lik linje med bekreftende faktoranalyse CFA - er SEM en “bekreftende metode”, altså bør det ligge solid teori bakgrunnen der vi modellerer å finne støtte eller ikke modellen.Vi vil også poengtere SEM-analyse trives best med større datautvalg. Kline (2016) foreskriver \\(\\frac{n}{q}\\) bør være \\(20:1\\), der \\(q\\) er antall parametere modellen. Altså, hvis antall parametere er 15 bør utvalgsstørrelsen være minst 300.Lin (2021b) illustrerer modellvalg SEM på denne måten (referansene til “Model 1A” etc. illustrasjonen referer til ulike modeller Lin bygger sin gjennomgang) 8:Visualisering av SEM, fra Lin (2021b)Siden vi foregående kapitler har sett på såvel enkel som multippel regresjon og faktoranalyse vil vi dette kapittelet bruke Lins eksempel å belyse multivariat regresjon, stianalyse og strukturell regresjon.","code":"\npacman::p_load(lavaan, lavaanPlot, haven, tidyverse, sjPlot)"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"datasettet","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.1 Datasettet","text":"Download worland5.csvDatasettet eksempelet ser på effektene av studenters bakgrunn på deres akademiske prestasjoner.Vi ser datasettet har 500 observasjoner av 9 observerte variabler. Hypotesen er de 9 observerte variablene utgjør 3 latente variabler: Tilpasning, risiko og prestasjoner på denne måten:Tilpasning (“Adjustment”)\nmotiv: Motivasjon\nharm: Harmoni\nstabi: Stabilitet\nmotiv: Motivasjonharm: Harmonistabi: StabilitetRisiko (“Risk”)\nppsych: (Negativ) foreldrepåvirkning\nses: SES - Sosioøkonomisk status\nverbal: Verbal IQ\nppsych: (Negativ) foreldrepåvirkningses: SES - Sosioøkonomisk statusverbal: Verbal IQPrestasjoner:\nread: Lesing\nartih: Regning\nspell: Staving\nread: Lesingartih: Regningspell: StavingVi kan se på kovariansmatrisen:En positiv kovarians betyr altså når f.eks. motivasjon og harmoni er positivt kovariert (77) øker den ene når den andre øker. Og motsatt - når motivasjon og foreldrepåvirkning er negativt kovariert (-25) betyr det negativ foreldrepåvirkning resulterer lavere motivasjon.","code":"\nsemdata <- read.csv(\"worland5.csv\")\nglimpse(semdata)\n#> Rows: 500\n#> Columns: 9\n#> $ motiv  <dbl> -7.907122, 1.751478, 14.472570, -1.165421, …\n#> $ harm   <dbl> -5.075312, -4.155847, -4.540677, -5.668406,…\n#> $ stabi  <dbl> -3.138836, 3.520752, 4.070600, 2.600437, -6…\n#> $ ppsych <dbl> -17.800210, 7.009367, 23.734260, 1.493158, …\n#> $ ses    <dbl> 4.766450, -6.048681, -16.970670, 1.396363, …\n#> $ verbal <dbl> -3.633360, -7.693461, -3.909941, 21.409450,…\n#> $ read   <dbl> -3.488981, -4.520552, -4.818170, -3.138441,…\n#> $ arith  <dbl> -9.989121, 8.196238, 7.529984, 5.730547, -0…\n#> $ spell  <dbl> -6.567873, 8.778973, -5.688716, -2.915676, …\nsemdatacov <- as.matrix(cov(semdata))\nsjPlot::tab_corr(semdatacov, triangle = \"lower\")"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"multivariat-regresjon","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.2 Multivariat regresjon","text":"Multivariat regresjon skiller seg fra multippel regresjon ved vi har flere avhengige variabler (multippel regresjon har flere uavhengige variabler, men kun 1 avhengig variabel).dette eksempelet er vår hypotese motivasjon predikerer både lesing og regning, mens foreldrepåvirkning kun predikerer lesing (sammen med motivasjon).SEM-modell multivariat regresjon\\(\\gamma_{11}, \\gamma_{12}\\ og\\ \\gamma_{22}\\ er\\ ladningen\\ på\\ de\\ latente\\ variablene\\)\n\\(\\phi_{11}\\ og \\phi_{22}\\ er\\ variansen\\ \\ x_1\\ og\\ x_2\\ (de\\ observerte\\ variablene\\ x_1\\ og\\ x_2)\\)\n\\(\\phi_{12}\\ er\\ kovariansen\\ mellom\\ de\\ \\ eksogene\\ variablene\\ x_1\\ og\\ x_2\\)\n\\(\\zeta_1\\ og\\ \\zeta_2\\ er\\ residualene\\ til\\ de\\ \\ latente\\ variablene\\ y_1\\ og\\ y_2\\)\n\\(\\psi_{11}\\ og\\ \\psi_{22}\\ er\\ variansen\\ \\ residualene\\ til\\ de\\ latente\\ variablene\\)\n\\(\\psi_{12}\\ er\\ kovariansen\\ mellom\\ residualene\\ til\\ de\\ latente\\ variablene\\)delen “Regressions” ser vi forholdet mellom lesing og foreldrepåvirkning er -0.216, og 0.476 mellom lesing og motivasjon. Dvs. hver enhet økning foreldrepåvirkning synker lesing med 0.216 når det er kontrollert påvirkningen fra motivasjon. På samme måte: en enhets økning motivasjon øker lesing med 0.476 når vi har kontrollert foreldrepåvirkningen. Vi ser også regning øker med 0.6 når motivasjon øker med en enhet.Vi kan imidlertid tenke oss foreldrepåvirkning også predikerer regning slik vi får følgende modell:SEM-modell #2 multivariat regresjonVi får da:Vi kan tolke dette på samme måte som første modell.","code":"\nsemmultivregr <- '\n    read ~ ppsych + motiv\n    arith ~ motiv'\nfitsemmultivregr <- sem(semmultivregr, data=semdata)\nsummary(fitsemmultivregr)\n#> lavaan 0.6-12 ended normally after 17 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         6\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 6.796\n#>   Degrees of freedom                                 1\n#>   P-value (Chi-square)                           0.009\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   read ~                                              \n#>     ppsych           -0.216    0.030   -7.289    0.000\n#>     motiv             0.476    0.037   12.918    0.000\n#>   arith ~                                             \n#>     motiv             0.600    0.036   16.771    0.000\n#> \n#> Covariances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>  .read ~~                                             \n#>    .arith            39.179    3.373   11.615    0.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read             65.032    4.113   15.811    0.000\n#>    .arith            63.872    4.040   15.811    0.000\nsemmultivregr2 <- '\n    read ~ 1 + ppsych + motiv\n    arith ~ 1 + ppsych + motiv'\nfitsemmultivregr2 <- sem(semmultivregr2, data=semdata)\nsummary(fitsemmultivregr2)\n#> lavaan 0.6-12 ended normally after 25 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         9\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 0.000\n#>   Degrees of freedom                                 0\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   read ~                                              \n#>     ppsych           -0.275    0.037   -7.385    0.000\n#>     motiv             0.461    0.037   12.404    0.000\n#>   arith ~                                             \n#>     ppsych           -0.096    0.037   -2.616    0.009\n#>     motiv             0.576    0.037   15.695    0.000\n#> \n#> Covariances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>  .read ~~                                             \n#>    .arith            38.651    3.338   11.579    0.000\n#> \n#> Intercepts:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read             -0.000    0.360   -0.000    1.000\n#>    .arith             0.000    0.355    0.000    1.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read             64.708    4.092   15.811    0.000\n#>    .arith            63.010    3.985   15.811    0.000"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"stianalyse","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.3 Stianalyse","text":"Vi skal bygge videre på den første multivariate regresjonsmodellen , men nå tenke oss de endogende variablene påvirker hverandre. Dette kan vi illustrere slik:SEM-modell stianalyseVi ser vi har en tilleggshypotese om lesing predikerer regning.Det vi først og fremst vil kommentere er regning predikeres større grad av lesing enn av motivasjon - en enhets økning motivasjon gir økning regning på 0.296, mens tilsvarende en enhets økning lesing gir økning på 0.573.Som Lin (2021b) poengterer er modellen “-saturated” (vi går ikke inn detaljer rundt det, men vi kan se antall frihetsgrader er positiv slik vi prinsippet kan legge til flere stier - avhengig av hvor mange frihetsgrader som er “tilgjengelig”). Hvis vi ikke har noen sterk hypotese om hvilken sti dette bør være kan man gjennomføre en kjikvadrattest (“modification index”) som vil vise hvordna kjikvadratverdien vil endre seg som følge av å inkludere en gitt parameter til modellen:Den klart mest innflytelsesrike parameteren er motiv ~ arith. Forventet endring regresjonskoeffisienten (“expected parameter change” - epc) er 8.874.\nVi går ikke videre med revidert modell, og det er verdt å poengtere Lin (2021b) advarsel om forsiktighet å endre modellen “fordi vi kan”. Faren Type feil kan stige drastisk.Vi skal til slutt se på hvor god modellen er (“fit statistics”).CFI - Confirmatory Factor Index: Vil være en verdi mellom 0 og 1. Verdier 0.9, evt 0.95 en mer konsvervativ tilnærming, vurderes som bra. Siden vi har 0.994 indikerer det en god modell.CFI - Confirmatory Factor Index: Vil være en verdi mellom 0 og 1. Verdier 0.9, evt 0.95 en mer konsvervativ tilnærming, vurderes som bra. Siden vi har 0.994 indikerer det en god modell.TLI - Tucker Lewis Index: Også verdier mellom 0 og 1, og 0.9 som en foreslått grense god modell. Vi har 0.971.TLI - Tucker Lewis Index: Også verdier mellom 0 og 1, og 0.9 som en foreslått grense god modell. Vi har 0.971.RMSEA - Root Mean Square Error Approximation: Kline (2016) foreslår verdier eller lik 0.05 er “close fit”, mellom 0.05 og 0.08 er “reasonable fit”, mens verdier 0.08 er “poor fit”. Siden vi har 0.088 har vi en “reasonable fit”.RMSEA - Root Mean Square Error Approximation: Kline (2016) foreslår verdier eller lik 0.05 er “close fit”, mellom 0.05 og 0.08 er “reasonable fit”, mens verdier 0.08 er “poor fit”. Siden vi har 0.088 har vi en “reasonable fit”.","code":"\nstianalyse <- '\n    read ~ 1 + ppsych + motiv\n    arith ~ 1 + motiv + read\n'\nfitstianalyse <- sem(stianalyse, data=semdata)\nsummary(fitstianalyse)\n#> lavaan 0.6-12 ended normally after 1 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         8\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 4.870\n#>   Degrees of freedom                                 1\n#>   P-value (Chi-square)                           0.027\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   read ~                                              \n#>     ppsych           -0.275    0.037   -7.385    0.000\n#>     motiv             0.461    0.037   12.404    0.000\n#>   arith ~                                             \n#>     motiv             0.296    0.034    8.841    0.000\n#>     read              0.573    0.034   17.093    0.000\n#> \n#> Intercepts:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read              0.000    0.360    0.000    1.000\n#>    .arith             0.000    0.284    0.000    1.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read             64.708    4.092   15.811    0.000\n#>    .arith            40.314    2.550   15.811    0.000\nmodindices(fitstianalyse, sort = TRUE)\n#>       lhs op    rhs     mi    epc sepc.lv sepc.all sepc.nox\n#> 21  motiv  ~  arith 68.073  8.874   8.874    8.874    8.874\n#> 14   read ~~  arith  4.847 16.034  16.034    0.314    0.314\n#> 16  arith  ~ ppsych  4.847  0.068   0.068    0.068    0.007\n#> 15   read  ~  arith  4.847  0.398   0.398    0.398    0.398\n#> 18 ppsych  ~  arith  2.188  0.071   0.071    0.071    0.071\n#> 20  motiv  ~   read  0.000  0.000   0.000    0.000    0.000\n#> 17 ppsych  ~   read  0.000  0.000   0.000    0.000    0.000\n#> 22  motiv  ~ ppsych  0.000  0.000   0.000    0.000    0.000\n#> 11  motiv ~~  motiv  0.000  0.000   0.000    0.000    0.000\n#> 10 ppsych ~~  motiv  0.000  0.000   0.000       NA    0.000\n#> 19 ppsych  ~  motiv  0.000  0.000   0.000    0.000    0.000\nsummary(fitstianalyse, fit.measures=TRUE)\n#> lavaan 0.6-12 ended normally after 1 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         8\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 4.870\n#>   Degrees of freedom                                 1\n#>   P-value (Chi-square)                           0.027\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                               674.748\n#>   Degrees of freedom                                 5\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    0.994\n#>   Tucker-Lewis Index (TLI)                       0.971\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)              -3385.584\n#>   Loglikelihood unrestricted model (H1)      -3383.149\n#>                                                       \n#>   Akaike (AIC)                                6787.168\n#>   Bayesian (BIC)                              6820.885\n#>   Sample-size adjusted Bayesian (BIC)         6795.492\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.088\n#>   90 Percent confidence interval - lower         0.024\n#>   90 Percent confidence interval - upper         0.172\n#>   P-value RMSEA <= 0.05                          0.139\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.015\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   read ~                                              \n#>     ppsych           -0.275    0.037   -7.385    0.000\n#>     motiv             0.461    0.037   12.404    0.000\n#>   arith ~                                             \n#>     motiv             0.296    0.034    8.841    0.000\n#>     read              0.573    0.034   17.093    0.000\n#> \n#> Intercepts:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read              0.000    0.360    0.000    1.000\n#>    .arith             0.000    0.284    0.000    1.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .read             64.708    4.092   15.811    0.000\n#>    .arith            40.314    2.550   15.811    0.000"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"strukturell-regresjon","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.4 Strukturell regresjon","text":"","code":""},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"en-endogen-variabel","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.4.1 En endogen variabel","text":"Vår hpotese denne modellen er de latente eksogene variablene Tilpasning og Risiko hver har 3 indikatorer (jfr. gjennomgang av datasettet starten av kapittelet). Vi har videre en latent endogen variabel - Prestasjon - med sine tre indikatorer. Begge “sidene” utgjør “measurement models”. tillegg har vi den strukturelle regresjonen der vi hypotetiserer Tilpasning positivt predikerer Prestasjon, mens Risiko negativt predikerer Prestasjon. viser vi modellen svært forenklet.SEM-modell strukturell regresjon - en endogen variabelVi kan også vise modellen grafisk. illustrasjonen har vi spesifisert vi kun ønsker signifikante stier (< 0.05).","code":"\nstrukregrsem <- '\nadjust =~ motiv + harm + stabi\nrisk =~ verbal + ppsych + ses\nachieve =~ read + arith + spell\nachieve ~ adjust + risk\n'\nfitstrukregrsem <- sem(strukregrsem, data=semdata)\nsummary(fitstrukregrsem, standardized=TRUE, fit.measures=TRUE)\n#> lavaan 0.6-12 ended normally after 130 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        21\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                               148.982\n#>   Degrees of freedom                                24\n#>   P-value (Chi-square)                           0.000\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                              2597.972\n#>   Degrees of freedom                                36\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    0.951\n#>   Tucker-Lewis Index (TLI)                       0.927\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)             -15517.857\n#>   Loglikelihood unrestricted model (H1)     -15443.366\n#>                                                       \n#>   Akaike (AIC)                               31077.713\n#>   Bayesian (BIC)                             31166.220\n#>   Sample-size adjusted Bayesian (BIC)        31099.565\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.102\n#>   90 Percent confidence interval - lower         0.087\n#>   90 Percent confidence interval - upper         0.118\n#>   P-value RMSEA <= 0.05                          0.000\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.041\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   adjust =~                                           \n#>     motiv             1.000                           \n#>     harm              0.884    0.041   21.774    0.000\n#>     stabi             0.695    0.043   15.987    0.000\n#>   risk =~                                             \n#>     verbal            1.000                           \n#>     ppsych           -0.770    0.075  -10.223    0.000\n#>     ses               0.807    0.076   10.607    0.000\n#>   achieve =~                                          \n#>     read              1.000                           \n#>     arith             0.837    0.034   24.437    0.000\n#>     spell             0.976    0.028   34.338    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     9.324    0.933\n#>     8.246    0.825\n#>     6.478    0.648\n#>                   \n#>     7.319    0.733\n#>    -5.636   -0.564\n#>     5.906    0.591\n#>                   \n#>     9.404    0.941\n#>     7.873    0.788\n#>     9.178    0.919\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   achieve ~                                           \n#>     adjust            0.375    0.046    8.085    0.000\n#>     risk              0.724    0.078    9.253    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     0.372    0.372\n#>     0.564    0.564\n#> \n#> Covariances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   adjust ~~                                           \n#>     risk             32.098    4.320    7.431    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     0.470    0.470\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .motiv            12.870    2.852    4.512    0.000\n#>    .harm             31.805    2.973   10.698    0.000\n#>    .stabi            57.836    3.990   14.494    0.000\n#>    .verbal           46.239    4.788    9.658    0.000\n#>    .ppsych           68.033    5.068   13.425    0.000\n#>    .ses              64.916    4.975   13.048    0.000\n#>    .read             11.372    1.608    7.074    0.000\n#>    .arith            37.818    2.680   14.109    0.000\n#>    .spell            15.560    1.699    9.160    0.000\n#>     adjust           86.930    6.830   12.727    0.000\n#>     risk             53.561    6.757    7.927    0.000\n#>    .achieve          30.685    3.449    8.896    0.000\n#>    Std.lv  Std.all\n#>    12.870    0.129\n#>    31.805    0.319\n#>    57.836    0.580\n#>    46.239    0.463\n#>    68.033    0.682\n#>    64.916    0.650\n#>    11.372    0.114\n#>    37.818    0.379\n#>    15.560    0.156\n#>     1.000    1.000\n#>     1.000    1.000\n#>     0.347    0.347\nlavaanPlot(model = fitstrukregrsem, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, sig = .05)"},{"path":"structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html","id":"to-endogene-variabler","chapter":"Kapittel 13 Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)","heading":"13.4.2 To endogene variabler","text":"En annen mulig hypotese er denne:SEM-modell strukturell regresjon - endogene variablerRisiko predikerer denne modellen Tilpasning, og den endogene variabelen Tilpasning og den eksogene variabelen Risiko predikerer den endogene variabelen Prestasjon.","code":"\nstrukregrsem2 <- '\nadjust =~ motiv + harm + stabi\nrisk =~ verbal + ses + ppsych\nachieve =~ read + arith + spell\nadjust ~ risk \nachieve ~ adjust + risk\n'\nfitstrukregrsem2 <- sem(strukregrsem2, data=semdata)\nsummary(fitstrukregrsem2, standardized=TRUE, fit.measures=TRUE)\n#> lavaan 0.6-12 ended normally after 112 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        21\n#> \n#>   Number of observations                           500\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                               148.982\n#>   Degrees of freedom                                24\n#>   P-value (Chi-square)                           0.000\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                              2597.972\n#>   Degrees of freedom                                36\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    0.951\n#>   Tucker-Lewis Index (TLI)                       0.927\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)             -15517.857\n#>   Loglikelihood unrestricted model (H1)     -15443.366\n#>                                                       \n#>   Akaike (AIC)                               31077.713\n#>   Bayesian (BIC)                             31166.220\n#>   Sample-size adjusted Bayesian (BIC)        31099.565\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.102\n#>   90 Percent confidence interval - lower         0.087\n#>   90 Percent confidence interval - upper         0.118\n#>   P-value RMSEA <= 0.05                          0.000\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.041\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   adjust =~                                           \n#>     motiv             1.000                           \n#>     harm              0.884    0.041   21.774    0.000\n#>     stabi             0.695    0.043   15.987    0.000\n#>   risk =~                                             \n#>     verbal            1.000                           \n#>     ses               0.807    0.076   10.607    0.000\n#>     ppsych           -0.770    0.075  -10.223    0.000\n#>   achieve =~                                          \n#>     read              1.000                           \n#>     arith             0.837    0.034   24.437    0.000\n#>     spell             0.976    0.028   34.338    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     9.324    0.933\n#>     8.246    0.825\n#>     6.478    0.648\n#>                   \n#>     7.319    0.733\n#>     5.906    0.591\n#>    -5.636   -0.564\n#>                   \n#>     9.404    0.941\n#>     7.873    0.788\n#>     9.178    0.919\n#> \n#> Regressions:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   adjust ~                                            \n#>     risk              0.599    0.076    7.837    0.000\n#>   achieve ~                                           \n#>     adjust            0.375    0.046    8.085    0.000\n#>     risk              0.724    0.078    9.253    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     0.470    0.470\n#>                   \n#>     0.372    0.372\n#>     0.564    0.564\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .motiv            12.870    2.852    4.512    0.000\n#>    .harm             31.805    2.973   10.698    0.000\n#>    .stabi            57.836    3.990   14.494    0.000\n#>    .verbal           46.239    4.788    9.658    0.000\n#>    .ses              64.916    4.975   13.048    0.000\n#>    .ppsych           68.033    5.068   13.425    0.000\n#>    .read             11.372    1.608    7.074    0.000\n#>    .arith            37.818    2.680   14.109    0.000\n#>    .spell            15.560    1.699    9.160    0.000\n#>    .adjust           67.694    6.066   11.160    0.000\n#>     risk             53.561    6.757    7.927    0.000\n#>    .achieve          30.685    3.449    8.896    0.000\n#>    Std.lv  Std.all\n#>    12.870    0.129\n#>    31.805    0.319\n#>    57.836    0.580\n#>    46.239    0.463\n#>    64.916    0.650\n#>    68.033    0.682\n#>    11.372    0.114\n#>    37.818    0.379\n#>    15.560    0.156\n#>     0.779    0.779\n#>     1.000    1.000\n#>     0.347    0.347\nlavaanPlot(model = fitstrukregrsem2, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, sig = .05)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","text":"R-pakker brukt dette kapittelet:","code":"\npacman::p_load(readxl, car, rgl, flextable, plotly, latex2exp, ggfortify, gridExtra, factoextra, corrplot, Directional, tidyverse, palmerpenguins, psych, paran, kableExtra, multiUS, xtable, GPArotation, EFAtools, nFactors, rstatix, calibrate, Matrix, summarytools, lavaan, lavaanPlot, haven, skimr, psycho, semPlot, gridExtra, kfa, multiUS)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"innledning-2","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.1 Innledning","text":"“En faktoranalyse er en analyseteknikk som brukes å forstå korrelasjonsstrukturen et sett av observerte variabler” (Bjerkan 2007, s.221). følge Mehmetoglu Mittner (2020) brukes disse statistiske teknikkene praksis som metoder som reduserer et større antall variabler til et mindre antall variabler uten å miste vesentlig informasjon om dataene prosessen. De omtales derfor ofte som datareduksjonsteknikker (“data reduction” eller “dimension reduction”). Ofte vil vi bruke faktoranalyse å kunne si noe om såkalte latente (eller skjulte) variabler samfunnsvitenskapene - forhold vi ikke kan måle direkte, men som vi kan uttrykke gjennom å måle/observere en rekke andre forhold/variabler som vi så “samler” en konstruert variabel gjennom nettopp faktoranalyse. Målet med faktoranalysen er da følge Tinsley Tinsley (1987) “achieve parsimony using smallest number explanatory concepts explain maximum amount common variance correlation matrix”. Det vi ønsker å finne er variabler som er korrelerte med hverandre, men relativt ukorrelerte med andre grupper/subset av variabler (som igjen er interkorrelerte egen gruppe/subset) (Tabachnik Fidell 2007). Den grunnleggende utfordringen er vi ønsker å representere et stort antall variabler på en enklere måte, men hvis vi velger få faktorer mister vi informasjon (noe som går ut påliteligheten) og hvis vi velger mange kan vi ende opp med en modell som er komplisert og vanskelig å tolke.Innledningsvis er det nødvendig å gjøre en grunnleggende begrepsavklaring rundt begrepet faktoranalyse da det mange sammenhenger framstår som om begreper blandes sammen og det kan være uklart hva man egentlig snakker om. Begrepene faktoranalyse og komponentanalyse (“Factor Analysis” - FA - og “Principal Component Analysis” - PCA) brukes ofte om hverandre og noen ganger er det uklart hva som er hva (eller hvert fall hva forfatteren mener). En klargjørende framstilling kan man f.eks. finne Jöreskog, Olsson, Wallentin (2016). Vi vil det følgende skille mellom faktoranalyse (EFA og CFA), og komponentanalyse (PCA).Felles begge er:de er metoder datareduksjonde brukes å uttrykke multivariate data gjennom færre dimensjoner enn det opprinnelige datasettetde er metodikker å identifisere mønstre korrelasjonen mellom variabler.En grunnleggende forskjell er PCA er en deskriptiv teknikk mens faktoranalyse er modelleringsteknikker (Unkel Trendafilov 2010). PCA blir dermed “en empirisk oppsummering av datamaterialet” (Bjerkan 2007, s.225).\nStrengt tatt er ikke PCA en faktoranalyse, men omtales (dessverre noe forvirrende) som sagt ofte som en faktoranalyse. SPSS heter f.eks. menypunktet «Data reduction» og man velger «Factor» neste valg hvilket innebærer en EFA. SPSS er begrenset til EFA, og man trenger AMOS SPSS som strengt tatt er et SEM-program (SEM = Structural Equation Modelling - en relatert metodikk til CFA som vi behandler et senere kapittel).Grafisk kan forskjellene vises slik:venstre del (PCA) kombineres fire målte variabler (\\(X_1...X_4\\)) til en komponent (\\(C\\)). Pilene indikerer det er variablene som bidrar til å skape komponenten, og de kan gjøre det med ulike styrke (vekt), som er vist med \\(w_1...w_4\\). Variablene \\(X_1...X_4\\) utgjør altså ulike størrelser på bidraget til komponenten \\(C\\).figurens høyre del ser vi en faktor \\(F\\) som skaper de fire målte variablene (\\(Y_1...Y_4\\)). Dette vises ved pilene går fra \\(F\\) til \\(Y_1...Y_4\\). \\(F\\) kan typisk være en latent variabel vi ikke kan observere direkte - som f.eks. intelligens eller angst. Også er det ulike vekter, så \\(F\\) kan påvirke \\(Y_1...Y_4\\) med ulik styrke. tillegg har vi et feilledd (\\(e_1...e_4\\)). \\(e_1\\) representerer f.eks. den delen av variansen \\(Y_1\\) som ikke forklares av \\(F\\). Vi kan uttrykke sammenhengen en enkelt varaibel som \\(Y_1\\) som en regresjonslikning: \\(Y_1 = b_1*F + e_1\\) (og tilsvarende \\(Y_2...Y_4\\)).Som illustrert figuren består variansen til variabelen X av tre deler: felles varians med andre variabler, unik varians selve variabel X og målefeil. PCA søker å forklare varians variabel X som en komponent (derav komponentanalyse), mens faktoranalyse kun søker å forklare den delen av den totale variansen som er felles (eller med andre ord: korrelasjonen mellom variablene).Modifisert fra Bjerkan (2007), s. 225, fig. 10.1Matematisk er forskjellen mellom faktoranalyse og PCA altså hvordan varians blir analysert – PCA blir varians analysert, faktoranalyse blir kun delt varians («shared variance») analysert. Eller med andre ord: PCA analyserer varians (felles varians, unik varians og målefeil), FA analyserer kovarians (variabelens felles varians med andre variabler).Teoretisk er forskjellen mellom de FA ses faktoren som årsaker til variabelen, mens PCA ses variablene som årsaken til komponentene; PCA er det ingen teoretisk forventning om hvilke variabler som forbindes med hvilke komponenter – det er kun empirisk assosiert (Tabachnik Fidell 2007).En annen måte å illustrere forskjellen mellom PCA og EFA er gitt av Bastos (2021).Fra Bastos (2021)figuren representerer ’ene spesifikk varians, B’ene felles varians og C’ene feilvarians (jfr. figuren fra Bjerkan (2007) lenger opp). Mens vi PCA vil bruke varians (, B og C) bruker vi kun B EFA.Praktisk er det imidlertid ikke helt trivielt å avgjøre om man skal bruke PCA eller EFA. Guadagnoli Velicer (1988) konkluderer også en litteraturundersøkelse med resultatene fra PCA stor grad er like som resultatene fra faktoranalyse. Med minst 30 variabler vil løsningene være mer eller mindre like, men med 20 variabler kan forskjeller inntreffe (J. P. Stevens 2002). Som Field (2009a) oppsummerer: “However, non-statistician difference principal component factor may difficult conceptualize (linear models), difference arises largely calculation” (s.760).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2 Principal Component Analysis (PCA)","text":"Tabachnik Fidell (2007) foreslår dersom du ønsker en «empirisk oppsummering» av datasettet og redusere et større antall variabler til et mindre antall komponenter er PCA riktig valg. en PCA transformeres et antall korrelerte variabler til et mindre antall «principal components».","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.1 PCA gjennom et lite eksempel","text":"La oss se på hva PCA er. Vi tar utgangspunkt et lite, kontruert datasett der vi har registrert karakterer ulike fag på 10 studenter. Dette eksempelet er modifisert fra Starmer (2018a), med innslag fra Pittard (2012).studentnrmatte1792763784755426457418469501049Vi kan plotte den ene variabelen - matte.Vi kan se studentgruppen deler seg klare grupper, en gruppe med høy score og en gruppe med lavere score. Studentene gruppa med høy score likner mer på hverandre enn på studenter den andre gruppa, og motsatt.Hvis vi legger til en variabel - f.eks. engelskkarakterer - får vi denne tabellen:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Som vi også kan plotte:Vi kan nå si vi har dimensjoner hver student: den første dimensjonen - x-aksen - inneholder mattekarakterer, den andre dimensjonen - y-aksen - inneholder engelskkarakterer. Vi kan se også med dimensjoner det er tydelige clustere.Så kan vi legge til nok en karakter - denne gangen norsk.studentnrmatteengelsknorsk17979672766868378716947570665425348645554674162478466050950645110494949Med tre dimensjoner må vi har tre akser plottet:Hvis vi nå legger til fysikkarakterer tillegg kan vi ikke lenger plotte dette siden det vil kreve fire dimensjoner. PCA stepper inn og lager et 2D plott av flere dimensjoner. PCA vil også kunne si oss noe om hvilken av karakterene (= hvilken av variablene) som er viktigst å skape klyngene/grupperingene av studenter.","code":"\nplot(karakterer$matte, xlab = \"studentnr\", ylab = \"matte\")\nplot(karakterer$matte, karakterer$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nfig <- plot_ly(karakterer, x = matte, y = norsk, z = engelsk)\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'Matte'),\n                                   yaxis = list(title = 'Norsk'),\n                                   zaxis = list(title = 'Engelsk')))\nfig"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sentrering","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.1.1 Sentrering","text":"Vi går tilbake til datasettet med karakterer (dimensjoner).studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi regner så ut gjennomsnittet de variablene matte og engelsk:Vi bruker disse gjennomsnittsverdiene til å kalkulere senterpunktet (x = 58.1, y = 63.1) som vi deretter - ved å beholde de innbyrdes avstandene x- og y-planet mellom datapunktene - sentrerer alle observasjonene rundt.Vi ser datapunktene ligger nøyaktig likt forhold til hverandre, men senterpunktet er nå (0, 0). Neste steg er å lage en regresjonslinje som passer best mulig til dataene. Vi begynner med å legge på en hvilken som helst linje som går gjennom (0, 0) og deretter roterer vi linja med (0, 0) som pivoteringspunkt til man finner linja som passer best (dette tilfellet den røde stiplede linja):Og hvordan vet PCA hvilken linje som passer best? å se på det skal vi ta en liten omveg ut av eksempelet vårt.","code":"\nkartabell2#> [1] 58.1\n#> [1] 63.1\ncenter_scale <- function(x) {\n    scale(x, scale = FALSE)\n}\nsentrert <- as.data.frame(center_scale(karakterer))  \nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,3), col = \"blue\", lty = 3)\nabline(coef = c(0,2.5), col = \"blue\", lty = 3)\nabline(coef = c(0,2), col = \"blue\", lty = 3)\nabline(coef = c(0,1.5), col = \"blue\", lty = 3)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nabline(coef = c(0,0.75), col = \"red\", lty = 3)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forskjell-i-utregning-av-avvik---ols-og-pca","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.1.2 Forskjell i utregning av avvik - OLS og PCA","text":"Med utgangspunkt Long (2010) kan vi illustrere den prinsippielle forskjellen hvordan hhv. OLS (lineær regresjon) og PCA kalkulerer beste tilpasning.en OLS (jfr. teori kapittel om regresjonsanalyse) vil man søke å finne beste tilpasning:OLS forsøker å minimere feilleddet mellom den avhengige variabelen og modellen ved å regne på alle avstandene (og kvadrere dem) mellom datapunktene og modellen. grafen er dette illustrert med oransje strek av datapunktene.Hvis vi bytter om på den avhengige og uavhengige variabelen ser det OLS slik ut av datapunktene:OLS forsøker alltid minimere y-avstanden (feilleddet = \\(y-\\hat{y}\\)). PCA vil minimere feilleddet ortogonalt (90\\(^\\circ\\) på modellen):PCA vil rotere modellen (“regresjonslinja”) rundt et senterpunkt og hele tiden kalkulere summen av de ortogonale feilleddene. Når du har en litt større mengde uavhengige variabler vil PCA gi deg hvilke lineære kombinasjoner som teller mest. En snasen forklaring og illustrasjon kan de se herÅ gå dybden på den matematiske utregningen av eigenvalues og eigenvectors er på grensen av dybdekunnskap et notat med tittel “Anvendt…” seg, og trolig kan man leve godt uten denne dybdekunnskapen. En anbefalt kilde å gå dybden kan være L. . Smith (2002). Likevel vil vi skissere hvordan dette gjøres det enkle eksempelet.","code":"\nset.seed(2)\nx <- 1:100\ny <- 20 + 3 * x\ne <- rnorm(100, 0, 60)\ny <- 20 + 3 * x + e\n# OLS-regresjon y ~ x\nplot(x,y)\nyx.lm <- lm(y ~ x)\nlines(x, predict(yx.lm), col=\"red\")\narrows(x0 = 61, x1= 61, y0 = 100, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 54, x1= 54, y0 = 260, y1 = 180, lwd = 2, col = \"orange\")\n# OLS-regresjon x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"blue\")\narrows(x0 = 84, x1= 55, y0 = 190, y1 = 190, lwd = 2, col = \"orange\")\narrows(x0 = 32, x1= 44, y0 = 137, y1 = 137, lwd = 2, col = \"orange\")\n# PCA x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"green\")\narrows(x0 = 54, x1= 57, y0 = 260, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 42, x1= 38, y0 = 32, y1 = 109, lwd = 2, col = \"orange\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel---del-2","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2 PCA gjennom et lite eksempel - del 2","text":"Vi tar utgangspunkt eksempelet der vi hadde variabler som er sentrert:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi legger på en tilfeldig linje som går gjennom (0, 0).Ettersom vi roterer linja vil avstanden \\(\\) ikke forandre seg. Lengden på både \\(b\\) og \\(c\\) vil imidlertid endre seg relativt til hverandre. Når \\(b\\) blir lengre, blir \\(c\\) kortere og motsatt. samme tanke som OLS-regresjon (se ovenfor) ønsker vi \\(b\\) skal være så kort som mulig da det betyr linja ligger så nærme datapunktet som mulig. å få \\(b\\) så kort som mulig jobber iidlertid PCA å maksimere \\(c\\) (det har samme effekt: når \\(c\\) er på sitt maksimale er \\(b\\) på sitt minimale). PCA finner altså den beste linja ved å maksimere den kvadrerte avstanden \\(c\\) (kvadrert pga. Pythagoras… \\(^2 + b^2 = c^2\\)).PCA summerer da \\(c^2\\) alle de 10 punktene dette eksempelet. Siden verdiene er kvadrerte slipper vi også problemet med positive og negative verdier nuller hverandre ut (som OLS - Ordinary Least Squares selv om vi ikke regner squares men avstand). Summen = sum squared distances SS(distances).","code":"\nkartabell2\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\ntext(10, -8, TeX('$\\\\c_{1}^2$ ... $\\\\c_{10}^2$ = SS(distances)'))"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"identifikasjon-av-principal-component-1-pc1-eigenvectors-og-loading-scores","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.1 Identifikasjon av Principal Component 1 (PC1), eigenvectors og loading scores","text":"PCA roterer som sagt på linja pivotert (0, 0) og til slutt finner den linja som maksimaliserer SS(Distances). Denne linja kalles Principal Component 1 (PC1).PC1 har et stigningstall på 0.4639586. Det betyr en økning på 1 x (matte) gir en økning på 0.46 y (engelsk). Det betyr matte har større innvirkning på grupepringen av studentene enn engelsk.PCA skaleres \\(c\\) alltid til 1. Matematisk gjør vi det ved å dele hver side av Pythagoras med \\(c\\) (1.1).Disse verdiene - 0.91 og 0.42 - kalles Eigenvector PC1, og de verdiene kalles gjerne Loading Scores. Som vist grafen er \\(SS(Distances) = Eigenvalue PC1\\). Eigenvalue er et begrep vi kommer tilbake til PCA når vi skal velge ut hvor mange komponenter vi skal beholde.Det neste vi kan se på da er Principal Component 2 (PC2). PC2 er linja som går vinkelrett på PC1 og gjennom (0, 0). PC2 er altså linja som reflekterer den nest største kilden til variasjon dataene, men som er ortogonal (vinkelrett) på PC1. Dette betyr eigenvectoren er “snudd” og blir -0.42 og 0.91 (som altså er loading scores PC2).å få fram det endelige PCA-plottet roteres løsningen slik PC1 utgjør x-aksen og PC2 y-aksen.Punktene plottes deretter på det roterte diagrammet. Til dette brukes trigonometri. La oss se på et enkelt eksempel ett punkt - dette gjør vi selvsagt ikke manuelt.Vi har det opprinnelige plottet et koordinatsystem og har funnet PC1 og PC2, og snur først aksene. Så må datapunktene posisjoneres etter de roterte aksene. La oss vise med ett punkt (1,0) - altså punktet 1 på x-aksen, 0 på y-aksen.Punkt (1,0) blir da - forutsatt \\(\\alpha=30\\):\\(y'= sin(\\alpha) = 0.5\\)\\(x'= cos(\\alpha) = 0.9\\)(1,0) blir da rotert (0.5, 0.9). Slik gjør man alle datapunkter.","code":"\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, TeX('Pythagoras gir: a = 1, b = 0.46, c = 1.1'))\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, \"Pythagoras gir: a = 1, b = 0.46, c = 1.1\", cex = 0.75)\ntext(11, -4, \"PCA-skalerte verdier: a = 0.91, b = 0.42, c = 1\", cex = 0.75)\ntext(11, -6.5, \"SS(Distances) = Eigenvalue for PC1\", cex = 0.75)\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\nabline(a = 0, b = -4.5, col = \"blue\") \ntext(-4, 8, \"PC2\", srt = 20)\nkaraktererX2 <- within(karakterer2, rm(studentnr))\n# Skalerer data\nstandardize <- function(x) {(x - mean(x))}\nskalert_karaktererX2 <- apply(karaktererX2,2,function(x) (x-mean(x)))\nplot(skalert_karaktererX2, cex=0.9, xlim=c(-20,20))\n# Finner Eigenvalues fra kovariansematrisen\nmy.cov <- cov(skalert_karaktererX2)\nmy.eigen <- eigen(my.cov)\nrownames(my.eigen$vectors) <- c(\"matte\",\"engelsk\")\ncolnames(my.eigen$vectors) <- c(\"PC1\",\"PC\")\n# Sum Eigenvalues = den totale variansen i dataene\nsum(my.eigen$values)\n#> [1] 357.9778\nvar(skalert_karaktererX2[,1]) + var(skalert_karaktererX2[,2])\n#> [1] 357.9778\n# Eigenvektorene er principal components.\nloadings <- my.eigen$vectors\npc1.slope <- my.eigen$vectors[1,1]/my.eigen$vectors[2,1]\npc2.slope <- my.eigen$vectors[1,2]/my.eigen$vectors[2,2]\nabline(0,pc1.slope,col=\"red\")\nabline(0,pc2.slope,col=\"blue\")\ntextxy(12,10,\"(-0.710,-0.695)\",cx=0.9,dcol=\"red\")\ntextxy(-12,10,\"(0.695,-0.719)\",cx=0.9,dcol=\"blue\")\n# Hvor mye varians foklarer hver eigenvector\npc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=2)\npc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)\nxlab=paste(\"PC1 - \",pc1.var,\" % of variation\",sep=\"\")\nylab=paste(\"PC2 - \",pc2.var,\" % of variation\",sep=\"\")\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\n\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"litt-mer-i-detalj-om-rotasjon","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.2 Litt mer i detalj om rotasjon","text":"Det vi har gjort ovenfor med et lite eksempel kjøres på hele datasettet det verktøyet vi bruker (R, Stata, SPSS, osv.).Med utgangspunkt en datamatrise roteres dataene en gitt vinkel gjennom en rotasjonsmatrise til et ny datamatrise (PC1). Denne igjen roteres på samme måte til PC2.Vi kan se på komponentene:Og matrisen etter en rotasjon som vi kan plotte (som vi kjenner igjen fra litt lenger oppe):","code":"\nut1 <- princomp(karaktererX2, cor = T)\nsummary(ut1)\n#> Importance of components:\n#>                           Comp.1     Comp.2\n#> Standard deviation     1.3533352 0.41046786\n#> Proportion of Variance 0.9157581 0.08424193\n#> Cumulative Proportion  0.9157581 1.00000000\nutdata <- ut1$score\nutdata\n#>           Comp.1      Comp.2\n#>  [1,]  2.2283619 -0.34268382\n#>  [2,]  1.2036707  0.41133586\n#>  [3,]  1.5364460  0.25900821\n#>  [4,]  1.3202598  0.20452296\n#>  [5,] -1.5428918  0.09028815\n#>  [6,] -1.2458551  0.06392291\n#>  [7,] -0.8603493 -0.68247820\n#>  [8,] -0.7964907 -0.29521764\n#>  [9,] -0.2926411 -0.43817195\n#> [10,] -1.5505104  0.72947352\nplot(utdata)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"utvelgelse-av-antall-komponenter","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.3 Utvelgelse av antall komponenter","text":"Et sentralt element PCA er valg av antall komponenter man vil beholde. Dette kan vi gjøre gjennom variansen komponentnen (: PC1 og PC2). kommer vi tilbake til begrepet eigenvalues som vi definerte lenger opp. Vi kan se på eigenvalue en PC som hvor mange variabler som representeres av den respektive PC. Eigenvalues forholder seg til forklart varians slik:\\(Forklart\\ varians = \\frac{Eigenvalue}{antall\\ opprinnelige\\ variabler}\\)Dette kan også uttrykkes slik:\\(\\frac{SS(Distances PC1)}{n - 1} = Varians PC1\\)\\(\\frac{SS(Distances PC2)}{n - 1} = Varians PC2\\)Et viktig poeng er en PCA ikke reduserer antall variabler seg selv. Hvis du opprinnelig har 13 variabler vil du få 13 PC, men spørsmålet er hvor mange du faktisk trenger å se på å forklare variansen (hvilket du ønsker skal være færre enn det opprinnelige antall variabler). Og det er vi kommer fram til hele poenget med PCA: Hvir mange komponenter skal vi beholde?å vise uilke metoder å vurdere antall komponenter som bør beholdes bruker vi et datasett med flere variabler enn vårt eksempel ovenfor, modifisert fra DataViz (2020)) - vi går ikke inn på hva dataene er.","code":"\npenguins_data <- penguins[,c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")]\npenguins_data <- na.omit(penguins_data)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.3.1 Kaisers kriterium","text":"Kaiser (1960) kriterium baserer seg på å beholde alle komponenter med eigenvalue på 1,0. Enhver komponent med eigenvalue 1 forklarer mer varianse enn en enkeltvariabel. Med andre ord – ut fra denne måten å vurdere ønsker vi ikke å beholde komponenter som forklarer mindre varians enn enkeltvariabler. Det er generelt anbefalt man ikke bruker Kaisers kriterium alene da metoden har en tendens til å overvurdere antallet komponenter. Samtidig hevdes det det er umulig å tillegge en komponent med verdi 1,01 som viktig og en annen med verdi 0,99 som uviktig (Fabrigar et al. 1999).Ut fra dette bør vi beholde 1 komponent, men vi ser komponent 2 er svært nærme 1.","code":"\npca_obj <- prcomp(drop_na(penguins_data), scale. = TRUE)\npca_obj_eigen <- ((pca_obj$sdev)^2)\npca_obj_eigen\n#> [1] 2.77008681 0.99348866 0.77191746 0.36520940 0.09929767"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.3.2 Scree plott","text":"Et hjelpemiddel dette er scree plot (Cattell 1966). Et Scree Plot er en grafisk framstilling av komponentene langs x-aksen og de korresponderende eigenvalues på y-aksen. Et scree plot viser hvor stor del av variansen variabelen forklarer rundt f.eks. PC1.Når vi vurderer Scree Plot ønsker vi å identifisere knekkpunktet (også kalt «albuen»).La oss forutsette variansen PC1 = 13 og PC2 = 4. Dvs. den totale variansen = 17. Videre beytr det PC1 forklarer \\(\\frac{13}{17}=0.765\\) - altså 76.5% av den totale variansen. PC2 forklarer på sin side \\(\\frac{4}{17}=0.235\\). Et scree plot er en grafisk framstilling av denne variansen.vårt eksempel kan vi dermed fremstille dette scree plottet:Cattell (1966) beskriver framgangsmåten som man finner albuen og dropper alle komponenter etter komponenten som starter albuen (eller sagt på en annen måte: vi beholder alle komponentene knekkpunktet). vårt tilfelle indikerer det vi beholder 1 komponent. vårt scree plott er det et tydelig knekkpunkt, men mange tilfeller er det ikke så tydelig, og det kan være vanskelig å identifisere «det rette» knekkpunktet.En alternativ, og ofte brukt meetode å illlustrere et scree plot på, er å kombinere det med et histogram (eksempel fra Szczęsna (2022)).","code":"\nvar_explained_df <- data.frame(PC= paste0(\"PC\",1:5), var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2))\nvar_explained_df %>%\n  ggplot(aes(x=PC,y=var_explained, group=1))+\n  geom_point(size=4)+\n  geom_line()+\n  labs(title=\"Scree plott\")\nsummary(pca_obj)\n#> Importance of components:\n#>                          PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     1.664 0.9967 0.8786 0.60433 0.31512\n#> Proportion of Variance 0.554 0.1987 0.1544 0.07304 0.01986\n#> Cumulative Proportion  0.554 0.7527 0.9071 0.98014 1.00000\nfviz_eig(pca_obj, col.var=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.2.3.3 Parallell analyse","text":"Parallell anlayse (PA) (Horn 1965) sammenlikner korrelasjonsmatrisen fra våre data med tilfeldig genererte korrelasjonsmatriser med samme antall variabler og observasjoner, å sammenlikne eigenvalues de genererte med den observerte. eksempelet ber vi om 5000 tilfeldige korrelasjonsmatriser.","code":"\nparan(penguins_data, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           2.617826    2.770086      0.152260\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (1 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel-pca","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3 Eksempel PCA","text":"En anvendelse av PCA kan være vi ønsker å se på den underliggende strukturen en skalavariabel. skal vi bruke Pallants datasett som vi også brukte deler av kapittelet om regresjonsanalyse som du finner . PCA-eksempelet skal vi se på en av skalaene - PANAS - og har modifisert datasettet til å kun inneholde spørsmålene knyttet til denne skalaen.Download Pallant_survey_PANAS.xlsxDownload Pallant_survey_PANAS.savDownload Pallant_survey_PANAS.dtaDatasettet består av 20 spørsmål som utgjør PANAS skalaen (“Positive Negative Affect Schedule”).","code":"\nPallant_survey_PANAS <- as.data.frame(read_excel(\"Pallant_survey_PANAS.xlsx\"))\nPallant_survey_PANAS <- na.omit(Pallant_survey_PANAS)\nsummarytools::descr(Pallant_survey_PANAS)\n#> Descriptive Statistics  \n#> Pallant_survey_PANAS  \n#> N: 435  \n#> \n#>                        pn1     pn10     pn11     pn12     pn13     pn14     pn15     pn16     pn17\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.79     1.86     2.43     2.87     3.26     1.66     3.19     1.68     3.37\n#>           Std.Dev     0.91     1.07     1.04     1.19     1.00     0.95     1.12     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     2.00     3.00     1.00     2.00     1.00     3.00\n#>            Median     4.00     2.00     2.00     3.00     3.00     1.00     3.00     1.00     3.00\n#>                Q3     4.00     2.00     3.00     4.00     4.00     2.00     4.00     2.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     1.48     1.48     0.00     1.48     0.00     1.48\n#>               IQR     1.00     1.00     1.00     2.00     1.00     1.00     2.00     1.00     1.00\n#>                CV     0.24     0.58     0.43     0.41     0.31     0.57     0.35     0.60     0.30\n#>          Skewness    -0.67     1.16     0.47    -0.01    -0.33     1.59    -0.29     1.53    -0.35\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.26     0.54    -0.40    -0.94    -0.35     2.10    -0.65     1.67    -0.24\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                       pn18     pn19      pn2     pn20      pn3      pn4      pn5      pn6      pn7\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.42     2.19     2.56     1.72     1.74     3.16     1.41     3.72     3.60\n#>           Std.Dev     0.99     1.17     1.20     1.08     1.00     1.11     0.82     1.01     1.10\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     1.00     1.00     2.00     1.00     3.00     3.00\n#>            Median     3.00     2.00     2.00     1.00     1.00     3.00     1.00     4.00     4.00\n#>                Q3     4.00     3.00     3.00     2.00     2.00     4.00     2.00     4.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     0.00     0.00     1.48     0.00     1.48     1.48\n#>               IQR     1.00     2.00     1.00     1.00     1.00     2.00     1.00     1.00     1.00\n#>                CV     0.29     0.54     0.47     0.63     0.58     0.35     0.58     0.27     0.31\n#>          Skewness    -0.48     0.74     0.40     1.47     1.41    -0.27     2.33    -0.66    -0.53\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.02    -0.45    -0.81     1.25     1.25    -0.69     5.31     0.11    -0.33\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                        pn8      pn9\n#> ----------------- -------- --------\n#>              Mean     2.20     3.29\n#>           Std.Dev     1.17     1.01\n#>               Min     1.00     1.00\n#>                Q1     1.00     3.00\n#>            Median     2.00     3.00\n#>                Q3     3.00     4.00\n#>               Max     5.00     5.00\n#>               MAD     1.48     1.48\n#>               IQR     2.00     1.00\n#>                CV     0.53     0.31\n#>          Skewness     0.77    -0.38\n#>       SE.Skewness     0.12     0.12\n#>          Kurtosis    -0.33    -0.12\n#>           N.Valid   435.00   435.00\n#>         Pct.Valid   100.00   100.00\nresultat.pca <- prcomp(Pallant_survey_PANAS, scale = TRUE)\nsummary(resultat.pca)\n#> Importance of components:\n#>                           PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     2.4973 1.8443 1.1063 1.07642 0.94816\n#> Proportion of Variance 0.3118 0.1701 0.0612 0.05793 0.04495\n#> Cumulative Proportion  0.3118 0.4819 0.5431 0.60104 0.64599\n#>                            PC6    PC7     PC8     PC9\n#> Standard deviation     0.88714 0.8556 0.81001 0.80669\n#> Proportion of Variance 0.03935 0.0366 0.03281 0.03254\n#> Cumulative Proportion  0.68534 0.7219 0.75475 0.78729\n#>                           PC10    PC11    PC12    PC13\n#> Standard deviation     0.77118 0.76610 0.70723 0.70109\n#> Proportion of Variance 0.02974 0.02935 0.02501 0.02458\n#> Cumulative Proportion  0.81702 0.84637 0.87138 0.89595\n#>                           PC14    PC15    PC16    PC17\n#> Standard deviation     0.62728 0.61274 0.57483 0.54724\n#> Proportion of Variance 0.01967 0.01877 0.01652 0.01497\n#> Cumulative Proportion  0.91563 0.93440 0.95092 0.96589\n#>                           PC18    PC19    PC20\n#> Standard deviation     0.53320 0.47228 0.41805\n#> Proportion of Variance 0.01422 0.01115 0.00874\n#> Cumulative Proportion  0.98011 0.99126 1.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"valg-av-antall-komponenter","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.1 Valg av antall komponenter","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-og-eigenvalues","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.1.1 Kaisers kriterium og eigenvalues","text":"Ut fra Kaisers kriterium beholder vi fire komponenter.","code":"\neig.val <- as.data.frame(get_eigenvalue(resultat.pca))\neig.val\n#>        eigenvalue variance.percent\n#> Dim.1   6.2365752       31.1828761\n#> Dim.2   3.4015750       17.0078752\n#> Dim.3   1.2239359        6.1196796\n#> Dim.4   1.1586859        5.7934294\n#> Dim.5   0.8990120        4.4950602\n#> Dim.6   0.7870241        3.9351204\n#> Dim.7   0.7320491        3.6602453\n#> Dim.8   0.6561184        3.2805918\n#> Dim.9   0.6507440        3.2537200\n#> Dim.10  0.5947166        2.9735829\n#> Dim.11  0.5869064        2.9345320\n#> Dim.12  0.5001778        2.5008892\n#> Dim.13  0.4915331        2.4576653\n#> Dim.14  0.3934826        1.9674130\n#> Dim.15  0.3754471        1.8772354\n#> Dim.16  0.3304249        1.6521243\n#> Dim.17  0.2994713        1.4973567\n#> Dim.18  0.2843020        1.4215102\n#> Dim.19  0.2230511        1.1152553\n#> Dim.20  0.1747675        0.8738377\n#>        cumulative.variance.percent\n#> Dim.1                     31.18288\n#> Dim.2                     48.19075\n#> Dim.3                     54.31043\n#> Dim.4                     60.10386\n#> Dim.5                     64.59892\n#> Dim.6                     68.53404\n#> Dim.7                     72.19429\n#> Dim.8                     75.47488\n#> Dim.9                     78.72860\n#> Dim.10                    81.70218\n#> Dim.11                    84.63671\n#> Dim.12                    87.13760\n#> Dim.13                    89.59527\n#> Dim.14                    91.56268\n#> Dim.15                    93.43992\n#> Dim.16                    95.09204\n#> Dim.17                    96.58940\n#> Dim.18                    98.01091\n#> Dim.19                    99.12616\n#> Dim.20                   100.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-1","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.1.2 Scree plott","text":"Dette skulle indikere vi beholder komponenter.","code":"\nfviz_eig(resultat.pca, addlabels = TRUE)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-1","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.1.3 Parallell analyse","text":"Dette peker også mot vi bør beholde komponenter.","code":"\nparan(Pallant_survey_PANAS, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           5.836032    6.236575      0.400542\n#> 2           3.074520    3.401575      0.327054\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (2 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-låst-til-to-komponenter","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.2 PCA låst til to komponenter","text":"Vi ønsker å se på modellen med komponenter på tre parametere. Først ønsker vi mindre enn 50% av residualene skal ha absoluttverdi > 0.05.vårt tilfelle er 12.5% av residualene > 0.05.Den neste parameteren er model fit som bør være > 0.9.er verdien 0.8705564.Til slutt ser vi på “communalities”.Pallant (2010) foreslår å se etter verdier på 0.3. En lav verdi indikerer den respektive variabelen ikke passer godt sammen med de andre variablene sin respektive komponent. Man kan vurdere å se om modellen blir bedre ved å ta vekk variabler med lav verdi (f.eks. 0.3). vårt tilfelle er variabelen pn5 terskelverdien på 0.3. Vi kan prøve å ta den bort. Fra før ser vi modellen forklarer 48% (se “Cumulative Var” tabellen).Vi ser ingen forbedring kumulativ varians forklart.Model fit er marginalt bedre.","code":"\npca2 <- psych::principal(Pallant_survey_PANAS, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca2\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.14 0.50 0.50 1.1\n#> pn2  -0.15  0.70 0.52 0.48 1.1\n#> pn3  -0.11  0.73 0.54 0.46 1.0\n#> pn4   0.54 -0.12 0.31 0.69 1.1\n#> pn5  -0.12  0.49 0.26 0.74 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.16  0.73 0.55 0.45 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.2\n#> pn10 -0.01  0.60 0.35 0.65 1.0\n#> pn11 -0.15  0.65 0.44 0.56 1.1\n#> pn12  0.76 -0.04 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.73 0.55 0.45 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.58 0.35 0.65 1.1\n#> pn17  0.82 -0.12 0.69 0.31 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.04  0.79 0.62 0.38 1.0\n#> pn20 -0.08  0.71 0.51 0.49 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.89 4.75\n#> Proportion Var        0.24 0.24\n#> Cumulative Var        0.24 0.48\n#> Proportion Explained  0.51 0.49\n#> Cumulative Proportion 0.51 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  830.29  with prob <  2.5e-94 \n#> \n#> Fit based upon off diagonal values = 0.95\nantall_over <- length(pca2$residual[pca2$residual>0.05])\nantall_residualverdier <- nrow(pca2$residual)*ncol(pca2$residual)\n(antall_over/antall_residualverdier)*100\n#> [1] 12.5\npca2$fit\n#> [1] 0.8705564\nsort(pca2$communality)\n#>       pn5       pn4      pn16      pn10       pn6      pn11 \n#> 0.2574873 0.3064322 0.3512776 0.3542041 0.3751836 0.4394038 \n#>       pn7      pn15       pn9       pn1      pn20       pn2 \n#> 0.4503231 0.4608460 0.4746693 0.5034580 0.5064648 0.5160759 \n#>      pn13       pn3      pn14       pn8      pn18      pn12 \n#> 0.5377154 0.5422017 0.5471146 0.5534260 0.5717316 0.5848691 \n#>      pn19      pn17 \n#> 0.6201048 0.6851615\nPallant_survey_PANAS2 <- subset(Pallant_survey_PANAS, select = -(pn5))\npca3 <- psych::principal(Pallant_survey_PANAS2, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca3\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS2, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.15 0.51 0.49 1.1\n#> pn2  -0.14  0.71 0.53 0.47 1.1\n#> pn3  -0.11  0.74 0.56 0.44 1.0\n#> pn4   0.54 -0.11 0.31 0.69 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.15  0.74 0.57 0.43 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.1\n#> pn10 -0.01  0.60 0.36 0.64 1.0\n#> pn11 -0.14  0.65 0.45 0.55 1.1\n#> pn12  0.76 -0.05 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.74 0.56 0.44 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.55 0.31 0.69 1.1\n#> pn17  0.82 -0.13 0.68 0.32 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.03  0.80 0.64 0.36 1.0\n#> pn20 -0.08  0.72 0.52 0.48 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.87 4.55\n#> Proportion Var        0.26 0.24\n#> Cumulative Var        0.26 0.50\n#> Proportion Explained  0.52 0.48\n#> Cumulative Proportion 0.52 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  708.34  with prob <  6.1e-79 \n#> \n#> Fit based upon off diagonal values = 0.95\npca2$fit\n#> [1] 0.8705564\npca3$fit\n#> [1] 0.8785797"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forutsetninger","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.3 Forutsetninger","text":"Så langt har vi ikke sett på hvilke forutsetninger som må ligge til grunn å kunne kjøre en PCA. Det skal vi nå.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"størrelse-på-datasettetutvalgsstørrelse","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.3.1 Størrelse på datasettet/utvalgsstørrelse","text":"Antall cases (sample size) og forholdstallet mellom antall respondenter og antall variabler kan være av betydning en faktoranalyse. små utvalg er korrelasjonskoeffisientene mellom variablene mindre pålitelige/konsistente.Det finnes ulike anbefalinger, og Hogarty et al. (2005) hevder det ikke finnes et minimum hva angår \\(N\\) og \\(\\frac{N}{variabler}\\) å oppnå en god faktoranalyse. Arrindell van der Ende (1985) fant verken et bestemt forholdstall eller et minimumsantall observasjoner hadde påvirkning på faktorstabiliteten. Guadagnoli Velicer (1988) viste en faktor med fire eller flere faktorladninger på 0,6 eller høyere er stabil uavhengig av utvalgsstørrelsen. En faktor med 10 eller flere ladninger større enn 0,4 var stabil dersom utvalgsstørrelsen er minst 150. Antallet caser kan imidlertid ses opp mot hvor sterkt variablene lader på faktorene (Tabachnik Fidell 2007) og korrelasjonene (MacCallum et al. 1999) – høyere korrelasjoner (>.80) krever mindre sample size (Guadagnoli Velicer 1988).vårt datasett har vi 435 caser/observasjoner. Vi får da forholdstallet 22.9. Antallet og forholdstallet skulle utgangspunktet ikke være til hinder en faktoranalyse . Både Hair Jr. et al. (2010) og Nunally (1978) anbefaler et forholdstall på 10:1.\n##### Sphericity - er datasettet “faktoriserbart”Vi gjennomfører tester: Bartletts test sfæritet og KMO. Først Bartletts:ser vi p-verdien er terskelverdi på 0.05, så vi får dermed indikert dataene er egnet PCA etter dette kriteriet.Deretter KMO. KMO er en utregning som indikerer andelen av varians skalavariabelen som kan forklares av de underliggende faktorene. En høy verdi indikerer en faktoranalyse er mulig.Pallant (2010) anbefaler en grenseverdi på 0.60.\nKaiser (1974) anbefaler følgende retningslinjer KMO-verdier:KMO er derfor “respektabel”.","code":"\nkorrelasjonsmatrise <- cor(Pallant_survey_PANAS2)\ncortest.bartlett(korrelasjonsmatrise, n = nrow(Pallant_survey_PANAS2))\n#> $chisq\n#> [1] 3781.425\n#> \n#> $p.value\n#> [1] 0\n#> \n#> $df\n#> [1] 171\nKMO(Pallant_survey_PANAS2)\n#> ℹ 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> \n#> ── Kaiser-Meyer-Olkin criterion (KMO) ──────────────────────\n#> \n#> ✔ The overall KMO value for your data is meritorious.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.876\n#> \n#>   For each variable:\n#>   pn1   pn2   pn3   pn4   pn6   pn7   pn8   pn9  pn10  pn11 \n#> 0.936 0.863 0.795 0.947 0.884 0.950 0.889 0.936 0.820 0.868 \n#>  pn12  pn13  pn14  pn15  pn16  pn17  pn18  pn19  pn20 \n#> 0.893 0.899 0.800 0.870 0.918 0.905 0.915 0.827 0.803"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"anti-image-korrelasjonsmatrise","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.2.3.3.2 Anti-image korrelasjonsmatrise","text":"resultatet har vi kun hentet ut de diagonale verdiene anti-image matrisen (som er de vi er interessert ) og sortert disse synkende rekkefølge. Disse verdiene er KMO verdier de individuelle variablene. Disse bør ifølge Field (2009a) være på 0,5. Verdier 0,5 kan bety vi bør ta denne variabelen ut.Siden denne matrisen blir stor har vi kun hentet ut antall korrelasjonsverdier som er 0.9 (og 1.0 siden det matrisen alltid vil være 1.0 diagonalene - der variablene korrelerer med seg selv). Field (2009a) peker på ingen korrelasjo-ner bør være 0.9. Samtidig bør det være godt med korrelasjoner 0.3. Det finnes ingen absolutte krav til hvor mange/hvor stor andel av korrelasjonene som bør være 0.3, men hvis man har få 0.3 indikerer det dataene kanskje ikke egner seg PCA.tabellen har vi fjernet alle korrelasjonsverdier +/- 0.3 og 1 å gjøre det litt lettere å se.Det kan se ut som vi har et greit antall korrelasjoner 0.3.","code":"\nantiimage <- antiImage(Pallant_survey_PANAS2)$AIR\nantiimage2 <- as.matrix(diag(antiimage), row.names = FALSE)\nsort(antiimage2, decreasing = TRUE)\n#>  [1] 0.9499796 0.9469195 0.9364141 0.9359988 0.9182026\n#>  [6] 0.9146945 0.9048130 0.8987604 0.8931080 0.8887645\n#> [11] 0.8837913 0.8703518 0.8682341 0.8629180 0.8265146\n#> [16] 0.8197264 0.8028766 0.8001013 0.7945513\ncorPallant_survey_PANAS2 <- cor(Pallant_survey_PANAS2)\ncorPallant_survey_PANAS2 <- round(corPallant_survey_PANAS2, 2)\nlength(corPallant_survey_PANAS2[corPallant_survey_PANAS2>0.9 & corPallant_survey_PANAS2<1])\n#> [1] 0\nunder03<- as.data.frame(apply(corPallant_survey_PANAS2, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\nunder03\n#>       pn1  pn2  pn3  pn4  pn6  pn7  pn8  pn9 pn10 pn11 pn12\n#> pn1                 0.34 0.35 0.41      0.41           0.48\n#> pn2            0.46                0.64      0.41  0.5     \n#> pn3       0.46                     0.49           0.33     \n#> pn4  0.34                     0.33       0.4           0.31\n#> pn6  0.35                     0.33      0.43            0.4\n#> pn7  0.41           0.33 0.33           0.48            0.4\n#> pn8       0.64 0.49                          0.38 0.46     \n#> pn9  0.41            0.4 0.43 0.48                     0.41\n#> pn10      0.41                     0.38           0.58     \n#> pn11       0.5 0.33                0.46      0.58          \n#> pn12 0.48           0.31  0.4  0.4      0.41               \n#> pn13 0.49           0.33 0.33 0.39      0.43           0.58\n#> pn14      0.41 0.81                0.46           0.32     \n#> pn15 0.41           0.32      0.33      0.36           0.51\n#> pn16      0.31 0.33                0.38      0.31 0.35     \n#> pn17 0.56           0.37 0.39 0.49      0.46           0.64\n#> pn18 0.47           0.34 0.45 0.46      0.46           0.47\n#> pn19      0.46 0.56                0.48      0.34 0.39     \n#> pn20      0.42 0.42                0.43      0.37 0.37     \n#>      pn13 pn14 pn15 pn16 pn17 pn18 pn19 pn20\n#> pn1  0.49      0.41      0.56 0.47          \n#> pn2       0.41      0.31           0.46 0.42\n#> pn3       0.81      0.33           0.56 0.42\n#> pn4  0.33      0.32      0.37 0.34          \n#> pn6  0.33                0.39 0.45          \n#> pn7  0.39      0.33      0.49 0.46          \n#> pn8       0.46      0.38           0.48 0.43\n#> pn9  0.43      0.36      0.46 0.46          \n#> pn10                0.31           0.34 0.37\n#> pn11      0.32      0.35           0.39 0.37\n#> pn12 0.58      0.51      0.64 0.47          \n#> pn13           0.36      0.55 0.58          \n#> pn14                0.36           0.56 0.45\n#> pn15 0.36                0.62 0.41          \n#> pn16      0.36                     0.35     \n#> pn17 0.55      0.62           0.58          \n#> pn18 0.58      0.41      0.58               \n#> pn19      0.56      0.35                0.75\n#> pn20      0.45                     0.75"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"faktoranalyse","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3 Faktoranalyse","text":"Det er flere teknikker assosiert med begrepet faktoranalyse, men hovedsak kan vi dele disse inn typer: eksplorerende og konfirmerende (Hoyle 2000; Hurley et al. 1997).Eksplorerende faktoranalyse som søker å gruppere variabler et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og mindre grad drevet av teori - “one can always subject data set EFA necessarily CFA” (Schriesheim Hurley et al. 1997, s.672).Konfirmerende faktoranalyse starter andre enden - med forhåndshypoteser om dataene og strukturen. ønsker vi å bekrefte (konfirmere) en antatt datastruktur vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksplorerende-faktoranalyse-efa","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1 Eksplorerende faktoranalyse (EFA)","text":"Hensikten med en ekspolerende faktoranalyse er altså å undersøke om vi har variabler som korrelerer med hverandre og se om disse kan grupperes på en meningsfull måte. Vi ser på graden av korrelasjon - variabler som er sterkt korrelerte grupperes og skilles fra andre som er mindre korrelerte (som igjen kan inneholde grupper av relativt sterkt korrelerte variabler). Målet er altså å få grupper av variabler som internt er sterkt korrelerte med hverandre, og lite korrelert med variabler utenfor gruppen.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"hva-er-en-faktor","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.1 Hva er en faktor?","text":"En faktor kan ses på som en skjult variabel - en variabel vi ikke kan observere eller måle direkte, men som påvirker flere andre synlige/målbare variabler.Variablene , B, C osv er med andre ord observerbare/målbare fenomen underliggende, skjulte faktorer. Når vi grupperer variabler som er høyt korrelerte antar vi deres variasjon og korrelasjon skyldes den underliggende/skjulte variabelen.Et typisk eksempel er den såkalte “five-factor model” (McCrae John 1992) som antar personlighetstrekk kan grupperes fem faktorer: Åpenhet, planmessighet, ekstroversjon, omgjengelighet og nevrotisisme (se f.eks. Kennair (2021) en kort introduksjon til modellen på norsk). Man kan imidlertid ikke måle disse fem faktorene direkte, men man antar de påvirker en rekke målbare fohold. Disse kan man spørre om/måle/observere. Faktoren planmessighet kan eksempel påvirke spørsmål/atferd som “Jeg er alltid forberedt”, “Jeg følger en plan” eller “Jeg utfører mine oppgaver med en gang de er gitt”.La oss se på dette visuelt på en forenklet framstilling. Vi har en teoretisk modell, der vi sier positive tilbakemeldinger på jobben predikerer jobbtilfredshet, økonomi predikerer tilfredshet hjemmet, og tilsammen predikerer jobbtilfredshet og tilfredshet hjemmet den totale personlige tilfredsheten:Dette er det vi teoretisk forventer og vår modell. Når vi samler data ser vi alltid dataene (selvsagt) aldri passer perfekt inn vår teoretiske modell. figuren er våre faktiske (empiriske) data fra vår undersøkelse representert gjennom de fargede sirklene som knyttes til sin respektive variabel.Så det vi faktisk ser - empirisk - er egentlig dette:Ikke alle målte variabler måler sterkest på den underliggende faktoret, det er overlapping mellom variablene og hva de måler (og det kan se langt verre ut enn figuren ).Faktoranalysen vil hjelpe oss å rydde litt opp dette, ved å se på hvilke variabler som faktisk (ikker teoretisk) korrelerer sterkt med hvilke, og hvilke som korrelerer svakt, så å hjelpe oss strukturere modellen vi tester.figuren kan vi se vi nok har en struktur av sterkt korrelerte variabler som måler “sine” underliggende faktorer, men vi ser også det er en gruppe som teoretisk burde ligge nørmere sine respektive faktorer, men som ser ut til å klumpe seg midten. Kanskje dette er en bedre representasjon?må vi trolig gå tilbake til vårt teoretiske utgangspunkt og spørreskjemate å se på om vi har funnet en ny faktor, eller om vi skal utelate enkelte målinger/sørsmål å få en bedre modell.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.2 Eksempel","text":"Vi skal bruke et generert datasett gjennomgang av eksplorerende faktoranalyse.Download fa_spm.xlsxDownload fa_spm.savDownload fa_spm.dtaVi kan se oss vi har stilt en rekke mennesker 9 spørsmål der de har svart på en skala fra 1-4. Det vi ønsker å se er om disse spørsmålene kan si noe om en eller flere latente variabler. De 9 spørsmålene er altså direkte målt, og vi vil se om vi kan si noe om latente variabler ut fra dette.Inngangsverderdiene en faktoranalyse er korrelasjonsmatrisen som “tygges” (programvare) til en struktur/et mønster.","code":"\nfa_spm <- as.data.frame(read_excel(\"fa_spm.xlsx\"))\nsummarytools::descr(fa_spm)\n#> Descriptive Statistics  \n#> fa_spm  \n#> N: 366  \n#> \n#>                      spm_1    spm_2    spm_3    spm_4    spm_5    spm_6    spm_7    spm_8    spm_9\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     1.42     1.28     1.58     2.10     2.86     2.64     1.94     2.48     1.95\n#>           Std.Dev     0.66     0.59     0.72     0.98     0.91     0.89     0.91     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     1.00     1.00     1.00     1.00     2.00     2.00     1.00     2.00     1.00\n#>            Median     1.00     1.00     1.00     2.00     3.00     3.00     2.00     2.50     2.00\n#>                Q3     2.00     1.00     2.00     3.00     4.00     3.00     3.00     3.00     3.00\n#>               Max     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00\n#>               MAD     0.00     0.00     0.00     1.48     1.48     1.48     1.48     0.74     1.48\n#>               IQR     1.00     0.00     1.00     2.00     2.00     1.00     2.00     1.00     2.00\n#>                CV     0.46     0.47     0.46     0.47     0.32     0.34     0.47     0.41     0.52\n#>          Skewness     1.50     2.24     1.04     0.32    -0.35     0.05     0.59     0.00     0.64\n#>       SE.Skewness     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13\n#>          Kurtosis     1.86     4.63     0.43    -1.08    -0.74    -0.84    -0.62    -1.11    -0.86\n#>           N.Valid   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"korrelasjonsmatrise","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.3 Korrelasjonsmatrise","text":"Vi ser på korrelasjonsmatrisen.Henter fram antall korrelasjoner 0.9.Videre tar vi vekk verdier 0.3 og de som = 1:","code":"\nfa_spm_cor <- round(cor(fa_spm, use=\"complete.obs\"),2)\npull_lower_triangle(fa_spm_cor, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4  0.19  0.22  0.21                              \n#> 5   spm_5  0.07  0.17  0.12  0.46                        \n#> 6   spm_6  0.17  0.23  0.21  0.47   0.7                  \n#> 7   spm_7  0.34  0.33  0.27  0.18  0.11  0.26            \n#> 8   spm_8  0.26  0.27  0.25  0.25  0.29  0.34  0.52      \n#> 9   spm_9  0.35   0.3  0.23  0.17  0.22  0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9\nlength(fa_spm_cor[fa_spm_cor>0.9 & fa_spm_cor<1])\n#> [1] 0\nunder03x<- as.data.frame(apply(fa_spm_cor, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\npull_lower_triangle(under03x, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4                                                \n#> 5   spm_5                    0.46                        \n#> 6   spm_6                    0.47   0.7                  \n#> 7   spm_7  0.34  0.33                                    \n#> 8   spm_8                                0.34  0.52      \n#> 9   spm_9  0.35                          0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kmo-og-bartletts","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.4 KMO og Bartletts","text":"Vi bruker KMO og Bartletts til å se på “faktoriserbarheten”. Kaiser (1974) anbefaler 0.60 som cut-verdi. Bartletts test er signifkant. Bartletts test sammenligner våre faktiske korrelasjonsmatrise med en “identity matrix”. “Identity matrix” er en konstruert korrelasjonsmatrise med verdi 1 diagnoalen og 0 på alle andre korrelasjoner:Vi forventer selvsagt korrelasjon vår korrelasjonsmatrise, og siden Bartletts test bruker nullhypotsene om det ikke er korrelasjon, forteller en signifikant test det er meningsfullt å gjennomføre en datareduksjonsteknikk. Hvis vår korrelasjonsmatrise ikke er signifikant forskjellig fra en matrise med null korrelasjon mellom variablene gir det ingen mening å se etter strukturer av korrelasjoner.tillegg kan vi se på “determinant”:En positiv verdi indikerer også datasettet egner seg faktoranalyse.","code":"\nKMO(fa_spm_cor)\n#> \n#> ── Kaiser-Meyer-Olkin criterion (KMO) ──────────────────────\n#> \n#> ✔ The overall KMO value for your data is middling.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.779\n#> \n#>   For each variable:\n#> spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 \n#> 0.791 0.790 0.821 0.870 0.659 0.729 0.775 0.863 0.783\nBARTLETT(fa_spm_cor, N = nrow(fa_spm))\n#> \n#> ✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   𝜒²(36) = 1157.84, p < .001#> [1] \"Identity Matrix\"\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]    1    0    0    0    0    0    0\n#> [2,]    0    1    0    0    0    0    0\n#> [3,]    0    0    1    0    0    0    0\n#> [4,]    0    0    0    1    0    0    0\n#> [5,]    0    0    0    0    1    0    0\n#> [6,]    0    0    0    0    0    1    0\n#> [7,]    0    0    0    0    0    0    1\ndet(fa_spm_cor)\n#> [1] 0.04052472"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"antall-faktorer","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.5 Antall faktorer","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-1","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.6 Kaisers kriterium","text":"Kaisers kriterium tilsier 3 faktorer.","code":"\neigenComputes(fa_spm)\n#> [1] 3.5289174 1.6219116 1.2294074 0.6032709 0.5332548\n#> [6] 0.4581451 0.4093023 0.3446454 0.2711450"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-2","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.7 Scree plott","text":"Det kan se ut til albuen/knekkpunktet tilsier 3 faktorer.","code":"\nSCREE(fa_spm_cor, eigen_type = \"EFA\")\n#> \n#> Eigenvalues were found using EFA."},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-2","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.8 Parallell analyse","text":"Dette peker mot tre faktorer.","code":"\nPARALLEL(fa_spm, eigen_type = \"EFA\")\n#> ℹ 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> Parallel Analysis performed using 1000 simulated random data sets\n#> Eigenvalues were found using EFA\n#> \n#> Decision rule used: means\n#> \n#> ── Number of factors to retain according to ────────────────\n#> \n#> ◌ EFA-determined eigenvalues:  3"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"analyse","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.9 Analyse","text":"Vi kjører en faktoranalyse med 3 faktorer uten rotasjon.Vi kan se på Unrotated loadings hvordan de ulike spørsmålene lader på de tre faktorene. Det er ikke gitt bildet er helt enkelt å tolke. noen spørsmål - som 1, 2, og 3 - ser vi loadings på alle tre faktorene. andre er det klarere loading på en faktor, eller positivt på en og negativt på en annen. Et hjelpemiddler å tolke modellen er rotasjon. Rotasjon innebærer egnetlig brae å se på variablene og faktorene fra en annen vinkelt. Som Hartmann, Krois, Waske (2018a) påpeker: “purpose rotation produce factors mix high low loadings moderate-sized loadings. idea give meaning factors, helps interpret . mathematical viewpoint, difference rotated unrotated matrix. fitted model , uniquenesses , proportion variance explained ”.Det finnes hovedgrupper rotasjoner: ortogonal og oblikk. Blant ortogonale rotasjonsteknikker finner vi varimax, quartimax og equimax. (Direct) oblimin og promax er vanlige oblikke rotasjoner. En hovedforskjell er ved ortogonal rotasjon tillates ikke faktorene er korrelerte, mens ved oblikk rotasjon kan faktorene korrelere. må vi altså gå tilbake til vår teoretiske forståelse av hva vi undersøker. Svært ofte samfunnsvitenskapene (vil vi hevde) ønsker vi å tillate faktorene kan korrelere ved rotasjon (vi antar veldig mange tilfeller vil dette være teoretisk fornuftig). så fall bør vi bruke oblikk rotasjon. Hvis vi har teoretiske vurderinger som tilsier faktorene ikke korrelerer velger vi ortogonalt.kapittelet om PCA viste vi ortogonal rotasjon. Dette innebærer aksene forblir ortogonale på hverandre, mens ved oblikk rotasjon kan aksenens vinkler på hverandre variere.Når vi kjører en ny faktoranalyse med 3 faktorer ser det slik ut:Vi ser en klar struktur hvilke spørsmål som lader på hvilke faktorer (dette eksempelet er konstruert å vise en veldig klar faktorstruktur, ofte vil det være større rom tolkning).","code":"\nurotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\")\n#> ℹ 'x' was not a correlation matrix. Correlations are found from entered raw data.\nurotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'none'.\n#> \n#> ── Unrotated Loadings ──────────────────────────────────────\n#> \n#>           F1      F2      F3  \n#> spm_1     .502    .454    .338\n#> spm_2     .556    .371    .418\n#> spm_3     .470    .318    .378\n#> spm_4     .499   -.241    .119\n#> spm_5     .635   -.592    .056\n#> spm_6     .711   -.428    .016\n#> spm_7     .594    .395   -.364\n#> spm_8     .605    .151   -.282\n#> spm_9     .638    .289   -.386\n#> \n#> ── Variances Accounted for ─────────────────────────────────\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> ── Model Fit ───────────────────────────────────────────────\n#> \n#> 𝜒²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50\nmrotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\", rotation = \"promax\")\n#> ℹ 'x' was not a correlation matrix. Correlations are found from entered raw data.\nmrotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'promax'.\n#> \n#> ── Rotated Loadings ────────────────────────────────────────\n#> \n#>           F1      F3      F2  \n#> spm_1    -.066    .076    .732\n#> spm_2     .056   -.022    .782\n#> spm_3     .049   -.043    .688\n#> spm_4     .521   -.018    .140\n#> spm_5     .903   -.055   -.069\n#> spm_6     .788    .094    .004\n#> spm_7    -.103    .814    .039\n#> spm_8     .142    .623   -.010\n#> spm_9     .013    .808   -.027\n#> \n#> ── Factor Intercorrelations ────────────────────────────────\n#> \n#>       F1      F2      F3  \n#> F1    1.000   0.380   0.268\n#> F2    0.380   1.000   0.504\n#> F3    0.268   0.504   1.000\n#> \n#> ── Variances Accounted for ─────────────────────────────────\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> ── Model Fit ───────────────────────────────────────────────\n#> \n#> 𝜒²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"model-fit","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.10 Model fit","text":"Vi ser vi presenteres flere mål på model fit (se f.eks. Finch (2020)). De ulike indeksene måler ulike aspekter av model fit. Vi skal gå inn på av dem. Husk dette datasettet er generert å vise en utmerket modell - med andre data vil du sjeldent oppleve så gode verdier på model fit som .CFI = Comparative Fit Index. Verdiene kan være mellom 0 og 1, og verdier 0.9 regnes som en god fit (Hu Bentler 1999) (en mer konservativ terskel kan være 0.95).RMSEA = Root Mean Square Error Approximation. oppgis ofte verdiene 0.01, 0.05 og 0.08 som henhodsvis utmerket, god og middels. Finch (2020) viser til 0.05 som en cut-verdi.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sammenlikning-uten-rotasjon-med-ortogonal-varimax-og-oblikk-promax-rotasjon","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.1.11 Sammenlikning uten rotasjon, med ortogonal (varimax) og oblikk (promax) rotasjon","text":"","code":"\nfa_ingenrot <- factanal(fa_spm, factors = 3, rotation = \"none\")\nfa_varimax <- factanal(fa_spm, factors = 3, rotation = \"varimax\")\nfa_promax <- factanal(fa_spm, factors = 3, rotation = \"promax\")\n\npar(mfrow = c(1,3))\nplot(fa_ingenrot$loadings[,1], \n     fa_ingenrot$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Uten rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_varimax$loadings[,1], \n     fa_varimax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med varimax rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_promax$loadings[,1], \n     fa_promax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med promax rotasjon\")\nabline(h = 0, v = 0)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"konfirmerende-faktoranalyse-cfa","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2 Konfirmerende faktoranalyse (CFA)","text":"Gjennomgangen av Confirmatory Factor Analysis (CFA) er basert på Lin (2021a). Vi håper vår gjennomgang kan framstå som like god som Lins original.forrige del beskrev vi forskjellen på EFA og CFA på følgende måte:Eksplorerende faktoranalyse som søker å gruppere variabler et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og mindre grad drevet av teori - “one can always subject data set EFA necessarily CFA” (Schriesheim Hurley et al. 1997, s.672).Konfirmerende faktoranalyse starter andre enden - med hypoteser om dataene og strukturen. ønsker vi å bekrefte (konfirmere) en antatt datastruktur vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"gjennomgang-av-teori-med-eksempel","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.1 Gjennomgang av teori med eksempel","text":"Data eksempelet er hentet fra Field (2009a) (dataene er konstruerte).Du kan laste ned datasettet SPSS-format - fila heter SAQ.sav.Vi har modifisert datasettet ved å ta vekk noen unødvendige variabler vårt formål og gitt variablene nye navn, og kalt denne fila SAQ2.sav:Download SAQ2.savDet grunnleggende ved faktoranalyse er det ser på korrelasjoner mellom spørsmål/enheter. vårt datasett - SAQ2 - har vi denne korrelasjonsmatrisen:CFA (og SEM) bruker vi imidlertid kovariansmatrise, ikke korrelasjonsmatrise.Vi kan legge merke til diagonalen korrelasjonsmatrisen er 1 (en variabel korrelerer alltid perfekt (1) med seg selv). en kovariansmatrise er diagnoalen ikke 1. Korrelasjon viser hvordan variabler er relatert til hverandre, kovarians viser hvordan variabler er ulike (korrelasjon er standardisert kovarians ved kovariansen deles på standardavviket hver variabel).","code":"\nSAQ <- read_sav(\"SAQ.sav\")\nSAQ2 <- select(SAQ, -c(Question_09:FAC4_2))\nSAQ2 <- rename(SAQ2, Spm1 = Question_01, Spm2 = Question_02, Spm3 = Question_03, Spm4 = Question_04, Spm5 = Question_05, Spm6 = Question_06, Spm7 = Question_07, Spm8 = Question_08)\nwrite_sav(SAQ2, \"SAQ2.sav\")\nSAQ2cor<-round(cor(SAQ2[,1:8]),2)\nupper <- SAQ2cor\nupper[upper.tri(SAQ2cor)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm1  Spm2  Spm3 Spm4 Spm5 Spm6 Spm7 Spm8\n#> Spm1     1                                     \n#> Spm2  -0.1     1                               \n#> Spm3 -0.34  0.32     1                         \n#> Spm4  0.44 -0.11 -0.38    1                    \n#> Spm5   0.4 -0.12 -0.31  0.4    1               \n#> Spm6  0.22 -0.07 -0.23 0.28 0.26    1          \n#> Spm7  0.31 -0.16 -0.38 0.41 0.34 0.51    1     \n#> Spm8  0.33 -0.05 -0.26 0.35 0.27 0.22  0.3    1\nSAQ2cov <- round(cov(SAQ2[,1:8]),2)\nupper <- SAQ2cov\nupper[upper.tri(SAQ2cov)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm1  Spm2  Spm3 Spm4 Spm5 Spm6 Spm7 Spm8\n#> Spm1  0.69                                     \n#> Spm2 -0.07  0.72                               \n#> Spm3  -0.3  0.29  1.16                         \n#> Spm4  0.34 -0.09 -0.39  0.9                    \n#> Spm5  0.32  -0.1 -0.32 0.37 0.93               \n#> Spm6   0.2 -0.07 -0.27  0.3 0.28 1.26          \n#> Spm7  0.28 -0.15 -0.45 0.43 0.36 0.64 1.22     \n#> Spm8  0.24 -0.04 -0.24 0.29 0.23 0.22 0.29 0.76"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"fra-lineær-regresjon-til-faktoranalyse","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.2 Fra lineær regresjon til faktoranalyse","text":"Fra kapittelet om regresjonsanalyse kjenner vi til:\\(y=b_0 + b_1x + e\\)der\\(y=den\\ avhengige\\ variabelen\\)\\(b_0=intercept\\)\\(b_1=stigningstallet\\ (slope)\\)\\(x=uavhengig\\ observert\\ variabel/prediktor\\)\\(e=feilledd/residual\\)faktoranalyse har vi tilsvarende:\\(y_1 = \\tau_1 + \\lambda_1\\eta_1 + e_1\\)der\\(y_1 = \"enheten\"/spørsmålet\\)\\(\\tau_1 = intercept\\ (= \"tau\")\\)\\(\\lambda_1 = koeffisient\\ (= lambda\\ , på\\ en\\ måte\\ som\\ slope\\ \\ lineær\\ regresjon,\\ men\\ som\\ \\ CFA\\ kalles\\ \"loading\")\\)\\(\\eta_1=faktoren\\ (=eta\\ ,uobservert/latent,\\ \\ motsetning\\ til\\ en\\ prediktor\\ \\ lineær\\ regresjon)\\)\\(e_1=feilleddet/residualen\\)En distinkt forskjell mellom OLS og CFA er dermed OLS er prediktoren observert, mens CFA er faktoren uobservert.Vi har altså:\n\\[y_1 = \\tau_1 + \\lambda_1\\eta_1 + e_1\\]\\[y_2 = \\tau_2 + \\lambda_2\\eta_1 + e_2\\]\\[y_3 = \\tau_3 + \\lambda_3\\eta_1 + e_3\\]Disse tre likningene (som kan være de tre første spørsmålene vårt datasett) er dermed tre separate regresjonslikninger.\nSom vi kan skrive slik på matriseform:\\[\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\end{pmatrix} = \\begin{pmatrix}\\tau_1\\\\\\tau_2\\\\\\tau_3\\end{pmatrix} + \\begin{pmatrix}\\lambda_1\\\\\\lambda_2\\\\\\lambda_3\\end{pmatrix}(\\eta_1) + \\begin{pmatrix}e_1\\\\e_2\\\\e_3\\end{pmatrix}\\]motsetning til OLS, som er univariat, er CFA multivariat. hver \\(y\\) har vi en intercept (\\(\\tau\\)), en loading (\\(\\lambda\\)) og en residual (\\(e\\)). Det alle \\(y\\) har felles er faktoren \\(\\eta\\). Og det vi egentlig sier er den felles faktoren predikerer alle \\(y\\).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"frihetsgrader-1","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.3 Frihetsgrader","text":"La oss anta vi ser på spm 3, 4 og 5, og sier vårt teoretiske utgangspunkt er (den uobserverte/latente) faktoren SPSS-angst beskriver de observerte skårene på de tre spørsmålene. Grafisk kan vi illustrere dette slik:Vi modifiserer datasettet til å kun inneholde disse tre spørsmålene.å se på antall frihetsgrader begynner vi med å se på antallet kjente størrelser (=totalt antall parametere = “known values”). Dette finner vi ved:\\(p(p+1)/2\\)vår tenkte modell har vi \\(p=3\\), altså 6 parametere. Dette kan vi forsåvidt se kovariansmatrisen også, men en stor kovariansmatrise er det raskere å bruke utregningen.Imidlertid ser vår modell slik ut:\\[\\Sigma(\\theta) = \\begin{pmatrix}\\lambda_{1} \\\\\\lambda_{2} \\\\\\lambda_{3}\\end{pmatrix}\\begin{pmatrix}\\psi_{11}\\end{pmatrix}\\begin{pmatrix}\\lambda_{1} & \\lambda_{2} & \\lambda_{3}\\end{pmatrix} + \\begin{pmatrix}\\theta_{11} &  0 & 0 \\\\\\theta_{21} & \\theta_{22} & 0 \\\\\\theta_{31} &  \\theta_{32} & \\theta_{33} \\\\\\end{pmatrix}\\]der\\(\\lambda\\) er kjent fra før\\(\\psi\\) er variansen faktorenog siste ledd likningen er kovariansmatrisen.modellen vår må vi finne antall unike parametere, som dette tilfellet er 10 (3 x \\(\\lambda\\), 1 x \\(\\psi\\) og 4 x \\(\\theta\\)). Vi kan da finne antallet frie parametere som er \\(Antall\\ unike\\ parametere\\ - antall\\ faste\\ parametere = 10 - 0 = 10\\)Antall frihetsgrader vil da være: \\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 6 - 10 = -4\\)Dette er uheldig - vi kan ikke ha et større antall frie parametere enn antall kjente parametere, da får vi et negativt antall frihetsgrader og en umulig modell. Dette kalles “underdefined model”, og kan sammenliknes med å skulle løse \\(x + y = 9\\): Det finnes uendelig mange løsninger den likningen. Der vi har 0 frihetsgrader er modellen “just-identified” (dette eer tilfelle regresjonsmodeller), og positivt antall frihetsgrader innebærer “-identified model”.Løsningen på en “underdefined model” er “fixed parameters” - altså vi låser et antall parametere til en verdi slik de ikke kan variere. Hvis vi f.eks. låser 3 x \\(\\lambda\\) og 1 x \\(\\psi\\) får vi 4 låste parametere som gir:\\(Antall\\ unike\\ parametere\\ - antall\\ faste\\ parametere = 10 - 4 = 6 (frie parrametere)\\)som gir\\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 10 - 6 = 4\\).Antall frihetsgrader blir da:\\(antall\\ kjente\\ parametere\\ - antall\\ frie\\ parametere = 6 - 4 = 2\\)","code":"\nSAQ3 <- select(SAQ2, -c(Spm1, Spm2, Spm6:Spm8))\nSAQ3cov <- round(cov(SAQ3[,1:3]),2)\nupper <- SAQ3cov\nupper[upper.tri(SAQ3cov)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>       Spm3 Spm4 Spm5\n#> Spm3  1.16          \n#> Spm4 -0.39  0.9     \n#> Spm5 -0.32 0.37 0.93"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kjøring-av-modell","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.4 Kjøring av modell","text":"tabellen ser vi resultatet av vår modell. Ladningene (“loadings”) - \\(\\lambda\\) - er gitt “Latent Variables” - “Estimates”. “Estimates” “Variances”, f.eks. 0.815 Spm3 er residualen Spm3.\nR-pakken lavaan som er brukt , låses ladning første spørsmål til 1. Vi ser dette “Latent Variables” der “Estimate” Spm3 = 1, noe vi også kan se grafisk . Siden ladningen til Spm3 er låst til 1 (“fixed parameter”) betyr det \\(\\lambda\\) de andre spørsmålene er vist relasjon til/skalert til Spm3. Dette gjør tolkning vanskelig.stedet å låse første ladning kan vi låse variansen faktoren:Vi ser \\(\\lambda\\) - ladningen - Spm3 ikke lenger er 1.","code":"\nmodell1 <- 'SPSSangst =~ Spm3 + Spm4 + Spm5'\nSPSS_3var <- cfa(modell1, data = SAQ3)\nsummary(SPSS_3var)\n#> lavaan 0.6-12 ended normally after 23 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         6\n#> \n#>   Number of observations                          2571\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 0.000\n#>   Degrees of freedom                                 0\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm3              1.000                           \n#>     Spm4             -1.139    0.073  -15.652    0.000\n#>     Spm5             -0.945    0.056  -16.840    0.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm3              0.815    0.031   26.484    0.000\n#>    .Spm4              0.458    0.030   15.359    0.000\n#>    .Spm5              0.626    0.025   24.599    0.000\n#>     SPSSangst         0.340    0.031   11.034    0.000\nnodenavnf <- c(\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_3var,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 10,\n         nodeNames = nodenavnf,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 8,\n         sizeLat = 10\n)\nmodell2 <- \"SPSSangst =~ Spm3 + Spm4 + Spm5\"\nSPSS_3var_2 <- cfa(modell2, data = SAQ3, std.lv = TRUE)\nsummary(SPSS_3var_2)\n#> lavaan 0.6-12 ended normally after 14 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                         6\n#> \n#>   Number of observations                          2571\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 0.000\n#>   Degrees of freedom                                 0\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm3              0.583    0.026   22.067    0.000\n#>     Spm4             -0.665    0.026  -25.605    0.000\n#>     Spm5             -0.551    0.024  -22.800    0.000\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm3              0.815    0.031   26.484    0.000\n#>    .Spm4              0.458    0.030   15.359    0.000\n#>    .Spm5              0.626    0.025   24.599    0.000\n#>     SPSSangst         1.000\nnodenavn <- c(\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_3var_2,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 10,\n         nodeNames = nodenavn,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 8,\n         sizeLat = 10\n)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"full-modell-på-datasettet","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.5 Full modell på datasettet","text":"Vi ønsker å lage en CFA-modell SPSSangst med alle åtte spørsmålene.En noe mer oversiktlig tabell parametrene, modifisert fra Dudek (2019):Table 14.1: FaktorladningerVi kan isolere/hente ut ladningene (\\(\\lambda\\) = lambda):Vi kan også se isolert på residualenes varians (\\(\\theta\\) = theta):Dere finner igjen lambda- og thetaverdiene såvel tabellearisk som grafisk output ovenfor.","code":"\nmodell3 <- \"SPSSangst =~ Spm1 + Spm2 + Spm3 + Spm4 + Spm5 + Spm6 + Spm7 + Spm8\"\nSPSS_8 <- cfa(modell3, data = SAQ2, std.lv = TRUE)\nsummary(SPSS_8, fit.measure = TRUE, standardized = TRUE)\n#> lavaan 0.6-12 ended normally after 15 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        16\n#> \n#>   Number of observations                          2571\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                               554.191\n#>   Degrees of freedom                                20\n#>   P-value (Chi-square)                           0.000\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                              4164.572\n#>   Degrees of freedom                                28\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    0.871\n#>   Tucker-Lewis Index (TLI)                       0.819\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)             -26629.559\n#>   Loglikelihood unrestricted model (H1)             NA\n#>                                                       \n#>   Akaike (AIC)                               53291.118\n#>   Bayesian (BIC)                             53384.751\n#>   Sample-size adjusted Bayesian (BIC)        53333.914\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.102\n#>   90 Percent confidence interval - lower         0.095\n#>   90 Percent confidence interval - upper         0.109\n#>   P-value RMSEA <= 0.05                          0.000\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.055\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>   SPSSangst =~                                        \n#>     Spm1              0.485    0.017   28.942    0.000\n#>     Spm2             -0.198    0.019  -10.633    0.000\n#>     Spm3             -0.612    0.022  -27.989    0.000\n#>     Spm4              0.632    0.019   33.810    0.000\n#>     Spm5              0.554    0.020   28.259    0.000\n#>     Spm6              0.554    0.023   23.742    0.000\n#>     Spm7              0.716    0.022   32.761    0.000\n#>     Spm8              0.424    0.018   23.292    0.000\n#>    Std.lv  Std.all\n#>                   \n#>     0.485    0.586\n#>    -0.198   -0.233\n#>    -0.612   -0.570\n#>     0.632    0.667\n#>     0.554    0.574\n#>     0.554    0.494\n#>     0.716    0.650\n#>     0.424    0.486\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)\n#>    .Spm1              0.450    0.015   30.734    0.000\n#>    .Spm2              0.685    0.019   35.300    0.000\n#>    .Spm3              0.780    0.025   31.157    0.000\n#>    .Spm4              0.499    0.018   27.989    0.000\n#>    .Spm5              0.623    0.020   31.040    0.000\n#>    .Spm6              0.951    0.029   32.711    0.000\n#>    .Spm7              0.702    0.024   28.678    0.000\n#>    .Spm8              0.581    0.018   32.849    0.000\n#>     SPSSangst         1.000                           \n#>    Std.lv  Std.all\n#>     0.450    0.656\n#>     0.685    0.946\n#>     0.780    0.675\n#>     0.499    0.555\n#>     0.623    0.670\n#>     0.951    0.756\n#>     0.702    0.578\n#>     0.581    0.764\n#>     1.000    1.000\nparameterEstimates(SPSS_8,standardized = FALSE) %>% \n    filter(op==\"=~\") %>%\n    select(Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue) %>%\n    knitr::kable(digits=3,booktabs=TRUE,format=\"markdown\",caption=\"Faktorladninger\")\nnodenavn2 <- c(\n    \"Statistics make me cry\",\n    \"My friends will think I'm stupid for not being able to cope with SPSS\",\n    \"Standard deviations excite me\",\n    \"I dream that Pearson is attacking me with correlation coefficients\",\n    \"I don't understand statistics\",\n    \"I have little experience of computers\",\n    \"All computers hate me\",\n    \"I have never been good at mathematics\",\n    \"SPSS Anxiety Questionnaire\"\n)\nsemPaths(SPSS_8,\n         what = \"std\",\n         whatLabels = \"est\",\n         style = \"lisrel\",\n         residScale = 8,\n         nodeNames = nodenavn2,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         manifests = paste0(\"Spm\", 1:8),\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 6,\n         sizeLat = 10\n)\nround(semMatrixAlgebra(SPSS_8, LY), digits = 3)\n#> model set to 'lisrel'\n#>      SPSSangst\n#> Spm1     0.485\n#> Spm2    -0.198\n#> Spm3    -0.612\n#> Spm4     0.632\n#> Spm5     0.554\n#> Spm6     0.554\n#> Spm7     0.716\n#> Spm8     0.424\nthetaverdier <- semMatrixAlgebra(SPSS_8, TE)\n#> model set to 'lisrel'\nupper <- round(thetaverdier, digits = 3)\nupper[upper.tri(thetaverdier)] <- \"\"\nupper <- as.data.frame(upper)\nupper\n#>      Spm1  Spm2 Spm3  Spm4  Spm5  Spm6  Spm7  Spm8\n#> Spm1 0.45                                         \n#> Spm2    0 0.685                                   \n#> Spm3    0     0 0.78                              \n#> Spm4    0     0    0 0.499                        \n#> Spm5    0     0    0     0 0.623                  \n#> Spm6    0     0    0     0     0 0.951            \n#> Spm7    0     0    0     0     0     0 0.702      \n#> Spm8    0     0    0     0     0     0     0 0.581"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"tolkning-av-resultatene","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.6 Tolkning av resultatene","text":"Det første vi kan se er “Estimate” SPSSangst er 1.000. Dette er fordi vi har låst denne parameteren, hvilket innebærer variansen er standardisert. Så hvis vi har en enhets økning SPSSangst, kan vi lese endringen de åtte spørsmålene. F.eks. vil Spm1 øke med 0.485 Spm1’s skala (Spm1:Spm8 er ikke standardisert, kun variansen) (se “Latent Variables” - “Estimate”), og Spm2 vil gå ned med 0.198 (“Estimate” er -0.198). den grafiske presentasjonen av modellen ser vi det samme, også illustrert med blå og rød farge dette tilfellet positiv eller negativ “Estimate”.å gjøre tolkningen lettere kan vi standardisere både variansen faktoren (til 1) og enhetene/spørsmålene seg selv:Table 14.2: FaktorladningerHer ønsker vi å se på kolonnen “Std.” originaloutput (vår forenkle tabell = “Std.Beta”), som vi kan sammenlikne med standardiserte betaverdier lineær regresjon. Tolkningen nå blir: ett standardavviks økning SPSSangst øker Spm1 med 0.586 standardavvik, mens Spm går ned med 0.233 standardavvik (-0.233). Dette vil vi (som regel) kalle standardiserte ladninger (“standardized loadings”). En fordel med standardiserte ladninger er vi kan lettere sammenlikne ladningen til hverandre fordi de er nettopp standardiserte.","code":"\nparameterEstimates(SPSS_8,standardized = TRUE) %>% \n    filter(op==\"=~\") %>%\n    select(Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue, Std.Beta = std.all) %>%\n    knitr::kable(digits=3,booktabs=TRUE,format=\"markdown\",caption=\"Faktorladninger\")\nsemPaths(SPSS_8,\n         what = \"std\",\n         whatLabels = \"std\",\n         style = \"lisrel\",\n         residScale = 8,\n         nodeNames = nodenavn,\n         theme = \"colorblind\",\n         nCharNodes = 0,\n         manifests = paste0(\"Spm\", 1:8),\n         reorder = FALSE,\n         legend.cex = 0.35,\n         rotation = 2,\n         sizeMan = 6,\n         sizeLat = 10\n)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"hvor-god-er-modellen-vår-model-fit-statistics","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.7 Hvor god er modellen vår (“Model Fit Statistics”)?","text":"Et viktig poeng før vi gir oss kast med vurdering av hvor god modellen er, er å kunne si noe om dette må modellen ha positivt antall frihetsgrader (jfr. delkapittel om frihetsgrader lenger opp).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kjikvadrattest","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.8 Kjikvadrattest","text":"en CFA ønsker vi p-verdien kjikvadratverdien er større enn 0.05 - vi ønsker altså ikke å forkaste nullhypotesen, men med en p-verdi < 0.001 må vi forkaste nullhypotesen (nullhypotsen innebærer det ikke er forskjell mellom vår modell og populasjonen, men det ønsker vi jo heller ikke - vi ønsker vår modell representerer populasjonen). Tilsynelatende har vi derfor forhold til kjikvadrattesten en dårlig modell. må vi imidlertid være klar store utvalgsstørrelser gir en stor risiko vi får en signifikant kjikvadrattest. dette tilfellet har vi 2571 observasjoner, noe som er høyt.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"approximate-fit-index---rmsea-cfi-og-tli","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.9 “Approximate Fit Index” - RMSEA, CFI og TLI","text":"Kjikvadrattest er en “Exact Fit”-test, hvilket innebærer vi hypotetiserer utvalget/modellen er lik populasjonen. Det finnes imidlertid andre tester som går betegnelsen “Approximate Fit Index”, hvilket innebærer man tilnærmer seg dette med “nærme nok” (altså ikke absolutt). Vi skal ikke gå inn detaljer på hvordan disse indeksene regnes ut, men begrense oss til å si disse bygger på å sammenlikne en verst mulig modell (= ingen kovariasjoner er med, kun varianser den enkelte enhet/spørsmål) med en best mulig modell (= alle varianser og kovarianser er med -> “just-identified” modell), og deretter plasserer vår modell inn dette bildet.CFI = “Confirmatory Factor Index” ønsker vi skal være 0.95TLI = “Tucker Lewis Index” ønsker vi skal være 0.90RMSEA = “Root Mean Square Error Approximation” bør være eller lik 0.05 “close fit”, og mellom 0.05 og 0.08 “reasonable approximate fit”. RMSEA 0.08 er “poor fit”.Alle disse tre målene på Model Fit er ikke særlig gode. Modellen vår er trolig ikke den beste modellen SPSSangst. Et tiltak vi kan se på er hvor store de enkelte standardiserte ladningen er. Vi ser (f.eks. det siste diagrammet) Spm2 lader med -0.23, noe som er en god del lavere enn de fleste andre. Hva skjer om vi tar bort dette spørsmålet en revidert modell?Vi ser forbedringene ikke er store, og modellen vår ikke ser kjempebra ut (den er ikke elendig, men cut-verdiene vi normalt opererer med).","code":"\nmodell4 <- \"SPSSangst =~ Spm1 + Spm3 + Spm4 + Spm5 + Spm6 + Spm7 + Spm8\"\nSPSS_7 <- cfa(modell4, data = SAQ2, std.lv = TRUE)\nfitMeasures(SPSS_7, c(\"cfi\",\"tli\", \"rmsea\"))\n#>   cfi   tli rmsea \n#> 0.906 0.859 0.100\n# summary(SPSS_7, fit.measure = TRUE, standardized = TRUE)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kryssvalidering-cross-validation","chapter":"Kapittel 14 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"14.3.2.10 Kryssvalidering (“cross-validation”)","text":"Dette er også omtalt delkapittelet om multippel regresjonsanalyse så en litt mer utførlig beskrivelse gis der.Kryssvalidering innebærer vi deler datasettet : en del vi utvikler modeller på, og en del vi tester vår utvalgte modell på. På den måten unngår vi vi bruker hele datasettet på å grave fram (litt sjansepreget) en modell som er ok uten å vite om vi bare har hatt flaks. Hvis vår valgte modell har en god fit med den delen av dataene vi tester på kan vi si noe mer sikkert om vår modell.R kan vi bruke pakken kfa som gjør dette ganske lett oss.hvert utvalg kjøres en EFA på alle utvalg unntatt et uvalg som er et valideringsutvalg. EFA-utvalgene og valideringsutvalget utgjør et “fold”. Neste iterasjon kjøres EFA på alle utvalg testdataene unntatt et nytt utvalg som blir valideringsutvalg. Dette utgjør et nytt “fold”. Slik kjøres prosedyren til alle utvalg har vært valideringsutvalg de andres EFA.Verdiene hhv. CFI og RMSEA kan sammenliknes med foreslåtte cutoff-verider lenger opp å bidra til vurderingen av modellen. Resultatene dette tilfellet bekrefter på mange måter inntrykket vi har før cross-validation.","code":"\nkfamodell <- kfa(\n    SAQ2,\n    k = 5,\n    m = 1,\n    seed = 1243)\n#> [1] \"Using 7 cores for parallelization.\"\n\nk_model_fit(kfamodell, index = \"default\", by.fold = FALSE)\n#> $`1-factor`\n#>   fold chisq.scaled df.scaled cfi.scaled rmsea.scaled\n#> 1    1    104.07114  16.31447  0.6960799   0.10219978\n#> 2    2     77.99306  16.11308  0.7438522   0.08627022\n#> 3    3     88.36224  15.57886  0.7165712   0.09543103\n#> 4    4     80.31738  16.19342  0.7728829   0.08785816\n#> 5    5     87.17257  15.55239  0.6907808   0.09465371"},{"path":"maskinlæring-machine-learning.html","id":"maskinlæring-machine-learning","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"Kapittel 15 Maskinlæring (Machine Learning)","text":"R-pakker brukt dette kapittelet:Hensikten med å analysere samfunnet rundt oss er å forstå verden. dette ligger vi ønsker å kunne forklare hva som skjer og hvorfor (faktuelt og kaulsalt) og predikere hva vi tror kommer til å skje. Forklaring og prediksjon kan ofte stå litt motsetning til hverandre. Som vi har nevnt et sted tidligere - alle modeller (sav samfunnet) er feil. Men noen modeller er mindre feil enn andre, og noen er også nyttige oss (selv om vi vet de inneholder feil). En modell som inneholder “alt” er ingen modell, men virkeligheten selv. Altså må vi hele tiden foreta valg når vi lager modeller. En modell som inneholder et svært høyt antall variabler kan være mer nøyaktig, men samtidig veldig vanskelig å forstå. Og en modell som forklarer hva som har skjedd godt trenger ikke å være en modell som predikerer godt hva som kommer til å skje.Vi antar avveiningen mellom forklaring og prediksjon er viktig og sentral alle felt innenfor samfunnsvitenskapene, og avveiningen har ulikt fokus og innretting fra fagfelt til fagfelt. Yarkoni Westfall (2017) påpker f.eks. psykologi som fagfelt “increased focus prediction, rather explanation, can ultimately lead us greater understanding behavior” (s.1100).Berepet “maskinlæring” har blitt et mer og mer sentralt begrep samfunnsvitenskapelig metode generelt og dataanalyse spesielt. Selve begrepet ble “coinet” (først brukt) av Samuel (1959). Endel av metodene maskinlæring kjenner vi fra før. Regresjon er f.eks. et sentralt element “pakken” av metoder som kan puttes inn begrepet maskinlæring. Og regresjon er jo ikke noe nytt, så er maskinlæring kun et moteord? Et fancy ord på ting vi har gjort før? Tja, kanskje svaret er både ja og nei. Det er unektelig slik vi har drevet med regresjonsanalyser lenge før begrepet maskinlæring hvert fall ble allment kjent og popularisert. Samtidig er maskinlæring en distinkt gruppe av analytiske metoder hvis hensikt er å - nettopp - lære. Med dette mener vi modeller som bruker data til å forbedre analyseopgavene vi har foran oss.","code":"\npacman::p_load(tidyverse, AmesHousing, outliers, EnvStats, caTools, Metrics, psych, tdr, readxl, e1071, caret, plotly, rpart, rpart.plot, knitr, hydroGOF, ggridges, tidymodels, sf, patchwork, RColorBrewer, grid, ragg) "},{"path":"maskinlæring-machine-learning.html","id":"assosiasjon-vs.-prediksjon","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.1 Assosiasjon vs. prediksjon","text":"Et sentralt element maskinlæring er prediksjon. Som nevnt ovenfor er f.eks. regresjonsanalyser et kjent begrep som vi har gått relativt grundig gjennom tidligere kapitler. Vi skal imidlertid være klar snakk om \\(R^2\\) ikke sier noe om hvor godt en regresjonsmodell evner å predikere. \\(R^2\\) forteller oss imidlertid noe om hvor godt den gitte regresjonsmodellen snitt forklarer variansen dataene vi har. Regresjonskoeffisientene beregnes å maksimere \\(R^2\\) (gjennom å minimere feilleddene). Av modeller samme data forklarer en modell med høyere \\(R^2\\) mer av variansen de foreliggende dataene enn en modell med lavere \\(R^2\\). Men vi vet ikke noe om de modellenes evne til å predikere. Derfor er maskinlæring fokusert på modeller må trenes og testes. Dvs dataene deles uavhengige deler, der modellen utvikles og trenes på en del, og testes på en annen del. Merk: Dette er ikke det samme som å utvikle en regresjonsmodell på et datasett, og “teste” ved å replikere analysen på nye data. Dette er realiteten modeller - tradisjonell replikasjon - siden koeffisientene modellen vil tilpasses sett data (selv om de uavhengige variablene er de samme). Merk: Dette er ikke en kritikk av replikasjon, men en klargjøring av maskinlæring er prediksjon det sentrale. Den prediktive lakkmustesten modeller ligger hvor godt den klarer å predikere nye verdier ift observerte verdier den ikke har sett (altså hvor nærme observasjonene kommer prediksjonene).","code":""},{"path":"maskinlæring-machine-learning.html","id":"konsepter-og-definisjoner","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.2 Konsepter og definisjoner","text":"Maskinlæring knyttes tett til kunstig intelligens (Articifical Intelligence, eller bare AI). Det er imildertid ulike oppfatninger av hvordan disse begrepene forholder seg til hverandre. tillegg har begrepet “deep learning” også kommet mer fokus. En vanlig måte å se sammenhengen på er denne (SuperDataScience 2022):Sammenhengen AI - ML - DL, fra SuperDataScienceDet er videre vanlig å sele inn maskinlæring ut fra hvordan dataanalysen skjer:Supervised learning: Modellering gjennom algoritmer som kjenner både input og ønsket output.\nRegresjon: F.eks. lineær, logistisk, polynomial, SVR. Regresjon predikerer en numerisk verdi.\nKlassifisering: F.eks. lineær, SVM, beslutningstre, k-nearest neighbor, random forest. Klassifisering predikerer et sett av ordnede eller uordnede kvalitative verdier.\nRegresjon: F.eks. lineær, logistisk, polynomial, SVR. Regresjon predikerer en numerisk verdi.Klassifisering: F.eks. lineær, SVM, beslutningstre, k-nearest neighbor, random forest. Klassifisering predikerer et sett av ordnede eller uordnede kvalitative verdier.Unsupervised learning: Modellering gjennom algortimer som kjenner kun input, og forsøker finne mønstre og grupper dataene gjennom sannsynligheter (noe tilhører den ene eller den andre gruppen eller kategorien).\nClustering: Gruppering av data basert på likhet eller forskjeller\nAssosiering: Identifisering av sammenhenger mellom variabler\nDimensionality reduction: Komponentanalyse (PCA), SVD o.l.\nClustering: Gruppering av data basert på likhet eller forskjellerAssosiering: Identifisering av sammenhenger mellom variablerDimensionality reduction: Komponentanalyse (PCA), SVD o.l.Semi-supervised learning: En form mellomting mellom supervised og unsupervised, dvs. vi kan ha mindre deler av data med input og output som kan brukes på data uten kjent/ønsket output. F.eks. supervised learning bruker unsupervised learning gjennom såkalt “feature engineering”.Reinforcement learning: Modellering gjennom algoritmer der henskten/målsetningen er å maksimere en oppfatning om kumulativ belønning/utkomme. Reinforcement learning brukes f.eks. til å lære maskiner å spille spill mot mennesker.tillegg skal man ikke holde på lenge med maskinlæring før begrepene Neural networks, Natural Language Processing (NLP) og Deep learning dukker opp. Disse kan, slik vi ser det, alle assosieres med såvel supervised som unsupervised learning. Vi kommer ikke til å gå inn på disse begrepene dette kapittelet, men heller ta oss disse begrepene eget kapittel.","code":""},{"path":"maskinlæring-machine-learning.html","id":"regresjon","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3 Regresjon","text":"Vi har tidligere kapitler gått gjennom enkel og multippel OLS og polynomial regresjon. Disse er også teknikker paraplyen maskinlæring. Selv om vi har vist regresjon tidligere kapittel introduserer vi et element maskinlæringskapittelet som viser splitting av datasett treningsdata og testdata. Dette er karakteristisk maskinlæring. Det innebærer vi deler datasettet (tilfeldig) inn grupper: den første gruppa - treningsdata - bruker vi til å lage/trene en modell. Den andre gruppa - testdata - bruker vi å se hvor god modellen vi lagde med treningsdataene klarer å predikere dataene som ligger testdatasettet. Testdatasettet består jo av “virkelige” data, så hvis modellen vår er god og klarer å predikere disse dataene kan vi si noe sikrere om hvor godt vi kan anta modellen vil predikere nye, hittil ikke målte/observerte verdier.delkapittelet om regresjon skal vi bruke datasettet AmesHousingDownload AmesHousing.csvDatasettet “AmesHousing” (De Cock 2011) inneholder salgsdata 2930 boliger Ames, Iowa tidsrommet 2006 til 2010 med tilhørende 82 variabler (23 nominelle, 23 ordinale, 14 diskrete og 20 kontinuerlige). Den uavhengige variabelen er salgspris. En oversikt variablene finnes flere stedet på nett, f.eks. og mer detaljert .Vi skal dette kapittelet ta utgangspunkt det opprinnelige datasettet. neste kapittel - der vi også tar oss maskinlæring - vil vi bruke en modifisert versjon av datasettet som ligger R-pakken “tidymodels”. Vi vil også velge en litt annen tilnærming analysen, og leseren kan dermed se både litt mer av bredden tilnærminger og kunne sammenlikne hva ulike valg gjør med analysene.","code":"\nAmes <- read.csv(\"AmesHousing.csv\")\nAmesdim <- as.data.frame(dim(Ames))\nAmesdim\n#>   dim(Ames)\n#> 1      2930\n#> 2        82"},{"path":"maskinlæring-machine-learning.html","id":"manglende-verdier-i-datasettet-nas","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3.1 Manglende verdier i datasettet (NAs)","text":"Det kan innledningsvis være verdt å se om vi har variabler med mange manglende verdier (“na”). Vi kan først se på hvilke kolonner (variabler) som har manglende verdier:Vi kan se 5 variabler har et veldig høyt antall manglende verdier (NAs). Jeg velger å droppe disse variablene fra datasettet.Det er flere måter å håndtere manglende verdier på. Vi velger å erstatte NA med gjennomsnitt “Lot.Frontage” som har mange manglende verdier, og lar øvrige variabler være hva angår manglende verdier.","code":"\nnas <- colSums(is.na(Ames))\nnas <- as.data.frame(sort(nas, decreasing = TRUE))\nnames(nas)[1] <- \"Antall_NA\"\nnas[nas==0] <- NA\nnas2<-as.data.frame(nas[complete.cases(nas),])\nnas2\n#>    nas[complete.cases(nas), ]\n#> 1                        2917\n#> 2                        2824\n#> 3                        2732\n#> 4                        2358\n#> 5                        1422\n#> 6                         490\n#> 7                         159\n#> 8                         158\n#> 9                         158\n#> 10                        157\n#> 11                        157\n#> 12                         79\n#> 13                         79\n#> 14                         79\n#> 15                         79\n#> 16                         79\n#> 17                         23\n#> 18                          2\n#> 19                          2\n#> 20                          1\n#> 21                          1\n#> 22                          1\n#> 23                          1\n#> 24                          1\n#> 25                          1\nAmes2 <- subset(Ames, select = -c(Pool.QC, Misc.Feature, Alley, Fence, Fireplace.Qu))\ndim(Ames2)\n#> [1] 2930   77\nAmes2$Lot.Frontage[is.na(Ames2$Lot.Frontage)] <- mean(Ames2$Lot.Frontage, na.rm = TRUE)\nnas3 <- colSums(is.na(Ames2))\nnas3 <- as.data.frame(sort(nas3, decreasing = TRUE))\nnas3[nas3==0] <- NA\nnas4 <- as.data.frame(nas3[complete.cases(nas3),])\nnas4\n#>    nas3[complete.cases(nas3), ]\n#> 1                           159\n#> 2                           158\n#> 3                           158\n#> 4                           157\n#> 5                           157\n#> 6                            79\n#> 7                            79\n#> 8                            79\n#> 9                            79\n#> 10                           79\n#> 11                           23\n#> 12                            2\n#> 13                            2\n#> 14                            1\n#> 15                            1\n#> 16                            1\n#> 17                            1\n#> 18                            1\n#> 19                            1"},{"path":"maskinlæring-machine-learning.html","id":"uteligger","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3.2 Uteligger","text":"Den avhengige variabelen er salgspris (“SalePrice”). Hvis vi ser på distribusjonen av boligenes verdi ser vi indikasjoner på det kan være enkeltboliger med unormalt høye salgsverdier.Dette kommer tydeligere fram histogrammet:Vi får også en klar indikasjon på uteliggere gjennom boksplottet:Uteliggere er en “tricky” greie. Man kan si data har verdifull informasjon. Samtidig påvirker uteliggere mange analyser og kan gi oss større avvik prediksjoner fordi modellene påvirkes uforholdsmessig grad av uteliggere forhold til andre observasjoner. Mange av teknikkene maskinlæring er sensitive ovenfor uteliggere. Vi gikk detalj inn på dette kapittelet om OLS, så vi begrenser oss til noen analyser og viser til dette kapittelet mer utførlig forklaringer.Vi starter med Grubbs test:Høyeste salgsverdi bør altså regnes som uteligger (hvis ikke høyeste verdi bør ses som en uteligger er det ingen grunn til å gå videre).Vi kan også bruke et såkalt Hampel filter som tar utgangspunkt median:Gjennom å identifisere uteliggere gjennom Hampelfilteret kan vi fjerne uteliggere (som sagt er det mange måter å forholde seg til uteliggere på og mye er kontekstavhengig - vi går ikke inn ulike måter å behandle uteliggere på , men velger å vise hvordan vi fjerner de identifiserte uteliggerne etter Hampel).Vi kan se hvordan datasettet har endret seg som følge av de justeringene vi nå har gjort:Table 15.1: Utvikling av datasettet gjennom ‘data cleaning’Vi sjekker nå Ames3 gjennom Rosners test (de 5 mest uteliggende verdiene blant gjenværende data):Vi kan også sjekke igjen med et boksplott:Til slutt merker vi oss variabelen “SaleCondition” er en variabel som har registrert om salget har skjedd\n- “Normal” - som vi tolker som etter markedsmessige betingelser)\n- “Abnorml” - “trade, foreclosure, short sale”\n- “Family” - “Sale family members”\n- “Partial” - “Home completed last assessed (associated New Homes)”\n- “AdjLand” - “Adjoining Land Purchase”\n- “Alloca” - “Allocation - two linked properties separate deeds, typically condo garage unit”Vi kan mistenke unormalt salg, som salg innad familie eller tvangssalg, kan ha en vesentlig påvirkning på salgspris, og siden vi er interessert å modellere markedsmessige betingelser (de normale salgene) ved salgsprisen bør vi undersøke dette.Siden p-verdien er signifikant kan vi si det er signifikant forskjell mellom en eller flere av gjennomsnittsverdiene de ulike gruppene Sale.Condition.Vi tar utgangspunkt “normal” er den markedsmessige salgsprosessen vi ønsker å modellere, noe vi forsåvidt også ser ut fra antallet salgsprosesser (som overveiende grad er normale). Vi ønsker derfor å ta ut de typene salgsprosesser som har signifikant forskjellig salgsverdi forhold til normal. Av ANOVA-resultatene ser vi det gjelder “abnorml”, “AdjLand” og “Partial” (vi kan også legge merke til det ikke er signifikant forskjell på en kommersiell salgsprosess og salgsprosess kategorien “familiy” som vi kanskje forventet). Dette stemmer bra overens med indikasjonene den grafiske framstillingen ovenfor.Samlet etter alle justeringer ser utviklingen datasettet slik ut:Table 15.2: Utvikling av datasettet til Ames Housing gjennom ‘data cleaning’Vårt datasett ender altså opp med 2250 boligsalg med 77 variabler.videre analyse vil vi bruke en versjon av datasettet som kommer pakken “modeldata”. denne versjonen er det ryddet opp litt mer og det er lagt til geodata (lengde- og breddegrad på salgsobjektene). Alt vi har gjort hittil kan imidlertid være nødvendige steg forberedelser av data til analyse. Datasettet kan lastes ned :Download ames.csvVi kan se litt nærmere på datasettet vi tar med oss videre analysen. Siden boligene er registrert solgt ulike nabolag vil det være interessant å se på hvordan salgspris slår ut ift nabolag. Vi kan først vise byen Ames og de ulike salgsobjektene plottet på et kart som også viser nabolagene. Det kan ligge mye informasjon innledningsvis et plott av denne type data på kart. Bildet er laget sin helhet på kode fra Kuhn Silge (2022), og koden finnes .Kart Ames med salgsobjektene plottet inn etter nabolag - Kuhn Silge (2022)Vi kan så se på hvordan salgsprisen gjennomsnitt ser ut de ulike nabolagene.","code":"\nAmessalgspris <- ggplot() +\n    geom_point((aes(x=seq_along(Ames2$SalePrice), y=Ames2$SalePrice)), color = \"red\") + \n    ggtitle(\"Boligens salgspris\") +\n    xlab(\"Bolig nr.\") +\n    ylab(\"Verdi\")\nAmessalgspris\nggplot(Ames2, aes(x=SalePrice)) + \n    geom_histogram(color=\"black\", fill=\"lightblue\", bins = 50) +\n    labs(title = \"Histogram salgsverdi\", x = \"Salgspris i $\", y = \"Antall\") +\n    theme_classic()\nggplot(data = Ames2, aes(x = \"\", y = SalePrice)) + \n  geom_boxplot(fill = 'lightblue') +\n  labs(title = \"Boksplott salgsverdi - Ames2\",\n       y = \"Verdi\")\ngrubbstest <- grubbs.test(Ames2$SalePrice)\ngrubbstest\n#> \n#>  Grubbs test for one outlier\n#> \n#> data:  Ames2$SalePrice\n#> G = 7.18773, U = 0.98236, p-value = 0.0000000007664\n#> alternative hypothesis: highest value 755000 is an outlier\nnedregrense <- median(Ames2$SalePrice) - 3*(mad(Ames2$SalePrice, constant = 1))\novregrense <- median(Ames2$SalePrice) + 3*(mad(Ames2$SalePrice, constant = 1))\nuteligger_ind <- which(Ames2$SalePrice < nedregrense |Ames2$SalePrice > ovregrense)\nuteligger_ind\n#>   [1]   16   18   37   38   39   40   42   45   47   48   49\n#>  [12]   60   61   63   64   66   72   92   93  182  229  242\n#>  [23]  254  264  265  297  301  322  340  344  348  350  351\n#>  [34]  367  368  380  421  422  423  424  425  428  429  430\n#>  [45]  431  432  433  434  435  436  437  439  440  441  442\n#>  [56]  443  444  445  448  449  450  457  458  459  461  495\n#>  [67]  496  497  498  499  501  505  508  510  511  514  522\n#>  [78]  524  529  564  566  575  709  710  727  728  802  803\n#>  [89]  819  820  821  822  825  833  867  872  880  892  938\n#> [100]  957  958  960  961  968  969 1000 1011 1012 1013 1015\n#> [111] 1023 1028 1033 1051 1052 1053 1054 1055 1056 1057 1058\n#> [122] 1059 1060 1061 1063 1064 1065 1066 1067 1068 1069 1070\n#> [133] 1071 1073 1075 1099 1100 1102 1103 1104 1105 1106 1107\n#> [144] 1108 1110 1119 1127 1158 1159 1168 1169 1170 1171 1172\n#> [155] 1178 1181 1184 1200 1303 1307 1321 1426 1427 1460 1461\n#> [166] 1462 1498 1538 1539 1554 1556 1560 1561 1563 1564 1573\n#> [177] 1574 1575 1586 1587 1588 1636 1637 1638 1640 1641 1642\n#> [188] 1643 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694\n#> [199] 1695 1696 1697 1698 1700 1701 1702 1704 1705 1706 1707\n#> [210] 1708 1709 1710 1711 1712 1723 1724 1725 1761 1762 1764\n#> [221] 1765 1766 1768 1770 1771 1772 1773 1775 1778 1779 1781\n#> [232] 1785 1791 1799 1800 1806 1828 1829 1833 1834 1852 1853\n#> [243] 1854 1861 1902 1946 2072 2093 2097 2098 2100 2101 2104\n#> [254] 2114 2116 2119 2145 2153 2155 2215 2219 2231 2246 2257\n#> [265] 2270 2273 2275 2276 2277 2317 2321 2322 2329 2330 2331\n#> [276] 2332 2333 2334 2335 2336 2337 2340 2341 2342 2354 2379\n#> [287] 2380 2381 2383 2384 2385 2386 2387 2388 2389 2390 2391\n#> [298] 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402\n#> [309] 2403 2405 2408 2409 2441 2443 2444 2445 2446 2447 2448\n#> [320] 2450 2451 2452 2453 2454 2456 2457 2458 2462 2501 2502\n#> [331] 2523 2618 2667 2726 2736 2738 2760 2767 2794 2801 2844\n#> [342] 2881 2884 2885 2892 2900 2901 2902 2903\nAmes3 <- subset(Ames2, Ames2$SalePrice > nedregrense & Ames2$SalePrice < ovregrense)\nv1 <- as.data.frame(dim(Ames))\nv2 <- as.data.frame(dim(Ames2))\nv3 <- as.data.frame(dim(Ames3))\nAmesendring <- data.frame(c(v1, v2, v3))\nrownames(Amesendring) <- c(\"Observasjoner\", \"Variabler\")\ncolnames(Amesendring) <- c(\"Ames\", \"Ames2\", \"Ames3\")\nkable(Amesendring, caption = \"Utvikling av datasettet gjennom 'data cleaning'\")\nrosnerstest <- rosnerTest(Ames3$SalePrice,\n  k = 5)\nrosnerstest$all.stats\n#>   i   Mean.i     SD.i  Value Obs.Num    R.i+1 lambda.i+1\n#> 1 0 159338.7 46945.31 270000     209 2.357239   4.264693\n#> 2 1 159295.8 46903.77 270000     224 2.360242   4.264604\n#> 3 2 159252.8 46862.12 270000     316 2.363257   4.264515\n#> 4 3 159209.8 46820.36 270000     447 2.366283   4.264426\n#> 5 4 159166.8 46778.49 270000    1563 2.369320   4.264338\n#>   Outlier\n#> 1   FALSE\n#> 2   FALSE\n#> 3   FALSE\n#> 4   FALSE\n#> 5   FALSE\nrosnerstest$n.outliers\n#> [1] 0\nggplot(data = Ames3, aes(x = \"\", y = SalePrice)) + \n  geom_boxplot(fill = 'lightblue') +\n  labs(title = \"Boksplott salgsverdi - Ames3\",\n       y = \"Verdi\")\ngroup_by(Ames3, Sale.Condition) %>%\n  summarise(\n    count = n(),\n    mean = mean(SalePrice, na.rm = TRUE))\n#> # A tibble: 6 × 3\n#>   Sale.Condition count    mean\n#>   <chr>          <int>   <dbl>\n#> 1 Abnorml          175 131845.\n#> 2 AdjLand           12 108917.\n#> 3 Alloca            22 147735.\n#> 4 Family            44 148477.\n#> 5 Normal          2184 159305.\n#> 6 Partial          142 203162.\nggplot(Ames3, aes(x=Sale.Condition, y=SalePrice, fill=Sale.Condition)) + \n    geom_boxplot()\nres.aov <- aov(SalePrice ~ Sale.Condition, data = Ames3)\nsummary(res.aov)\n#>                  Df        Sum Sq     Mean Sq F value\n#> Sale.Condition    5  443659381081 88731876216   43.59\n#> Residuals      2573 5237897079256  2035715927        \n#>                             Pr(>F)    \n#> Sale.Condition <0.0000000000000002 ***\n#> Residuals                             \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(res.aov)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = SalePrice ~ Sale.Condition, data = Ames3)\n#> \n#> $Sale.Condition\n#>                        diff        lwr       upr     p adj\n#> AdjLand-Abnorml -22928.4076 -61325.447  15468.63 0.5298822\n#> Alloca-Abnorml   15890.3348 -13216.121  44996.79 0.6271519\n#> Family-Abnorml   16631.6303  -5068.547  38331.81 0.2446787\n#> Normal-Abnorml   27460.0338  17351.103  37568.96 0.0000000\n#> Partial-Abnorml  71317.2708  56784.346  85850.20 0.0000000\n#> Alloca-AdjLand   38818.7424  -7358.083  84995.57 0.1573930\n#> Family-AdjLand   39560.0379  -2344.749  81464.82 0.0770790\n#> Normal-AdjLand   50388.4414  13141.912  87634.97 0.0016320\n#> Partial-AdjLand  94245.6784  55563.391 132927.97 0.0000000\n#> Family-Alloca      741.2955 -32857.280  34339.87 0.9999999\n#> Normal-Alloca    11569.6990 -16001.248  39140.65 0.8385986\n#> Partial-Alloca   55426.9360  25945.204  84908.67 0.0000013\n#> Normal-Family    10828.4035  -8764.172  30420.98 0.6144474\n#> Partial-Family   54685.6405  32484.640  76886.64 0.0000000\n#> Partial-Normal   43857.2370  32713.754  55000.72 0.0000000\nAmes4 <- Ames3[Ames3$Sale.Condition != \"Abnorml\" & Ames3$Sale.Condition != \"AdjLand\" & Ames3$Sale.Condition != \"Partial\",]\ngroup_by(Ames4, Sale.Condition) %>%\n  summarise(\n    count = n(),\n    mean = mean(SalePrice, na.rm = TRUE))\n#> # A tibble: 3 × 3\n#>   Sale.Condition count    mean\n#>   <chr>          <int>   <dbl>\n#> 1 Alloca            22 147735.\n#> 2 Family            44 148477.\n#> 3 Normal          2184 159305.\nv4 <- as.data.frame(dim(Ames4))\nAmesendring2 <- data.frame(c(v1, v2, v3, v4))\nrownames(Amesendring2) <- c(\"Observasjoner\", \"Variabler\")\ncolnames(Amesendring2) <- c(\"Ames\", \"Ames2\", \"Ames3\", \"Ames4\")\nkable(Amesendring2, caption = \"Utvikling av datasettet til Ames Housing gjennom 'data cleaning'\")#> Reading layer `iowa_highway' from data source \n#>   `C:\\Users\\nilsk\\OneDrive - Høgskolen i Innlandet\\Dokumenter\\Undervisning\\AKA3\\iowa_highway.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 343866 features and 0 fields\n#> Geometry type: LINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: -96.64263 ymin: 40.37181 xmax: -90.12934 ymax: 43.50618\n#> CRS:           NA\n#> png \n#>   2\n#> png \n#>   2\n#> Warning: Removed 2856 rows containing missing values\n#> (geom_point).\n#> png \n#>   2\n#> png \n#>   2\n#> png \n#>   2\n#> png \n#>   2\names %>% \n  ggplot(aes(x = Sale_Price/1000, y = Neighborhood, fill = Neighborhood)) + \n    geom_density_ridges() + \n    theme_minimal() + \n    labs(x='Salgspris 1000$', y = 'Nabolag') +\n    theme_ridges(font_size = 11, center_axis_labels = TRUE) + \n    theme(legend.position = 'none') + \n    labs(caption = \"Salgspris pr nabolag\")\n#> Picking joint bandwidth of 16"},{"path":"maskinlæring-machine-learning.html","id":"deling-av-datasettet-i-trenings--og-testdatasett","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3.3 Deling av datasettet i trenings- og testdatasett","text":"Vi deler datasettet trenings- og testsett siden vi vil trenge dette regresjon som metodikk maksinlæring.Table 15.3: Splitting av datasettet Ames Housing fra pakken ‘modeldata’","code":"\nset.seed(123)\nsplitt <- sample.split(ames$Sale_Price, SplitRatio = .8)\names_trening <- subset(ames, splitt == TRUE)\names_test <- subset(ames, splitt == FALSE)\ndim1 <- as.data.frame(dim(ames_trening))\ndim2 <- as.data.frame(dim(ames_test))\ndimensjoner <- data.frame(dim1, dim2)\nrownames(dimensjoner) <- c(\"Observasjoner\", \"Variabler\")\ncolnames(dimensjoner) <- c(\"Treningssett\", \"Testsett\")\nkable(dimensjoner, caption = \"Splitting av datasettet Ames Housing fra pakken 'modeldata'\")"},{"path":"maskinlæring-machine-learning.html","id":"enkel-lineær-regresjon","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3.4 Enkel lineær regresjon","text":"den enkle lineære regresjonsn ser vi på forholdet mellom salgspris som avhengig variabel og “Gr.Liv.Area” som er boligareal (ikke inkludert kjeller).Det vil være rimelig å anta det er en sammenheng mellom boligareal og salgspris.Vi kan se det er en sammenheng, men også det er et slags konisk/traktmønster der spredningen verdiene øker med boligarealet. Dette kan tyde på det er en (eller flere) andre variabler som påvirker sammenhengen mellom boligareal og salgspris. Dette kan f.eks. være beliggenhet/hvilket område av byen boligen ligger (registrert variabelen “Neighborhood”). Vi kan se på medianprisen de uilke områdene:Så kan vi se på hvilke verdier modellen vil predikere på testdataene:Table 15.4: Første 10 differanser mellom predikert og observert verdiAv plasshensyn viser vi kun de første 10 sammenlikningene mellom predikert og faktisk verdi. Den venstre kolonnen tabellen er verdien modellen predikerer, den høyre kolonnen tilhørende observerte verdi testsettet.kapittelet om polynomisk regresjon beskrev vi “Mean Square Error” (MSE) slik:\\(MSE = \\frac{1}{n}*\\sum(faktisk\\ verdi - predikert\\ verdi)\\)Vi kan regne ut MSE slik:En annen vanlig måte å se på modellens prediksjonsevne er RMSE.MSE/RMSE kan fortelle oss noe om hvor god modellen er ved den sammenlikner observerte verdier mot predikerte verdier. RMSE forteller oss f.eks. den gjennomsnittlige avstanden mellom en predikert verdi og den “tilhørende” observerte verdien. Jo nærmere 0 verdien er, jo bedre predikerer modellen (0 = perfekt prediksjon). RMSE er 50882.51. Dvs gjennomsnitt bommer modellens prediksjoner med $50882.51. Vi kan tolke dette kontekst av dataene. Det er vanskelig å si noe fornuftig om den gjennomsnittlig feilen prediksjonen uten å se på den avhengige variabelen. La oss derfor se på den.Table 15.5: Deskriptiv statistikk den avhengige variabelen SalePriceVi kan se boligprisen varierer mellom 12789 og 755000, med et standardavvik (= verdienes gjennomsnittlig avstand fra gjennomsnittet) på 79886.69. det perspektivet bør altså vurdere om en gjennomsnittlig feil på 50882.51 indikerer en modell vi er fornøyd med eller ikke. Jeg vil være tilbøyelig til å si dette virker som et noe høyt gjennomsnittlig avvik prediksjonene.Vi kan enklere tolke og sammenlikne RMSE hvis vi normaliserer verdien.Det finnes fire beskrevne måter å normalisere RMSE (Otto 2019):Table 15.6: Sammenlikning av RMSE-verdi 4 ulike beregningsmåterEn lettere veg R er gjennom pakken tdr (hensikten med utregning ovenfor er å vise - utregningen):Table 15.7: Teststatistikk modell og prediksjon fra pakken ‘tdr’Normalisert RMSE er 0.14 (ut fra 0.143331).Merk “tdr”-pakken bruker snitt til å normalisere kolonnen “cvrmse” og MaksMin å normalisere kolonnen “nrmse” (hvilket vi ser gjennom de like verdiene fra pakken og fra vår manuelle utregning).Jaha…så hvilken metode skal man bruke siden det gir fire ulike normaliserte verdier?Otto (2019) finner sin simulering “normalization method (one tested ) obviously superior”, men gir følgende råd:Gjennomsnitt er ok hvis man sammenlikner modeller som baserer seg på samme responsvariabel. Hvis ikke, bruk en av de andre tre.Maks-min metoden sies å være mer sensitiv utvalgsstørrelse, men Otto fant ingen større forskjeller utvalg helt ned til 5IQR sies å være mindre sensitiv ekstreme verdierVi må rett og slett ta et valg ut fra de (få) retningslinjene som f.eks. Otto (2019) gir, men vi bør hvert fall være obs på dette hvis vi bruker normalisert RSME til å sammenlikne modeller slik vi ikke sammenlikner epler og pærer.","code":"\nenkelOLS <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_trening)\nsjPlot::tab_model(enkelOLS)\nenkelOLSplott <- ggplot() +\n    geom_point(aes(x = ames_trening$Gr_Liv_Area, y = ames_trening$Sale_Price), col = \"red\") +\n    geom_line(aes(x = ames_trening$Gr_Liv_Area, y = predict(enkelOLS, newdata = ames_trening)), col = \"blue\") +\n    ggtitle(\"Enkel OLS - treningsdata\") +\n    xlab(\"Boligareal (ikke inkl. kjeller)\") +\n    ylab(\"Salgspris\") + \n    theme_bw()\nenkelOLSplott\nnabolag <- tapply(ames$Sale_Price, ames$Neighborhood, median)\nnabolag <- sort(nabolag, decreasing = TRUE)\nnabolag\n#>                             Stone_Brook \n#>                                319000.0 \n#>                      Northridge_Heights \n#>                                317750.0 \n#>                              Northridge \n#>                                302000.0 \n#>                             Green_Hills \n#>                                280000.0 \n#>                                 Veenker \n#>                                250250.0 \n#>                              Timberland \n#>                                232106.5 \n#>                                Somerset \n#>                                225500.0 \n#>                                Crawford \n#>                                200624.0 \n#>                           College_Creek \n#>                                200000.0 \n#>                                  Greens \n#>                                198000.0 \n#>                             Clear_Creek \n#>                                197500.0 \n#>                     Bloomington_Heights \n#>                                191500.0 \n#>                                 Gilbert \n#>                                183000.0 \n#>                          Northwest_Ames \n#>                                181000.0 \n#>                             Sawyer_West \n#>                                180000.0 \n#>                                Mitchell \n#>                                153500.0 \n#>                         Northpark_Villa \n#>                                143750.0 \n#>                              North_Ames \n#>                                140000.0 \n#>                                Landmark \n#>                                137000.0 \n#> South_and_West_of_Iowa_State_University \n#>                                136200.0 \n#>                                  Sawyer \n#>                                135000.0 \n#>                                 Blueste \n#>                                130500.0 \n#>                               Brookside \n#>                                126750.0 \n#>                                 Edwards \n#>                                125000.0 \n#>                                Old_Town \n#>                                119900.0 \n#>                  Iowa_DOT_and_Rail_Road \n#>                                106500.0 \n#>                               Briardale \n#>                                106000.0 \n#>                          Meadow_Village \n#>                                 88250.0\n# dotchart(nabolag, pch = 21, bg = \"red\",\n         # cex = 0.85,\n         # xlab=\"Medianpris solgt bolig\",\n         # main = \"Sortering av boliger etter median salgspris fordelt nabolag - Ames4\")\ny_prediksjon1 <- predict(enkelOLS, newdata = ames_test)\nOLSpred <- as.data.frame(y_prediksjon1)\nobs1 <- as.data.frame(ames_test$Sale_Price)\ndf1 <- as.data.frame(c(OLSpred, obs1))\ndf1$Differanse <- round(abs(df1$y_prediksjon1 - df1$ames_test.Sale_Price),0)\ncolnames(df1) <- c(\"Predikert\", \"Observert\", \"Abs differanse\")\nkable(head(df1, n = 10), caption = \"Første 10 differanser mellom predikert og observert verdi\")\n((1/(nrow(df1))))*sum((df1$Observert - df1$Predikert)^2)\n#> [1] 2589030075\n# Alternativt:\nmseOLS <- mean((df1$Observert - df1$Predikert)^2)\nmseOLS\n#> [1] 2589030075\nrmsepred1 <- sqrt(mean((df1$Observert - df1$Predikert)^2))\nrmsepred1\n#> [1] 50882.51\ndeskSalePrice <- describe(ames$Sale_Price)\nkable(deskSalePrice, caption = \"Deskriptiv statistikk for den avhengige variabelen SalePrice\")\ndeskdata1 <- describe(ames_test$Sale_Price)\n# Gjennom maks og min verdier\negennrmse1 <- as.data.frame(rmsepred1/(deskdata1$max-deskdata1$min))\n# Gjennom gjennomsnitt\negennrmse2 <- as.data.frame(rmsepred1/deskdata1$mean)\n# Gjennom standardavvik\negennrmse3 <- as.data.frame(rmsepred1/deskdata1$sd)\n# Gjennom interkvartil bredde (IQR)\ninterkvart <- IQR(ames$Sale_Price)\negennrmse4 <- as.data.frame(rmsepred1/(interkvart))\nnormrmse <- data.frame(c(egennrmse1, egennrmse2, egennrmse3, egennrmse4))\nnormrmse <- rename(normrmse, MaksMin = rmsepred1..deskdata1.max...deskdata1.min.,\n       Snitt = rmsepred1.deskdata1.mean,\n       Standardavvik = rmsepred1.deskdata1.sd,\n       IQR = rmsepred1..interkvart.)\nkable(normrmse, caption = \"Sammenlikning av RMSE-verdi for 4 ulike beregningsmåter\")\nnormrmse1 <- as.data.frame(tdStats(df1$Predikert, ames_test$Sale_Price))\nnormrmseverdi <- normrmse1[12,]\nkable(normrmse1, caption = \"Teststatistikk for modell og prediksjon fra pakken 'tdr'\")"},{"path":"maskinlæring-machine-learning.html","id":"multippel-regresjon","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.3.5 Multippel regresjon","text":"Table 15.8: Korrelasjoner større enn +/- 0.5 blant numeriske variabler datasettetUt fra dette utvider vi med åtte uavhengige variabler. tillegg til boligstørrelse tar vi nå inn “Year_Built”, “Year_Remod_Add”, “Mas_Vnr_Area”, “Total_Bsmt_SF”, “First_Flr_SF”, “Full_Bath”, “Garage_Cars” og “Garage_Area” som er hhv. byggeår, nyeste år oppussing, murflisareal, totalt kjellerareal, areal første etasje, antall fullverdige bad, størrelse garasje ut fra antall biler og størrelse garasjen areal.Lager modell på treningssettet:Prediksjon ift testdata (10 første verdier):Table 15.9: Første 10 differanser mellom predikert og observert verdiVi kan sammenlikne MSE mellom multippel og enkel lineær:Table 15.10: Sammenlikning MSE hhv enkel lineær og multippel regresjonDet ser ut til den enkle OLS har vesentlig høyere MSE enn den multiple regresjonsmodellen.Vi kan også se på RMSE:Table 15.11: Teststatistikk modell og prediksjon fra pakken ‘tdr’den enkle lineære var NRMSE 0.14, mens det den multiple var 0.12. \\(R^2\\) hadde den enkle lineære 0.52 og den multiple 0.78.","code":"\nnumerisk <- ames %>% select(where(is.numeric))\nkorrelasjoner <- as.data.frame(cor(numerisk, ames$Sale_Price))\nkorrelasjoner <- na.omit(korrelasjoner)\nkorrelasjoner <- korrelasjoner %>%\n    filter(V1 > 0.5 | V1 < -0.5)\nkable(korrelasjoner, caption = \"Korrelasjoner større enn +/- 0.5 blant numeriske variabler i datasettet\")\nmultiregressor <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Year_Remod_Add + Mas_Vnr_Area + Total_Bsmt_SF + First_Flr_SF + Full_Bath + Garage_Cars + Garage_Area, data = ames_trening)\nsjPlot::tab_model(multiregressor)\ny_prediksjon2 <- predict(multiregressor, newdata = ames_test)\ny_pred2 <- as.data.frame(y_prediksjon2)\nobs2 <- as.data.frame(ames_test$Sale_Price)\ndf2 <- as.data.frame(c(y_pred2, obs2))\ndf2$Differanse <- round(abs(df2$y_prediksjon2 - df2$ames_test.Sale_Price),0)\ncolnames(df2) <- c(\"Predikert\", \"Observert\", \"Abs differanse\")\nkable(head(df2, n = 10), caption = \"Første 10 differanser mellom predikert og observert verdi\")\nmsemultippel <- mean((df2$Observert - df2$Predikert)^2)\nmsetab <- data.frame(mseOLS, msemultippel)\nmsetab$Differanse <- round(msetab$mseOLS - msetab$msemultippel, 0)\ncolnames(msetab) <- c(\"MSE enkel linear\", \"MSE multippel\", \"Differanse\")\nkable(msetab, caption = \"Sammenlikning MSE for hhv enkel lineær og multippel regresjon\")\nnormrmse2 <- as.data.frame(tdStats(y_prediksjon2, ames_test$Sale_Price))\nnormrmseverdi2 <- normrmse2[12,]\nkable(normrmse2, caption = \"Teststatistikk for modell og prediksjon fra pakken 'tdr'\")\nrmse1 <- nrmse(y_prediksjon1, ames_test$Sale_Price)\nrmse2 <- nrmse(y_prediksjon2, ames_test$Sale_Price)\nrmsesammenlikning <- data.frame(c(rmse1, rmse2))\nny <- c(\"Enkel linear\", \"Multippel\")\nrmsesammenlikning$Modell <- ny\nrmsesammenlikning <- rmsesammenlikning[, c(2,1)]\nny2 <- c(\"Gr_Liv_Area\", \"Gr_Liv_Area, Year_Built, Year_Remod_Add, Mas_Vnr_Area, Total_Bsmt_SF, First_Flr_SF, Full_Bath, Garage_Cars, Garage_Area\")\nrmsesammenlikning$Uavhengig <- ny2\ncolnames(rmsesammenlikning) <- c(\"Modell\", \"RMSE\", \"Uavhengige variabler\")\nkable(rmsesammenlikning)"},{"path":"maskinlæring-machine-learning.html","id":"polynomisk-regresjon-1","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.4 Polynomisk regresjon","text":"La oss anta vi ønsker å se på sammenhengen mellom byggeår og salgspris. Vi illustrerer dette gjennom en enkel lineær regresjon:Vi kan se den lineære regresjonen kanskje ikke er den beste modellen. Det kan se ut til trenden datapunktene ligger en svak kurve. Vi kan forsøke uilke polynomialledd å se om vi kan finne en bedre modell med det.Vi kan se av plottet store deler av datasettet, la oss si fra ca. 1930 til 2010, har rimelig lik prediksjon de ulike polynomlikningene. Den lineære predikerer, ikke uventet, høyere verdier fram til ca. 1990 og deretter lavere verdier enn de polynome.Vi kan sammenlikne modellene ved å se på Residual Sum Squares (RSS). RSS finnes slik:\\(RSS = \\sum(\\hat{Y_i-Y_i)^2}\\)Fra tidligere har vi regnet ut MSE:\\(MSE = \\frac{1}{n}\\sum(\\hat{Y_i-Y_i)^2}\\)og RMSE:\\(\\sqrt{MSE}=\\sqrt{\\frac{1}{n}*RSS}\\)Alle måler med andre ord forskjellen/avvikene mellom predikerte og observerte verdier. et rent modellvalg har det forsåvidt ikke så mye å si hvilken av de tre vi bruker - lavest verdi vil indikere best modell prediksjon. tolkning vil det imidlertid være lettere å tolke RMSE. vårt eksempel vil RSS-verdien være gjennomsnittlig avvik mellom predikert og observert målt \\(dollar^2\\), mens RMSE vil være samme avvik målt \\(dollar\\). Så en direkte tolkning vil RMSE være lettere å bruke enn RSS.Vi regner derfor ut RSS både trenings- og testdataene. Først lager vi prediksjonene modellene:Deretter kan vi sette opp en sammenlikning av RSS-verdiene en tabell, der vi sorterer fra laveste til høyeste verdi RSS-verdien testsettet.Table 15.12: RSS sammenlikning trenings - og testdataSom vi kunne forvente ut fra plottet av modellene lenger opp er forskjellene RSS-verdien ganske liten mellom modellene. Vi kan også lage et interessant plott der vi plotter RSS-verdiene “mot hverandre”.Grafen illustrerer det vi kan se fra tabellen - modell 9 ser ut til å predikere best. Vi kan vise det samme med RMSE (som sagt er RMSE, RSS og MSE forsåvidt mål på det samme).Table 15.13: RMSE testdataSom sagt kan RMSE tolkes direkte, så en RMSE på 47123.327 beste modell innebærer modellen gjennomsnitt bommer med 47123.327dollar på boligprisen. Samtidig ser vi forskjellene, som vi har sett tidligere, er små mellom modellene. Selv med den lineære er forskjellen mellom beste og dårligste modell dette tilfellet 2333.652.Vår modell blir dermed (på hele datasettet):Oppsummert kan vi se på RMSE de tre modellene vi så langt har sett på:Table 15.14: Sammenlikning av RMSE valgte modeller","code":"\ntheme_set(theme_bw())\n\nggplot(ames_trening, aes(Year_Built, Sale_Price)) +\n    labs(x = \"Bolig bygd\", y = \"Salgspris\") +\n  geom_point(color = \"red\") +\n  geom_smooth(method = 'lm', se = FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\ntheme_set(theme_classic())\n\npoly_reg1 <- lm(formula = Sale_Price ~ poly(Year_Built, 1), data = ames_trening)\npoly_reg2 <- lm(formula = Sale_Price ~ poly(Year_Built, 2), data = ames_trening)\npoly_reg3 <- lm(formula = Sale_Price ~ poly(Year_Built, 3), data = ames_trening)\npoly_reg4 <- lm(formula = Sale_Price ~ poly(Year_Built, 4), data = ames_trening)\npoly_reg5 <- lm(formula = Sale_Price ~ poly(Year_Built, 5), data = ames_trening)\npoly_reg6 <- lm(formula = Sale_Price ~ poly(Year_Built, 6), data = ames_trening)\npoly_reg7 <- lm(formula = Sale_Price ~ poly(Year_Built, 7), data = ames_trening)\npoly_reg8 <- lm(formula = Sale_Price ~ poly(Year_Built, 8), data = ames_trening)\npoly_reg9 <- lm(formula = Sale_Price ~ poly(Year_Built, 9), data = ames_trening)\npoly_reg10 <- lm(formula = Sale_Price ~ poly(Year_Built, 10), data = ames_trening)\n\n# Viser kun 5 av de 10 polynomfunksjonene (+ den lineære)\npolykurver <- ggplot(ames_trening) +\n  geom_point(aes(Year_Built, Sale_Price, col = \"lightgrey\"), cex = 2) +\n    labs(x = \"Bolig bygd\", y = \"Salgspris\") +\n    scale_x_continuous(breaks = seq(min(ames_trening$Year_Built), max(ames_trening$Year_Built), by = 10)) +\n    scale_y_continuous(breaks = seq(min(0), max(ames_trening$Sale_Price), by = 50000)) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x,1), aes(Year_Built, poly_reg1$fitted.values, col = \"Poly 1\"), se = FALSE) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x,2), aes(Year_Built, poly_reg2$fitted.values, col = \"Poly 2\"), se = FALSE) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x,4), aes(Year_Built, poly_reg4$fitted.values, col = \"Poly 4\"), se = FALSE) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x,6), aes(Year_Built, poly_reg6$fitted.values, col = \"Poly 6\"), se = FALSE) +\n  stat_smooth(method = \"lm\", formula = y  ~poly(x,8), aes(Year_Built, poly_reg8$fitted.values, col = \"Poly 8\"), se = FALSE) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x,10), aes(Year_Built, poly_reg10$fitted.values, col = \"Poly 10\"), se = FALSE) +\n  scale_colour_manual(\"\",\n                      breaks = c(\"Original\",  \"Poly 1\", \"Poly 2\", \"Poly 4\", \"Poly 6\", \"Poly 8\", \"Poly 10\"),\n                      values = c(\"blue\",\"red\", \"purple\",\"orange\",\"sienna\", \"black\", \"#FF689F\")) \npolykurver\npolykurver2 <- polykurver +\n    xlim(1935,2010)\n#> Scale for 'x' is already present. Adding another scale\n#> for 'x', which will replace the existing scale.\npolykurver2\npolypred1 <- predict(poly_reg1, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred2 <- predict(poly_reg2, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred3 <- predict(poly_reg3, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred4 <- predict(poly_reg4, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred5 <- predict(poly_reg5, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred6 <- predict(poly_reg6, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred7 <- predict(poly_reg7, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred8 <- predict(poly_reg8, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred9 <- predict(poly_reg9, newdata = data.frame(Year_Built = ames_test$Year_Built))\npolypred10 <- predict(poly_reg10, newdata = data.frame(Year_Built = ames_test$Year_Built))\ntrening_rss1 <- mean((ames_trening$Sale_Price - poly_reg1$fitted.values)^2)  \ntrening_rss2 <- mean((ames_trening$Sale_Price - poly_reg2$fitted.values)^2)  \ntrening_rss3 <- mean((ames_trening$Sale_Price - poly_reg3$fitted.values)^2)  \ntrening_rss4 <- mean((ames_trening$Sale_Price - poly_reg4$fitted.values)^2)\ntrening_rss5 <- mean((ames_trening$Sale_Price - poly_reg5$fitted.values)^2)\ntrening_rss6 <- mean((ames_trening$Sale_Price - poly_reg6$fitted.values)^2)\ntrening_rss7 <- mean((ames_trening$Sale_Price - poly_reg7$fitted.values)^2)\ntrening_rss8 <- mean((ames_trening$Sale_Price - poly_reg8$fitted.values)^2)\ntrening_rss9 <- mean((ames_trening$Sale_Price - poly_reg9$fitted.values)^2)\ntrening_rss10 <- mean((ames_trening$Sale_Price - poly_reg10$fitted.values)^2)\n\ntest_rss1 <- mean((ames_test$Sale_Price - polypred1)^2)  \ntest_rss2 <- mean((ames_test$Sale_Price - polypred2)^2)  \ntest_rss3 <- mean((ames_test$Sale_Price - polypred3)^2)  \ntest_rss4 <- mean((ames_test$Sale_Price - polypred4)^2)\ntest_rss5 <- mean((ames_test$Sale_Price - polypred5)^2)  \ntest_rss6 <- mean((ames_test$Sale_Price - polypred6)^2)\ntest_rss7 <- mean((ames_test$Sale_Price - polypred7)^2)\ntest_rss8 <- mean((ames_test$Sale_Price - polypred8)^2)\ntest_rss9 <- mean((ames_test$Sale_Price - polypred9)^2)\ntest_rss10 <- mean((ames_test$Sale_Price - polypred10)^2)\n\nrss_sammenlikning_trening <- data.frame(c(trening_rss1, trening_rss2, trening_rss3, trening_rss4, trening_rss5, trening_rss6, trening_rss7, trening_rss8, trening_rss9, trening_rss10))\nny3 <- c(\"Enkel linear\", \"Poly 2\", \"Poly 3\", \"Poly 4\", \"Poly 5\", \"Poly 6\",\"Poly 7\",\"Poly 8\",\"Poly 9\", \"Poly 10\")\nrss_sammenlikning_trening$Modell <- ny3\nrss_sammenlikning_trening <- rss_sammenlikning_trening[, c(2,1)]\ncolnames(rss_sammenlikning_trening) <- c(\"Modell\", \"RSS Trening\")\n\nrss_sammenlikning_test <- data.frame(c(test_rss1, test_rss2, test_rss3, test_rss4, test_rss5, test_rss6, test_rss7, test_rss8, test_rss9, test_rss10))\nny4 <- c(\"Enkel linear\", \"Poly 2\", \"Poly 3\", \"Poly 4\", \"Poly 5\", \"Poly 6\", \"Poly 7\", \"Poly 8\", \"Poly 9\", \"Poly 10\")\nrss_sammenlikning_test$Modell <- ny4\nrss_sammenlikning_test <- rss_sammenlikning_test[, c(2,1)]\ncolnames(rss_sammenlikning_test) <- c(\"Modell\", \"RSS Test\")\n\nrss_sammenlikning <- merge(rss_sammenlikning_trening, rss_sammenlikning_test, by = \"Modell\")\nrss_sammenlikning <- rss_sammenlikning %>% arrange(rss_sammenlikning$`RSS Test`) \nkable(rss_sammenlikning, caption = \"RSS sammenlikning for trenings - og testdata\")\ntrening_rss <- scale(c(trening_rss2, trening_rss3, trening_rss4, trening_rss5, trening_rss6, trening_rss7, trening_rss8, trening_rss9, trening_rss10)) # scaling \ntest_rss <- scale(c(test_rss2, test_rss3, test_rss4, test_rss5,  test_rss6, test_rss7, test_rss8, test_rss9, test_rss10))      # scaling\norders <- c(2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nggplot() +\n  geom_line(aes(orders, trening_rss, col = \"Trening RSS\")) +\n  geom_line(aes(orders, test_rss, col = \"Test RSS\")) +\n  scale_x_continuous(breaks = seq(min(2), max(10), by = 1)) +    \n  scale_colour_manual(\"\", \n                      breaks = c(\"Trening RSS\", \"Test RSS\"), \n                      values = c(\"blue\", \"red\")) +\n  ylab(\"RSS\") +\n  xlab(\"Polynomfunksjon\") \npolyrmse1 <- as.data.frame(tdStats(polypred1, ames_test$Sale_Price))\npolyrmse1 <- polyrmse1[7,]\npolyrmse1 <- round(polyrmse1, 3)   \npolyrmse2 <- as.data.frame(tdStats(polypred2, ames_test$Sale_Price))\npolyrmse2 <- polyrmse2[7,]\npolyrmse2 <- round(polyrmse2, 3) \npolyrmse3 <- as.data.frame(tdStats(polypred3, ames_test$Sale_Price))\npolyrmse3 <- polyrmse3[7,]\npolyrmse3 <- round(polyrmse3, 3) \npolyrmse4 <- as.data.frame(tdStats(polypred4, ames_test$Sale_Price))\npolyrmse4 <- polyrmse4[7,]\npolyrmse4 <- round(polyrmse4, 3) \npolyrmse5 <- as.data.frame(tdStats(polypred5, ames_test$Sale_Price))\npolyrmse5 <- polyrmse5[7,]\npolyrmse5 <- round(polyrmse5, 3) \npolyrmse6 <- as.data.frame(tdStats(polypred6, ames_test$Sale_Price))\npolyrmse6 <- polyrmse6[7,]\npolyrmse6 <- round(polyrmse6, 3) \npolyrmse7 <- as.data.frame(tdStats(polypred7, ames_test$Sale_Price))\npolyrmse7 <- polyrmse7[7,]\npolyrmse7 <- round(polyrmse7, 3) \npolyrmse8 <- as.data.frame(tdStats(polypred8, ames_test$Sale_Price))\npolyrmse8 <- polyrmse8[7,]\npolyrmse8 <- round(polyrmse8, 3) \npolyrmse9 <- as.data.frame(tdStats(polypred9, ames_test$Sale_Price))\npolyrmse9 <- polyrmse9[7,]\npolyrmse9 <- round(polyrmse9, 3) \npolyrmse10 <- as.data.frame(tdStats(polypred10, ames_test$Sale_Price))\npolyrmse10 <- polyrmse10[7,]\npolyrmse10 <- round(polyrmse10, 3)\n\nrmsesammenlikning2 <- data.frame(c(polyrmse1, polyrmse2, polyrmse3, polyrmse4, polyrmse5, polyrmse6, polyrmse7, polyrmse8, polyrmse9, polyrmse10))\nny2 <- c(\"Enkel linear\", \"Poly 2\", \"Poly 3\", \"Poly 4\", \"Poly 5\", \"Poly 6\", \"Poly 7\", \"Poly 8\", \"Poly 9\",\"Poly 10\")\nrmsesammenlikning2$Modell <- ny2\nrmsesammenlikning2 <- rmsesammenlikning2[, c(2,1)]\ncolnames(rmsesammenlikning2) <- c(\"Modell\", \"RMSE\")\nrmsesammenlikning2 <- rmsesammenlikning2 %>% arrange(rmsesammenlikning2$`RMSE`) \nkable(rmsesammenlikning2, caption = \"RMSE for testdata\")\npolykurver3 <- ggplot(ames_trening) +\n  geom_point(aes(Year_Built, Sale_Price), cex = 2) +\n    labs(x = \"Bolig bygd\", y = \"Salgspris\") +\n    scale_x_continuous(breaks = seq(min(ames$Year_Built), max(ames$Year_Built), by = 10)) +\n    scale_y_continuous(breaks = seq(min(0), max(ames$Sale_Price), by = 50000)) +\n  stat_smooth(method = \"lm\", formula = y~poly(x,9), aes(Year_Built, poly_reg9$fitted.values, col = \"Poly 9\"), se = TRUE) \npolykurver3\nrmse3 <- as_tibble(nrmse(polypred9, ames_test$Sale_Price))\nrmse3$Modell <- \"Polynomial\"\nrmse3$Uavhengig <- \"Year_Built\"\ncolnames(rmse3) <- c(\"RMSE\", \"Modell\", \"Uavhengige variabler\")\nrmse3 <- rmse3[, c(2,1,3)]\nrmsesammenlikning2 <- bind_rows(rmsesammenlikning2, rmse3)\nkable(rmsesammenlikning2, caption = \"Sammenlikning av RMSE for valgte modeller\")"},{"path":"maskinlæring-machine-learning.html","id":"linear-support-vector-regression-svr","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.5 Linear Support Vector Regression (SVR)","text":"Så kommer vi til den første nye teknikken vi knytter til maskinlæring.SVR skiller seg fra OLS regresjon ved metoden ikke forsøker å finne en regresjonslinje som gir minste kvadraters sum avstandene mellom observasjon og regresjonslinje (minimalisere feil), men stedet lar oss definere hvilken feilmargin vi anser som formålstjenlig vårt formål/vår kontekst. Grafisk kan vi illustrere dette slik (datapunktene hhv. venstre og høyre del av illustrasjonen er like):Lineær regresjon vs. SVR-regresjonI figuren ser vi den venstre delen en “tradisjonell” lineær regresjon, der vi altså forsøker å finne den linja som minimerer summen av kvadrerte avvik, der linja blir vår modell som vi vil predikere ut fra. den høyre delen viser vi prinsippet med SVR. Vi tillater et feilnivå - målt ved \\(\\epsilon\\). Vi får et bånd - en slags “margin error” - som vi tilater vår modell predikerer innenfor (vi bryr oss ikke om “feilene” innenfor dette båndet). Der vi linær regresjon bryr oss om hvert enkelt punkts avstand til regresjonslinja gjør vi ikke det SVR de punktene som ligger innenfor grensene på båndet vårt. Derimot bryr vi oss SVR om de punktene som ligger utenfor båndet (“båndet” omtales gjerne på engelsk som \\(\\epsilon\\ insensitive\\ tube\\)). disse punktene måles avstanden fra det enkelte punkt til kanten på båndet. figuren ser vi disse som \\(\\xi_1, \\xi_2...osv\\). Punktene utenfor båndet kalles “slack variables”. Så teknikken SVR går da ut på å minimere summen av avstandene fra hvert enkelt “slack variabel” til kanten av båndet (husk vi selv definerer hvor bred/smal båndet skal være). Hensikten er ulike praktiske analyser kan rettferdiggjøre ulike tilnærminger til hvor bred båndet skal være, og dermed hvor stor variasjon vi fanger opp innenfor båndet. Modellen vil derfor kunne variere med hvilken verdi vi setter på \\(\\epsilon\\).Hvorfor begrepet Support Vector Regression? Vel, alle datapunktene er vektorer (med en retning og avstand ut fra origo). Gitt vi har definert båndet (gjennom \\(\\epsilon\\)) vil punktene innenfor båndet ikke ha noen påvirkning på modellen, kun punktene utenfor. Derfor er det punktene utenfor som definerer modellen og dermed “støtter” (“supports”) modellen.Og predikerer på bakgrunn av treningssettet og sammenlikner med testdataene:Table 15.15: Første 10 differanser mellom predikert og observert verdiOppsummert kan vi se på RMSE de fire modellene vi så langt har sett på:Table 15.16: Sammenlikning av RMSE valgte modeller","code":"\nsvrreg2 <- svm(Sale_Price ~ Gr_Liv_Area + Year_Built + Year_Remod_Add + Mas_Vnr_Area + Total_Bsmt_SF + First_Flr_SF + Full_Bath + Garage_Cars + Garage_Area, data = ames_trening, type = \"eps-regression\")\nsvrreg2\n#> \n#> Call:\n#> svm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + \n#>     Year_Remod_Add + Mas_Vnr_Area + Total_Bsmt_SF + \n#>     First_Flr_SF + Full_Bath + Garage_Cars + Garage_Area, \n#>     data = ames_trening, type = \"eps-regression\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  eps-regression \n#>  SVM-Kernel:  radial \n#>        cost:  1 \n#>       gamma:  0.1111111 \n#>     epsilon:  0.1 \n#> \n#> \n#> Number of Support Vectors:  1648\nsvrpred <-  predict(svrreg2, newdata = ames_test)\nsvrpred <- as.data.frame(svrpred)\nobservert <- as.data.frame(ames_test$Sale_Price)\ndf2 <- as.data.frame(c(svrpred, observert))\ndf2$Differanse <- round(abs(df2$svrpred - df2$ames_test.Sale_Price),0)\ncolnames(df2) <- c(\"Predikert\", \"Observert\", \"Abs differanse\")\nkable(head(df1, n = 10), caption = \"Første 10 differanser mellom predikert og observert verdi\")\nmsesvr <- mean((df2$Observert - df2$Predikert)^2)\nmsesvr\n#> [1] 701249854\n\n# x <- 1:length(ames_trening$Sale_Price)\n# medianverdi <- ggplot() +\n#     geom_point(aes(x = x, y = ames_trening$Sale_Price), \n#                color = \"red\") +\n#     geom_line(aes(x = x, y = predict(svrpred, newdata = ames_test)),\n#     color = \"blue\") +\n#     ggtitle(\"Median boligverdi (regresjon på testsett)\") +\n#     xlab(\"Bolig nr.\") + \n#     ylab(\"Medianverdi\")\n# medianverdi \nrmse4 <- as_tibble(nrmse(svrpred,  ames_test$Sale_Price))\nrmse4$Modell <- \"SVR\"\nrmse4$Uavhengig <- \"Gr_Liv_Area + Year_Built + Year_Remod_Add + Mas_Vnr_Area + Total_Bsmt_SF + First_Flr_SF + Full_Bath + Garage_Cars + Garage_Area\"\ncolnames(rmse4) <- c(\"RMSE\", \"Modell\", \"Uavhengige variabler\")\nrmse4 <- rmse4[, c(2,1,3)]\nrmsesammenlikning3 <- bind_rows(rmsesammenlikning2, rmse4)\nkable(rmsesammenlikning3, caption = \"Sammenlikning av RMSE for valgte modeller\")"},{"path":"maskinlæring-machine-learning.html","id":"svr-eksempel-2","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.5.1 SVR eksempel 2","text":"Vi kan illustrere SVR på en annen type datasett. Datasettet “Boston” ligger integrert R-pakken “MASS” og ble opprinnelig publisert 1978. Datasettet inneholder boligdata fra Boston (N = 506) og var en undersøkelse “measure willingness pay clean air” (Harrison Rubinfeld 1978, s.81). dette eksempelet bruker vi variabelen “medv” som avhengig variabel (som er boligenens medianverdi målt $1000). Vi har tatt ut en variabel - “B-1000” - fra det opprinnelige datasettet.Download bostondata.xlsxVi kan se på variabelen “medv” grafisk:Vi lager SVR-modellen:Og predikerer på bakgrunn av treningssettet og sammenlikner med testdataene:Vi kan til slutt sjekke hvor godt modellen predikerer:RMSE er 3.44. La oss se på verdien kontekst. Gitt den avhengige variabelen “medv” er median boligpris målt 1000, betyr det RMSE på 3.44 * 1000 betyr modellen gjennomsnitt bommer med 3440. Er det bra eller dårlig? Vel, la oss se på variabelen:Vi kan se boligprisen varierer mellom 5000 og 50000, med et standardavvik (= verdienes gjennomsnittlig avstand fra gjennomsnittet) på 9200 (9.2). det perspektivet kan modellen se ut til å predikere rimelig bra.Jaha…så hvilken metode skal man bruke siden det gir fire ulike normaliserte verdier?vårt datasett har vi ikke noe problem med utvalgsstørrelsen. Hvis vi ser på et boksplott vil vi se vi har endel uteliggere og kanskje ekstreme verdier:Vi kan gå videre med å se på hvor inflytelsesrike uteliggerne er å se om det taler bruk av IQR, men det gjør vi ikke .","code":"#> Rows: 506\n#> Columns: 13\n#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.0690…\n#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5,…\n#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, …\n#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, …\n#> $ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, …\n#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, …\n#> $ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.…\n#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, …\n#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 31…\n#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, …\n#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43,…\n#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, …\nggplot() +\n    geom_point((aes(x=seq_along(bostondata$medv), y=bostondata$medv)), color = \"red\") + \n    ggtitle(\"Boligens medianverdi\") +\n    xlab(\"Bolig nr.\") +\n    ylab(\"Verdi\")\nset.seed(321)\nsplitt3 <- sample.split(bostondata$medv, SplitRatio = .8)\nbtreningssett <- subset(bostondata, splitt3 == TRUE)\nbtestsett <- subset(bostondata, splitt3 == FALSE)\nsvrreg <- svm(medv~., data=btreningssett)\nprint(svrreg)\n#> \n#> Call:\n#> svm(formula = medv ~ ., data = btreningssett)\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  eps-regression \n#>  SVM-Kernel:  radial \n#>        cost:  1 \n#>       gamma:  0.08333333 \n#>     epsilon:  0.1 \n#> \n#> \n#> Number of Support Vectors:  292\npred4 <-  predict(svrreg, btestsett)\nx = 1:length(btestsett$medv)\nggplot() +\n    geom_point(aes(x = x, y = btestsett$medv), \n               color = \"red\") +\n    geom_line(aes(x = x, y = predict(svrreg, newdata = btestsett)),\n              color = \"blue\") +\n    ggtitle(\"Median boligverdi (testsett)\") +\n    xlab(\"Bolig nr.\") + \n    ylab(\"Medianverdi\")\nnormrmse3 <- as.data.frame(tdStats(pred4, btestsett$medv))\nnormrmseverdi3 <- normrmse3[12,]\nnormrmseverdi4 <- normrmse3[7,]\nnormrmse3\n#>        tdStats(pred4, btestsett$medv)\n#> mo                        21.51250000\n#> mm                        20.97258013\n#> sdo                        7.54979596\n#> sdm                        5.72835018\n#> mbe                       -0.53991987\n#> mae                        2.14037091\n#> rmse                       3.43943880\n#> nmbe                      -0.01261495\n#> cvmbe                     -0.02509796\n#> nmae                       0.05000867\n#> cvmae                      0.09949429\n#> nrmse                      0.08036072\n#> cvrmse                     0.15988094\n#> r2                         0.81555551\n#> tStone                     1.33933423\ndeskdata <- describe(btestsett$medv)\ndeskdata\n#>    vars  n  mean   sd median trimmed mad min max range skew\n#> X1    1 72 21.51 7.55  20.55   20.59 4.6 7.2  50  42.8 2.06\n#>    kurtosis   se\n#> X1     6.04 0.89\nnormrmse2 <- tdStats(pred4, btestsett$medv)\nnormrmse2\n#>          mo          mm         sdo         sdm         mbe \n#> 21.51250000 20.97258013  7.54979596  5.72835018 -0.53991987 \n#>         mae        rmse        nmbe       cvmbe        nmae \n#>  2.14037091  3.43943880 -0.01261495 -0.02509796  0.05000867 \n#>       cvmae       nrmse      cvrmse          r2      tStone \n#>  0.09949429  0.08036072  0.15988094  0.81555551  1.33933423\nggplot(bostondata, aes(x = \"\", y = medv)) + geom_boxplot()"},{"path":"maskinlæring-machine-learning.html","id":"decision-tree-regression","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.6 Decision Tree Regression","text":"Vi illustrerer DTR gjennom et eksempel basert på plotly (2022). pakken “plotly” ligger et kjent datasett - mtcars - som er brukt mange lærebøker og eksempler.Download mtcars.xlsxEt tips R å se på innhold og hva de enkelte variabler er datasett som følger pakker er:Hvor man (RStudio) får opp tekst om datasettet hjelpevinduet (normalt nede til høyre skjermen). Vi kan se variabelen “wt” er vekt, “hp” er hestekrefter og “qsec” er tid bilen bruker fra 0 til 1/4 mile. Vi kan se oss wt og hp er uavhengige variabler, og tid den avhengige variabelen.Vi kan plotte dataene. Vi kan først se på de uavhengige variablene wt og hp.Siden vi har tre variabler kan vi framstille dette som et 3D-plott (vi kan ha flere variabler regresjonen naturligvis, men det klarer vi ikke å plotte).DTR starter med alle dataene (treningssettet), noe som kalles rotnode. Deretter vil en algoritme splitte dataene (noden) eller flere sub-noder. Dersom en sub-node igjen splittes kalles denne sub-noden en beslutningsnode. Dersom den ikke splittes kalles sub-noden en leaf eller terminalnode.Prinsippiell skisse beslutningstreHver node treet utgjør en test en case/et dilemma/et valg inntil det ikke kan tas flere valg akkurat den respektive linjen. DTR bruker et antall algoritmer å beslutte hvordan splitting foregår. Hver splitt søker å øke homogeniteten (hver subsett av data skal inneholder samme verdi en eller annen attributt) de resulterende sub-nodene sett opp mot den uavhengige variabelen.Modellen begynner med hele datasettet og leter gjennom alle verdier alle inputvariabler (uavhengige variabler) å finne splittverdien som deler datasettet og samtidig gir lavest mulig “Sums Squares Error”. Deretter gjentas denne prosessen inntil en stoppverdi er nådd.La oss gå tilbake til eksempelet med mtcars og se på kun de uavhengige variablene:BeslutningstreI grafen har vi visualisert beslutningstreet. Første splitt skjer ved wt = 3.6. Neste splitt skjer ved hp = 170, men bare området wt = 3.6. Deretter kommer splitt 3 ved hp = 75, men bare området wt = 3.6 og hp = 170. Splitt 4 skjer ved wt = 2.5, men bare området wt = 3.6, hp = 170 og hp = 75.Vi har fått delt opp de uavhengige variablene bokser. Hvis vi nå får et nytt punkt (eller vil predikere verdien på den avhengige variabelen ut fra gitt verdier av de uavhengige variablene) kan vi legge det nye punktet inn riktig boks ut fra verdiene på hhv. wt og hp. Den predikerte Y-verdien - verdien på den avhengige variabelen - blir ganske enkelt gjennomsnittsverdien av alle y-verdiene den respektive boksen (jfr \\(\\hat{y}_n\\) figuren ). Inndeingen boksene gjennom belsutningstreet har realiteten tilført informasjon til våre data. Vi trenger altså predikere ut fra et mye snevrere utfallsområde enn en tradisjonell tilnærming som vil ta hensyn til alle datapunktene (ikke bare de rette boks).","code":"\nglimpse(mtcars)\n#> Rows: 32\n#> Columns: 11\n#> $ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.…\n#> $ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, …\n#> $ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360…\n#> $ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123…\n#> $ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.6…\n#> $ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.5…\n#> $ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.…\n#> $ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, …\n#> $ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, …\n#> $ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, …\n# ?mtcars\nggplot(mtcars, aes(x=wt, y=hp)) + geom_point(color = \"red\") +\n    scale_x_continuous(breaks = round(seq(min(mtcars$wt), max(mtcars$wt), by = 0.5),1))\nmtcars$am[which(mtcars$am == 0)] <- 'Automatic'\nmtcars$am[which(mtcars$am == 1)] <- 'Manual'\nmtcars$am <- as.factor(mtcars$am)\nfig <- plot_ly(mtcars, x = ~wt, y = ~hp, z = ~qsec, color = ~am, colors = c('#BF382A', '#0C4B8E'))\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'wt'),\n                     yaxis = list(title = 'hp'),\n                     zaxis = list(title = '1/4 mile tid')))\nfig\ndtrplott <- ggplot(mtcars, aes(x=wt, y=hp)) + geom_point(color = \"red\")\ndtrplott +\n    geom_segment(aes(x = 1.5, y = 170, xend = 3.6, yend = 170)) +\n    annotate(geom =\"text\", x = 1.5, y = 180, label = \"Splitt 2\",\n              color = \"blue\") +\n    geom_segment(aes(x = 3.6, y = 0, xend = 3.6, yend = 350)) +\n    annotate(geom =\"text\", x = 3.4, y = 350, label = \"Splitt 1\",\n              color = \"blue\") +   \n    geom_segment(aes(x = 1.3, y = 75, xend = 3.6, yend = 75)) +\n    annotate(geom =\"text\", x= 1.5, y = 85, label = \"Splitt 3\",\n              color = \"blue\") +\n    annotate(geom =\"text\", x= 2.3, y = 20, label = \"Splitt 4\",\n              color = \"blue\") +\n    geom_segment(aes(x = 2.5, y = 0, xend = 2.5, yend = 75)) +\n    annotate(geom = \"text\", x = 3.7, y = 350, label = \"3.6\") +\n    annotate(geom = \"text\", x = 1.5, y = 160, label = \"170\") +\n    annotate(geom = \"text\", x = 1.5, y = 65, label = \"75\")  +\n    annotate(geom = \"text\", x = 2.6, y = 20, label = \"2.5\")"},{"path":"maskinlæring-machine-learning.html","id":"eksempel-dtr-på-ames-housing-data","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.6.1 Eksempel DTR på Ames Housing Data","text":"dette eksempelet skal vi bruke datasettet fra pakken “AmesHousing” som .Output beskriver stegene byggingen av beslutningstreet (splittene). Vi ser vi starter med 2477 salgsobjekter. Den første splitten skjer gjennom variabelen “Garage” (splitt på 2.5). Dette danner utgangspunkt gren nr 2 (Garage < 2.5) og 3 (Garage > 2.5). Gren 2 omfatter 2113 boliger med gjennomsnittlig salgspris på 162117,2 og gren 3 omfatter 364 boliger med gjennomsnittlig salgspris på 310039,6. Grenene 2 og 3 splittes videre hhv. 4 og 5, og 6 og 7. Osv.Plottet kan fort se smått og uoversiktlig ut (et mindre datasett med færre grener og splitter vil dette se mer oversiktlig ut), men med litt zooming vil man se beslutningstreet og f.eks. hvor mange prosent av observasjonene som ligger de repsektive grenene.Hvis vi ser litt nøyere på beslutningstreet ser vi vi ender opp med 12 terminalnoder ut av 2930 observasjoner 74 variabler treningssettet. Dette skjer fordi algoritmen rpart-pakken foretar beregninger å se hvor det ikke gir mer å inkludere flere (gjennom en kryssvalidering). Vi ser forsåvidt det samme beslutningstreoversikten lenger opp.Vi kan se dette følgende plott:grafen ser vi på y-aksen relative feil gjennom kryssvalidering. Den nedre x-aksen er “cost complexity” verdi. Jo flere terminalnoder et belsutningstre har, jo mer kompleks blir treet (modellen). Cost complexity er en terskelverdi - algoritmen vil splitte noden kun hvis modellen forbedres med en verdi større enn terskelverdien. vår modell/vårt tre skjer dette ved 11 terminalnoder. Den stiplede linjen viser en alternativ tilnærming som dette tilfellet skulle tilsi 10 terminalnoder.RMSE er 32731.97. Dvs gjennomsnitt bommer modellens prediksjoner med $32731.97 dollar.Decision Tree Regression kan seg selv være en noe lunefull og dårlig modell prediksjon. Det er ikke sjeldent å finne en god fit til dataene, men ofte en overfit (altså vi får en god fit til dataene, men lav prediksjonsnøyaktighet). Derimot er den grunnstammen (pun intended) “Random Forests” modellering som vi skal se på neste delkapittel.Oppsummert kan vi se på RMSE de fem modellene vi så langt har sett på:Table 15.17: Sammenlikning av RMSE valgte modeller","code":"\ndim(ames)\n#> [1] 2930   74\ndtregress <- rpart(\n  formula = Sale_Price ~ .,\n  data    = ames_trening)\ndtregress\n#> n= 2477 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 2477 17175510000000 183854.7  \n#>    2) Garage_Cars< 2.5 2113  6480950000000 162117.2  \n#>      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1198  1729327000000 132490.3  \n#>        8) First_Flr_SF< 1235.5 964   893631400000 124113.4 *\n#>        9) First_Flr_SF>=1235.5 234   489370500000 167000.3 *\n#>      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Greens,Green_Hills 915  2323299000000 200907.3  \n#>       10) Gr_Liv_Area< 1537 456   635546300000 175604.5  \n#>         20) Total_Bsmt_SF< 1217.5 264   157284500000 156761.3 *\n#>         21) Total_Bsmt_SF>=1217.5 192   255635500000 201513.9 *\n#>       11) Gr_Liv_Area>=1537 459  1105767000000 226044.8  \n#>         22) Total_Bsmt_SF< 1015 241   256733000000 202170.0 *\n#>         23) Total_Bsmt_SF>=1015 218   559797700000 252438.5 *\n#>    3) Garage_Cars>=2.5 364  3900291000000 310039.6  \n#>      6) Neighborhood=North_Ames,College_Creek,Old_Town,Edwards,Somerset,Gilbert,Sawyer,Northwest_Ames,Sawyer_West,Mitchell,Crawford,Iowa_DOT_and_Rail_Road,Timberland,Bloomington_Heights,Veenker 194  1056008000000 253412.6  \n#>       12) Year_Remod_Add< 1977.5 29    42316690000 148910.3 *\n#>       13) Year_Remod_Add>=1977.5 165   641327800000 271779.7  \n#>         26) First_Flr_SF< 1696 125   261432100000 253435.6 *\n#>         27) First_Flr_SF>=1696 40   206384000000 329105.2 *\n#>      7) Neighborhood=Northridge_Heights,Northridge,Stone_Brook 170  1512294000000 374660.9  \n#>       14) First_Flr_SF< 2217 154   827639300000 356888.9  \n#>         28) Gr_Liv_Area< 2647.5 124   337495700000 334857.9 *\n#>         29) Gr_Liv_Area>=2647.5 30   181193100000 447950.3 *\n#>       15) First_Flr_SF>=2217 16   167853300000 545716.7 *\nrpart.plot(dtregress)\nplotcp(dtregress)\ndtrprediksjon <- predict(dtregress, newdata = ames_test)\nmodelprediksjon <- sqrt(mean((ames_test$Sale_Price - dtrprediksjon)^2))\nrmse5 <- as_tibble(nrmse(dtrprediksjon, ames_test$Sale_Price))\nrmse5$Modell <- \"DTR\"\nrmse5$Uavhengig <- \"Alle\"\ncolnames(rmse5) <- c(\"RMSE\", \"Modell\", \"Uavhengige variabler\")\nrmse5 <- rmse5[, c(2,1,3)]\nrmsesammenlikning3 <- bind_rows(rmsesammenlikning2, rmse5)\nkable(rmsesammenlikning3, caption = \"Sammenlikning av RMSE for valgte modeller\")"},{"path":"maskinlæring-machine-learning.html","id":"random-forest-regresjon","chapter":"Kapittel 15 Maskinlæring (Machine Learning)","heading":"15.7 Random Forest regresjon","text":"","code":""},{"path":"maskinlæring-gjennom-tidymodels.html","id":"maskinlæring-gjennom-tidymodels","chapter":"Kapittel 16 Maskinlæring gjennom “tidymodels”","heading":"Kapittel 16 Maskinlæring gjennom “tidymodels”","text":"R-pakker brukt dette kapittelet:Formålet dette kapittelet er å utvikle ulike modeller å predikere salgspris på hus basert på datasettet Ames Housing Data. Vi bruker et bearbeidet datasett. Datasettet ligger ferdig bearbeidet (rådata, hvilket ikke betyr klar til analyse…) pakken “tidymodels”.Datasett:Download ames.csvVi ser på distribusjonen av den avhengige variabelen - salgsprisen:Vi kan belyse det samme et boksplott:situasjoner som dette foreskrives det ofte man ser på en transformasjon av variabelen. Feng et al. (2014) finner imidlertid atUsing transformations general log transformation particular can quite problematic. approach used, researcher must mindful limitations, particularly interpreting relevance analysis transformed data hypothesis interest original data. example, demonstrated circumstances log transformation help make data less variable normal may, circumstances, make data variable skewed. Furthermore, log-transformed data usually facilitate inferences concerning original data, since shares little common original data (s.108).Det finnes likevel andre gode, og praktiske, grunner til nettopp anvendt analyse bruker f.eks. log transformasjoner. En god redegjørelse finner du . Vi går ikke dypere inn diskusjonen utover å si log transformasjon kan være et svært godt verktøy, men man skal anvende det bevisst (som alt annet…). Et viktig poeng å tenke på før man går en log transformasjon er testverdier vi senere vil være interessert - f.eks. RMSE, jfr. forrige kapittel - også vil være på log skala og dermed vanskeligere å tolke.Siden fordelingen ser ut til å være ganske nær en normalfordeling transformerer vi variabelen.Datafordelingen ser da (selvsagt) annerledes ut.kapittelet om regresjonsanalyse brukte vi R-funksjonen “lm” lineære modeller. Det kan vi også bruke , men vi velger å bruket pakken “caret” fordi den inneholder en rekke modeller maskinlæring og funksjoner som er hendige ulike teknikker maskinlæring.Caret, gjennom funksjonen train(), gir oss en modell med følgende informasjon:Vi kan aksessere detaljer alle disse gjennom f.eks.Regresjonslikningen vår modell blir da:\\[\n\\operatorname{SalePrice} = \\alpha + \\beta_{1}(\\operatorname{Gr.Liv.Area}) + \\epsilon\n\\]","code":"\npacman::p_load(tidyverse, tidymodels, sf, patchwork, RColorBrewer, grid, ragg)\ndim(ames)\n#> [1] 2930   74\ntidymodels_prefer()\nggplot(ames, aes(x=Sale_Price)) + \n    geom_histogram(color=\"black\",\n                   fill=\"lightblue\",\n                   bins = 50) +\n    labs(x = \"Salgspris i $\",\n         y = \"Antall\",\n         caption = \"Salgspris for boliger i Ames, Iowa i perioden 2006-2010\") +\n    theme_bw()\nggplot(data = ames, aes(x = \"\", y = Sale_Price)) + \n  geom_boxplot(fill = 'lightblue') +\n  labs(y = \"Verdi\",\n       caption = \"Salgspris for boliger i Ames, Iowa i perioden 2006-2010\")\nggplot(ames, aes(x=Sale_Price)) + \n    geom_histogram(color=\"black\",\n                   fill=\"lightblue\",\n                   bins = 50) +\n    labs(x = \"Salgspris i $\",\n         y = \"Antall\",\n         caption = \"Salgspris for boliger i Ames, Iowa i perioden 2006-2010 etter log 10 transformasjon\") +\n    theme_bw() +\n    scale_x_log10()\names <- ames %>% \n    mutate(Sale_Price = log10(Sale_Price))\nggplot(data = ames, aes(x = \"\", y = Sale_Price)) + \n  geom_boxplot(fill = 'lightblue') +\n  labs(y = \"Verdi\",\n       caption = \"Salgspris for boliger i Ames, Iowa i perioden 2006-2010 etter log 10 transformasjon\") +\n  theme_bw()\npacman::p_load(tidyverse, caret, readxl, equatiomatic)\nAmes4_trening2 <- read_xlsx(\"Ames4_trening.xlsx\")\nAmes4_test2 <- read_xlsx(\"Ames4_test.xlsx\")\ntrn_kontrl <- trainControl(method = \"none\")\nenkelregr <- train(SalePrice ~ Gr.Liv.Area, data = Ames4_trening2, method = \"glm\", trControl = trn_kontrl)\nnames(enkelregr$finalModel)\n#>  [1] \"coefficients\"      \"residuals\"        \n#>  [3] \"fitted.values\"     \"effects\"          \n#>  [5] \"R\"                 \"rank\"             \n#>  [7] \"qr\"                \"family\"           \n#>  [9] \"linear.predictors\" \"deviance\"         \n#> [11] \"aic\"               \"null.deviance\"    \n#> [13] \"iter\"              \"weights\"          \n#> [15] \"prior.weights\"     \"df.residual\"      \n#> [17] \"df.null\"           \"y\"                \n#> [19] \"converged\"         \"boundary\"         \n#> [21] \"model\"             \"formula\"          \n#> [23] \"terms\"             \"data\"             \n#> [25] \"offset\"            \"control\"          \n#> [27] \"method\"            \"contrasts\"        \n#> [29] \"xlevels\"           \"xNames\"           \n#> [31] \"problemType\"       \"tuneValue\"        \n#> [33] \"obsLevels\"         \"param\"\ncoef(enkelregr$finalModel)\n#> (Intercept) Gr.Liv.Area \n#>  55322.5073     73.7485\nenkelregr2 <- lm(SalePrice ~ Gr.Liv.Area, data = Ames4_trening2)\nextract_eq(enkelregr2)"},{"path":"referanser.html","id":"referanser","chapter":"Referanser","heading":"Referanser","text":"","code":""},{"path":"vedlegg-a---sentralgrenseteoremet-central-limit-theorem.html","id":"vedlegg-a---sentralgrenseteoremet-central-limit-theorem","chapter":"Vedlegg A - Sentralgrenseteoremet (Central Limit Theorem)","heading":"Vedlegg A - Sentralgrenseteoremet (Central Limit Theorem)","text":"Koden brukt dette eksempelet er stor grad hentet fra Fedit (2018).Dette er et noe komplisert begrep som vi ikke skal gå veldig dybden på, men det har et par viktige konsekvenser oss når vi skal tenke på distribusjon av populasjoner og utvalg. belyser vi forhold som følger av sentralgrenseteoremet:Gjennomsnittsverdien (mean) av tilfeldige utvalg fra en populasjon vil være tilnærmet lik gjennomsnittsverdien populasjonen hvis størrelsen på utvalgene er tilstrekkelig stort.Fordelingen til tilfeldige utvalg fra en populasjon vil være tilnærmet normalfordelt uavhengig av fordelingen på populasjonen. Dette innebærer selv om populasjonen er langt fra normalfordelt vil et tilstrekkelig stort utvalg vise seg å være tilnærmet normalfordelt.La oss se på dette gjennom et eksempel der vi starter med en bimodal fordeling (altså langt fra normalfordeling). Vi generer et datasett og plotter et Q-Q diagram (mer om dette et annet sted notatet, men per nå trenger vi bare vite dette er en effektiv måte å sjekke om en variabel er normalfordelt eller ikke).\nFigure 16.1: Ikke-normal fordeling og Q-Q plott\nSom sagt har vi gjort rede Q-Q plott et annet sted boka, så kan vi nøye oss med å slå fast denne variabelen definitivt ikke er normalfordelt.\nDet framkommer også tydelig når vi plotter et histogram:\nFigure 16.2: Histogram bimodal fordeling\nUt fra denne populasjonen tar vi 100 utvalg med 30 hvert utvalg. Fordelingen ser da slik ut:\nFigure 6.1: Histogram 100 utvalg fra bimodal fordeling\nVi kan allerede nå ane en tilnærming mot normalfordeling, og hvert fall en endret form enn populasjonen viste. Vi tar nå 1000 utvalg med 30 hvert utvalg.\nFigure 4.1: Histogram 1000 utvalg fra bimodal fordeling\nDet er videre åpenbart dette begynner å se mer og mer ut som en normalfordeling. Til slutt øker vi til 10000 utvalg av 30.\nFigure 4.2: Histogram 10000 utvalg fra bimodal fordeling\nDet vi kan se bekrefter hva Central Limit Theorem sier vi bør forvente. Vi kan starte med en hvilken som helst fordeling (kontinuerlig eller diskret) som har et definert gjennomsnitt og definert varians (og dermed definert standardavvik) og ta tilfeldige utvalg fra denne fordelingen – og vi vil få en tilnærmet normalfordelt fordeling. det virkelige liv har vi ofte populasjonsfordelinger som har alt annet enn normalfordeling. Likevel kan vi ta tilfeldige utvalg og få en tilnærmet normalfordelt frekvensplott (av f.eks. gjennomsnittsverdier). Størrelsen på utvalget og antallet ganger vi tar utvalg vil påvirke -> jo større utvalg og jo flere utvalg, jo nærmere normalfordeling vil frekvensplottet være.En interessant illustrasjon av CLT ligger .","code":""},{"path":"vedlegg-b---chebyshevs-teorem.html","id":"vedlegg-b---chebyshevs-teorem","chapter":"Vedlegg B - Chebyshevs teorem","heading":"Vedlegg B - Chebyshevs teorem","text":"Dette vedlegget er stor grad bygget på Hartmann, Krois, Waske (2018a).Vi diskuterte noe detalj hvordan vi kan bruke normalfordelingen til å si noe om hvordan verdier et datasett kan antas å falle innenfor en gitt avstand fra gjennomsnittet (Hartmann, Krois, Waske 2018b):\nFigure 16.1: Normalfordeling med standardavvik\nVi kan ut fra normalfordeingen si at68 % av observajsonene vil ligge innenfor ett standardavvik fra gjennomsnittsverdien95 % av observasjonene vil ligge innenfor standradavvik fra gjennomsnittsverdien99.7 % av observasjonene vil ligge innenfor tre standardaavik fra gjennomsnittsverdienDette kalles ofte den empiriske regelen (“empirical rule”), og gjelder kun normalfordelte data. Chebyshevs teorem gjelder imidlertid alle fordelinger. Normalfordelingen gir oss datapunkter med en viss sannsynlighet ligger innenfor en viss avstand fra gjennomsnittsverdien. Det samme sier Chebyshevs teorem om datafordelinger som ikke er normalfordelte: bare en gitt mengde datapunkter kan ligge mer enn en gitt avstand fra gjennomsnittsverdien.Teoremet uttrykkes slik (Hartmann, Krois, Waske 2018b):ethvert nummer k større enn 1 vil minst \\(1-1/\\)k\\(^2\\) av dataverdiene ligge innenfor k standardavvik fra gjennomsnittet.Teoremet kan generisk kan framstilles slik:\nFigure 16.2: Chebyshevs teorem\nethvert numerisk datasett gjelder:Minst ¾ av datapunktene ligger innenfor standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm2s\\) et utvalg og \\(\\overline{x}\\pm2\\sigma\\) populasjoner.Minst 8/9 av datapunktene ligger innenfor tre standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm3s\\) et utvalg og \\(\\overline{x}\\pm3\\sigma\\) populasjoner.Minst \\(1-1/\\)k\\(^2\\) av datapunktene ligger mellom k standardavvik av gjennomsnittet – altså intervallet mellom endepunktene \\(\\overline{x}\\pm\\)k\\(s\\) et utvalg og \\(\\overline{x}\\pm\\)k\\(\\sigma\\) populasjoner.Ut fra tabellen ser vi dersom vi velger scroller til k = 2 vil 75 % av verdiene ligge innenfor (altså 75 % innenfor 2 standardavvik).Vi kan også vise en grafisk framstilling av Chebyshevs teorem med fokus på prosenter (y-aksen) mot k (x-aksen).\nFigure 4.1: Chebyshevs teorem - prosent\nNår vi vet minst 75% av distribusjonen ligger innenfor \\(\\overline{x}\\pm2s\\) vet vi også maksimalt 25% ligger utenfor. Likeledes \\(\\overline{x}\\pm3s\\) vil maksimalt 11,11 % av distribusjonen ligge utenfor. Så mens reglene normalfordeling kun gjelder normalfordelte eller tilnærmet-normalfordelte datasett, er Chebyshevs teorem et faktum som gjelder alle datadistribusjoner og som beskriver minimumsandelen av observasjoner/datapunkter som ligger innenfor hhv +/- 1, 2 og 3 standardavvik fra gjennomsnittet.","code":""},{"path":"vedlegg-x---session-info.html","id":"vedlegg-x---session-info","chapter":"Vedlegg X - Session Info","heading":"Vedlegg X - Session Info","text":"","code":"#> R version 4.2.1 (2022-06-23 ucrt)\n#> Platform: x86_64-w64-mingw32/x64 (64-bit)\n#> Running under: Windows 10 x64 (build 19044)\n#> \n#> Locale:\n#>   LC_COLLATE=Norwegian Bokmål_Norway.utf8 \n#>   LC_CTYPE=Norwegian Bokmål_Norway.utf8   \n#>   LC_MONETARY=Norwegian Bokmål_Norway.utf8\n#>   LC_NUMERIC=C                            \n#>   LC_TIME=Norwegian Bokmål_Norway.utf8    \n#> \n#> Package version:\n#>   base64enc_0.1.3 bookdown_0.27   brio_1.1.3     \n#>   bslib_0.3.1     cachem_1.0.6    cli_3.3.0      \n#>   compiler_4.2.1  desc_1.4.1      digest_0.6.29  \n#>   downlit_0.4.2   evaluate_0.15   fansi_1.0.3    \n#>   fastmap_1.1.0   fs_1.5.2        glue_1.6.2     \n#>   graphics_4.2.1  grDevices_4.2.1 highr_0.9      \n#>   htmltools_0.5.2 jquerylib_0.1.4 jsonlite_1.8.0 \n#>   knitr_1.39      magrittr_2.0.3  memoise_2.0.1  \n#>   methods_4.2.1   R6_2.5.1        rappdirs_0.3.3 \n#>   rlang_1.0.3     rmarkdown_2.14  rprojroot_2.0.3\n#>   rstudioapi_0.13 sass_0.4.1      stats_4.2.1    \n#>   stringi_1.7.8   stringr_1.4.0   tinytex_0.40   \n#>   tools_4.2.1     utils_4.2.1     vctrs_0.4.1    \n#>   withr_2.5.0     xfun_0.31       xml2_1.3.3     \n#>   yaml_2.3.5"}]
