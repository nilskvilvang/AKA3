[{"path":"index.html","id":"introduksjon","chapter":"Introduksjon","heading":"Introduksjon","text":"Dette notatet gir en introduksjon til anvendt kvantitativ analyse rettet mot samfunnsvitenskapene.Innholdet er utvikling og oppdateres jevnlig om enn noe uregelmessig. Tilbakemeldinger og innspill bes gitt til Nils Kvilvang.Bilde: Ukjent opphav","code":""},{"path":"index.html","id":"versjoner","chapter":"Introduksjon","heading":"Versjoner","text":"","code":""},{"path":"hvorfor-dette-notatet.html","id":"hvorfor-dette-notatet","chapter":"Hvorfor dette notatet?","heading":"Hvorfor dette notatet?","text":"Dette notatet oppsummerer ulike forelesningsnotater og undervisningsopplegg anvendt kvantitativ analyse. Hensikten er å forsøke å formidle sentrale konsepter og teknikker, samt gi “oppskrift” på prosedyre ulike verktøy.","code":""},{"path":"hvorfor-dette-notatet.html","id":"programvare","chapter":"Hvorfor dette notatet?","heading":"Programvare","text":"Notatet er skrevet med bruk av R (R Core Team 2021) og RStudio (RStudio Team 2022). R er en velkjent programvare innenfor statistikk, dataanalyse, datamodellering osv. R har noen store fordeler; det er gratis, det kjører på “alle” operativsystemer, og det har et svært stort bruker- og utviklermiljø som hovedsak deler alt gratis. Det er også enkelt å finne løsninger på det meste gjennom veiledninger og brukerforum på nett. Selve R er et programmeringsspråk og utviklermiljø statistikk som gir en kjernefunksjonalitet innenfor datahåndtering, kalkulasjoner, dataanalyse, datamodellering, grafisk framstilling av data o.l.R kommer med 14 basis “pakker”. Det store potensialet ligger imidlertid brukerne av R utvikler tilleggspakker som man bruker R, det finnes pr. april 2022 19000 ulike pakker som bygger på kjernen R. Alle pakkene tilbyr ulike tilrettelagte løsninger ulike problemer/analyser.Den største ulempen med R er brukergrensesnittet er veldig ulikt hva vi kjenner fra Microsoft Office-typen brukergrensesnitt, så det vil ta litt tid å bli kjent med programmet. tillegg er brukergrensesnittet kodebasert og ikke menybasert, og kan (dessverre) virke avskrekkende. Likevel, det er et utrolig kraftig verktøy hvis man tar seg tid til å lære seg det grunnleggende.Brukere av notatet vil sannsynligvis kjøre programvare som enten SPSS eller Stata. Konsepter og eksempler notatet er som sagt laget og kjørt R, men tilhørende videoer framgangsmåte SPSS og Stata å få fram tilsvarende analyser på samme eksempler som notatet ligger tilgjengelig.tillegg er det på sin plass å nevne alternativer til store og dyre programvarepakker som SPSS og Stata som mer og mer framstår som reelle og gode alternativer. av disse, jamovi og JASP, har sterkt voksende bruk (også akademia) rundt om verden. Dette er grafisk tiltalende og funksjonsrike statistikkpakker som kjører med R bakgrunnen (alle analyser jamovi og JASP bruker R), og som også kan inkludere R kode direkte. Det gjør man kan utnytte alle pakkene skrevet R direkte de grafiske brukergrensesnitt. tillegg kan man (varierende grad mellom de ) hente ut R-kode fra analyser man gjør gjennom det grafiske brukergrensesnittet - noe som kan gjøre en overgang/introduksjon til R lettere om man vil den veien.Leseren står selvsagt fritt til å hoppe elegant alle verktøy som ikke er interessante. Det er klare fordeler og ulemper med alle, men forhåpentligvis vil de fleste finne et verktøy de kan bruke utvalget vi har tatt med.Der det er naturlig, som ved bruk av R og RStudio, er kodingen inkludert slik eksempler skal kunne replikeres av leseren, men vi går ikke inn på R utover dette. Kodingen er gjengitt fortløpende der analysen er gjort. Vi har brukt R/RStudio og rmarkdown (Allaire et al. 2022) – en såkalt pakke til R – produksjonen av dette notatet. R baserer seg som sagt på bruk av ulike “pakker” som er utviklet av forskjellige utviklere og som er fritt tilgjengelig. Mange av disse har også datasett inkludert slik det er enkelt å replikere eksempler. Så langt det er mulig har vi basert oss på det vi viser som eksempler skal være replikerbare leseren.Vi har lite fokus på matematikk og formler den forstand vi ikke utleder dybden forklaring på ulike formler. Vi tror det er fullt mulig å ha en praktisk forståelse og anvendt bruk av kvantitative analyser uten å ha dyptgående kjennskap til de matematiske eller statistiske forklaringene. Likevel, det er heller ikke slik vi absolutt skal unngå dette. Vi har derfor inkludert noe bakgrunnskunnskap å skape en\nramme rundt kjernen notatet der vi tenker det kan være interessant de som ønsker å fordype seg noe mer.","code":""},{"path":"regresjonsanalyse---ols.html","id":"regresjonsanalyse---ols","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"Kapittel 1 Regresjonsanalyse - OLS","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"innledning","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.1 Innledning","text":"Regresjonsanalyse er et særtilfelle av variansanalyse, og er følge Mehmetoglu Mittner (2020) muligens den mest brukte analysemetoden dataanalyse, eller arbeidshesten forskning på økonomiske og sosiale forhold (Thrane 2019). Det er først og fremst metodens er fleksibilitet som en hovedgrunn til dette.En regresjonsanalyse er en statistisk analyse som undersøker sammenhengen mellom en kontinuerlig avhengig variabel og en eller flere kontinuerlige og/eller kategoriske uavhengige variabler. Selv om korrelasjon kan være veldig hjelpsomt å forstå vil en regresjonsanalyse søke å ta vår forståelse av sammenhengen litt videre, til eksempel å forsøke å predikere nivået en avhengig variabel ut fra nivået på den/de uavhengige variabler. Hvis vi lykkes med dette vil vi kunne klare å si noe om forventet verdi på et fenomen vi er interessert ut fra kjente verdier på andre variabler. La oss anta vi har variabler som beliggenhet (avstand fra sentrum), areal, etasje, solforhold, antall rom, antall bad, standard på bad og liknende en leilighet kan vi bruke disse uavhengige variablene til å predikere en salgssum denne boligen (som en avhengig variabel). Vi lager da en modell dette forholdet - dette tilfellet en regresjonsmodell. Vi går dermed fra å spørre om det er en sammenheng til å spørre hvilken sammenheng det er.La oss forsøke å illustrere prinsippet med regresjonsanalyse gjennom et såkalt Venndiagram.Den gule sirkelen illustrerer det forholdet vi er interessert å “finne ut noe om”. Den representerer det vi kaller den avhengige variabelen - fordi det vi ønsker å finne ut er avhengig av andre forhold (andre variabler). Vi kan si den gule sirkelen viser variasjonen drivstofforbruket til alle biler vi har med undersøkelsen vår, og vi betegner denne variabelen \\(Y\\). Biler har ulikt drivstofforbruk, så vi har altså en variasjon drivstofforbruket mellom bilene. Den blå sirkelen viser variasjonen motorstørrelse (vi kaller denne variabelen \\(x_1\\)). Ulike biler har ulik motorstørrelse, og vi tenker større motor betyr mer drivstofforbruk enn mindre motor. Den grønne sirkelen representerer en variabel vi har kalt kjørestil (\\(x_2\\)).Vi har en hypotese om vi kan predikere (forutsi) drifstofforbruket til en gitt bil ut fra motorstørrelse og kjørestil. Så det vi ønsker å se på er hvor mye av korrelasjonen mellom drivstofforbruk og motorstørrelse skyldes faktisk motorstørrelse, og hvor mye skyldes kjørestil. Vi tenker også kjørestil og drivstofforbruk er korrelert (det er naturlig å tenke seg personer med en aggresiv kjørestil har biler med større motorer - det er altså en korrelasjon mellom kjørestil og motorstørrelse). Vi ser dette figuren . Korrelasjonen mellom drivstofforbruk og motorstørrelse er gitt områdene merket 1 og 2. Korrelasjonen mellom drivstofforbruk og kjørestil er gitt områdene 2 og 3. Korrelasjonen mellom kjørestil og motorstørrelse er gitt 2 og 4.Området 2 viser den delte variasjonen mellom drivstofforbruk, motorstørrelse og kjørestil. Det vil innebære vi kan bruke regresjonsanalsye til å isolere ut område 1 ved å se på motorstørrelsens totale korrelasjon med drivstofforbruk og trekke fra den delen av den totale korrelasjonen som deles med kjørestil (område 2). Da finner vi motorstørrelsens (\\(x_1\\)’s) unike bidrag.Det samme kan vi gjøre kjørestil, der det unike bidraget utgjøres av område 3. Vi kan selvsagt ha flere prediktorer (uavhengige variabler) - noe vi veldig ofte vil ha. Det vi gjør er prinsippet det samme: vi tar bort biter av korrelasjonen mellom motorstørrelse og drivstofforbruk som skyldes samvariasjon med andre variabler slik vi får isolert den delen av korrelasjonen som utelukkende skyldes motorstørelse. Man kan tenke seg en ny variabel med rød sirkel. Igjen - regresjonsanalysen forsøker å isolere den unike delen korrelasjonen mellom motorstørrelse og drivstofforbruk (og det samme de andre variablene: den unike delen). Når vi klarer å isolere den unike korrelasjonen kan vi også si vi har isolert den unike kausale effekten motorstørrelse har på drivstofforbruket (gitt vi har inkludert alle relevant uavhengige variabler modellen, noe vi praksis sjelden vil klare).Den videre innledningen til regresjonsanalyse tar utgangspunkt eksempelet Løvås (2013) (boka kom ut 4. utgave 2018). Illustrasjonene som er brukt er hentet fra bokas nettressurser. Løvås’ bok «Statistikk universiteter og høgskoler» kan anbefales som introduksjonsbok til statistikk på universitets- og høgskolenivået. En annen bok som fungerer fint til dette formålet er Jan Ubøes “Statistikk økonomifag” (vi har brukt 4. utgave, 2014 - 5. utgave kom 2015) (Ubøe 2014).","code":""},{"path":"regresjonsanalyse---ols.html","id":"teori","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.2 Teori","text":"Eksempelet Løvås (2013) dreier seg om sammenhengen mellom motorstørrelse og drivstofforbruk. Vi kan måle motorstørrelse hestekrefter (hk) og drivstofforbruk liter/mil.å vise sammenhengen kan vi sette opp ligningen \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) der\\(Y=drivstofforbruket\\)\\(\\alpha=konstantleddet\\) (krysningspunktet på y-aksen, altså Y-verdien om x er 0)\\(\\beta=linjens\\:stigningstall\\) (dersom x øker med 1, øker y med \\(\\beta\\)\\(e=forstyrrelsen\\) (vi antar det er flere ting som forstyrrer forholdet mellom motorstørrelse og drivstofforbruk - drivstofforbruket er ikke bare avhengig av motorstørrelse). Vi skal snakke mye om residualer regresjonsanalyse - residualer er dette restleddet/feilleddet/forstyrrelsen. En av forutsetningene regresjonsanalyse er knyttet til fordelingen av disse residualene, men det kommer vi tilbake til.Dersom vi ikke hadde hatt et feilledd kunne vi framstilt denne ligningen slik:Når vi plotter inn et antall observasjoner av motorstørrelse og drivstofforbruk kan det se slik ut:Det vi en lineær regresjonsanalyse gjør er å finne den rette linja som best passer til disse observasjonene. Vi ønsker altså å finne en rett linje som best «beskriver» observasjonene. Tenk deg vi trekker den rette linja som samlet sett ligger nærmest punktene og deretter tar bort punktene. Det vi sitter igjen med er regresjonslinja. Denne linja gir oss da “tilgang til” alle punkter som ligger på linja som en modell på sammenhengen mellom de variablene. Selv om vi bare hadde noen observasjoner på gitte punkter på x-aksen har vi gjennom regresjonslinja fått tilgang til alle tenkelige punkter på x-linja og kan anta et drivstofforbruk ut fra det (ved å gå opp fra x-aksen, finne skjæringspunktet med regresjonslinja, og deretter gå inn på y-aksen og lese av drivstofforbruket). Den prediksjonen vi da gjør er vår beste gjetning på hvor stort drivstofforbruket vil være en gitt motorstørrelse. Dette vil selvsagt være en kvalifisert gjetning - nettopp fordi det er en modell. Og alle modeller er feil, men noen modeller er nyttige likevel.eksempelet kan vi eksempel tenke oss mulige linjer:Begge linjene er forsøk på å lage en rett linje som har kortest mulig avvik. Vi kan deretter legge sammen de absolutte vertikale avstandene (de stiplede linjene) fra observasjonspunktene ned til den rette linja. prinsippet er da den rette linja som medfører minst samlet avstand fra observasjonspunktene den rette linja som best representerer observasjonspunktene, og vi kan si vi har laget en modell sammenhengen mellom motorstørrelse og drivstofforbruk. Siden vi har en sammenhengende rett linje har vi også mulighet til å mene noe om drivstofforbruk på motorstørrelser vi ikke har målt/har observasjoner på. Vi har med andre ord en modell å predikere drivstofforbruk ut fra motorstørrelse. Uavhengige variabler regresjonsanalyser kalles også ofte prediktorer, fordi vi bruker de til å predikere en verdi den avhengige variabelen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"minste-kvadratsum-ordinary-least-squares---ols","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.2.1 Minste kvadratsum (Ordinary Least Squares - OLS)","text":"Imidlertid er absoluttverdier matematisk problematiske (Løvås 2013). Pre-datamaskiner ble det derfor utviklet en alternativ måte som kalles «minste kvadraters metode» - derav begrepet OLS («Ordinary Least Squares»). Det finnes andre måter å tilnærme seg dette, men dette kurset går vi kun inn på OLS-regresjon. Hvis vi fortsetter eksempelet kan vi tenke oss en mengde forslag på ulike linjer som forsøker å beskrive sammenhengen mellom de variablene:Man regner deretter ut kvadratene som dannes av hvert punkt og avstanden til den rette linja. Den linja som har den laveste kvadratsummen («least squares») er den linja som best representerer datapunktene og som derfor er den beste lineære modellen av forholdet mellom variablene. Regresjonslinja er således en modell. Som Thrane (2019) beskriver: den diagonale linja oppsummerer den typiske trenden det statistiske forholdet mellom de variablene - en linje vi kjenner som regresjonslinja.\nHvis vi har et stort antall datapunkter er dette selvsagt en omfattende prosess å gjøre manuelt. Det statistikkprogrammer gjør oss er å regne ut kvadratsummen et stort antall mulige linjer og deretter fortelle oss hvilken som har lavest kvadratsum.Hvis vi tenker tilbake til formelen modellen vår: \\(Y_i=\\alpha\\:+\\beta x_i+e_i\\) kan vi nå fylle ut med verdier fra eksempelet.Det vi egentlig har gjort når vi finner den rette linja som gir minste kvadratsum er å identifisere \\(\\alpha\\) (skjæringspunktet på y-aksen) og \\(\\beta\\) (stigningstallet). Statistikkprogrammer vil gi oss verdiene på dette. Vi går ikke inn på en manuell utregning , men bruker de verdiene Løvås (2013) viser (\\(\\alpha=0.211\\) og \\(\\beta=0.00576\\)). Vår modell ser da slik ut: \\(Y_i=0,211\\:+0,00576x_i\\).Som sagt har vi ønsket å lage en modell som predikerer drivstofforbruk ut fra motorstørrelse – eller sagt på en annen måte: hvilket drivstofforbruk kan vi forvente med en motor på 100 hk? Vi får da: \\[Y_i=0,211\\:+0,00576x_i=0,211\\:+\\:0,00576\\times100\\:=\\:0,787\\]Dette blir vårt “best guess”, vår antakelse (vår prediksjon av verdien på y-aksen som er drivstofforbruket ut fra verdien på x-aksen som er motorstørrelse), om forventet drivstofforbruk en motor med 100 hk basert på den modellen vi har laget om sammenhengen mellom motorstørrelse og drivstofforbruk (som er basert på de observasjonene vi har).Vi kan naturligvis umiddelbart tenke drivstofforbruket er avhengig av mange andre faktorer enn motorstørrelse, eksempel bilens design (luftmotstand), vekt, rullemotstand, temperatur, type motor og så videre. Dette belyser så vidt et sentralt problem når vi ønsker å lage modeller prediksjon: Virkeligheten er utrolig sammensatt, mange relevante variabler er vanskelig å måle, og man ønsker en modell som er enkel nok til å kunne brukes og sammensatt nok til å gi relevante prediksjoner. Tenk eksempel bare på «klimamodellene» som brukes å analysere og predikere temperatur, issmelting, global oppvarming og liknende. Det er klart de fleste tilfeller trenger vi flere prediktorer enn en – og regresjonssammenheng snakker vi da om multippel regresjonsanalyse. Vi kan ha som en tommelfingerregel vi skal ha med så mange variabler modellen har praktisk verdi, men likevel så få som mulig.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konfidensintervall","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.2.2 Konfidensintervall","text":"Avslutningsvis denne introduksjonen til regresjonsanalyse kan vi se kort på begrepet konfidensintervall (se eksempel Løvås (2013) eller Hinkle, Wiersma, Jurs (2003)). de som ønsker å fordype seg effektstørrelser og konfidensintervaller anbefales Cumming Calin-Jageman (2017) “Introduction new statistics: Estimation, open science, & beyond”.Man kan si estimatet på stigningstallet \\(\\beta\\) er det viktigste resultatet en regresjonsanalyse fordi dette sier noe om hvor sterk sammenhengen mellom de variablene er. Løvås (2013) illustrerer dette slik:De røde stiplede linjene utgjør konfidensgrensene 95 % konfidensintervall. grafen har vi kun 5 observasjoner, noe som selvsagt er lite. Vi kan gå litt dypere inn hvordan konfidensgrensene framkommer en regresjonsanalyse.La oss anta vi har et datasett der vi har plottet korrelasjonen mellom en prediktor (den uavhengige variabelen) på x-aksen og en avhengig varaibel på y-aksen (se graf ). Den røde prikken markerer verdien x=8. Verdien på y-aksen (10,458) er vår prediksjon (vår buest guess) på hva verdien den avhengige variabelen vil være ved den observerte verdien x=8.punktet x=8 har vi en hel populasjon av mulige normalfordelte verdier. Vårt beste estimat av gjennomsnittsverdien denne populasjonen er 10,458. Dette er et punktestimat. Det tilhørende intervallestimatet er vårt konfidensintervall. Vi må tenke på konfidensintervallet som en vertikal linje. Så stedet å tenke punkt- og intervallestimat slikKan vi tenke det slik:Overført til vårt eksempel får vi:Den røde streken er vårt 95 % konfidensintervall punktestimatet. Hvis vi legger på 95 % konfidensintervaller på alle punktestimatene (alle punktene som utgjør regresjonslinja) kan vi lage de stiplede linjene som toucher endepunktene på alle konfidensintervallene. Disse stiplede linjene utgjør da konfidensgrensene regresjonslinja.Vi ser konfidensgrensene er lett buede mot hverandre med minst avstand mellom dem “på midten”. Vi skal kort se på hvorfor det er slik. Vi har nå lagt på et nytt kryss grafen . Dette krysset markerer punktet der gjennomsnittene av X og Y krysser. Regresjonslinja må gå gjennom dette punktet, slik alle alternative regresjonslinjer må pivotere rundt dette punktet. Dette medfører det er litt større usikkerhet rundt punktestimatenes konfidensintervaller endene forhold til midten. Konfidensintervallene hvert enkelt punkt blir derfor litt lenger jo lenger ut fra krysningspunket vi går, og resultatet blir en form buet linje.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-i-analyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.2.3 Steg i analyse","text":"Vi anbefaler en analyse går gjennom disse stegene:Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterVi skal det følgende gå gjennom disse stegene en regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"enkel-lineær-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3 Enkel, lineær regresjonsanalyse","text":"Dette eksempelet bygger på Field (2009). Hvis vi ønsker å se på hvilken grad vi kan predikere salgstall gjennom hvor mye vi bruker på reklame før lansering kan vi gjøre en lineær regresjonsanalyse med salg som avhengig variabel og reklame (adverts) som uavhengig variabel.Du kan laste ned datasettet ulike formater :Download Field_datasett_OLS.xlsxDownload Field_datasett_OLS.savDownload Field_datasett_OLS.dtaDatasettet kan også finnes herVi skal nå gå gjennom våre anbefalte steg analysen, og starter med en analyse av dataene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-1-analyse-av-dataene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1 Steg 1: Analyse av dataene","text":"“soon collected data, compute statistics, look data. Data screening data snooping. opportunity discard data change values favor hypotheses. However, assess hypotheses without examining data, risk publishing nonsense” (Wilkinson Task Force Statistical Inference 1999).Vi ser på datasettet og noen nøkkeltall datasettet.Descriptive Statistics\nField_OLS_data\nN: 200Vi kan se datasettet består av 200 obervasjoner av 4 variabler. Hver av observasjonene er en CD:Adverts: Dette er summen brukt på reklame før lanseringsdatoSales: Dette er salgtall per ukeAirplay: Antall ganger et spor fra CDen ble spilt på radio uka før lanseringsdatoImage: En rating på hvor attraktiv gruppen/artisten (positivt image)Ofte er det imidlertid mer hensiktsmessig å se på dataene grafisk en utforskende hensikt (Tukey 1977).","code":"\n# Bruker pakken: readxl\nField_OLS_data <- read_excel(\"Field_datasett_OLS.xlsx\")\n# Bruker pakken: summarytools\nsummarytools::descr(Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"histogram","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.1 Histogram","text":"Det kan se ut som salgstallene er rimelig normalfordelte, mens reklamevariabelen er klart skjev.","code":"\n# Bruker pakkene: ggplot2 og gridExtra\n\nannotations <- data.frame(\n  x = c(round(min(Field_OLS_data$Adverts), 2), round(mean(Field_OLS_data$Adverts), 2), round(max(Field_OLS_data$Adverts), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n  \nplott1 <- ggplot(Field_OLS_data, aes(Adverts)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Adverts)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\n\nannotations2 <- data.frame(\n  x = c(round(min(Field_OLS_data$Sales), 2), round(mean(Field_OLS_data$Sales), 2), round(max(Field_OLS_data$Sales), 2)),\n  y = c(4, 52, 5),\n  label = c(\"Min:\", \"Gjennomsnitt:\", \"Maks:\"))\n\nplott2 <- ggplot(Field_OLS_data, aes(Sales)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") + \n    geom_vline(aes(xintercept = mean(Sales)), color = \"#000000\", size = 0.5, linetype = \"dashed\") + \n geom_text(data = annotations2, aes(x = x, y = y, label = paste(label, x)), size = 2, fontface = \"bold\") +\n      theme_classic()\ngrid.arrange(plott1, plott2, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"quantile-quantile-plott-qq","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2 Quantile-Quantile plott (qq)","text":"Q-Q plottet (“quantile-quantile plot”) kan tolkes ved å se om dataverdiene ligger langs en rett linje med ca 45 graders vinkel. Q-Q plottet innebærer å se distribusjoner mot hverandre – empirisk fordeling (dataene) og teoretisk forventning ut fra en fordelingsmodell (som normalfordeling om vi snakker om “normal Q-Q plott” - dvs vi ser om vår empiriske datafordeling og normalfordelingen er lik). Om de samsvarer perfekt ligger de på en helt rett linje (x = y). eksempelet vil da alle punktene ligge perfekt oppå den rette linjen. Siden vi vet den teoretiske distribusjonen til normalfordelingen, kan vi bruke denne teoretiske fordelingen til å plotte den mot datasettet vi sitter med.Som vi fikk indikert gjennom historgrammene er salgsvariabelen rimelig normalfordelt, mens reklamevariabelen viser avvik fra normalfordelingen - dette tilfellet (ut fra histogram og qq-plott vil vi si den er høyreskjev).viser vi typiske mønstre histogram og “tilhørende” qq-plott som kan være til hjelp tolkning av dataene dine. Dette er genererte tall og ikke tallene fra eksempelet :","code":"\n# Bruker pakken: car\npar(mfrow=(c(1,2)))\nqqSales2 <- car::qqPlot(Field_OLS_data$Adverts)\nqqAdverts2 <- car::qqPlot(Field_OLS_data$Sales)"},{"path":"regresjonsanalyse---ols.html","id":"normalfordelt","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.1 Normalfordelt","text":"Download QQ_norm.xlsx\nFigure 1.1: Q-Q plott normalfordeling\nVi ser dette Q-Q plottet viser oss vi kan være ganske sikre på dette datasettet er normalfordelt (noe som gir meninig siden vi har brukt R til å lage et normalfordelt datasett).","code":"\nset.seed(89)\n# base R\n# Bruker pakken: tibble (Tidyverse)\nqqnorm <- as_tibble(rnorm(10000, mean=90, sd=5))\n# Bruker pakken: writexl\nwrite_xlsx(qqnorm,\"QQ_norm.xlsx\")\n# Bruker pakken: ggpubr\nggqqplot(qqnorm$value) + ggtitle(\"Normal Q-Q plott\") + labs(x = \"Teoretisk forventning\", y = \"Data\")"},{"path":"regresjonsanalyse---ols.html","id":"skjevhet-høyre","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.2 Skjevhet høyre","text":"Download QQ_norm_rs.xlsx\nFigure 1.2: Q-Q plott - fordeling skjevhet høyre\net datasett med høyreskjevhet vil ofte Q-Q plottet vise en bananform med bunnen/midten av bananen ned mot høyre hjørne og endene pekende oppover/utover fra den rette linjen.","code":"\n# Lage datasett med right skew\n# base R\n# Bruker pakken: tibble\nset.seed(90)\nN <- 5000\nqqrightskew <- as_tibble(rnbinom(N, 10, .1))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqrightskew,\"QQ_norm_rs.xlsx\")\n# Plotte histogram og Q-Q plott\"\n# Bruker pakken: ggplot2\nqqrighthist <- ggplot(qqrightskew, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqrightskew_plott <- ggqqplot(qqrightskew$value) + ggtitle(\"Normal Q-Q plott - skjevhet høyre\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqrighthist, qqrightskew_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"skjevhet-venstre","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.3 Skjevhet venstre","text":"Download QQ_norm_ls.xlsx\nFigure 1.3: Q-Q plott - fordeling skjevhet venstre\ndette datasettet har vi generert en kraftig skjevhet til venstre. Q-Q plottet får da en omvendt bananform forhold til høyre skjevhet, altså en topp på midten og ender som svinger nedover ift den rette linja.","code":"\n# Lage datasett med left skew\n# base R\n# Bruker pakken: tibble\nset.seed(91)\nN=5000\nqqleftskew <- as_tibble(rbeta(N,5,1,ncp=0))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqleftskew,\"QQ_norm_ls.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken: ggplot2\nqqlefthist <- ggplot(qqleftskew, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqleftskew_plott <- ggqqplot(qqleftskew$value) + ggtitle(\"Normal Q-Q plott - skjevhet venstre\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqlefthist, qqleftskew_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"tunge-haler","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.4 Tunge haler","text":"Download QQ_ht.xlsx\nFigure 1.4: Q-Q plott - ‘heavy-tail’\n“Heavy-tailed” (fete/tunge haler) har større sannsynlighet ekstreme verdier vil forekomme). Fordelinger med tunge haler vil ofte følge en slags S-form, men den er ofte mer “liggende” enn S-formen til fordeling med lette haler. Den starter med å vokse raskere enn normalfordelingen og ender med å vokse saktere.","code":"\nset.seed(14)\nN=100\n# Bruker pakken: tibble\n# Base R\nqqcauchy <- as_tibble(rcauchy(N, scale = 5)) \n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqcauchy,\"QQ_ht.xlsx\")\n# Bruker pakken: ggplot2\nqqcauchyhist <- ggplot(qqcauchy, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqcauchy_plott <- ggqqplot(qqcauchy$value) + ggtitle(\"Normal Q-Q plott - tung hale\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqcauchyhist, qqcauchy_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"lette-haler","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.5 Lette haler","text":"Download QQ_lt.xlsx\nFigure 1.5: Q-Q plott - ‘light-tail’\n“Light-tailed” (lette haler) har liten sannsynlighet ekstreme verdier og utvalg tenderer til å ikke fravike gjennomsnittet med mye. Q-Q plottet en fordeling med lette haler har ofte en S-form. Dataene vokser saktere enn normalfordelingen starten før den følger vekstraten til normalfordelingen. Mot slutten vokser den raskere enn normalfordelingen. Derfor bøyer den av fra normalfordelingen.","code":"\nset.seed(81)\n# Base R\n# Bruker pakken: tibble\nqqlt <- as_tibble(runif(n = 1000, min = -1, max = 1))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqlt,\"QQ_lt.xlsx\")\n# Bruker pakken: ggplot2\nqqlthist <- ggplot(qqlt, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken ggpubr\nqqlt_plott <- ggqqplot(qqlt$value) + ggtitle(\"Normal Q-Q plott - lett hale\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqlthist, qqlt_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"bimodalitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.6 Bimodalitet","text":"Download QQ_bimod.xlsx\nFigure 1.6: Q-Q plott - bimodal\nDen bimodiale fordelingen viser ofte et brudd eller et distinkt knekkpunkt rundt krysning av den rette linja, med en del av linja på hver side av den rette linja.","code":"\nset.seed(10) \n# Base R\n# Bruker pakken: ggplot2\n# Bruker pakken: tibble\nmode1 <- rnorm(50,2,1)\nmode1 <- mode1[mode1 > 0] \nmode2 <- rnorm(50,6,1)\nmode2 <- mode2[mode2 > 0] \nqqbimod <- as_tibble(sort(c(mode1,mode2)))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqbimod,\"QQ_bimod.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken ggplot2\nqqbimodhist <- ggplot(qqbimod, aes(x=value)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqbimod_plott <- ggqqplot(qqbimod$value) + ggtitle(\"Normal Q-Q plott - bimodial\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqbimodhist, qqbimod_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"multimodalitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.2.7 Multimodalitet","text":"Download QQ_multimod.xlsx\nFigure 1.7: Q-Q plott - multimodal\nMultimodale fordelinger vil som regel vise flere brudd.","code":"\n# Bruker pakken: readxl\n# Bruker pakken: tibble\nqqmultimod <- as_tibble(read_xlsx(\"Multimodal.xlsx\"))\n# Eksportere datasettet\n# Bruker pakken: writexl\nwrite_xlsx(qqmultimod,\"QQ_multimod.xlsx\")\n# Plotte histogram og Q-Q plott\n# Bruker pakken ggplot2\nqqmultimodhist <- ggplot(qqmultimod, aes(x=Verdi)) + geom_histogram(color=\"black\", fill=\"lightblue\")\n# Bruker pakken: ggpubr\nqqmultimod_plott <- ggqqplot(qqmultimod$Verdi) + ggtitle(\"Normal Q-Q plott - multimodial\") + labs(x = \"Teoretisk forventning\", y = \"Data\")\n# Bruker pakken: gridExtra\ngrid.arrange(qqmultimodhist, qqmultimod_plott, ncol=2)"},{"path":"regresjonsanalyse---ols.html","id":"statistiske-tester-for-vurdering-av-dataenes-distribusjon","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.3 Statistiske tester for vurdering av dataenes distribusjon","text":"Vi har nå sett på noen typiske eksempler på mønstre Q-Q plott. Det kan imidlertid være vanskelig å bedømme fordelinger som ligger nære normalfordelingen, men likevel ikke perfekt oppå (du vil trolig aldri se en perfekt match med mindre du har generert et normalfordelt datasett med mange datapunkter). Vi kan supplere Q-Q plottene med visse statistiske tester (men husk: disse statistiske testene har sine egne forutsetninger og er heller ikke uten utfordringer).","code":""},{"path":"regresjonsanalyse---ols.html","id":"anderson-darling","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.3.1 Anderson-Darling","text":"Anderson-Darlings test er en test å se om et datasett kommer fra en gitt fordeling, f.eks. normalfordelingen (Anderson Darling 1952; Anderson Darling 1954). Testen setter opp hypoteser:\\(H_0\\): Dataene følger normalfordelingen\\(H_1\\): Dataene følger ikke normalfordelingenDownload Anderson-Darling_raw.xlsxSiden vi vet nullhypotesen er datasettet har en normalfordeling vil vi forkaste nullhypotesen dersom vi har en signifikant p-verdi (grensen hva som er signifikant bestemmer vi forsåvidt selv, men vanlige verdier er 0.01, 0.05 og 0.1). Altså - dette tilfellet har vi en p-verdi=0.04. Vi forkaster derfor nullhypotesen og aksepterer \\(H_1\\) som sier dataene er trolig ikke er normalfordelte (med andre ord: p-verdien må være større enn signifikansverdien vi skal si dataene trolig er normalfordelte) - en huskeregel: “p low, null must go” (: “low” = terskelverdien vi har satt, ofte 0.05).Generisk ser dette slik ut (Hartmann, Krois, Waske 2018):Det er verdt å merke seg Anderson-Darling testen egentlig ikke forteller deg dataene dine er normalfordelte, men det er usannsynlig de ikke er det om testen viser det. Dette synes kanskje som samme sak, men er realiteten en viktig erkjennelse – en tørr gressplen er et bevis det ikke har regnet, men en våt gressplen er ikke bevis det har regnet. En våt gressplen kan skyldes andre ting enn regn. Altså – en signifikant p-verdi på testen gjør vi forkaster \\(H_0\\) og antar fordelingen er ikke-normal. En ikke-signifikant p-verdi på gjør vi med f.eks. 95% konfidens kan si vi ikke har funnet avvik fra normalfordelingen.Det finnes flere andre statistiske tester som kan kjøres å teste normalitet, f.eks. Kolmogorov-Smirnov, Shapiro-Wilks og Cramer Von-Mises test. Anderson-Darling er en modifisering/videreutvikling av Kolmogorov-Smirnov og anses ofte som en bedre test av de . Andre kilder (se f.eks. Razali Wah (2011)) finner Shapiro-Wilks presterer best 10 000 simuleringer på ulike distribusjoner.Tolkning Kolmogorov-Smirnov: Hvis p-verdien er valgte signifikansnivå (f.eks. 0.05) skal vi anta datasettet ikke er normalfordelt. vil testen peke på datasettet ikke er normalfordelt.Tolkning av Shapiro-Wilks og Cramer-von Mieses test er lik som Kolmogorov-Smirnov.Som et siste eksempel på en statistisk test normalitet kan vi bruke Jarque-Bera test. Denne skiller seg litt ut fra de andre ved den spesifikt ser på skjevhet og kurtosis datasettet opp mot hva en normalfordeling vil ha. å gjøre lykken komplett finnes det versjoner av testen:Tolkningen er lik som før - hvis p-verdien er mindre enn valgte signifikansnivå peker det mot datasettet ikke er normalfordelt. , motsetning til de øvrige testene, er p-verdien større enn signifikansnivået (0,05) så det peker mot datasettet er normalfordelt.Dette er altså ikke så enkelt. Det finnes mange statistiske tester, som kan gi motsatte indikasjoner på om et datasett er normalfordelt eller ikke siden de ser på dataene fra “ulik vinkel” (fokuserer på ulike aspekter ved dataene). Vårt råd blir: Start alltid med Q-Q plott. Velg evt en teststatistikk, men vær klar alle teststatistikker bygger på forutsetninger eller tester ulike sider av distribusjonen. Det vi også kan huske på er henhold til sentralgrenseteoremet (“Central Limit Theorem”) vil populasjonens fordeling være av mindre interesse dersom utvalgsstørrelsen er stor nok. Hva er stor nok? De fleste kilder peker mot 30 er “stort nok”.","code":"\n# Bruker pakken: tibble\n# Bruker pakken: readxl\naddata <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Bruker pakken: nortest\nad.test(addata$Values)\n#> \n#>  Anderson-Darling normality test\n#> \n#> data:  addata$Values\n#> A = 0.74573, p-value = 0.04046\noptions(scipen=999)\n# Bruker pakken: tibble\n# Bruker pakken: readxl\naddata5 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Base R\nks.test(addata5, \"pnorm\")\n#> \n#>  One-sample Kolmogorov-Smirnov test\n#> \n#> data:  addata5\n#> D = 0.88493, p-value = 0.0000000000000171\n#> alternative hypothesis: two-sided\n# Base R\nshapiro.test(addata5$Values)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  addata5$Values\n#> W = 0.87521, p-value = 0.04027\n# bruker pakken: nortest\ncvm.test(addata$Values)\n#> \n#>  Cramer-von Mises normality test\n#> \n#> data:  addata$Values\n#> W = 0.12634, p-value = 0.04326\n# Bruker pakken: tibble\n# bruker pakken: readxl\naddata6 <- as_tibble(read_excel(\"Anderson-Darling_raw.xlsx\"))\n# Bruker pakken: tseries\njarque.bera.test(addata6$Values)\n#> \n#>  Jarque Bera Test\n#> \n#> data:  addata6$Values\n#> X-squared = 2.1953, df = 2, p-value = 0.3337\n# Bruker pakken: normtest\najb.norm.test(addata6$Values, nrepl=2000)\n#> \n#>  Adjusted Jarque-Bera test for normality\n#> \n#> data:  addata6$Values\n#> AJB = 3.1014, p-value = 0.131"},{"path":"regresjonsanalyse---ols.html","id":"boxplott","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.4 Boxplott","text":"Et boxplott forteller oss mye om dataenes distribusjon.Selve boksen representerer 50 % av observasjonene/casene, det vil si nedre kant representerer første kvartil (= 25.prosentil) og øvre kant tredje kvartil (= 75.prosentil).Den tykkere horisontale streken boksen viser medianverdien (= andre kvartil = 50.prosentil)Dersom en observasjon ligger utenfor en terkselverdi (jfr figur ) vises dette med en liten sirkel. Dette defineres som uteliggere. Å identifisere uteliggere kan være viktig mange statistiske tester.Galarnyk (2018) illustrerer boxplott slik:","code":"\n# Base R\npar(mfrow=(c(1,2)))\nBoxplot(Field_OLS_data$Adverts, id = list(n=Inf), ylab = \"\", main = \"Adverts\", col = \"Blue\")\n#> [1]  43  87 184\nBoxplot(Field_OLS_data$Sales, id = list(n=Inf), ylab = \"\", main = \"Sales\", col = \"Green\")"},{"path":"regresjonsanalyse---ols.html","id":"scatterplott","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.1.5 Scatterplott","text":"Et scatterplott viser oss på en god visuell måte hvordan de variablene forholder seg til hverandre (vi plotter hver enkelt observasjon gjennom verdiene de har på de variablene). Mønsteret kan derfor si oss mye om sammenhengen mellom de .En god måte å fremstille et scatterplott på R (gjennom pakken car) er denne:kombinerer vi scatterplott og boxplott. Den rette blå linja er en minste kvadratssums regresjonslinje (OLS). Den stiplede blå linja bruker en ikke-parametrisk tilnærming. tillegg får vi visualisert de fire mest ekstreme tilfellene (lengst vekk fra gjennomsnitt).","code":"\n# Bruker pakken: ggplot2\nggplot(Field_OLS_data, aes(x = Adverts, y = Sales)) +\n  geom_point(colour = 4)\n# Bruker pakken: car\nscatterplot(Adverts ~ Sales, data = Field_OLS_data, id = list(n=4))#> [1]   1  87 169 184"},{"path":"regresjonsanalyse---ols.html","id":"steg-2-evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.2 Steg 2: Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Dette er ikke relevant en enkel lineær regresjonsanalyse. Når vi skal gjøre en multippel regresjonsanalyse - altså vi har eller flere uavhengige variabler (prediktorer) vil analysen av dataene våre - og hvilken rekkefølge vi legger de uavhengige variablene inn regresjonsmodellen (mer om det eksempelet multippel regresjon) kunne informeres av analysen vi gjør forkant. Derfor viser vi dette nå selv om det altså ikke er relevant enkel regresjonsanalyse.Vi lager en korrelasjonstabell de tre uavhengige og den avhengige variabelen (Sales, Adverts, Airplay, Image):p < .0001**** , p < .001*** , p < .01**, p < .05*standard multippel regresjonsanalyse legger vi alle de uavhengige variablene inn samtidig. Dersom vi skal gjøre en stegvis regresjonsanalyse vil vi legge de uavhengige variablene inn en og en ut fra statistiske kriterier - som hvor stor korrelasjonen er. Den uavhengige variabelen med størst korrelasjon legges inn først og så videre. det eksempelet vi har ser vi Sales korrelerer høyest med Airplay, og nesten like mye med Adverts. Korrelasjonen med Image er noe lavere.","code":"\n# Bruker pakken: sjPlot\ntab_corr(Field_OLS_data, triangle = \"lower\")"},{"path":"regresjonsanalyse---ols.html","id":"steg-3-lage-modell-og-kjøre-regresjonsanalysen","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.3 Steg 3: Lage modell (og kjøre regresjonsanalysen)","text":"Vi ønsker å se om Adverts kan predikere Sales. Grafisk kan vi vise dette:","code":"\n# Bruker pakken: olsrr\nFieldOLS_reg <- ols_regress(Sales ~ Adverts, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"steg-4-analyse-av-resultatene-diagnostikk","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.4 Steg 4: Analyse av resultatene (diagnostikk)","text":"","code":"\n# Bruker pakken: olsrr\nFieldOLS_reg\n#>                          Model Summary                           \n#> ----------------------------------------------------------------\n#> R                       0.578       RMSE                 65.991 \n#> R-Squared               0.335       Coef. Var            34.157 \n#> Adj. R-Squared          0.331       MSE                4354.870 \n#> Pred R-Squared          0.323       MAE                  50.869 \n#> ----------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                  ANOVA                                   \n#> ------------------------------------------------------------------------\n#>                    Sum of                                               \n#>                   Squares         DF    Mean Square      F         Sig. \n#> ------------------------------------------------------------------------\n#> Regression     433687.833          1     433687.833    99.587    0.0000 \n#> Residual       862264.167        198       4354.870                     \n#> Total         1295952.000        199                                    \n#> ------------------------------------------------------------------------\n#> \n#>                                     Parameter Estimates                                     \n#> -------------------------------------------------------------------------------------------\n#>       model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n#> -------------------------------------------------------------------------------------------\n#> (Intercept)    134.140         7.537                 17.799    0.000    119.278    149.002 \n#>     Adverts      0.096         0.010        0.578     9.979    0.000      0.077      0.115 \n#> -------------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-mye-forklarer-modellen-vår","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.4.1 Hvor mye forklarer modellen vår?","text":"Det første vi kan se på er \\(R^{2}\\) som forteller oss hvor stor del av den totale variansen modellen forklarer. dette tilfellet er \\(R^{2} = 0.3346\\). Det innebærer modellen vår kan forklare 33.46 % av den totale variansen. Det betyr reklame forklarer («accounts ») 33.5 % av variansen salget. Det er med andre ord mange andre faktorer som kan forklare hvorfor noen plater selger bedre enn andre, men reklame kan forklare drøye 33 % av den totale variansen. Dette kan vi også se er statistisk signifikant p < .001.Endel programmer vil også gi en R verdi. Siden vi kun har en uavhengig variabel (en prediktor) vil verdien R utgjøre den bivariate korrelasjonen (korrelasjonskoeffisienten mellom de variablene - vi ser dette er samme verdi som tabellen korrelasjonskoeffisienter lenger opp).Adjusted \\(R^{2}\\) er en “modifisert versjon” av \\(R^{2}\\) der det legges inn en korreksjon antall prediktorer modellen. Motivasjonen dette er det å legge til flere prodeiktorer alltid vil øke \\(R^{2}\\) verdien (Navarro Foxcroft 2019). Navarro Foxcroft (2019) påpeker imidlertid man ikke kan tolke adjusted \\(R^{2}\\) like rett fram som \\(R^{2}\\), og anbefaler man bruker \\(R^{2}\\). Det vi også kan si er dersom verdiene på henholdsvis \\(R^{2}\\) og adjusted \\(R^{2}\\) er nærme hverandre (eller like) indikerer dette en god kryssvaliditet modellen, noe som kan gjøre oss sikrere generalisering av funnene våre.","code":""},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-og-regresjonslikning","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.4.2 Modellens koeffisienter og regresjonslikning","text":"Koeffisientene vil fortelle oss mer multippel regresjonsanalyse, men gir oss noen interessante opplysninger også . introduksjonen snakket vi om punktet der regresjonslinja skjærer y-aksen (konstanten). Fra analysen ser vi (Intercept) = 134.14 og Adverts = 0.096. 134.14 er punktet på y-aksen regresjonslinja “begynner” (der x = 0). Altså, ettersom x-aksen angir verdier hva vi bruker på reklame er Intercept estimatet antallet plater vi kan forvente å selge dersom vi bruker 0 kroner på reklame. Estimatet på «Adverts» på 0,096 er stigningstallet regresjonslinja – hvis prediktoren (reklame) stiger med 1 enhet stiger salget med 0,096 plater. Vi kan da lage følgende likning: \\[ Sales=134,14\\:+\\:0,096\\left(Adverts\\right) \\]","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.4.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi ønsker å ha en formening om hvor god modellen er («goodness fit»). Altså, er regresjonsmodellen vår bedre enn en modell der vi ikke vet noe om forholdet mellom reklame og platesalg? Vi kan bruke gjennomsnitt av salgstallene som en modell ingen sammenheng mellom reklame og platesalg, og deretter sammenlikne regresjonsmodellen med gjennomsnittsmodellen. Sammenlikningen mellom modellene skjer gjennom å se på forskjellene mellom de observerte målingene (salgstall) og verdier predikert de ulike modellene. Dersom regresjonsmodellen signifikant predikerer bedre er det en bedre modell enn alternativet.Analysen vår har gitt oss F verdien 99.59 med p < .001. Vi ser verdien er statistisk signifikant. F verdien er et mål på forbedring prediksjonen sett opp mot unøyaktigheter modellen (alle modeller er unøyaktige (eller “feil”)).\nVi kan sjekke F-verdien opp mot antall frihetsgrader (df) gjennom tabeller som ofte finnes statistikkbøker, eller bruke onlineressurser som herVi ser av resultatene fra analysen antall df teller er 1 og antall df nevner er 198. Hvis vi leser av tabellen ser vi df 1/df 200 er kritisk verdi 3,89 α = 0,05 og 11,15 α = 0,001. Vår F er med andre ord langt kritisk verdi. Vi kan derfor si vår regresjonsmodell gir en signifikant bedre prediksjon av platesalg enn alternativet. Reklame er med andre ord en god prediktor platesalg.Helt nøyaktig kan vi regne ut kritisk verdi (som vi har gjort R) df 1 og df 198:Ut fra dette kan vi si det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.Til slutt vil vi se på konfidensintervallet:","code":"\n# Base R\nqf(p=.05, df1=1, df2=198, lower.tail=FALSE)\n#> [1] 3.888853\n# Bruker pakken: car\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\nConfint(FieldOLS_reg)\n#>                 Estimate        2.5 %      97.5 %\n#> (Intercept) 134.13993781 119.27768082 149.0021948\n#> Adverts       0.09612449   0.07712929   0.1151197"},{"path":"regresjonsanalyse---ols.html","id":"steg-5-sjekk-av-forutsetningene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5 Steg 5: Sjekk av forutsetningene","text":"Dette er et tema som behandles og framstilles på noe ulike måter litteraturen. Det som er klart er brudd på forutsetningene kan gjøre modellen vår mer usikker opp til et punkt hvor regresjonsanalyse ikke bør gjennomføres. Noen av forutsetningene er empirisk testbare (vi kan få ut en eller annen form analyse av et statistikkprogram som SPSS, Stata, R og så videre) mens noen er ikke empirisk testbare (det vil si vi må bruke egen vurdering). Vi skal dette delkapittelet gå gjennom forutsetningene lineær regresjon. Selv om noen ikke er aktuelle enkel regresjon tar vi med alle forutsetningene oversiktens skyld.Ved regresjonsanalyse gjør vi en rekke sjekker av datamaterialet vi har å avgjøre om regresjonsanalyse er en egnet teknikk og hvorvidt vi mener vi kan generalisere funnene. Dersom forutsetningene brytes gjør det vi kan sette spørsmålstegn ved hvor nærme regresjonskoeffisienten er populasjonskoeffisienten – eller med andre ord: Hvis regresjonskoeffisienten er helt forventningsrett («unbiased», dvs 0) så vil regresjonskoeffisienten være lik populasjonskoeffisienten («estimatet er lik virkeligheten»). Nå vil det praksis aldri være tilfelle, men ved å sette visse forutsetninger kan vi fastslå om våre data egner seg regresjonsanalyse og hvor sikre vi føler oss funnene kan generaliseres. Det er verdt å merke seg hva Field et. al (2012, s.298) skriver:“’s worth remembering can perfectly good model data (outliers, influential cases, etc.) can use model draw conclusions sample, even assumptions violated. However, ’s much interesting generalize regression model assumptions become important. violated generalize findings beyond sample.”Med andre ord: Vi kan ha brudd på forutsetningene og likevel si noe meningsfullt om vårt utvalg/våre data, men resultatene våre blir mer usikre og vi skal være veldig forsiktige med å kreve generaliserbarhet dersom vi har brudd på forutsetningene. Så alt håp er ikke ute med brudd på forutsetningene, men vi skal behandle konklusjonene våre deretter.Regresjonsforutsetninger behandles ulikt av ulke kilder, og får ulik plass diskusjoner om regresjon. Vi har undersøkt en rekke kilder å framstille dette (blant annet Green (1991), Berry (1993), Miles Shevlin (2001), Hinkle, Wiersma, Jurs (2003), Tabachnik Fidell (2007), Eikemo Clausen (2007), Hair Jr. et al. (2010), Lomax Hahs-Vaughn (2012)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.1 Kausalitet","text":"Forutsetningen om kausalitet hviler fagunnskap og teoretiske vurderinger. Det sier seg så vidt selv vi ikke er interesserte å ha med irrelevante variabler modellen. Med irrelevant menes variabler som korrelerer med den avhengige variabelen, men hvor korrelasjonen er ikke har noe med årsakssammenheng å gjøre (sammenhenger medfører ikke seg selv kausalitet som vi har vært inne på en tidligere modul).Kausalitet er dermed en forutsetning. Den uavhengige variabelen må variere korrelert med de avhengige, det er en kausal sammenheng (hvis ikke det er kausalitet har den/de uavhengige variablene ingen effekt på den avhengige – det kan like gjerne være motsatt). Når vi velger en avhengig variabel og en eller flere uavhengige variabler har vi også gjort en antakelse om kausalitet og retning på kausaliteten, og forutsatt denne er tilstede - basert på teoretisk kunnskap om det vi undersøker. Men det er viktig å understreke verken korrelasjon eller regresjon indikerer kausalitet.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil våre data. Thoresen (2003) viser eksempel til en studie av MacMahon et al. (1990) der de fant en 60% sterkere sammneheng mellom blodtrykk og hjerte-karsykdommer en stor metastudie når de korrigerte skjevhet estimatene de tidligere studiene (som var inkludert metastudien).Vi skal også være oppmerksom på utfordringer dersom feil den ene variabelen korrelerer med feil en annen variabel. “Dersom målt eksponering og målt helseutfall er rammet av avhengige feil, blir resultatet oftest en falskt forhøyet sammenheng mellom de . Slik resultatskjevhet er sannsynligvis ikke uvanlig tverrsnittsstudier, hvor data om eksponering og utfall skaffes til veie gjennom spørreskjema” (Kristensen 2005).","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.3 Relevante og irrelevante variabler","text":"Alle relevante uavhengige variabler må være inkludert modellen, og alle irrelevante uvhengige variabler er fjernet/er ikke med modellen. Man kan si en hovedgrunn til vi veldig ofte kjører mulitippel regresjonsanalyse stedet bivariat regresjonsanalyse er å unngå vi ikke inkluderer relevante variabler (såkalt “omitted variable bias”) (Thrane 2019). Imidlertid, som Thrane (2019) påpeker, er dette praksis umulig, så det vi tilstreber er å inkludere de mest relevante variablene. Dette faller igjen tilbake på teoretiske betraktninger og faglig kjennskap til området man holder på med. Du skal hvert fall kunne begrunne valget av hvilke uavhengige variabler som er inkludert og hvilke som kanskje kunne tenkes å være inkludert, men som du har valgt å ikke inkludere.Teoretisk sett skal vi også forsikre oss om ikke-relevante variabler ikke er inkludert modellen. Igjen er dette delvis umulig og delvis forvirrende/unøyaktig. Det er delvis umulig fordi vi vanskelig kan vite eksakt hvilke potensielle variabler som er relevante og ikke. Det er delvis forvirrende/unøyaktig fordi det kan være viktig å identifisere variabler som ikke har noen effekt - dette kan være viktig policyrevisjon/-utforming (jfr Thrane (2019)).","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Forholdstallet mellom respondenter/caser/observasjoner og uavhengig variabler er av stor betydning dersom man skal gjennomføre en multippel regresjon. Dette gjelder spesielt ved skjevdistribusjon av den avhengige variabelen, effektstørrelsen er forventet liten eller man kan mistenke vesentlige målefeil (Tabachnik Fidell 2007).Det finnes ulike anbefalte normer vurdering av forholdstallet (vi tar utgangspunkt standard multippel regresjonsanalyse - stegvis multippel regresjonsanalyse kan gi andre vurderinger rundt forholdstallet). viser vi noen eksempler på hvordan man kan vurdere dette:\nTable 1.1: Forholdstall\nTable 1.1: ForholdstallKildeAntallMarks (1966, Harris (2013)Minimum 200 uansettSchmidt (1971)15-1 til 25-1Nunally (1978)2-3 IV = minst 100, 9-10 IV = 300-400Stevens (1996)15 pr IVGreen (1991)N=50+8m (m=antall uavhengige variabler) ved «medium-sized relationship IVs DV, =.05 ß=.20Miles & Shevlin (2001)Som Green (2001). Utvalgsstørrelse avhenger av størrelse på effekt og statistisk styrke (se Cohen, 1988 effektstørrelser). Stor effekt: 80 respondenter er alltid nok opptil 20 IVs. Middels effekt: 200 respondenter vil alltid være nok opptil 20 IVs, 100 er nok opptil 6 eller færre IVs. Lav effekt: Minst 600.Det er også verdt å merke seg det ikke er ønskelig med mange respondenter, da et svært stort antall respondenter vil gi statistisk signifikans nesten enhver multippel korrelasjon – “statistical practical reasons, , one wants measure smallest number cases decent chance revealing relationship specified size” (Tabachnik Fidell 2007, s.123). Dette har med andre ord mye å si hvordan man planlegger en studie. Miles Shevlin (2001), som angitt siste rad tabellen , ser på sammenhengen mellom effektstørrelse, statistisk styrke og utvalgsstørrelse. Field (2009, s.223, figur 7.10) har modifisert grafisk denne sammenhengen:","code":""},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Denne forutsetningen gjelder om man har minst uavhengige variabler - altså multippel regresjon. Med dette menes vi må forvente variasjonen den avhengige variabelen er en funksjon av sum av endringer de uavhengige variablene. Vi kan også uttrykke dette som effekten av den uavhengige variabelen \\(x_1\\) på den avhengige variabelen \\(y\\) er uavhengig av eventuelle andre uavhengige variablers effekt på \\(y\\). Forutsetningen om additivitet betyr altså det ikke er interaksjon mellom eller flere uavhengige variabler.\nMed andre ord, hvis effekten av \\(x_1\\) på \\(y\\) er avhengig av hvordan \\(x_2...x_n\\) påvirker \\(y\\) brytes forutsetningen om additivitet. praksis er det imidlertid ikke uvanlig denne forutsetningen brytes en eller annen grad.La oss vise dette med et eksempel fra Thrane (2019) (se figur 6.1). Dersom vi har data kvinners og menns inntekt relatert til antall års utdannelse vil vi kunne se en regresjonsanalyse antall års utdanning predikerer inntekt. Forutsetningen om additivitet sier da antall års utdanning har lik effekt på inntekt menn og kvinner. Slik er det imidlertid ikke. Generisk kan det framstilles som grafen :Vi ser linjene har ulikt stigningstall. Kvinner starter menn, men har et høyere stigningstall. Det vil si kvinner har større effekt av et (ekstra) års utdanning enn menn. Da er forutsetningen om additivitet brutt. Som Thrane (2019) påpeker: “Additivity thus means parallel regression lines”.Vi kan altså sjekke dette ved å kjøre regresjonsanalyser. En annen måte, som vi ikke går inn på , er å lage en interaksjonsvariabel som vi dernest bruker modellen. En interaksjonsvariabel er et produkt av (minst) uavhengige variabler.La oss se på et annet eksempel å vise interaksjonsvariabel og -effekt basert på et eksempel fra Thomas (2017).Datasettet inneholder ti variabler tillegg til det vi vil bruke som avhengig variabel: Sales. Vi vil altså se om vi kan bruke de ni variablene til å predikere salg.Denne modellen kan altså forklare 87 % av variansen Sales.\nVi kan imidlertid mistenke det kan være en interaksjon mellom variablene Income og Population - jo større befolkning, jo større inntekt tilgjengelig.Forskjellen mellom modellene ligger altså den nederste koeffisienten (Income:Population) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Vi ser en klar interaksjon ved linjene krysser (jfr. Thranes merknad om forutsetningen om additivitet gir parallelle linjer).R har vi et hjelpemiddel pakken interactions:Tolkningen av plottet fra pakken interactions er den samme: Parallelle linjer indikerer fravær av interaksjoneseffekt, mens ikke-parallelle linjer indikerer tilstedeværelse av interaksjoneseffekt.Vi ser tillegg interaksjonseffekten (Income:Population) er statistisk signifikant.","code":"\n# Bruker pakken: ISLR\ndata(Carseats)\n# Bruker pakken: summarytools\nsummarytools::descr(Carseats, stats = \"common\")\n#> Non-numerical variable(s) ignored: ShelveLoc, Urban, US\n#> Descriptive Statistics  \n#> Carseats  \n#> N: 400  \n#> \n#>                   Advertising      Age   CompPrice   Education   Income   Population    Price    Sales\n#> --------------- ------------- -------- ----------- ----------- -------- ------------ -------- --------\n#>            Mean          6.64    53.32      124.97       13.90    68.66       264.84   115.80     7.50\n#>         Std.Dev          6.65    16.20       15.33        2.62    27.99       147.38    23.68     2.82\n#>             Min          0.00    25.00       77.00       10.00    21.00        10.00    24.00     0.00\n#>          Median          5.00    54.50      125.00       14.00    69.00       272.00   117.00     7.49\n#>             Max         29.00    80.00      175.00       18.00   120.00       509.00   191.00    16.27\n#>         N.Valid        400.00   400.00      400.00      400.00   400.00       400.00   400.00   400.00\n#>       Pct.Valid        100.00   100.00      100.00      100.00   100.00       100.00   100.00   100.00\n# Bruker pakken: olsrr\nsaleslm <- ols_regress(Sales~. ,data = Carseats)\nsaleslm\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.935       RMSE                1.019 \n#> R-Squared               0.873       Coef. Var          13.592 \n#> Adj. R-Squared          0.870       MSE                 1.038 \n#> Pred R-Squared          0.865       MAE                 0.804 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                 Sum of                                                \n#>                Squares         DF    Mean Square       F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression    2779.441         11        252.676    243.372    0.0000 \n#> Residual       402.834        388          1.038                      \n#> Total         3182.275        399                                     \n#> ----------------------------------------------------------------------\n#> \n#>                                      Parameter Estimates                                      \n#> ---------------------------------------------------------------------------------------------\n#>           model      Beta    Std. Error    Std. Beta       t        Sig      lower     upper \n#> ---------------------------------------------------------------------------------------------\n#>     (Intercept)     5.661         0.603                   9.380    0.000     4.474     6.847 \n#>       CompPrice     0.093         0.004        0.504     22.378    0.000     0.085     0.101 \n#>          Income     0.016         0.002        0.157      8.565    0.000     0.012     0.019 \n#>     Advertising     0.123         0.011        0.290     11.066    0.000     0.101     0.145 \n#>      Population     0.000         0.000        0.011      0.561    0.575    -0.001     0.001 \n#>           Price    -0.095         0.003       -0.799    -35.700    0.000    -0.101    -0.090 \n#>   ShelveLocGood     4.850         0.153        0.703     31.678    0.000     4.549     5.151 \n#> ShelveLocMedium     1.957         0.126        0.345     15.516    0.000     1.709     2.205 \n#>             Age    -0.046         0.003       -0.264    -14.472    0.000    -0.052    -0.040 \n#>       Education    -0.021         0.020       -0.020     -1.070    0.285    -0.060     0.018 \n#>        UrbanYes     0.123         0.113        0.020      1.088    0.277    -0.099     0.345 \n#>           USYes    -0.184         0.150       -0.031     -1.229    0.220    -0.479     0.111 \n#> ---------------------------------------------------------------------------------------------\n# Base R\nsaleslm2 <- lm(Sales~. ,Carseats)\nsaleslm1 <- lm(Sales~.+Population*Income, Carseats)\n# Bruker pakken: car\ncompareCoefs(saleslm2, saleslm1)\n#> Calls:\n#> 1: lm(formula = Sales ~ ., data = Carseats)\n#> 2: lm(formula = Sales ~ . + Population * Income, data = \n#>   Carseats)\n#> \n#>                     Model 1   Model 2\n#> (Intercept)           5.661     6.195\n#> SE                    0.603     0.644\n#>                                      \n#> CompPrice           0.09282   0.09262\n#> SE                  0.00415   0.00413\n#>                                      \n#> Income              0.01580   0.00797\n#> SE                  0.00185   0.00387\n#>                                      \n#> Advertising          0.1231    0.1237\n#> SE                   0.0111    0.0111\n#>                                      \n#> Population         0.000208 -0.001811\n#> SE                 0.000370  0.000952\n#>                                      \n#> Price              -0.09536  -0.09511\n#> SE                  0.00267   0.00266\n#>                                      \n#> ShelveLocGood         4.850     4.859\n#> SE                    0.153     0.152\n#>                                      \n#> ShelveLocMedium       1.957     1.964\n#> SE                    0.126     0.125\n#>                                      \n#> Age                -0.04605  -0.04566\n#> SE                  0.00318   0.00317\n#>                                      \n#> Education           -0.0211   -0.0216\n#> SE                   0.0197    0.0196\n#>                                      \n#> UrbanYes              0.123     0.133\n#> SE                    0.113     0.112\n#>                                      \n#> USYes                -0.184    -0.216\n#> SE                    0.150     0.150\n#>                                      \n#> Income:Population           0.0000288\n#> SE                          0.0000125\n#> \n# Bruker pakken: ggplot2\nggplot(data=Carseats, aes(x=Income, y=Sales, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(Population,Sales), method=lm, se=F,color=\"black\")+xlab(\"Income and Population\")+labs(\n        title=\"Inntekt i blått - Befolkning i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\n# Bruker pakken: interactions\ninteract_plot(saleslm1, pred = Population, modx = Income)\n# Bruker pakken: jtools\nsumm(saleslm1)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.6 Linearitet","text":"Vi tar, som navnet “linær regresjonsanalyse” ganske klart indikerer, utgangspunkt forholdet mellom den/de uavhengige varaibelen(e) og den avhengige variabelen kan beskrives som en lineær funksjon (se eksempel Ringdal (2007)). Sammenhengen mellom variablene må ikke være perfekt lineær, men må hvert fall være tilnærmet lineær.Vi har allerede sett en grafisk framstilling punktet analyse av dataene som lar oss visuelt vurdere denne forutsetningen:Den stiplede linjen som buer ca midt det skraverte området gjør ingen forutsetninger, men plotter bare dataene (ofte kalt “scatterplot smoother”). Vi kan vurdere om denne er nærme eller langt fra en rett linje. grafen ligger det både en stiplet buet linje og en rett linje (regresjonslinje). vårt tilfelle vil vi raskt konkludere med forholdet mellom de variablene er tilnærmet lineært.","code":"\n# Bruker pakken: car\nscatterplot(Adverts ~ Sales, data = Field_OLS_data)"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.7 Residualene skal være normalfordelte","text":"Forutsetningen er residualene modellen er tilfeldige, normalfordelte variabler med gjennomsnittsverdi 0 (Field, Miles, Field 2012), hvilket innebærer forskjellen mellom modellen og de observerte dataene er 0 eller nær 0 de fleste tilfeller (og ulikhet skyldes tilfeldigheter). Poenget er dersom regresjonsmodellen er god skal det være omtrent like stor sannsynlighet den underestimerer som den overestimerer. Er den det vil fordelingen være tilnærmet symmetrisk (perfekt normalfordeling vil praksis ikke inntreffe).En tilnærming til å se på denne forutsetningen er å se på et histogram residualene. Når vi lagrer regresjonsmodellen som et objekt (R) kan vi se hvilke parametere som lagres modellen:Residualene lagres altså modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nnames(FieldOLS_reg)\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"      \n#>  [4] \"rank\"          \"fitted.values\" \"assign\"       \n#>  [7] \"qr\"            \"df.residual\"   \"xlevels\"      \n#> [10] \"call\"          \"terms\"         \"model\"\n# Base R\n\nFieldOLS_reg <- lm(Sales ~ Adverts, Field_OLS_data)\n\nhist(FieldOLS_reg$residuals)\n\nFieldOLSresid <- FieldOLS_reg$residuals\n\nplott3 <- ggplot(data = FieldOLS_reg, aes(FieldOLS_reg$residuals)) + \n  geom_histogram(bins = 10, color = \"#000000\", fill = \"#0099F8\") +\n    theme_classic() +\n    labs(title = 'Histogram av residualer', x = 'Residualer', y = 'Antall')\n\nplott3\n# Bruker pakken: olsrr\nols_plot_resid_qq(FieldOLS_reg)\nols_test_normality(FieldOLS_reg)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9899         0.1757 \n#> Kolmogorov-Smirnov        0.0634         0.3970 \n#> Cramer-von Mises          16.055         0.0000 \n#> Anderson-Darling          0.4298         0.3057 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.8 Fravær av multikolinearitet","text":"Multikolinearitet innebærer det er korrelasjon mellom de uavhengige varaiblene (multippel regresjon). Det kan ofte forekomme vi har en viss korrelasjon, men hvis korrelasjonen blir stor blir det vanskelig å skille mellom effekten den enkelte uavhengige variabel har på den avhengige variabelen.Multikolinearitet inntreffer dersom vi har sterk korrelasjon mellom eller flere av de uavhengige variablene. Multikollinearitet handler altså om det innbyrdes forholdet mellom de uavhengige variablene. Hvis disse er høyt korrelerte har vi multikollinearitet, altså det kan være (tilnærmet) perfekt linearitet mellom uavhengige variabler (Berry 1993) hvilket innebærer muligheten ingen av korrelasjonskoeffisientene er signifikante pga størrelsen på standardfeil. En perfekt kolinearitet har koeffisienten 1.Berry (1993) angir angir \\(r=.9\\) gir en dobling av standardfeil regresjonskoeffisienten, og selv \\(0.5\\) og \\(0.6\\) kan gi utfordringer tolkningen av regresjonskoeffisientene. Field (2009) anser \\(0.8\\) til \\(0.9\\) som høy korrelasjon.Samtidig sier Pallant (2010) det (naturligvis) bør være en viss korrelasjon mellom de uavhengige og den avhengige variabelen, og hevder de bør være på \\(0.3\\), men samtidig bivariat korrelasjon mellom de uavhengige variablene ikke bør være \\(0.7\\).Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de tre uavhengige variablene korrelerer mellom \\(0.33\\) og \\(0.60\\) med den avhengige. De bivariate korrelasjonene mellom de uavhengige variablene erpå hhv. \\(0.08\\), \\(0.10\\) og \\(0.18\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Dette er aktuelt multippel regresjonsanalyse, så å vise dette lager vi en slik datasettet til Field der vi inkluderer tre uavhengige variabler:Vi kan også se på Variance Inflation Factor (VIF), som måler hvor mye variansen til en estimert regresjonskoeffisient øker pga. multikollinearitet.Myers (1990) og Hair Jr. et al. (2010) opererer med en akseptabel grense VIF på 10.0 (toleranse på .1). er rådene (også) litt ulike. En VIF-verdi på 10 VIF anses f.eks. pakken “olsrr” R (som vi har brukt ) som et tegn på alvorlig multikolinearitet (jfr. Belsley, Kuh, Welsch 1980). Der settes VIF på 4 som en grense der man bør se nærmere på om multikolinearitet kan være et problem. Bowerman O’Connell (1990) tilføyer snittet av VIF de uavhengige variablene ikke bør være vesentlig 1.","code":"\n# Base R\nFieldKorr <- cor(Field_OLS_data, method = \"pearson\")\nround(FieldKorr, 2)\n#>         Adverts Sales Airplay Image\n#> Adverts    1.00  0.58    0.10  0.08\n#> Sales      0.58  1.00    0.60  0.33\n#> Airplay    0.10  0.60    1.00  0.18\n#> Image      0.08  0.33    0.18  1.00\n# Base R\nFieldOLS_mult_reg <- lm(Sales ~ ., data = Field_OLS_data)\nbrief(FieldOLS_mult_reg)\n#>            (Intercept) Adverts Airplay Image\n#> Estimate         -26.6 0.08488   3.367 11.09\n#> Std. Error        17.4 0.00692   0.278  2.44\n#> \n#>  Residual SD = 47.1 on 196 df, R-squared = 0.665\n# Bruker pakken: olsrr\nols_vif_tol(FieldOLS_mult_reg)\n#>   Variables Tolerance      VIF\n#> 1   Adverts 0.9856172 1.014593\n#> 2   Airplay 0.9592287 1.042504\n#> 3     Image 0.9629695 1.038455"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.9 Fravær av heteroskedasisitet","text":"Variansen til residualene de uavhengige variablene skal være lik (Miles Shevlin 2001). Heteroskedasitsitet innebærer residualene ikke har konstant varianse (motsatt: vi ønsker lik varians residualene alle x-verdier og kaller dette homoskedastisitet). Hvis vi har homoskedastisitet predikerer modellen vår likt på alle predikerte verdier av Y, noe vi ønsker.Variansen til residualen kan altså ikke avhenge av de uavhengige variablene, men være lik på alle nivåer av verdier prediktorene. Dersom vi har heteroskedastisitet vil spredningen rundt regresjonslinja variere med X.Forutsetningen om homoskedastisitet er godt illustrert av Miles Shevlin (2001), s.100-101, fig. 4.22, 4.23 og 4.24:Figuren til venstre viser et scatterplot residualer. midten har vi samme fordeling av residualer med et antall normalfordelingskurver. Til høyre ser vi et alternativt scatterplot residualer som bryter med forutsetningen om homoskedastisitet.Løvås (2013) illustrerer det samme slik eksempelet om motorstørrelse og drivstofforbruk:Variasjonen residualene er like stor uansett verdien av x.Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi p-verdien er \\(0.4027\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Base R\nFieldOLS_mult2 <- lm(Sales ~ Adverts + Airplay + Image, data = Field_OLS_data)\n# Bruker pakken: lmtest\nbptest(FieldOLS_mult2, ~ Adverts*Airplay*Image + I(Adverts^2) + I(Airplay^2)+ I(Image^2), data = Field_OLS_data)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  FieldOLS_mult2\n#> BP = 10.44, df = 10, p-value = 0.4027"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.10 Fravær av autokorrelasjon","text":"Dette er typisk et problem tidsserieanalyser eller geografiske analyser (Eikemo Clausen 2007, .124), der “verdien på variabel X enhet N stor grad er bestemt av verdien på variabel X enhet N-1”. Et annet kjent eksempel er fra aksjemarkedet: Hvis en aksje stiger dag er det mer sannsynlig den stiger morgen. Verdien av aksjen morgen avhenger (delvis) av verdien dag. Verdiene dataene autokorrelerer. Grad av autokorrelasjon er således et mål på forholdet mellom en variabels verdi på tidspunkt X og verdien på tidspunkt før X.tilfeldige observasjoner bør ikke ha korrelasjon residualen. Dette kan testes gjennom en Durbin-Watson test (Durbin Watson 1951).\nDurbin-Watson testen er en test på autokorrelasjon residualene, og vil ha en verdi på mellom 0 og 4. Verdien 2 indikerer ingen autokorrelasjon. Verdier mellom 0 og 2 indikerer en positiv autokorrelasjon, mens verdier mellom 2 og 4 indikerer en negativ autokorrelasjon. En konservativ tommelfingerregel sier verdier 1 og 3 er bekymringsfullt (Field, Miles, Field 2012).viser verdien \\(2.03\\) noe som ikke gir grunn til bekymring.","code":"\n# Bruker pakken: car\ndurbinWatsonTest(FieldOLS_reg)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1     -0.04394305      2.032324   0.762\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene 1 og 169 identifiseres som statistisk signifikante uteliggere","code":"\n# Bruker pakken: olsrr\ncar::qqPlot(FieldOLS_reg, id.method=\"identify\", main=\"Q-Q Plott\")#> [1]   1 169"},{"path":"regresjonsanalyse---ols.html","id":"hampel-filter","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.11.1 Hampel filter","text":"Hampel filter innebærer man ser alle observasjoner som ligger utenfor intervallet \\(median +/- 3 median\\ absolute\\ deviation\\ (MAS)\\).","code":"\n#Base R\nnedregrense <- median(Field_OLS_data$Adverts) - 3*(mad(Field_OLS_data$Adverts, constant = 1))\nnedregrense\n#> [1] -457.7405\n\novregrense <- median(Field_OLS_data$Adverts) + 3*(mad(Field_OLS_data$Adverts, constant = 1))\novregrense\n#> [1] 1521.572\n\nuteligger_ind <- which(Field_OLS_data$Adverts < nedregrense |Field_OLS_data$Adverts > ovregrense)\nuteligger_ind\n#>  [1]  11  23  28  43  55  87  88  93 126 175 184"},{"path":"regresjonsanalyse---ols.html","id":"grubbs-test","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.11.2 Grubbs’ test","text":"Grubbs’ test ser om den høyeste verdien variabelen bør regnes som en uteligger (hvis den høyeste ikke er det vil ingen andre heller være det).","code":"\n# Bruker pakken: outliers\ngrubbstest <- grubbs.test(Field_OLS_data$Adverts)\ngrubbstest\n#> \n#>  Grubbs test for one outlier\n#> \n#> data:  Field_OLS_data$Adverts\n#> G = 3.41281, U = 0.94118, p-value = 0.05396\n#> alternative hypothesis: highest value 2271.86 is an outlier"},{"path":"regresjonsanalyse---ols.html","id":"rosners-test","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.11.3 Rosners test","text":"angir vi det antallet vi tror er uteliggere, f.eks. fra et box plott.","code":"\n# Bruker pakken: EnvStats\nrosnerstest <- rosnerTest(Field_OLS_data$Adverts,\n  k = 3\n)\nrosnerstest$all.stats\n#>   i   Mean.i     SD.i    Value Obs.Num    R.i+1 lambda.i+1\n#> 1 0 614.4123 485.6552 2271.860     184 3.412808   3.605525\n#> 2 1 606.0834 472.3432 2000.000      43 2.951068   3.604019\n#> 3 2 599.0434 462.9555 1985.119      87 2.993971   3.602505\n#>   Outlier\n#> 1   FALSE\n#> 2   FALSE\n#> 3   FALSE"},{"path":"regresjonsanalyse---ols.html","id":"influential-cases","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.5.12 Influential cases","text":"Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):Det som grafen kalles “hat values” er et vanlig mål å finne observasjoner/caser som er relativt langt fra senter av prediksjonsrommet og som derfor potensielt har stor innflytelse på OLS-regresjonskoeffisientene (“leverage”) (Fox Weisberg 2019). Huber (1981) anbefaler følgende grenseverdier: verdier \\(0.2\\) er ønskelig, verdier \\(0.5\\) uønskede, og verdier mellom \\(0.2\\) og \\(0.5\\) problematiske.“Hat values” er altså et mål på potensiell innflytelse. Neste mål - DfBetas - er et mål på observasjonens effekt på regresjonskoeffisinenten hver variabel med og uten den innflytelsesrike observasjonen - eller med andre ord: observasjonenes innflytelse på variablene. Belsley, Kuh, Welsch (1980) anbefaler 2 som cut-verdi å indikere innflytelsesrike observasjoner.Pakken legger automatisk inn cutoff-verdi (dette tilfellet 0.14). Formelen utregning av dfbeta cutoff-verdi er \\(\\frac{2}{\\sqrt{n}}\\), der n=antall observasjoner.Vi kan også se på dffit (Welsch Kuh 1977).Cutoff-verdi dette tilfellet er 0.2. Formel utregning er \\(2*\\frac{\\sqrt{(k+1)}}{(n-k-1)}\\), der k = antall prediktorer og n = antall observasjoner.Det siste målet vi ønsker å se på (og trolig den mest brukte) er “Cook’s distance”, som gir et mål på observasjonens totale innflytelse på regresjonsmodellen.grafen vises Cook’s distance - et mål på vektet kvadratsum forskjellene mellom de individuelle elementene til koeffisienten. Sagt på en annen måte: Vi bruker Cook’s distance til å se hvilke observasjoner/caser som kan påvirke modellen vår uforholdsmessig mye (totalt sett). Dersom vi har mange caser med høy verdi på Cook’s distance kan det være en indikasjon på lineær regresjon kanskje ikke er en egnet analyse det foreliggende datasettet.Så hva er høy verdi på Cook’s distance? Kilder som Cook Weisberg (1982) og Tabachnik Fidell (2007) angir verdier 1 er bekymringsfullt. Andre, som Fox (2020), advarer mot en ren numerisk vurdering (og fremhever viktigheten av både grafisk presentasjon og vurdering av hvert enkelt tilfelle). En tilnærming som er anbefalt (se f.eks. Zach (2019)) er å bruker forholdstallet \\(4/N\\) - vårt tilfelle \\(4/200=0.02\\).La oss hente opp Cook’s distance de største verdiene de enkelte observasjoner:kjenner vi igjen observasjonene 1, 169 og 42 som de med høyest verdi på Cook’s distance, men også casene 10, 55 og 125 har verdier anbefalingen som kommer fra \\(4/n\\). Men vi ser også verdien er relativt lave hvis man tar utgangspunkt 1 som bekymringsfullt. å vise eventuell justering av modellen som følge av uteliggere viser vi likevel framgangsmåte.Vi bør også undersøke “added variable plots” - en regresjon kan observasjonene ha både en individuell og en sammensatt/felles påvirkning.sier Fox Weisberg (2019), s.44 “Points extreme left right plot correspond cases high leverage corresponding coefficients consequenlty potentially influential”.","code":"\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_reg, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(FieldOLS_reg, print_plot = TRUE)\n# Bruker pakken: olsrr\nols_plot_dffits(FieldOLS_reg)\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_reg, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier <- cooks.distance(FieldOLS_reg)\nmineCDverdier <- round(mineCDverdier, 3)\nhead(sort(mineCDverdier, decreasing = TRUE), n = 10)\n#>     1   169    42    10    55   125     3   148    86    72 \n#> 0.057 0.051 0.041 0.024 0.024 0.023 0.018 0.018 0.017 0.016\n# Bruker pakken: car\navPlots(FieldOLS_reg, id=list(cex=0.75, n=3, method=\"mahal\"))"},{"path":"regresjonsanalyse---ols.html","id":"steg-6-eventuell-revisjon-av-modell","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.6 Steg 6: Eventuell revisjon av modell","text":"kan vi eksempel se hvordan modellens presterer ved bortfall av visse ekstreme verdier (spesielt innflytelsesrike observasjoner/caser, jfr. diskusjon regresjonsforutsetnigner) eller ved inkludering/eksklusjon av gitte variabler modellen (først og fremst ved multippel regresjonsanalyse).Vi bør vurdere punktene og vurdere om vi ønsker å lage en revidert modell der vi tar ut veldig innflytelsesrike caser/observasjoner. Som tidligere nevnt har vi ikke svært store verdier , men la oss som et eksempel si vi ønsker å se om en modell uten observasjon 169. Vi anbefaler å ta bort en og en observasjon siden (som nevnt) observasjonene/casene har både en individuell og felles påvirkning.Som forventet ser vi ikke de store forskjellene. Vi kan ta bort de andre observasjonene illustrasjonens skyld:Igjen, ikke de store endringene. Vi kan se Intercept (\\(\\beta\\)) går litt ned etter hvert som vi tar bort caser, og betydningen av Adverts går litt opp (men det er marginalt).La oss, eksempelets skyld, manipulere datasettet slik en analyse av Cook’s distance ser slik ut:Hvis vi nå kjører denne modellen opp mot en modell der vi tar bort 11 og 23 får vi:ser vi koeffisienten Adverts stiger fra 0.01 til 0.09, eller en endring på 11.1%.","code":"\nFieldOLS_reg2 <- update(FieldOLS_reg, subset = -169)\n# Bruker pakken: car\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> \n#>             Model 1 Model 2\n#> (Intercept)  134.14  131.76\n#> SE             7.54    7.39\n#>                            \n#> Adverts     0.09612 0.09826\n#> SE          0.00963 0.00942\n#> \nFieldOLS_reg3 <- update(FieldOLS_reg, subset = -c(1, 42, 169, 184))\ncompareCoefs(FieldOLS_reg, FieldOLS_reg2, FieldOLS_reg3)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -169)\n#> 3: lm(formula = Sales ~ Adverts, data = Field_OLS_data, \n#>   subset = -c(1, 42, 169, 184))\n#> \n#>             Model 1 Model 2 Model 3\n#> (Intercept)  134.14  131.76  126.14\n#> SE             7.54    7.39    7.28\n#>                                    \n#> Adverts     0.09612 0.09826 0.10461\n#> SE          0.00963 0.00942 0.00942\n#> \n# Bruker pakken: readxl\nField_OLS_data2 <- read_excel(\"Field_datasett_OLS2.xlsx\")\n# Base R\nFieldOLS_man <- lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\n# Bruker pakken: car\ninfluenceIndexPlot(FieldOLS_man, vars = c(\"Cook\"), id = list(n=3))\nFieldOLS_man2 <- update(FieldOLS_man, subset = -c(11, 23))\n# Bruker pakken: car\ncompareCoefs(FieldOLS_man, FieldOLS_man2)\n#> Calls:\n#> 1: lm(formula = Sales ~ Adverts, data = Field_OLS_data2)\n#> 2: lm(formula = Sales ~ Adverts, data = Field_OLS_data2,\n#>    subset = -c(11, 23))\n#> \n#>             Model 1 Model 2\n#> (Intercept)  183.77  134.14\n#> SE             5.97    7.64\n#>                            \n#> Adverts     0.01207 0.09621\n#> SE          0.00298 0.00999\n#> "},{"path":"regresjonsanalyse---ols.html","id":"steg-7-eventuell-analyse-av-revidert-modell","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.7 Steg 7: Eventuell analyse av revidert modell","text":"vil vi prinsippet bare gjenta samme analyser som ved analyse av den opprinnelige modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"steg-8-konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.8 Steg 8: Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Vi viser rapportering av en enkel lineær regresjonsanalyse etter APA-standard:En enkel lineær regresjon ble gjennomført å predikere salgstall per uke basert på sum brukt på reklame uka før lansering. Vi fant en signifikant regresjonslikning (F(1,198) = 99.59, \\(\\beta\\) = 134.14p < .001) med en \\(R^2\\) på .335, 95% CI [119.28, 149.00].","code":"\n# Bruker pakken: table1\ntable1::label(Field_OLS_data$Adverts) <- \"Adverts\"\ntable1::label(Field_OLS_data$Sales) <- \"Sales\"\ntable1::table1(~Adverts + Sales, data = Field_OLS_data)\n# Bruker pakken: sjPlot\nFieldOLSkorr <- tab_corr(Field_OLS_data, triangle = \"lower\")\nFieldOLSkorr\n# Bruker pakken: sjPlot\ntab_model(FieldOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"til-slutt-for-r-brukere","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.3.9 Til slutt for R-brukere…","text":"Mehmetoglu Mittner (2020) har skrevet en veldig god bok om på norsk: “Innføring R statistiske analyser” som vi varmt kan anbefale. Den kommer med en pakke (“rnorsk”) som kan lastes ned gjennom kommandoen devtools::install_github(“ihrke/rnorsk”) (forutsetter pakken “devtools” er på plass, hvis ikke så kjør “package.install(”devtools”)).Forfatterne har laget en samling av regresjonsdiagnostikk som vi viser på Fields data der vi laget en multippel regresjonsmodell:Analysen gir en samlet oversikt et antall parametere og søker å hjelpe til med beslutning om forutsetninger er ok/ikke ok, men vi vil understreke kunnskap om hva som ligger bak de ulike testene og kriteriene er essensielt. Mer informasjon om parameterene finner dere .","code":"\noptions(scipen=999)\n# Bruker pakken: rnorsk\nregression.diagnostics(FieldOLS_mult_reg)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 1/11 (9.1 %) checks failed\n#> \n#> \n#> Identified problems: \n#>  functional form\n#> Summary:\n#> # A tibble: 11 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   6.19e+0  0.103   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   3.03e-1  0.582   0.05 No Pro~\n#>  3 multicoll~ Adverts  Vari~   1.01e+0 NA       5    No Pro~\n#>  4 multicoll~ Airplay  Vari~   1.04e+0 NA       5    No Pro~\n#>  5 multicoll~ Image    Vari~   1.04e+0 NA       5    No Pro~\n#>  6 normality  global   Shap~   9.95e-1  0.725   0.01 No Pro~\n#>  7 model spe~ global   Stat~  -6.99e-5  0.916   0.05 No Pro~\n#>  8 functiona~ global   RESE~   3.72e+0  0.0261  0.05 Problem\n#>  9 outliers   global   Cook~   7.08e-2 NA       1    No Pro~\n#> 10 outliers   global   Bonf~   3.16e+0  0.362   0.05 No Pro~\n#> 11 autocorre~ global   Durb~   2.70e-3  0.814   0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"standard-multippel-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4 Standard multippel regresjonsanalyse","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-standard-multippel-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.1 Eksempel standard multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på multippel regresjonsanalyse, ved å følge stegene .Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultater","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.2 Analyse av dataene","text":"Vi skal bruke et datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaDatasettet er stort til å vises fram sin helhet, men vi skal bruke uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) mot den avhengige variabelen tpstress (Perceived stress).Vi ønsker altså å se om en gruppe studenters (N = 439) egenrapporterte oppfattelse av sin kontroll eksterne forhold som kan skape stress og deres evne til å kontrollere deres følelser, tanker og fysiske reaksjoner (Pallant 2000). Vi kan derfor se nærmere på variablene.","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\n# Base R\nPallant_survey2 <- select(Pallant_survey, tmast, tpcoiss, tpstress)\nPallant_survey2 <- na.omit(Pallant_survey2) \n# Bruker pakken: summarytools\nsummarytools::descr(Pallant_survey2, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey2  \n#> N: 426  \n#> \n#>                    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- ----------\n#>            Mean    21.74     60.56      26.75\n#>         Std.Dev     3.97     11.96       5.84\n#>             Min     8.00     20.00      12.00\n#>          Median    22.00     62.00      26.00\n#>             Max    28.00     88.00      46.00\n#>         N.Valid   426.00    426.00     426.00\n#>       Pct.Valid   100.00    100.00     100.00\n# Base R\npar(mfrow=(c(2,2)))\nhisttmast <- with(Pallant_survey2, hist(tmast))\nhisttpcoiss <- with(Pallant_survey2, hist(tpcoiss))\nhisttpstress <- with(Pallant_survey2, hist(tpstress))\n# Bruker pakken: car\nqqtmast <- car::qqPlot(~ tmast, data = Pallant_survey2)\nqqtpcoiss <- car::qqPlot(~ tpcoiss, data = Pallant_survey2)\nqqtpstress <- car::qqPlot(~ tpstress, data = Pallant_survey2)\n# Base R\nboxplot(Pallant_survey2)"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.3 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.4 Lage modell (kjøre regresjonsanalysen)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.5 Analyse av resultatene (diagnostikk)","text":"","code":""},{"path":"regresjonsanalyse---ols.html","id":"antall-prediktoreruavhengige-variabler-overfitting-og-predicted-r-square","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.5.1 Antall prediktorer/uavhengige variabler, overfitting og predicted R-square","text":"multippel regresjonsanalyse har vi mer enn en prediktor/uavhengig variabel. Som regel kan vi ønske å inkludere mer enn en prediktor fordi vi sjeldent klarer å fange nok av variansen en avhengig variabel gjennom en prediktor. Samtidig ønsker vi ikke flere uavhengige variabler enn nødvendig å lage en så enkel modell som mulig som gir oss prediksjoner vi kan bruke. Matematisk er det også slik enhver uavhengig variabel som har en grad av korrelasjon med den avhengige variabelen vil bidra til å øke \\(R^2\\) uten det nødvendigvis gjør modellen “riktigere” (men kan gjøre den vanskeligere å tolke). Dersom vi legger til unødvendige uavhengige variabler risikerer vi det som kalles overfitting - altså modellen blir god på å beskrive tilfeldige feil dataene heller enn å beskrive forholdet mellom variablene. Resultatet er modellen ikke kan generaliseres. Vi kan også se tilbake på regresjonsforutsetningene og vil se et stort antall uavhengige variabler er en invitasjon til multikolinearitet.En måte å sjekke dette er å dele datasettet slik man gjør regresjonsanalysen på en del av datasettet og deretter tester modellen på den andre delen av datasettet. Dette kalles kryssvalidering. En annen måte er å se på “predicted R-square” som innebærer følgende prosedyre (som statistikkprogrammer gjør oss naturligvis):Et datapunkt/observasjon tas ut av datasettetRegresjonslikningen kalkuleresModellens evne til å predikere det datapunktet/observasjonen som ble tatt ut evalueres (altså - hvor nærme datapunktet kommer vår modell sin prediksjon?)Dette gjentas alle datapunktene datasettetTolkningen av dette er ganske grei. Man sammenlikner R-square med predicted R-square. Dersom det er liten forskjell mellom disse verdiene har man trolig liten sannsynlighet du har overfitting av modellen. Dersom forskjellen er stor er det grunn til å tro man kan ha overfitting.å unngå dette er det viktig å tenke på forholdstallene som ble diskutert foregående punkt om regresjonsforutsetninger. Kjennskap til tidligere forskning og resultater vil gi informasjon om og et teoretisk grunnlag hvilke variabler som bør inkluderes modellen.Vi skal illustrere overfitting basert på et eksempel fra Frost (2022) (dette eksempelet har altså ikke noe direkte med analysen vi er inne - Pallant sitt datasett - men er tatt med å illustrere poenget med overfitting).Datasettet inneholder variabler: Hvordan historikere rangerer amerikanske presidenter (Historians.rank) og hvor stor generell støtte presidenter har hatt befolkningen (Approval.High).Vi ser R-squared er \\(0.0068\\) - hvilket vi vil tolke som det praksis ikke er noen sammenheng mellom variablene.Vi kan så bruke en polynomisk likning (har vi brukt \\(x^3\\) å lage regresjonslinjen):Vi har nå en R-squared på \\(0.66\\) denne modellen mot \\(0.0068\\) den lineære modellen. Dette betyr den polynomiske regresjonsmodellen forklarer drøye 66% av variansen den avhengige variabelen. Modellen vår er ut fra dette en god modell våre data.\nImidlertid gir en analyse av Predicted R-square oss et annet bilde av modellen:realiteten forteller både verdien på predicted r-square på 0, og forskjellen mellom R-square (\\(0.664\\)) og predicted R-square \\(0\\), vi har en seriøs overfitting. Vi har med andre ord funnet en modell som beskriver dataene våre veldig godt, men som ikke kan brukes på andre data enn de vi har (vel, den kan jo brukes, men vil ikke kunne gi oss noe av verdi). Vi kan altså ikke predikere noe ut fra modellen.Vi ser \\(R^2 = 0.466\\). Modellen kan altså forklare \\(46.6%\\) av variansen den avhengige variabelen. Vi ser vi får en noe lavere verdi «Adjusted R Squared» forhold til «R Squared». Når vi legger til en uavhengig variabel en regresjonsanalyse er det lite sannsynlig korrelasjonen mellom den nye uavhengige variabelen og den avhengige variabelen vil være nøyaktig 0. Den vil stedet fluktuere rundt 0. Pga. denne tilfeldige fluktuasjonen rundt 0 vil \\(R^2\\) alltid øke litt når man legger til en ny uavhengig variabel. Adjusted \\(R^2\\) søker å kompensere dette å få fram en mer korrekt verdi. Jo større antall uavhengige variabler, jo større forskjell vil man se mellom \\(R^2\\) og Adjusted \\(R^2\\). Det samme vil være tilfelle ved mindre utvalgsstørrelser fordi variasjonen rundt 0 vil være større mindre utvalg.Vi kan også legge merke til Predicted R-Squared er \\(0.456\\) og dermed svært lik R-squared.","code":"\n# Base R\nPresidentRanking <- read.csv(\"PresidentRanking.csv\")\n\n# Bruker pakken: summarytools\nsummarytools::descr(PresidentRanking)\n#> Descriptive Statistics  \n#> PresidentRanking  \n#> N: 12  \n#> \n#>                     Approval.High   Historians.rank\n#> ----------------- --------------- -----------------\n#>              Mean           78.75             17.00\n#>           Std.Dev            8.01             10.90\n#>               Min           67.00              2.00\n#>                Q1           72.00              8.50\n#>            Median           79.00             15.00\n#>                Q3           85.50             25.00\n#>               Max           90.00             38.00\n#>               MAD           10.38             11.86\n#>               IQR           12.25             15.75\n#>                CV            0.10              0.64\n#>          Skewness           -0.05              0.37\n#>       SE.Skewness            0.64              0.64\n#>          Kurtosis           -1.58             -1.20\n#>           N.Valid           12.00             12.00\n#>         Pct.Valid          100.00            100.00\n# Base R\npresidentlm <- lm(formula = Historians.rank ~ Approval.High, data = PresidentRanking)\nplot(Historians.rank ~ Approval.High, data = PresidentRanking)\nabline(presidentlm, lwd = 2, col = \"red\")\nsumm(presidentlm)\n# Base R\npresidentlm2 <- lm(Historians.rank ~ poly(Approval.High, degree=3), data=PresidentRanking)\n# Bruker pakken: ggplot2\nggplot(data=PresidentRanking, aes(Approval.High,Historians.rank)) +\n    geom_point() + \n    geom_smooth(method=\"lm\", formula=y~I(x^3)+I(x^2))\nsumm(presidentlm2)\n# Bruker pakken: olsrr\nols_pred_rsq(presidentlm2)\n#> [1] -0.2162257\n# Bruker pakken: summarytools\nPallantOLS_reg <- lm(tpstress ~ tmast + tpcoiss, data = Pallant_survey2)\nsumm(PallantOLS_reg)"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.5.2 Modellens koeffisienter","text":"Vi ser først på tallene “Std.Beta” “Parameter Estimated”. Vi ser tmast bidrar større grad enn tpcoiss (fortegn er denne sammenheng irrelevant). Begge bidrar signifikant.","code":""},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.5.3 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 184.5. Vi kan regne ut (bruke R) til å finne kritiske verdi.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=2, df2=423, lower.tail=FALSE)\n#> [1] 3.017049"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"variablene-er-uten-målefeil-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.2 Variablene er uten målefeil","text":"Vi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.3 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.4 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\n# Base R\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.5 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten, men grafisk kan vi også se interaksjonseffekten:Det kan se ut som en interaksjonseffekt ved linjene ikke er parallelle. Det kan imidlertid være noe vanskelig å tolke interaksjonseffekt. Man kan f.eks. ha en interaksjonseffekt som ikke er statistisk signifikant. R kan vi bruke pakken jtools som hjelp:Vi ser interaksjonen tmast:tpcoiss ikke er statistisk signifikant.Tolkningen er “som før”: Parallelle linjer indikerer fravær av interaksjonseffekt.","code":"\n# Base R\nPallantOLS_reg2 <- lm(tpstress ~ tmast + tpcoiss, Pallant_survey2)\nPallantOLS_inter <- lm(tpstress ~.+tmast*tpcoiss, Pallant_survey2)\n# Bruker pakken: car\ncompareCoefs(PallantOLS_reg2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ tmast + tpcoiss, data = \n#>   Pallant_survey2)\n#> 2: lm(formula = tpstress ~ . + tmast * tpcoiss, data = \n#>   Pallant_survey2)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     50.83   52.66\n#> SE               1.27    4.34\n#>                              \n#> tmast         -0.6207 -0.7093\n#> SE             0.0614  0.2103\n#>                              \n#> tpcoiss       -0.1747 -0.2071\n#> SE             0.0204  0.0763\n#>                              \n#> tmast:tpcoiss         0.00154\n#> SE                    0.00348\n#> \n# Bruker pakken: ggplot2\nggplot(data=Pallant_survey2, aes(x=tmast, y=tpstress, group=1)) +geom_smooth(method=lm,se=F)+ \n    geom_smooth(aes(tmast,tpcoiss), method=lm, se=F,color=\"black\")+xlab(\"tmast og tpcoiss\")+labs(\n        title=\"tmast i blått - tpcoiss i svart\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\n# Bruker pakken: jtools\nsumm(PallantOLS_inter)\n# Bruker pakken: interactions\ninteract_plot(PallantOLS_inter, pred = tmast, modx = tpcoiss)"},{"path":"regresjonsanalyse---ols.html","id":"linearitet-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.6 Linearitet","text":"Vi kan ikke se på samme type scatterplott sjekk av linearitet som vi gjorde enkel OLS (på et todimensjonalt plott). Vi kan imidlertid lage “added variable plots” (Mosteller Tukey 1977).X-aksene representerer en enkelt uavhengig variabel (per graf ovenfor). Y-aksen = den avhengige variabelen. Den blå linjen viser sammenhengen mellom den uavhengige variabelen og den avhengige variabelen når alle andre uavhengige variabler holdes konstant. Jo sterkere lineær sammenheng plottene, jo sterkere er den respektive uavhengige variabelens bidrag modellen.vårt tilfelle vil vi nok konkludere med forutsetningen om linearitet er (nok) oppfylt.","code":"\n# Bruker pakken: olsrr\nols_plot_added_variable(PallantOLS_reg2, print_plot = TRUE)\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.7 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik.En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nhist(PallantOLS_reg2$residuals)\n# Bruker pakken: olsrr\nols_plot_resid_qq(PallantOLS_reg2)\nols_test_normality(PallantOLS_reg2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9911         0.0115 \n#> Kolmogorov-Smirnov        0.0509         0.2197 \n#> Cramer-von Mises         31.6502         0.0000 \n#> Anderson-Darling          1.0978         0.0070 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.8 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.58\\) og \\(r=-0.61\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.53\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\n# Base R\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\", use=\"pairwise.complete.obs\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\n# Bruker pakken. olsrr\nols_vif_tol(PallantOLS_reg2)\n#>   Variables Tolerance      VIF\n#> 1     tmast 0.7220785 1.384891\n#> 2   tpcoiss 0.7220785 1.384891"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.9 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Bruker pakken: lmtestbptest(PallantOLS_reg2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.10 Fravær av autokorrelasjon","text":"viser verdien \\(1.826\\) noe som ikke gir grunn til bekymring.","code":"\n# Bruker pakken: car\ndurbinWatsonTest(PallantOLS_reg2)\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08185218      1.825972   0.078\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.11 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:dffit:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\n# Bruker pakken: car\ncar::qqPlot(PallantOLS_reg2, id = list(n=3))#>  22 194 269 \n#>  21 190 263\n# Bruker pakken: car\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(PallantOLS_reg2, print_plot = TRUE)\n# Bruker pakken: olsrr\nols_plot_dffits(PallantOLS_reg2, print_plot = TRUE)\n# Bruker pakken: car\ninfluenceIndexPlot(PallantOLS_reg2, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier2 <- cooks.distance(PallantOLS_reg2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22     268      23     191     413     194 \n#> 0.09556 0.05833 0.05434 0.04374 0.03689 0.03601"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.6.12 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\n# Bruker pakken: rnorsk\nregression.diagnostics(PallantOLS_reg2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/10 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 10 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   3.95     0.139   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   2.91     0.0881  0.05 No Pro~\n#>  3 multicoll~ tmast    Vari~   1.38    NA       5    No Pro~\n#>  4 multicoll~ tpcoiss  Vari~   1.38    NA       5    No Pro~\n#>  5 normality  global   Shap~   0.991    0.0115  0.01 No Pro~\n#>  6 model spe~ global   Stat~   0.00495  0.568   0.05 No Pro~\n#>  7 functiona~ global   RESE~   0.419    0.658   0.05 No Pro~\n#>  8 outliers   global   Cook~   0.0956  NA       1    No Pro~\n#>  9 outliers   global   Bonf~   3.56     0.177   0.05 No Pro~\n#> 10 autocorre~ global   Durb~   0.0819   0.0620  0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.7 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.8 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.4.9 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:En multippel lineær regresjonsanalyse ble gjennomført å teste om respondentenes oppfattede nivå av indre kontroll og kontroll på ytre faktorer predikerer totalt nivå av oppfattet stress. Analyser ble gjennomført å sikre det ikke var brudd på forutsetningene en multippel lineær regresjonsanalyse. En statistisk signifikant regresjonsmodell ble funnet (F (2, 423) = 184.5, p < .001), med en \\(R^2\\) på .466. Både nivå av indre kontroll og kontroll på ytre faktorer var signifikante prediktorer.","code":"\n# Bruker pakken: table1\ntable1::label(Pallant_survey$tmast) <- \"tmast\"\ntable1::label(Pallant_survey$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey$tpstress) <- \"tpstress\"\ntable1::table1(~tmast + tpcoiss + tpstress, data = Pallant_survey)\n# Bruker pakken: sjPlot\nPallantkorr1 <- tab_corr(Pallant_survey2, triangle = \"lower\")\nPallantkorr1\n# Bruker pakken: sjPlot\ntab_model(PallantOLS_reg2)"},{"path":"regresjonsanalyse---ols.html","id":"hierarkisk-multippel-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5 Hierarkisk multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på hierarksik multippel regresjonsanalyse, ved å følge stegene . Hierarkisk vil dere også kunne se omtalt som blockwise entry fordi data legges inn modellen blokker).Analyse av dataeneAnalyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneEvtentuelt valg av prediktorer ut fra analyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)Sjekk av forutsetningeneSjekk av forutsetningeneEventuell revisjon av modellenEventuell revisjon av modellenEventuell analyse av revidert modellEventuell analyse av revidert modellKonklusjon / oppsummering / rapportering av resultaterKonklusjon / oppsummering / rapportering av resultaterEksempelet utvider eksempelet standard multippel regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.1 Analyse av dataene","text":"Vi skal bruke samme datasett fra Pallant (2010) som du kan finne .Download Pallant_survey.xlsxDownload Pallant_survey.savDownload Pallant_survey.dtaI tillegg til de uavhengige variabler - tmast (Control external events) og tpcioss (Control internal states) vil vi bruke variablene alder (age) og “social desirability” (tmarlow) som kontrollvariabler.“Social desirability bias” er et kjent fenomen der respondenter har en tendens til å ønske å framstå et bedre lys heller enn et sant/reelt lys (Preiss et al. 2015). Respondenter kan f.eks. ønske å framstå som mer ærlige enn de realiteten er, og vil derfor også svare deretter på spørsmål. tillegg kan man anta alder kan ha påvirkning på opplevelsen av totalt stressnivå.Vi velger å lage et nytt datasett der vi trekker ut de relevante variablene, og fjerne observasjoner med NA (ingen verdi).denne hierarkiske multipple regresjonsanalysen ønsker vi derfor å bruke alder og sosial ønskverdighet som kontrollvariabler. Vi legger første blokk inn de variablene vi ønsker å kontrollere , før vi blokk legger inn de samme uavhengige variablene som forrige eksempel. Hensikten med dette er vi ønsker å fjerne mulige effekter av de “kontrollvariablene”.","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey <- as.data.frame(read_excel(\"Pallant_survey.xlsx\"))\nPallant_subset <- Pallant_survey[c(\"age\", \"tmarlow\", \"tmast\", \"tpcoiss\", \"tpstress\")]\n# Bruker pakken: summarytools\nsummarytools::descr(Pallant_subset, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_subset  \n#> N: 439  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.44      5.30    21.76     60.63      26.73\n#>         Std.Dev    13.20      2.04     3.97     11.99       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   439.00    433.00   436.00    430.00     433.00\n#>       Pct.Valid   100.00     98.63    99.32     97.95      98.63\n# Base R\nPallant_survey3 <- select(Pallant_survey, age, tmarlow, tmast, tpcoiss, tpstress)\nPallant_survey3 <- na.omit(Pallant_survey3)\nsummarytools::descr(Pallant_survey3, stats = \"common\")\n#> Descriptive Statistics  \n#> Pallant_survey3  \n#> N: 423  \n#> \n#>                      age   tmarlow    tmast   tpcoiss   tpstress\n#> --------------- -------- --------- -------- --------- ----------\n#>            Mean    37.32      5.30    21.77     60.56      26.74\n#>         Std.Dev    13.09      2.02     3.97     12.00       5.85\n#>             Min    18.00      0.00     8.00     20.00      12.00\n#>          Median    36.00      5.00    22.00     62.00      26.00\n#>             Max    82.00     10.00    28.00     88.00      46.00\n#>         N.Valid   423.00    423.00   423.00    423.00     423.00\n#>       Pct.Valid   100.00    100.00   100.00    100.00     100.00\n# Base R\npar(mfrow=(c(1,2)))\nhistage <- with(Pallant_survey3, hist(age))\nhisttmarlow <- with(Pallant_survey3, hist(tmarlow))"},{"path":"regresjonsanalyse---ols.html","id":"evtentuelt-valg-av-prediktorer-ut-fra-analyse-av-dataene-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.2 Evtentuelt valg av prediktorer ut fra analyse av dataene","text":"Vi gjør ingen analyse av dette da vi har valgt kontrollvariabler og uavhengige variabler ut fra eksempelet til Pallant (2010).","code":""},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.3 Lage modell (kjøre regresjonsanalysen)","text":"lager vi tre modeller: Modell 0 inneholder kun Intercept (som en referansemodell). Modell 1 inneholder kun age og tmarlow, modell 2 inneholder tillegg tmast og tpcoiss.","code":"\n# Base R\nmodell0 <- lm(tpstress ~ 1, data = Pallant_survey3)\nmodell1 <- lm(tpstress ~ age + tmarlow, data = Pallant_survey3)\nmodell2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey3)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.4 Analyse av resultatene (diagnostikk)","text":"Den første modellen er modellen med kun alder og social desirability som forklarer 5,9 % av variansen. Den andre modellen, som består av både kontrollvariablene og de tidligere uavhengige variablene forklarer til sammen 47 %.Vi kan oppsummere modellene:Modell 0: \\(SS_Total\\) = 14450 (ingen prediktorer)Modell 1: \\(SS_Residual\\) = 13598.0, \\(SS_Difference\\) = 852.4, \\(F\\)(2, 420) = 23.26, \\(p\\)<.001 (etter å ha lagt til age og tmarlow)Modell 2: \\(SS_Residual\\) = 7658.8, \\(SS_Difference\\) = 5939.2, \\(F\\)(2, 418) = 162.07, \\(p\\)<.001 (etter å ha lagt til tmast og tpcoiss)Det vi jo ønsket å finne ut av var hvor mye våre uavhengige variabler forklarer etter effektene fra de kontrollvariablene er tatt bort. Dette finner vi «R Square Change» - altså hvor mye \\(R^2\\) endrer seg fra modell 1 til modell 2. Vi har tillegg med hvor mye \\(R^2\\) endrer seg fra modell 0 ti lmodell 1.modell 2 er verdien 0.411. Altså – de uavhengige variablene forklarer 41.1 % av variansen den avhengige variabelen etter vi har kontrollert alder og sosial ønskverdighet. Vi kan så se på de uavhengige variablenes unike bidrag (altså bidrag etter interaksjonseffekter er tatt bort).","code":"\n# Base R\nsumm(modell1)\nsumm(modell2)\n# Base R\nanova(modell0, modell1, modell2)\n#> Analysis of Variance Table\n#> \n#> Model 1: tpstress ~ 1\n#> Model 2: tpstress ~ age + tmarlow\n#> Model 3: tpstress ~ age + tmarlow + tmast + tpcoiss\n#>   Res.Df     RSS Df Sum of Sq      F                Pr(>F)\n#> 1    422 14450.3                                          \n#> 2    420 13598.0  2     852.4  23.26       0.0000000002642\n#> 3    418  7658.8  2    5939.2 162.07 < 0.00000000000000022\n#>      \n#> 1    \n#> 2 ***\n#> 3 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Base R\nrsq_diff1 <- summary(modell1)$r.squared - summary(modell0)$r.squared\nrsq_diff2 <- summary(modell2)$r.squared - summary(modell1)$r.squared\nrsq_diff1\n#> [1] 0.05898557\nrsq_diff2\n#> [1] 0.4110042"},{"path":"regresjonsanalyse---ols.html","id":"modellens-koeffisienter-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.4.1 Modellens koeffisienter","text":"Vi ser på tallene “Std.Beta” “Parameter Estimates”. Vi ser alder og social desirability ikke bidrar signifikant seg selv. Beta verdiene tabellen representerer de unike bidragene hver variabel etter overlappende effekter fra de andre variablene er fjernet. tmast bidrar større grad (beta = -0,631) enn pcoiss (beta = -0,160).","code":"\n# Bruker pakken: olsrr\nmodell2x <- ols_regress(tpstress ~ age + tmarlow + tmast + tpcoiss, data = Pallant_survey)\nmodell2x\n#>                         Model Summary                          \n#> --------------------------------------------------------------\n#> R                       0.686       RMSE                4.280 \n#> R-Squared               0.470       Coef. Var          16.011 \n#> Adj. R-Squared          0.465       MSE                18.323 \n#> Pred R-Squared          0.455       MAE                 3.277 \n#> --------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                 ANOVA                                  \n#> ----------------------------------------------------------------------\n#>                  Sum of                                               \n#>                 Squares         DF    Mean Square      F         Sig. \n#> ----------------------------------------------------------------------\n#> Regression     6791.514          4       1697.879    92.666    0.0000 \n#> Residual       7658.831        418         18.323                     \n#> Total         14450.345        422                                    \n#> ----------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                    \n#> ----------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n#> ----------------------------------------------------------------------------------------\n#> (Intercept)    51.716         1.371                 37.713    0.000    49.020    54.411 \n#>         age    -0.021         0.017       -0.046    -1.201    0.231    -0.054     0.013 \n#>     tmarlow    -0.147         0.111       -0.051    -1.328    0.185    -0.364     0.071 \n#>       tmast    -0.631         0.063       -0.429    -9.997    0.000    -0.756    -0.507 \n#>     tpcoiss    -0.160         0.022       -0.328    -7.311    0.000    -0.203    -0.117 \n#> ----------------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.4.2 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 92.67.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=4, df2=418, lower.tail=FALSE)\n#> [1] 2.393283"},{"path":"regresjonsanalyse---ols.html","id":"sjekk-av-forutsetningene-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5 Sjekk av forutsetningene","text":"Vi viser resultater av tester og grafer, men diskuterer ikke disse nærmere. stedet viser vi til gjennomgangen av forutsetningene lenger opp .","code":""},{"path":"regresjonsanalyse---ols.html","id":"kausalitet-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.1 Kausalitet","text":"Vi antar det foreligger godt teoretisk grunnlag modellen.\n#### Variablene er uten målefeilVi må forutsette vi ikke har systematiske målefeil variablene.","code":""},{"path":"regresjonsanalyse---ols.html","id":"relevante-og-irrelevante-variabler-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.2 Relevante og irrelevante variabler","text":"Også dette forutsetter vi er på plass.","code":""},{"path":"regresjonsanalyse---ols.html","id":"forholdstall-mellom-caserobservasjoner-og-uavhengige-variabler-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.3 Forholdstall mellom caser/observasjoner og uavhengige variabler","text":"Vi har altså 426 observasjoner. forhold til tabellen vist forutsetninger lenger opp tilfredsstiller dette godt de ulike måtene å betrakte forholdstallet vi har vist til.","code":"\n# Base R\nnrow(Pallant_survey2)\n#> [1] 426"},{"path":"regresjonsanalyse---ols.html","id":"de-uavhengige-variablene-er-additiv-for-den-avhengige-variabelen-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.4 De uavhengige variablene er additiv for den avhengige variabelen","text":"Vi kan mistenke det kan være en interaksjon mellom variablene tmast og tpcoiss - jo større kontroll på eksterne hendelser, jo større er kontrollen følelser, tanker og fysisker reaksjoner. Vi ser derfor etter en interaksjonseffekt.Forskjellen mellom modellene ligger altså den nederste koeffisienten (tmast:tpcoiss) som da er den kombinerte og samtidige effekten av de variablene. Vi ser effekten er veldig liten (vi gjentar ikke den grafiske framstillingen som er lik som forrige analyse).","code":"\n# Base R\nPallantOLS2 <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey)\nPallantOLS_inter <- lm(tpstress ~ age + tmarlow + tmast + tpcoiss + tmast*tpcoiss, Pallant_survey)\n# Bruker pakken: car\ncompareCoefs(PallantOLS2, PallantOLS_inter)\n#> Calls:\n#> 1: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss, data = Pallant_survey)\n#> 2: lm(formula = tpstress ~ age + tmarlow + tmast + \n#>   tpcoiss + tmast * tpcoiss, data = Pallant_survey)\n#> \n#>               Model 1 Model 2\n#> (Intercept)     51.72   53.80\n#> SE               1.37    4.38\n#>                              \n#> age           -0.0206 -0.0206\n#> SE             0.0171  0.0171\n#>                              \n#> tmarlow        -0.147  -0.148\n#> SE              0.111   0.111\n#>                              \n#> tmast         -0.6314 -0.7324\n#> SE             0.0632  0.2111\n#>                              \n#> tpcoiss       -0.1600 -0.1969\n#> SE             0.0219  0.0768\n#>                              \n#> tmast:tpcoiss         0.00175\n#> SE                    0.00349\n#> "},{"path":"regresjonsanalyse---ols.html","id":"linearitet-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.5 Linearitet","text":"Se forrige regresjonsanalyse.","code":""},{"path":"regresjonsanalyse---ols.html","id":"residualene-skal-være-normalfordelte-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.6 Residualene skal være normalfordelte","text":"Side vi vet residualene lagres modellen og vi kan plotte ut disse:Vi kan også se på et Q-Q plott og hente ut testverdier ulike normalitetstester:Q-Q plottet viser noe avvik (jfr. )En hendig graf er også et plott av residualene på y-aksen og “fitted values” på x-aksen:forhold til forutsetningen om normalfordelte residualer skal disse være spredd tilfeldig rundt 0.Sett ett kan det dette tilfellet se ut som residualene er tilnærmet (nok) normalfordeling.","code":"\n# Base R\nhist(PallantOLS2$residuals)\n# Bruekr pakken: olsrr\nols_plot_resid_qq(PallantOLS2)\nols_test_normality(PallantOLS2)\n#> -----------------------------------------------\n#>        Test             Statistic       pvalue  \n#> -----------------------------------------------\n#> Shapiro-Wilk              0.9926         0.0345 \n#> Kolmogorov-Smirnov        0.038          0.5743 \n#> Cramer-von Mises         31.3398         0.0000 \n#> Anderson-Darling          0.8705         0.0255 \n#> -----------------------------------------------\n# Bruker pakken: olsrr\nols_plot_resid_fit(PallantOLS2)"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-multikolinearitet-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.7 Fravær av multikolinearitet","text":"Vi kan derfor sjekke multikolinearitet gjennom å se på en korrelasjonsmatrisen:Hvis vi følger Pallant (2010) ser vi først alle de uavhengige variablene korrelerer mellom \\(r=0.52\\) og \\(r=-0.58\\) med den avhengige. Den bivariate korrelasjonen mellom de uavhengige variablene er på \\(0.52\\). Ut fra korrelasjonsmatrisen bør det ikke være grunn til å frykte multikolinearitet.Det er ingenting som indikerer vi har multikolinearitet dataene denne modellen.","code":"\n# Base R\nPallantKorr <- cor(Pallant_survey2, method = \"pearson\")\nround(PallantKorr, 2)\n#>          tmast tpcoiss tpstress\n#> tmast     1.00    0.53    -0.61\n#> tpcoiss   0.53    1.00    -0.58\n#> tpstress -0.61   -0.58     1.00\n# Bruker pakken: olsrr\nols_vif_tol(PallantOLS2)\n#>   Variables Tolerance      VIF\n#> 1       age 0.8636921 1.157820\n#> 2   tmarlow 0.8729179 1.145583\n#> 3     tmast 0.6897936 1.449709\n#> 4   tpcoiss 0.6290247 1.589763"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-heteroskedasisitet-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.8 Fravær av heteroskedasisitet","text":"Den såkalte White’s test vil også kunne være et godt hjelpemiddel:ser vi \\(p < 0.001\\), dvs. vi kan ikke forkaste nullhypotesen = vi har ikke grunn til å konkludere med vi har heteroskedasisitet regresjonsmodellen.","code":"\n# Bruker pakken: lmtest\nbptest(PallantOLS2, ~ tmast*tpcoiss + I(tmast^2) + I(tpcoiss^2), data = Pallant_survey2)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  PallantOLS2\n#> BP = 40.384, df = 5, p-value = 0.0000001249"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-autokorrelasjon-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.9 Fravær av autokorrelasjon","text":"viser verdien \\(1.817\\) med p < 0.05 noe som indikerer vi har autokorrelasjon.","code":"\n# Bruker pakke: car\ndurbinWatsonTest(lm(tpstress ~ age + tmarlow + tmast + tpcoiss, Pallant_survey))\n#>  lag Autocorrelation D-W Statistic p-value\n#>    1      0.08634585      1.817403   0.046\n#>  Alternative hypothesis: rho != 0"},{"path":"regresjonsanalyse---ols.html","id":"fravær-av-innflytelsesrike-observasjonercaser-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.10 Fravær av innflytelsesrike observasjoner/caser","text":"Vi så punktet “Analyse av dataene” (se eksempel Boxplottet av Adverts) vi har noen observasjoner modellen som kan defineres som uteliggere. Vi kan identifisere disse gjennom residualene:Vi ser residualene ligger innenfor 95% konfidenstintervall. Det er lite ved residualplottet som vekker bekymring.Vi kan også kjøre analyser som identifiserer betydning/innvirkning (“influential cases”):DfBetas:Cook’s distance:Det er ingenting ved hat values, DfBetas eller Cook’s disgance som er bekymringsfullt.","code":"\n# Bruker pakke: car\ncar::qqPlot(PallantOLS2, id = list(n=3))#> [1]  22 194 269\n# Bruker pakke: car\ninfluenceIndexPlot(PallantOLS2, vars = \"hat\", id = list(n=3))\n# Bruker pakken: olsrr\nols_plot_dfbetas(PallantOLS2, print_plot = TRUE)\n# Bruker pakke: car\ninfluenceIndexPlot(PallantOLS2, vars = \"Cook\", id = list(n=3))\n# Base R\nmineCDverdier2 <- cooks.distance(PallantOLS2)\nmineCDverdier2 <- round(mineCDverdier2, 5)\nhead(sort(mineCDverdier2, decreasing = TRUE))\n#>      22      23     268     389     191     413 \n#> 0.05880 0.04913 0.03618 0.02859 0.02728 0.02530"},{"path":"regresjonsanalyse---ols.html","id":"oppsummert-om-forutsetningerdiagnostikk-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.5.11 Oppsummert om forutsetninger/diagnostikk","text":"","code":"\noptions(scipen=999)\n# Bruekr pakken: rnorsk\nregression.diagnostics(PallantOLS2)\n#> Tests of linear model assumptions\n#> ---------------------------------\n#> \n#> 0/12 (0.0 %) checks failed\n#> \n#> \n#> Identified problems: NONE\n#> Summary:\n#> # A tibble: 12 x 8\n#>    assumption variable test  statistic p.value  crit problem\n#>    <chr>      <chr>    <chr>     <dbl>   <dbl> <dbl> <chr>  \n#>  1 heteroske~ global   stud~   7.01     0.135   0.05 No Pro~\n#>  2 heteroske~ global   Non-~   1.86     0.173   0.05 No Pro~\n#>  3 multicoll~ age      Vari~   1.16    NA       5    No Pro~\n#>  4 multicoll~ tmarlow  Vari~   1.15    NA       5    No Pro~\n#>  5 multicoll~ tmast    Vari~   1.45    NA       5    No Pro~\n#>  6 multicoll~ tpcoiss  Vari~   1.59    NA       5    No Pro~\n#>  7 normality  global   Shap~   0.993    0.0345  0.01 No Pro~\n#>  8 model spe~ global   Stat~   0.00772  0.366   0.05 No Pro~\n#>  9 functiona~ global   RESE~   0.897    0.409   0.05 No Pro~\n#> 10 outliers   global   Cook~   0.0588  NA       1    No Pro~\n#> 11 outliers   global   Bonf~   3.54     0.190   0.05 No Pro~\n#> 12 autocorre~ global   Durb~   0.0863   0.0660  0.05 No Pro~\n#> # ... with 1 more variable: decision <chr>\n#> \n#> Outliers:\n#> -----------\n#> Cook's distance (criterion=1.00): No outliers\n#> Outlier test (criterion=0.05): No outliers"},{"path":"regresjonsanalyse---ols.html","id":"eventuell-revisjon-av-modellen-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.6 Eventuell revisjon av modellen","text":"Vi har ut fra foregående punkt der vi så på eventuelle innflytelsesrike observasjoner ikke behov å revidere modellen.","code":""},{"path":"regresjonsanalyse---ols.html","id":"eventuell-analyse-av-revidert-modell-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.7 Eventuell analyse av revidert modell","text":"Se forrige punkt.","code":""},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-1","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.5.8 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*Modell0. modell1 og modell2:","code":"\n# Bruker pakken: table1\ntable1::label(Pallant_survey3$age) <- \"age\"\ntable1::label(Pallant_survey3$tmarlow) <- \"tmarlow\"\ntable1::label(Pallant_survey3$tmast) <- \"tmast\"\ntable1::label(Pallant_survey3$tpcoiss) <- \"tpcoiss\"\ntable1::label(Pallant_survey3$tpstress) <- \"tpstress\"\ntable1::table1(~age + tmarlow + tmast + tpcoiss + tpstress, data = Pallant_survey3)\n# Bruker pakken: sjPlot\nPallantkorr <- tab_corr(Pallant_survey3, triangle = \"lower\")\nPallantkorr\n# Bruker pakken: sjPlot\ntab_model(modell0, modell1, modell2)"},{"path":"regresjonsanalyse---ols.html","id":"stegvis-mutlippel-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6 Stegvis mutlippel regresjonsanalyse","text":"Stegvis regresjon er en iterativ multippel regresjonsanalyse der prosessen gradvis (steg steg = stegvis) setter inn eller tar bort uavhengige variabler som ikke bidrar til forklaring av variansen den avhengige variabelen. Man kan f.eks. ha en situasjon der man har et stort antall uavhengige variabler der problemet er å avgjøre hvor mange og hvilke variabler som skal være med regresjonsanalysen. Det finnes ulike strategier og teknikker å velge de uavhengige variablene som til slutt skal gå inn modellen. Målet er å finne den kombinasjonen av uavhengige variabler blant et større antall variabler som best forklarer variansen den avhengige variabelen.“Stepwise selection” kan foretas på ulike måter: forward, backward og bidirectional (toveis).“Forward Selection” (også omtalt som «Step-») innebærer altså man starter med null variabler. Først settes den variabelen med lavest p-verdi F. hvert steg deretter settes den variabelen som ennå ikke er lagt inn modellen med lavest p-verdi F inn. Når ingen av de gjenværende variablene er signifikante stoppes prosessen. Denne prosedyren kan ha sin nytte en analyse av et stort antall «kandidatvariabler» til en regresjonsmodell. Det som gjøres er:Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Finn den uavhengige variabelen som alene forklarer mest av variansen. Legg denne inn modellen dersom p-verdien er terskelverdien (f.eks. 0.05).Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Sjekk p-verdiene til alle variablene modellen. Dersom noen av variablene modellen har p-verdi terskelverdien (f.eks. 0.10) skal variabelen tas ut av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.Gjenta steg 1 og 2 inntil alle signifikante uavhengige variabler er inkludert modellen og alle ikke-signifikante verdier er ute av modellen.“Backward selection” (også omtalt som “Step-” eller “Backward Elimination”) starter motsatt ende av forward. puttes alle de uavhengige variablene inn først. Deretter tar man hver gang bort variabelen som har høyest p-verdi F inntil man ikke har flere ikke-signifikante variabler igjen modellen.\n“Bidirectional” utvelgelse er en kombinasjon av de foregående. Den starter som forward selection, men hver gang en variabel settes inn blir alle variablene modellen kontrollert å se om p-verdien F har endret seg til en definert terskelverdi som følge av den nye variabelen. Hvis en ikke-signifikant variabel da blir funnet fjernes den. Når ingen variabler tilfredsstiller terskelverdiene enten inkludering eller ekskludering modellen stopper prosessen. Dette krever definerte signifikansnivåer: En å inkludere og en å ekskludere, der terskelverdien å inkludere må være lavere enn å ekskludere (med mindre man ønsker en uendelig loop der man aldri finner en løsning).Det skal bemerkes dette er en rent matematisk/statistisk operasjon. Det ligger ingen teoretiske vurderinger til grunn. Man har selvsagt ingen garanti de uavhengige variablene man ender opp med er fornuftige eller har noen praktisk signifikans. Ethvert resultat fra en stegvis regresjon må derfor vurderes kritisk. Man står også fare såvel overfitting som underfitting (jfr. tidligere punkt om overfitting) (Field 2009). Det mangler ikke på advarsler om å bruke stegvis regresjon, f.eks. fra Miles Shevlin (2001) som advarer om stegvis regresjonsanalyse “used extreme caution” (s.38). Singer Willett (2003) hevder på sin side “Never let computer select predictors mechanically. computer know research questions literature upon rest. distinguish predictors direct substantive interest whose effects want control”. Man skal hvert fall se på resultatene med et veldig kritisk blikk siden “data analyst knows computer” (Henderson Velleman 1981, s.391).Et alvorlig problem med stegvis regresjonsanalyse er man får en \\(R^2\\)-verdi som har svært unøyaktig høy (Miles Shevlin 2001).Dette skyldes statistikkprogrammet tråler gjennom et stort antall uavhengige variabler på søken etter statistisk signifikante variabler og velger ut alle disse. En andel variabler vil være signifikante av tilfeldighet noe som øker verdien på \\(R^2\\).Forutsetningene er de samme som standard multippel regresjon. Spesielt skal man være observant på uteliggere. En tommelfingerregel som angis er minimum 5 caser per variabel (50 variabler = minimum 250 caser).Vi skal se på et eksempel som er hentet fra  (van den Berg 2018).\nDu kan laste ned datasettet ulike formater :Download magazine_reg.xlsxDownload magazine_reg.savDownload magazine_reg.dta","code":""},{"path":"regresjonsanalyse---ols.html","id":"eksempel-stegvis-multippel-regresjonsanalyse","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.1 Eksempel stegvis multippel regresjonsanalyse","text":"Vi skal gå gjennom et eksempel på stegvis multippel regresjonsanalyse, ved å følge stegene 1-4 .Analyse av dataeneAnalyse av dataeneLage modell (kjøre regresjonsanalysen)Lage modell (kjøre regresjonsanalysen)Analyse av resultatene (diagnostikk)Analyse av resultatene (diagnostikk)","code":""},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-dataene-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.2 Analyse av dataene","text":"","code":"\n# Base R\nmagazine_data <- as.data.frame(read_excel(\"magazine_reg.xlsx\"))\n# Bruker pakken: summarytools\nsummarytools::descr(magazine_data, stats = \"common\")\n#> Non-numerical variable(s) ignored: age\n#> Descriptive Statistics  \n#> magazine_data  \n#> N: 637  \n#> \n#>                     educ    filt1   gender     intnr     mis1     prof     sat1     sat2     sat3\n#> --------------- -------- -------- -------- --------- -------- -------- -------- -------- --------\n#>            Mean     5.11     0.99     1.13   1267.38     0.31     2.31     4.11     4.37     3.77\n#>         Std.Dev     1.13     0.10     0.34    712.79     0.76     0.72     0.88     0.75     0.95\n#>             Min     1.00     0.00     1.00     46.00     0.00     1.00     1.00     2.00     1.00\n#>          Median     5.00     1.00     1.00   1259.00     0.00     2.00     4.00     4.00     4.00\n#>             Max     6.00     1.00     2.00   2497.00     6.00     5.00     6.00     6.00     6.00\n#>         N.Valid   637.00   637.00   637.00    637.00   637.00   637.00   637.00   637.00   637.00\n#>       Pct.Valid   100.00   100.00   100.00    100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                     sat4     sat5     sat6     sat7     sat8     sat9    satov   whours\n#> --------------- -------- -------- -------- -------- -------- -------- -------- --------\n#>            Mean     4.00     4.67     4.41     4.41     3.91     3.97     7.49     4.33\n#>         Std.Dev     1.06     0.59     0.91     0.85     0.92     0.99     0.77     0.65\n#>             Min     1.00     2.00     1.00     1.00     2.00     1.00     5.00     1.00\n#>          Median     4.00     5.00     5.00     5.00     4.00     4.00     8.00     4.00\n#>             Max     6.00     6.00     6.00     5.00     6.00     6.00    10.00     5.00\n#>         N.Valid   637.00   637.00   637.00   637.00   637.00   637.00   637.00   477.00\n#>       Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00    74.88\n# Base R\npar(mfrow=(c(2,3)))\nhisttsat1 <- with(magazine_data, hist(sat1))\nhisttsat2 <- with(magazine_data, hist(sat2))\nhisttsat3 <- with(magazine_data, hist(sat3))\nhisttsat4 <- with(magazine_data, hist(sat4))\nhisttsat5 <- with(magazine_data, hist(sat5))\nhisttsat6 <- with(magazine_data, hist(sat6))\nhisttsat7 <- with(magazine_data, hist(sat7))\nhisttsat8 <- with(magazine_data, hist(sat8))\nhisttsat9 <- with(magazine_data, hist(sat9))"},{"path":"regresjonsanalyse---ols.html","id":"lage-modell-kjøre-regresjonsanalysen-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.3 Lage modell (kjøre regresjonsanalysen)","text":"Hvilke av de 9 uavhengige variablene (sat1…sat9), som er ulike mål på kunders oppfatning av produktet, bidrar signifikant til å forklare variansen hvordan kundene totalt sett skårer produktet (magasinet) («rate magazine altogether?»). De ulike målene er f.eks. grundighet, objektivitet, lesbarhet og pålitelighet. Spørsmålet er hvilke aspekter (sat1…sat9) har størst innflytelse på kundetilfredsheten?","code":"\n# Base R\nmagazine_intercept <- lm(satov ~ 1, data = magazine_data)\nmagazine_all <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9, data = magazine_data)\nmagazine_forward <- step(magazine_intercept, direction = \"forward\", scope = formula(magazine_all), trace = 0)"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---forward","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.4 Analyse av resultatene (diagnostikk) - forward","text":"Første modell er kun intercept. Vi ser på AIC verdiene tabellen . AIC er forkortelse Akaike Information Criterion som er en estimator prediksjonsfeil og brukes som et mål på relativ kvalitet statistiske modeller (hvor godt modellen passer til dataene modellen ble laget av - derfor er AIC egnet til å sammenlikne ulike mulige modeller fra det samme datasettet).skal vi legge merke til fortegnet til AIC verdien er irrelevant - vi ser på absoluttverdien, der lavere AIC verdi er bedre enn høyere AIC verdi.\nNeste steg er alle mulige modeller med en prediktor testes, og prediktoren som produserer den laveste AIC verdien inkluderes modellen. Det er vårt tilfelle sat1. Neste steg er å teste alle modeller/kombinasjoner med prediktorer, hvor den ene er sat1. Den neste prediktoren som inkluderes er da sat3 fordi det er prediktoren som gir lavest AIC av alle mulige kombinasjoner av modeller med prediktorer. Neste steg innebærer å teste alle modeller/kombinasjoner med tre prediktorer, der den første er sat1 og den andre er sat3. Da blir sat5 lagt til, osv.Modellen blir med denne prosedyren:R-brukere kan vi kjøre hele analysen en linje med pakken olsrr:","code":"\n# Base R\nmagazine_forward$anova\n#>     Step Df   Deviance Resid. Df Resid. Dev       AIC\n#> 1        NA         NA       636   381.2308 -325.0134\n#> 2 + sat1 -1 71.7435240       635   309.4872 -455.8202\n#> 3 + sat3 -1 33.8817998       634   275.6054 -527.6782\n#> 4 + sat5 -1 20.9219246       633   254.6835 -575.9685\n#> 5 + sat7 -1  9.7484194       632   244.9351 -598.8295\n#> 6 + sat9 -1  6.5282509       631   238.4069 -614.0379\n#> 7 + sat2 -1  2.2298960       630   236.1770 -618.0240\n#> 8 + sat6 -1  0.8045547       629   235.3724 -618.1977\n#> 9 + sat4 -1  1.0320596       628   234.3403 -618.9969\n# Base R\nmagazine_forward$coefficients\n#> (Intercept)        sat1        sat3        sat5        sat7 \n#>  3.78591494  0.17370968  0.17786494  0.19804309  0.14463696 \n#>        sat9        sat2        sat6        sat4 \n#>  0.10736930  0.08912079 -0.05363069  0.04538282\n# Bruker pakken: olsrr\nols_step_forward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Forward Selection Method \n#> ------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered: \n#> \n#> - sat1 \n#> - sat3 \n#> - sat5 \n#> - sat7 \n#> - sat9 \n#> - sat2 \n#> - sat6 \n#> - sat4 \n#> \n#> No more variables to be added.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#>                          Selection Summary                          \n#> -------------------------------------------------------------------\n#> Variable       AIC       Sum Sq       RSS       R-Sq      Adj. R-Sq \n#> -------------------------------------------------------------------\n#> sat1         1353.907     71.744    309.487    0.18819      0.18691 \n#> sat3         1282.050    105.625    275.605    0.27706      0.27478 \n#> sat5         1233.759    126.547    254.684    0.33194      0.32878 \n#> sat7         1210.898    136.296    244.935    0.35751      0.35345 \n#> sat9         1195.690    142.824    238.407    0.37464      0.36968 \n#> sat2         1191.704    145.054    236.177    0.38049      0.37459 \n#> sat6         1191.530    145.858    235.372    0.38260      0.37573 \n#> sat4         1190.731    146.890    234.340    0.38531      0.37748 \n#> -------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---backward","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.5 Analyse av resultatene (diagnostikk) - backward","text":"","code":"\n# Bruker pakken: olsrr\nols_step_backward_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Backward Elimination Method \n#> ---------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Removed: \n#> \n#> - sat8 \n#> \n#> No more variables to be removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                      Backward Elimination Summary                     \n#> --------------------------------------------------------------------\n#> Variable        AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> --------------------------------------------------------------------\n#> Full Model    1191.783    233.992    147.239    0.38622      0.37741 \n#> sat8          1190.731    234.340    146.890    0.38531      0.37748 \n#> --------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"analyse-av-resultatene-diagnostikk---bidirectional","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.6 Analyse av resultatene (diagnostikk) - bidirectional","text":"Vi kan se modellene gir likt resultat alle tre måtene, men det trenger absolutt ikke hende. Vi kan få ulike modeller ved de tre metodene. Husk inkludering/eksludering av variabler skjer rent matematisk.","code":"\n# Bruker pakken: olsrr\nols_step_both_aic(magazine_all, progress = TRUE, details = FALSE)\n#> Stepwise Selection Method \n#> -------------------------\n#> \n#> Candidate Terms: \n#> \n#> 1 . sat1 \n#> 2 . sat2 \n#> 3 . sat3 \n#> 4 . sat4 \n#> 5 . sat5 \n#> 6 . sat6 \n#> 7 . sat7 \n#> 8 . sat8 \n#> 9 . sat9 \n#> \n#> \n#> Variables Entered/Removed: \n#> \n#> - sat1 added \n#> - sat3 added \n#> - sat5 added \n#> - sat7 added \n#> - sat9 added \n#> - sat2 added \n#> - sat6 added \n#> - sat4 added \n#> \n#> No more variables to be added or removed.\n#> \n#> Final Model Output \n#> ------------------\n#> \n#>                         Model Summary                         \n#> -------------------------------------------------------------\n#> R                       0.621       RMSE               0.611 \n#> R-Squared               0.385       Coef. Var          8.151 \n#> Adj. R-Squared          0.377       MSE                0.373 \n#> Pred R-Squared          0.367       MAE                0.477 \n#> -------------------------------------------------------------\n#>  RMSE: Root Mean Square Error \n#>  MSE: Mean Square Error \n#>  MAE: Mean Absolute Error \n#> \n#>                                ANOVA                                 \n#> --------------------------------------------------------------------\n#>                Sum of                                               \n#>               Squares         DF    Mean Square      F         Sig. \n#> --------------------------------------------------------------------\n#> Regression    146.890          8         18.361    49.206    0.0000 \n#> Residual      234.340        628          0.373                     \n#> Total         381.231        636                                    \n#> --------------------------------------------------------------------\n#> \n#>                                   Parameter Estimates                                   \n#> ---------------------------------------------------------------------------------------\n#>       model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n#> ---------------------------------------------------------------------------------------\n#> (Intercept)     3.786         0.229                 16.546    0.000     3.337    4.235 \n#>        sat1     0.174         0.032        0.197     5.439    0.000     0.111    0.236 \n#>        sat3     0.178         0.030        0.218     5.865    0.000     0.118    0.237 \n#>        sat5     0.198         0.046        0.151     4.317    0.000     0.108    0.288 \n#>        sat7     0.145         0.032        0.159     4.479    0.000     0.081    0.208 \n#>        sat9     0.107         0.029        0.138     3.665    0.000     0.050    0.165 \n#>        sat2     0.089         0.042        0.086     2.142    0.033     0.007    0.171 \n#>        sat6    -0.054         0.031       -0.063    -1.725    0.085    -0.115    0.007 \n#>        sat4     0.045         0.027        0.062     1.663    0.097    -0.008    0.099 \n#> ---------------------------------------------------------------------------------------\n#> \n#> \n#>                                 Stepwise Summary                                \n#> ------------------------------------------------------------------------------\n#> Variable     Method       AIC         RSS      Sum Sq      R-Sq      Adj. R-Sq \n#> ------------------------------------------------------------------------------\n#> sat1        addition    1353.907    309.487     71.744    0.18819      0.18691 \n#> sat3        addition    1282.050    275.605    105.625    0.27706      0.27478 \n#> sat5        addition    1233.759    254.684    126.547    0.33194      0.32878 \n#> sat7        addition    1210.898    244.935    136.296    0.35751      0.35345 \n#> sat9        addition    1195.690    238.407    142.824    0.37464      0.36968 \n#> sat2        addition    1191.704    236.177    145.054    0.38049      0.37459 \n#> sat6        addition    1191.530    235.372    145.858    0.38260      0.37573 \n#> sat4        addition    1190.731    234.340    146.890    0.38531      0.37748 \n#> ------------------------------------------------------------------------------"},{"path":"regresjonsanalyse---ols.html","id":"hvor-god-er-modellen-vår-goodness-of-fit-3","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.6.1 Hvor god er modellen vår (goodness of fit)?","text":"Vi kan se F-verdien er 49.206.F-verdien er dermed langt kritisk verdi (p < 0.001).Det ser derfor ut til det er svært usannsynlig forbedringen fra referansemodellen (gjennomsnittet) til vår modell er tilfeldig. Vi kan også si dersom forbedringen ved regresjonsmodellen er større enn unøyaktigheten modellen så vil F > 1.","code":"\n# Base R\nqf(p=.05, df1=8, df2=628, lower.tail=FALSE)\n#> [1] 1.953131"},{"path":"regresjonsanalyse---ols.html","id":"konklusjon-oppsummering-rapportering-av-resultater-2","chapter":"Kapittel 1 Regresjonsanalyse - OLS","heading":"1.6.7 Konklusjon / oppsummering / rapportering av resultater","text":"Tabell 1: Deskriptiv statistikkp < .0001**** , p < .001*** , p < .01**, p < .05*","code":"\n# Bruker pakken: table1\ntable1::label(magazine_data$sat1) <- \"sat1\"\ntable1::label(magazine_data$sat2) <- \"sat2\"\ntable1::label(magazine_data$sat3) <- \"sat3\"\ntable1::label(magazine_data$sat4) <- \"sat4\"\ntable1::label(magazine_data$sat5) <- \"sat5\"\ntable1::label(magazine_data$sat6) <- \"sat6\"\ntable1::label(magazine_data$sat7) <- \"sat7\"\ntable1::label(magazine_data$sat8) <- \"sat8\"\ntable1::label(magazine_data$sat9) <- \"sat9\"\ntable1::label(magazine_data$satov) <- \"satov\"\ntable1::table1(~sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat8 + sat9 + satov, data = magazine_data)\n# Bruker pakken: sjPlot\nmagazine_data2 <- magazine_data[, c(\"sat1\", \"sat2\", \"sat3\", \"sat4\", \"sat5\", \"sat6\", \"sat7\", \"sat9\", \"satov\")]\nmagazinekorr <- tab_corr(magazine_data2, triangle = \"lower\")\nmagazinekorr\n# Bruker pakken: sjPlot\nmagazine_finalmod <- lm(satov ~ sat1 + sat2 + sat3 + sat4 + sat5 + sat6 + sat7 + sat9, data = magazine_data)\ntab_model(magazine_finalmod)"},{"path":"iv-regresjon.html","id":"iv-regresjon","chapter":"Kapittel 2 IV regresjon","heading":"Kapittel 2 IV regresjon","text":"","code":""},{"path":"iv-regresjon.html","id":"innledning-1","chapter":"Kapittel 2 IV regresjon","heading":"2.1 Innledning","text":"Multippel OLS-regresjon av tverrsnittsdata kan kalles en tradisjonell empirisk metode å se på sammenhengen mellom en uavhengig variabel og en eller flere avhengige variabler. regresjonen finner vi den gjennomsnittlige endringen en uavhengig variabel - Y - når en avhengig variabel X endres med en enhet, og vi samtidig holder alle andre variabler konstante. Dette (koeffisienten X) er en betinget korrelasjon mellom X og Y, men det er alltid en usikkerhet rundt hvilken grad dette er et godt estimat den kausale sammenhengen. Vi vil sannsynligvis kunne si vi ikke vet om vi har utelatt variabler som påvirker både X og Y. Det kan også være sånn ikke bare påvirker X -> Y, en kanskje også Y -> X. En mulighet “forkludring” av estimatet kausalitet kan også være vi ikke helt forstår kontrollvariablers påvirkning på X, slik deler av X’s effekt på Y forsvinner. Selv om kontrollvariabler kan klargjøre X’s effekt på Y og dermed variasjonen Y kan det tåkelegge den kausale effekten mellom variablene. Vi kjenner til fra kapittelet om OLS-regresjon det alltid er et feilledd regresjonslikningen. Feil, som beskrevet ovenfor, kan føre til en uavhenigig variabel korrelerer med feilleddet og dermed påvirker kausaleffekten. IV-regresjon (Instrumentell Variabel regresjon) kan ses på som et kvasi-eksperiment (se f.eks. Galiani et al. (2017), Andriano Monden (2019) og DiPrete, Burik, Koellinger (2018)).En definisjon av IV er “use additional ‘instrumental’ variables, contained equation interest, estimate unknown parameters equation” (Stock Trebbi 2003, s.179). La oss tenke oss vi har en avhengig variabel Y - inntekt - som påvirkes av sosioøkonomisk status - X. Sosioøkonomisk status påvirker altså inntekten (X -> Y), men samtidig kan inntekt påvirke sosioøkonomisk status (Y -> X). En såkalt instrumentvariabel - Z - som må korrelere sterkt med X, men ikke med Y på andre måter enn gjennom X kan gi oss en omveg rundt problemet med retning på kausaliteten. Å finne en god instrumentvariabel er derimot krevende og vil trolig forutsette svært god kunnskap om tematikken som undersøkes.Oppsummert blir da en IV-regresjon man først gjennomfører en regresjon der X-verdiene predikeres av Z. Deretter gjennomfører man en regresjon med de predikerte X-verdiene mot Y. IV regresjon kalles derfor også ofte “two-stage least squares”.Det hviler forutsetninger dette: Z må være korrelert med X (det såkalte relevanskriteriet) og Z kun påvirker Y gjennom X - altså ingen direkte påvirkning fra Z til Y (det såkalte eksluderingskriteriet).","code":""},{"path":"iv-regresjon.html","id":"et-konstruert-eksempel-for-konseptualisering-av-iv-regresjon","chapter":"Kapittel 2 IV regresjon","heading":"2.2 Et konstruert eksempel for konseptualisering av IV-regresjon","text":"La oss tenke oss vi ønsker å undersøke sammenhengen mellom lengde på utdanning og inntekt. Er det slik et år ekstra utdanning gir deg x mer inntekt, eller x+…? mer inntekt? Dette eksempelet er basert på Masten (2015).La oss videre tenke oss vi har data:Vi ser det er en klar trend disse dataene folk med lengre utdanning har høyere inntekt enn folk med kortere utdanning. Betyr det lengde på utdanning har en kausal effekt på inntekt? Svaret er nei. Fordi lengde på utdanning er ikke tilfeldig (“randomly assigned”) - folk velger hvor lang utdannelse de vil ta. Likevel ser vi jo en klar trend/sammenheng dataene. Så hvorfor gjør vi det?En forklaring kan være denne sammenhengen skyldes en uobservert variabel som “forstyrrer” bildet. La oss videre eksempelet si dette er IQ, og vi har data på dette.Lengde på utdanning ser ut til å henge sammen med IQ.Men også inntekt ser ut til å henge sammen med IQ, uavhengig av hvor mye utdannelse de har. virkeligheten vil vi kanskje ikke se dette - vi har data på lengde på utdannelse og inntekt (vi har generert dataene nettopp å illustrere). Men virkeligheten kan korrelasjonen vi kanskje faktisk ser og som vi viste første diagrammet - korrelasjonen mellom lengde på utdanning og inntekt - altså være drevet av en underliggende, uobservert variabel og IKKE av en kausal sammenheng mellom lengde på utdanning og inntekt.å mitigere dette problemet kommer instrumentet inn bildet. eksempelet fra Masten (2015) vises det til avstand fra hjemadressen til nærmeste studiested (heretter: avstand) kan være et slikt instrument. Vi bygger videre på samme måte.Et instrument må altså kunne påvirke Y gjennom X (og ikke direkte), og være korrelert med X. vårt eksempel betyr det avstand må påvirke inntekt gjennom lengde på utdannelse, og avstand må være korrelert med lengde på utdannelse.Vi må derfor kunne se på X, Y og Z slik:Lav avstand har sammenheng med lengre utdannelse. Forhold som trekkes fram denne sammenhengen kan være større kjennskap til universitetet, lavere kostnad bolig og reise, større påvirkning av eldre studenter nærmiljøet gjennom oppveksten (og sikkert flere) - som sum antyder det kan være en kausal sammenheng mellom avstand og om folk studerer og hvor lenge. Altså en korrelasjon mellom Z og X (som vi sa var en forutsetning).Sammenhengen mellom IQ og avstand er imidlertid tilfeldig. Vi antar det ikke er noen grunn til IQ har sammenheng med avstand. Folk er født “og der” med ulik IQ. Vi ser bort fra det kanskje faktisk kan være en sammenheng… ved universitets ansatte bor nærheten av iuniversitete, og om IQ er arvelig kan man kanskje tenke seg det bor flere unge med høyere IQ nærheten av nuviersitetet enn lenger unna - vi går ikke inn denne “problemstillingen” , men legger til grunn dataene våre viser det ikke er slik. Om det faktisk er sånn det er en sammenheng vil det gjøre det vanskelig å bruke avstand som et instrument.Den siste forutsetningen er avstand ikke kan ha en direkte kausal effekt på inntekt. Dette virker rimelig, da det er vanskelig å se arbeidsgivere har noe forhold til hvor langt unna et unviersitet folk har vokst opp, og avstand dermed skulle kunne ha en kausal effekt på inntekt.Vi kan si instrumentet “avstand” tilfredsstiller forutsetningene, og vi kan se på korrelasjonen mellom avstand og inntekt.Vi ser avstand og inntekt korrelerer. Vi kan dermed si korrelasjonen mellom avstand og inntekt representerer en kausal effekt av lengde på utdanning og inntekt. Vi ser en kausal effekt mellom X og Y gjennom korrelasjonen mellom Z og Y.","code":"#> Descriptive Statistics  \n#> IVregrdata1  \n#> Label: jamovi data set  \n#> N: 161  \n#> \n#>                      Inntekt   Utdanning\n#> --------------- ------------ -----------\n#>            Mean    795578.74       15.20\n#>         Std.Dev    153427.64        1.52\n#>             Min    501846.67       12.02\n#>          Median    801972.01       15.24\n#>             Max   1049757.30       17.89\n#>         N.Valid       161.00      161.00\n#>       Pct.Valid       100.00      100.00#> Descriptive Statistics  \n#> IVregrdata2  \n#> N: 161  \n#> \n#>                       IQ   Utdanning\n#> --------------- -------- -----------\n#>            Mean   119.27       15.20\n#>         Std.Dev    11.25        1.52\n#>             Min    90.01       12.02\n#>          Median   122.55       15.24\n#>             Max   134.95       17.89\n#>         N.Valid   161.00      161.00\n#>       Pct.Valid   100.00      100.00"},{"path":"iv-regresjon.html","id":"eksempel-1-med-data-fra-pakken-aer","chapter":"Kapittel 2 IV regresjon","heading":"2.3 Eksempel 1 med data fra pakken “AER”","text":"datasettet har vi en rekke variabler:Vi skal se på sammenhengen mellom “packs” (= y -> antall sigarettpakker per capita)som avhengig variabel, “price” (= x -> gjennomsnittlig pris pr år inkl skatter og avgifter) som uavhengig variabel, og “taxs” (= z -> gjennomsnittlig skatt pr år) som instrumentvariabel.Forutsetningene er altså:z og x må være korrelerte: pris og skatte-/avgiftsnivå korrelerer virker rimeligz må ikke påvirke y direkte: skatte-/avgiftsnivået ikke seg selv påvirker antall sigarettpakker som selges direkte virker rimelig, men skatte-/avgiftsnivået påvirker antall sigarettpakker som selges gjennom prisnivåt virker også rimelig (dette er en forutsetning som alltid kan diskuteres, men eksempelets skyld antar vi dette).","code":"\ndata(\"CigarettesSW\")\n\n# Bruker pakken: summarytools\nsummarytools::descr(CigarettesSW, stats = \"common\")\n#> Non-numerical variable(s) ignored: state, year\n#> Descriptive Statistics  \n#> CigarettesSW  \n#> N: 96  \n#> \n#>                      cpi         income    packs    population    price      tax     taxs\n#> --------------- -------- -------------- -------- ------------- -------- -------- --------\n#>            Mean     1.30    99878735.74   109.18    5168866.32   143.45    42.68    48.33\n#>         Std.Dev     0.23   120541138.18    25.87    5442344.66    43.89    16.14    19.33\n#>             Min     1.08     6887097.00    49.27     478447.00    84.97    18.00    21.27\n#>          Median     1.30    61661644.00   110.16    3697471.50   137.72    37.00    41.05\n#>             Max     1.52   771470144.00   197.99   31493524.00   240.85    99.00   112.63\n#>         N.Valid    96.00          96.00    96.00         96.00    96.00    96.00    96.00\n#>       Pct.Valid   100.00         100.00   100.00        100.00   100.00   100.00   100.00\ncor(CigarettesSW$price, CigarettesSW$taxs)\n#> [1] 0.9203278"},{"path":"iv-regresjon.html","id":"modell-1","chapter":"Kapittel 2 IV regresjon","heading":"2.3.1 Modell 1","text":"Vi ser på koeffisientene log(price) er signifikant, og pris påvirker salget (antall sigarettpakker) negativt (negativ koeffisient).","code":"\nmodel1 <- ivreg(log(packs) ~ log(price) | taxs, data = CigarettesSW)\nsummary(model1)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) | taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.597468 -0.105477 -0.008609  0.101021  0.525352 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value\n#> (Intercept)  7.66548    0.34514  22.210\n#> log(price)  -0.60993    0.07004  -8.708\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)     0.000000000000103 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1845 on 94 degrees of freedom\n#> Multiple R-Squared: 0.4324,  Adjusted R-squared: 0.4264 \n#> Wald test: 75.83 on 1 and 94 DF,  p-value: 0.0000000000001025"},{"path":"iv-regresjon.html","id":"modell-2","chapter":"Kapittel 2 IV regresjon","heading":"2.3.2 Modell 2","text":"legger vi inn en eksogen variabel - inntekt - altså en variabel hvis verdi er bestemt utenfor modellen så å legges inn modellen. Pris ser vi på som en endogen variabel, altså en variabel hvis verdi bestemmes modellen. En tilfeldig endogen variabel en modell korrelerer med feilleddet (som vi forsåvidt har beskrevet begynnelsen av dette kapittelet), mens en eksogen variabel ikke er korrelert med feilleddet (hvilket er naturlig siden vi sier verdien er bestemt utenfor modellen).Vi kan legge merke til log(income) ikke er signifikant.","code":"\nmodel2 <- ivreg(log(packs) ~ log(price) + log(income) | log(income) + taxs, data = CigarettesSW)\nsummary(model2)\n#> \n#> Call:\n#> ivreg(formula = log(packs) ~ log(price) + log(income) | log(income) + \n#>     taxs, data = CigarettesSW)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.599938 -0.104454 -0.007223  0.101973  0.525855 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value\n#> (Intercept)  7.698702   0.399560  19.268\n#> log(price)  -0.605735   0.075878  -7.983\n#> log(income) -0.003015   0.018990  -0.159\n#>                         Pr(>|t|)    \n#> (Intercept) < 0.0000000000000002 ***\n#> log(price)      0.00000000000368 ***\n#> log(income)                0.874    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1853 on 93 degrees of freedom\n#> Multiple R-Squared: 0.4335,  Adjusted R-squared: 0.4214 \n#> Wald test: 37.67 on 2 and 93 DF,  p-value: 0.000000000001038"},{"path":"iv-regresjon.html","id":"eksempel-2","chapter":"Kapittel 2 IV regresjon","heading":"2.4 Eksempel 2","text":"Data dette eksempelet er hentet fra Becker Lincoln (2021) (“Mroz.csv”).","code":"\nMrozdata <- read.csv(file = 'Mroz.csv', na.strings = \".\")\nMrozdata <- subset(Mrozdata, is.na(wage) == FALSE) \nhead(Mrozdata)\n#>   inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs\n#> 1    1  1610       1       0  32   12 3.3540    2.65   2708\n#> 2    1  1656       0       2  30   12 1.3889    2.65   2310\n#> 3    1  1980       1       3  35   12 4.5455    4.04   3072\n#> 4    1   456       0       3  34   12 1.0965    3.25   1920\n#> 5    1  1568       1       2  31   14 4.5918    3.60   2000\n#> 6    1  2032       0       0  54   12 4.7421    4.70   1040\n#>   husage huseduc huswage faminc    mtr motheduc fatheduc\n#> 1     34      12  4.0288  16310 0.7215       12        7\n#> 2     30       9  8.4416  21800 0.6615        7        7\n#> 3     40      12  3.5807  21040 0.6915       12        7\n#> 4     53      10  3.5417   7300 0.7815        7        7\n#> 5     32      12 10.0000  27300 0.6215       12       14\n#> 6     57      11  6.7106  19495 0.6915       14        7\n#>   unem city exper  nwifeinc     lwage expersq\n#> 1  5.0    0    14 10.910060 1.2101540     196\n#> 2 11.0    1     5 19.499980 0.3285121      25\n#> 3  5.0    0    15 12.039910 1.5141380     225\n#> 4  5.0    0     6  6.799996 0.0921233      36\n#> 5  9.5    1     7 20.100060 1.5242720      49\n#> 6  7.5    1    33  9.859054 1.5564800    1089"},{"path":"iv-regresjon.html","id":"modell-1-standard-ols","chapter":"Kapittel 2 IV regresjon","heading":"2.4.1 Modell 1: Standard OLS","text":"Vi ønsker å kjøre en regresjon med kvinners lønn (lwage) som avhengig variabel og utdanning (educ) som uavhengig variabel. Eksempelet tar utgangspunkt Wooldridge (2016) (se kap. 15).Vi ser vi får et statistisk signifikant resultat og et års lengre utdanning øker lønn med nesten 11 % (Esitmate = 0.1086). Vi kan imidlertid forvente utdanning korrelerer med mange individuelle karakteristika hos et individ som er viktige den personens lønn, men som ikke fanges opp av utdanningsvariabelen og derfor ikke fanges opp denne modellen. dette tilfellet, og å følge eksempelet Wooldridge (2016) velger vi fars utdanning (fatheduc) som instrument (vi diskuterer ikke instrumentet og forutsetningene ).","code":"\nIVregmod1 <- lm(lwage ~ educ, data = Mrozdata)\nsummary(IVregmod1)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.10256 -0.31473  0.06434  0.40081  2.10029 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept)  -0.1852     0.1852  -1.000             0.318\n#> educ          0.1086     0.0144   7.545 0.000000000000276\n#>                \n#> (Intercept)    \n#> educ        ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.68 on 426 degrees of freedom\n#> Multiple R-squared:  0.1179, Adjusted R-squared:  0.1158 \n#> F-statistic: 56.93 on 1 and 426 DF,  p-value: 0.0000000000002761"},{"path":"iv-regresjon.html","id":"modell-2-iv-regresjon","chapter":"Kapittel 2 IV regresjon","heading":"2.4.2 Modell 2: IV regresjon","text":"Vi kan se “verdien” av et års ekstra utdanning nå kun er 5.9% økning, og signifikant p < 0.1.","code":"\nIVregmod2 <- ivreg(lwage ~ educ | fatheduc, data = Mrozdata)\nprint(summary(IVregmod2))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ | fatheduc, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0870 -0.3393  0.0525  0.4042  2.0677 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.44110    0.44610   0.989   0.3233  \n#> educ         0.05917    0.03514   1.684   0.0929 .\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6894 on 426 degrees of freedom\n#> Multiple R-Squared: 0.09344, Adjusted R-squared: 0.09131 \n#> Wald test: 2.835 on 1 and 426 DF,  p-value: 0.09294"},{"path":"iv-regresjon.html","id":"modell-3-ols-med-flere-prediktorer","chapter":"Kapittel 2 IV regresjon","heading":"2.4.3 Modell 3: OLS med flere prediktorer","text":"Estiemrt koeffisient educ er nå 0.1075 med standardfeil 0.0141","code":"\nIVregmod3 <- lm(lwage ~ educ + age + exper + expersq, data = Mrozdata)\nsummary(IVregmod3)\n#> \n#> Call:\n#> lm(formula = lwage ~ educ + age + exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.08521 -0.30587  0.04946  0.37523  2.37077 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value          Pr(>|t|)\n#> (Intercept) -0.5333762  0.2778430  -1.920           0.05557\n#> educ         0.1075228  0.0141745   7.586 0.000000000000212\n#> age          0.0002836  0.0048553   0.058           0.95344\n#> exper        0.0415623  0.0131909   3.151           0.00174\n#> expersq     -0.0008152  0.0003996  -2.040           0.04195\n#>                \n#> (Intercept) .  \n#> educ        ***\n#> age            \n#> exper       ** \n#> expersq     *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6672 on 423 degrees of freedom\n#> Multiple R-squared:  0.1568, Adjusted R-squared:  0.1489 \n#> F-statistic: 19.67 on 4 and 423 DF,  p-value: 0.000000000000007328"},{"path":"iv-regresjon.html","id":"modell-4-iv-regresjon-med-flere-to-instrumenter","chapter":"Kapittel 2 IV regresjon","heading":"2.4.4 Modell 4: IV regresjon med flere (to) instrumenter","text":"Vi legger til både fars og mors utdannelse som instrumenter:fatheduc og motheduc er instrumentene, exper og expersq er eksogene uavhengige variabler.Også ser vi en vesentlig reduksjon estimatet educ fra OLS til IV regresjon; fra 0.1075 med standardfeil 0.0141 til 0.0613 med standardfeil 0.0314. IV estiamtene vil ha større standardfeil enn OLS estimater.","code":"\nIVregmod4 <- ivreg(lwage ~ educ  +exper + expersq | fatheduc + motheduc + exper + expersq, data = Mrozdata)\nprint(summary(IVregmod4))\n#> \n#> Call:\n#> ivreg(formula = lwage ~ educ + exper + expersq | fatheduc + motheduc + \n#>     exper + expersq, data = Mrozdata)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0986 -0.3196  0.0551  0.3689  2.3493 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)  0.0481003  0.4003281   0.120  0.90442   \n#> educ         0.0613966  0.0314367   1.953  0.05147 . \n#> exper        0.0441704  0.0134325   3.288  0.00109 **\n#> expersq     -0.0008990  0.0004017  -2.238  0.02574 * \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6747 on 424 degrees of freedom\n#> Multiple R-Squared: 0.1357,  Adjusted R-squared: 0.1296 \n#> Wald test: 8.141 on 3 and 424 DF,  p-value: 0.00002787"},{"path":"iv-regresjon.html","id":"test-av-relevansen-av-instrumentene","chapter":"Kapittel 2 IV regresjon","heading":"2.4.5 Test av relevansen av instrumentene","text":"Det er selvsagt avgjørende instrumentet/-ene er relevante. Som beskrevet må instrumentene være tilstrekkelig sterkt korrelerte med den/de endogene uavhengige variablene. dette eksempelet (modell 4) må altså fatheduc og motheduc forklare en tilstrekkelig del av variasjonen educ å kunne være relevant instrumenter. Dette kan vi gjøre gjennom en vanlig F-test:F-verdien er 54.943 med p < .001. Vi kan finne kritisk F-verdi:Vi kan klart forkaste nullhypotesen om instrumentene er irrelevante.","code":"\nsteg1 <- lm(educ ~ age + exper + expersq + fatheduc + motheduc, data = Mrozdata)\n\ninstrumentF <- waldtest(steg1, .~. -fatheduc -motheduc)\ninstrumentF\n#> Wald test\n#> \n#> Model 1: educ ~ age + exper + expersq + fatheduc + motheduc\n#> Model 2: educ ~ age + exper + expersq\n#>   Res.Df Df      F                Pr(>F)    \n#> 1    422                                    \n#> 2    424 -2 54.943 < 0.00000000000000022 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nqf(0.05, df1 = 422, df2 = 424)\n#> [1] 0.8520168"},{"path":"logistisk-regresjon.html","id":"logistisk-regresjon","chapter":"Kapittel 3 Logistisk regresjon","heading":"Kapittel 3 Logistisk regresjon","text":"","code":"\npacman::p_load(readxl, car, rgl, flextable, plotly, latex2exp, ggfortify, gridExtra, factoextra, corrplot, Directional, tidyverse, palmerpenguins, psych, paran, kableExtra, multiUS, xtable, GPArotation, EFAtools, nFactors, rstatix, calibrate, Matrix, summarytools)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"innledning-2","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.1 Innledning","text":"“En faktoranalyse er en analyseteknikk som brukes å forstå korrelasjonsstrukturen et sett av observerte variabler” (Bjerkan 2007, s.221). følge Mehmetoglu Mittner (2020) brukes disse statistiske teknikkene praksis som metoder som reduserer et større antall variabler til et mindre antall variabler uten å miste vesentlig informasjon om dataene prosessen. De omtales derfor ofte som datareduksjonsteknikker (“data reduction” eller “dimension reduction”). Ofte vil vi bruke faktoranalyse å kunne si noe om såkalte latente (eller skjulte) variabler samfunnsvitenskapene - forhold vi ikke kan måle direkte, men som vi kan uttrykke gjennom å måle/observere en rekke andre forhold/variabler som vi så “samler” en konstruert variabel gjennom nettopp faktoranalyse. Målet med faktoranalysen er da følge Tinsley Tinsley (1987) “achieve parsimony using smallest number explanatory concepts explain maximum amount common variance correlation matrix”. Det vi ønsker å finne er variabler som er korrelerte med hverandre, men relativt ukorrelerte med andre grupper/subset av variabler (som igjen er interkorrelerte egen gruppe/subset) (Tabachnik Fidell 2007). Den grunnleggende utfordringen er vi ønsker å representere et stort antall variabler på en enklere måte, men hvis vi velger få faktorer mister vi informasjon (noe som går ut påliteligheten) og hvis vi velger mange kan vi ende opp med en modell som er komplisert og vanskelig å tolke.Innledningsvis er det nødvendig å gjøre en grunnleggende begrepsavklaring rundt begrepet faktoranalyse da det mange sammenhenger framstår som om begreper blandes sammen og det kan være uklart hva man egentlig snakker om. Begrepene faktoranalyse og komponentanalyse (“Factor Analysis” - FA - og “Principal Component Analysis” - PCA) brukes ofte om hverandre og noen ganger er det uklart hva som er hva (eller hvert fall hva forfatteren mener). En klargjørende framstilling kan man f.eks. finne Jöreskog, Olsson, Wallentin (2016). Vi vil det følgende skille mellom faktoranalyse (EFA og CFA, og komponentanalyse (PCA).Felles begge er:de er metoder datareduksjonde brukes å uttrykke multivariate data gjennom færre dimensjoner enn det opprinnelige datasettetde er metodikker å identifisere mønstre korrelasjonen mellom variabler.En grunnleggende forskjell er PCA er en deskriptiv teknikk mens faktoranalyse er modelleringsteknikker (Unkel Trendafilov 2010). PCA blir dermed “en empirisk oppsummering av datamaterialet” (Bjerkan 2007, s.225).\nStrengt tatt er ikke PCA en faktoranalyse, men omtales (dessverre noe forvirrende) som sagt ofte som en faktoranalyse. SPSS heter f.eks. menypunktet «Data reduction» og man velger «Factor» neste valg uansett hvilken av metodene (PCA/EFA/CFA) man skal bruke.Grafisk kan forskjellene vises slik:venstre del (PCA) kombineres fire målte variabler (\\(X_1...X_4\\)) til en komponent (\\(C\\)). Pilene indikerer det er variablene som bidrar til å skape komponenten, og de kan gjøre det med ulike styrke (vekt), som er vist med \\(w_1...w_4\\). Variablene \\(X_1...X_4\\) utgjør altså ulike størrelser på bidraget til komponenten \\(C\\).figurens høyre del ser vi en faktor \\(F\\) som skaper de fire målte variablene (\\(Y_1...Y_4\\)). Dette vises ved pilene går fra \\(F\\) til \\(Y_1...Y_4\\). \\(F\\) kan typisk være en latent variabel vi ikke kan observere direkte - som f.eks. intelligens eller angst. Også er det ulike vekter, så \\(F\\) kan påvirke \\(Y_1...Y_4\\) med ulik styrke. tillegg har vi et feilledd (\\(e_1...e_4\\)). \\(e_1\\) representerer f.eks. den delen av variansen \\(Y_1\\) som ikke forklares av \\(F\\). Vi kan uttrykke sammenhengen en enkelt varaibel som \\(Y_1\\) som en regresjonslikning: \\(Y_1 = b_1*F + e_1\\) (og tilsvarende \\(Y_2...Y_4\\)).Som illustrert figuren består variansen til variabelen X av tre deler: felles varians med andre variabler, unik varians selve variabel X og målefeil. PCA søker å forklare varians variabel X som en komponent (derav komponentanalyse), mens faktoranalyse kun søker å forklare den delen av den totale variansen som er felles (eller med andre ord: korrelasjonen mellom variablene).Modifisert fra Bjerkan (2007), s. 225, fig. 10.1Matematisk er forskjellen mellom faktoranalyse og PCA altså hvordan varians blir analysert – PCA blir varians analysert, faktoranalyse blir kun delt varians («shared variance») analysert. Eller med andre ord: PCA analyserer varians (felles varians, unik varians og målefeil), FA analyserer kovarians (variabelens felles varians med andre variabler).Teoretisk er forskjellen mellom de FA ses faktoren som årsaker til variabelen, mens PCA ses variablene som årsaken til komponentene; PCA er det ingen teoretisk forventning om hvilke variabler som forbindes med hvilke komponenter – det er kun empirisk assosiert (Tabachnik Fidell 2007).En annen måte å illustrere forskjellen mellom PCA og EFA er gitt av Bastos (2021).Fra Bastos (2021)figuren representerer ’ene spesifikk varians, B’ene felles varians og C’ene feilvarians (jfr. figuren fra Bjerkan (2007) lenger opp). Mens vi PCA vil bruke varians (, B og C) bruker vi kun B EFA.Praktisk er det imidlertid ikke helt trivielt å avgjøre om man skal bruke PCA eller EFA. Guadagnoli Velicer (1988) konkluderer også en litteraturundersøkelse med resultatene fra PCA stor grad er like som resultatene fra faktoranalyse. Med minst 30 variabler vil løsningene være mer eller mindre like, men med 20 variabler kan forskjeller inntreffe (Stevens 2002). Som Field (2009) oppsummerer: “However, non-statistician difference principal component factor may difficult conceptualize (linear models), difference arises largely calculation” (s.760).","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2 Principal Component Analysis (PCA)","text":"Tabachnik Fidell (2007) foreslår dersom du ønsker en «empirisk oppsummering» av datasettet og redusere et større antall variabler til et mindre antall komponenter er PCA riktig valg. en PCA transformeres et antall korrelerte variabler til et mindre antall «principal components».","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.1 PCA gjennom et lite eksempel","text":"La oss se på hva PCA er. Vi tar utgangspunkt et lite, kontruert datasett der vi har registrert karakterer ulike fag på 10 studenter. Dette eksempelet er modifisert fra Starmer (2018), med innslag fra Pittard (2012).studentnrmatte1792763784755426457418469501049Vi kan plotte den ene variabelen - matte.Vi kan se studentgruppen deler seg klare grupper, en gruppe med høy score og en gruppe med lavere score. Studentene gruppa med høy score likner mer på hverandre enn på studenter den andre gruppa, og motsatt.Hvis vi legger til en variabel - f.eks. engelskkarakterer - får vi denne tabellen:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Som vi også kan plotte:Vi kan nå si vi har dimensjoner hver student: den første dimensjonen - x-aksen - inneholder mattekarakterer, den andre dimensjonen - y-aksen - inneholder engelskkarakterer. Vi kan se også med dimensjoner det er tydelige clustere.Så kan vi legge til nok en karakter - denne gangen norsk.studentnrmatteengelsknorsk17979672766868378716947570665425348645554674162478466050950645110494949Med tre dimensjoner må vi har tre akser plottet:Hvis vi nå legger til fysikkarakterer tillegg kan vi ikke lenger plotte dette siden det vil kreve fire dimensjoner. PCA stepper inn og lager et 2D plott av flere dimensjoner. PCA vil også kunne si oss noe om hvilken av karakterene (= hvilken av variablene) som er viktigst å skape klyngene/grupperingene av studenter.","code":"\n# Base R\nplot(karakterer$matte, xlab = \"studentnr\", ylab = \"matte\")\n# Base R\nplot(karakterer$matte, karakterer$engelsk, xlab = \"matte\", ylab = \"engelsk\")\n# Bruker pakken: plotly\nfig <- plot_ly(karakterer, x = matte, y = norsk, z = engelsk)\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'Matte'),\n                                   yaxis = list(title = 'Norsk'),\n                                   zaxis = list(title = 'Engelsk')))\nfig"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sentrering","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.1.1 Sentrering","text":"Vi går tilbake til datasettet med karakterer (dimensjoner).studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi regner så ut gjennomsnittet de variablene matte og engelsk:Vi bruker disse gjennomsnittsverdiene til å kalkulere senterpunktet (x = 58.1, y = 63.1) som vi deretter - ved å beholde de innbyrdes avstandene x- og y-planet mellom datapunktene - sentrerer alle observasjonene rundt.Vi ser datapunktene ligger nøyaktig likt forhold til hverandre, men senterpunktet er nå (0, 0). Neste steg er å lage en regresjonslinje som passer best mulig til dataene. Vi begynner med å legge på en hvilken som helst linje som går gjennom (0, 0) og deretter roterer vi linja med (0, 0) som pivoteringspunkt til man finner linja som passer best (dette tilfellet den røde stiplede linja):Og hvordan vet PCA hvilken linje som passer best? å se på det skal vi ta en liten omveg ut av eksempelet vårt.","code":"\n# Base R\nkartabell2#> [1] 58.1\n#> [1] 63.1\n# Base R\ncenter_scale <- function(x) {\n    scale(x, scale = FALSE)\n}\nsentrert <- as.data.frame(center_scale(karakterer))  \nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,3), col = \"blue\", lty = 3)\nabline(coef = c(0,2.5), col = \"blue\", lty = 3)\nabline(coef = c(0,2), col = \"blue\", lty = 3)\nabline(coef = c(0,1.5), col = \"blue\", lty = 3)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nabline(coef = c(0,0.75), col = \"red\", lty = 3)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forskjell-i-utregning-av-avvik---ols-og-pca","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.1.2 Forskjell i utregning av avvik - OLS og PCA","text":"Med utgangspunkt Long (2010) kan vi illustrere den prinsippielle forskjellen hvordan hhv. OLS (lineær regresjon) og PCA kalkulerer beste tilpasning.en OLS (jfr. teori kapittel om regresjonsanalyse) vil man søke å finne beste tilpasning:OLS forsøker å minimere feilleddet mellom den avhengige variabelen og modellen ved å regne på alle avstandene (og kvadrere dem) mellom datapunktene og modellen. grafen er dette illustrert med oransje strek av datapunktene.Hvis vi bytter om på den avhengige og uavhengige variabelen ser det OLS slik ut av datapunktene:OLS forsøker alltid minimere y-avstanden (feilleddet = \\(y-\\hat{y}\\)). PCA vil minimere feilleddet ortogonalt (90\\(^\\circ\\) på modellen):PCA vil rotere modellen (“regresjonslinja”) rundt et senterpunkt og hele tiden kalkulere summen av de ortogonale feilleddene. Når du har en litt større mengde uavhengige variabler vil PCA gi deg hvilke lineære kombinasjoner som teller mest. En snasen forklaring og illustrasjon kan de se herÅ gå dybden på den matematiske utregningen av eigenvalues og eigenvectors er på grensen av dybdekunnskap et notat med tittel “Anvendt…” seg, og trolig kan man leve godt uten denne dybdekunnskapen. En anbefalt kilde å gå dybden kan være Smith (2002). Likevel vil vi skissere hvordan dette gjøres det enkle eksempelet.","code":"\n# Base R\nset.seed(2)\nx <- 1:100\ny <- 20 + 3 * x\ne <- rnorm(100, 0, 60)\ny <- 20 + 3 * x + e\n\n# OLS-regresjon y ~ x\nplot(x,y)\nyx.lm <- lm(y ~ x)\nlines(x, predict(yx.lm), col=\"red\")\narrows(x0 = 61, x1= 61, y0 = 100, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 54, x1= 54, y0 = 260, y1 = 180, lwd = 2, col = \"orange\")\n# Base R\n# OLS-regresjon x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"blue\")\narrows(x0 = 84, x1= 55, y0 = 190, y1 = 190, lwd = 2, col = \"orange\")\narrows(x0 = 32, x1= 44, y0 = 137, y1 = 137, lwd = 2, col = \"orange\")\n# Base R\n# PCA x ~ y\nplot(x,y)\nxy.lm <- lm(x ~ y)\nlines(predict(xy.lm), y, col=\"green\")\narrows(x0 = 54, x1= 57, y0 = 260, y1 = 200, lwd = 2, col = \"orange\")\narrows(x0 = 42, x1= 38, y0 = 32, y1 = 109, lwd = 2, col = \"orange\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-gjennom-et-lite-eksempel---del-2","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2 PCA gjennom et lite eksempel - del 2","text":"Vi tar utgangspunkt eksempelet der vi hadde variabler som er sentrert:studentnrmatteengelsk179792766837871475705425364555741628466095064104949Vi legger på en tilfeldig linje som går gjennom (0, 0).Ettersom vi roterer linja vil avstanden \\(\\) ikke forandre seg. Lengden på både \\(b\\) og \\(c\\) vil imidlertid endre seg relativt til hverandre. Når \\(b\\) blir lengre, blir \\(c\\) kortere og motsatt. samme tanke som OLS-regresjon (se ovenfor) ønsker vi \\(b\\) skal være så kort som mulig da det betyr linja ligger så nærme datapunktet som mulig. å få \\(b\\) så kort som mulig jobber iidlertid PCA å maksimere \\(c\\) (det har samme effekt: når \\(c\\) er på sitt maksimale er \\(b\\) på sitt minimale). PCA finner altså den beste linja ved å maksimere den kvadrerte avstanden \\(c\\) (kvadrert pga. Pythagoras… \\(^2 + b^2 = c^2\\)).PCA summerer da \\(c^2\\) alle de 10 punktene dette eksempelet. Siden verdiene er kvadrerte slipper vi også problemet med positive og negative verdier nuller hverandre ut (som OLS - Ordinary Least Squares selv om vi ikke regner squares men avstand). Summen = sum squared distances SS(distances).","code":"\n# Base R\nkartabell2\n# Base R}\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\n# Base R}\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(coef = c(0,1), col = \"blue\", lty = 3)\nlines(x=c(0,16.9), y=c(0, 6.9), col = \"orange\")\nlines(x=c(16.9, 13.4), y=c(6.9, 13.6), col = \"orange\")\ntext(10, 2, \"a\")\ntext(16, 11, \"b\")\ntext(8, 10, \"c\")\ntext(10, -8, TeX('$\\\\c_{1}^2$ ... $\\\\c_{10}^2$ = SS(distances)'))"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"identifikasjon-av-principal-component-1-pc1-eigenvectors-og-loading-scores","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.1 Identifikasjon av Principal Component 1 (PC1), eigenvectors og loading scores","text":"PCA roterer som sagt på linja pivotert (0, 0) og til slutt finner den linja som maksimaliserer SS(Distances). Denne linja kalles Principal Component 1 (PC1).PC1 har et stigningstall på 0.4639586. Det betyr en økning på 1 x (matte) gir en økning på 0.46 y (engelsk). Det betyr matte har større innvirkning på grupepringen av studentene enn engelsk.PCA skaleres \\(c\\) alltid til 1. Matematisk gjør vi det ved å dele hver side av Pythagoras med \\(c\\) (1.1).Disse verdiene - 0.91 og 0.42 - kalles Eigenvector PC1, og de verdiene kalles gjerne Loading Scores. Som vist grafen er \\(SS(Distances) = Eigenvalue PC1\\). Eigenvalue er et begrep vi kommer tilbake til PCA når vi skal velge ut hvor mange komponenter vi skal beholde.Det neste vi kan se på da er Principal Component 2 (PC2). PC2 er linja som går vinkelrett på PC1 og gjennom (0, 0). PC2 er altså linja som reflekterer den nest største kilden til variasjon dataene, men som er ortogonal (vinkelrett) på PC1. Dette betyr eigenvectoren er “snudd” og blir -0.42 og 0.91 (som altså er loading scores PC2).å få fram det endelige PCA-plottet roteres løsningen slik PC1 utgjør x-aksen og PC2 y-aksen.Punktene plottes deretter på det roterte diagrammet. Til dette brukes trigonometri. La oss se på et enkelt eksempel ett punkt - dette gjør vi selvsagt ikke manuelt.Vi har det opprinnelige plottet et koordinatsystem og har funnet PC1 og PC2, og snur først aksene. Så må datapunktene posisjoneres etter de roterte aksene. La oss vise med ett punkt (1,0) - altså punktet 1 på x-aksen, 0 på y-aksen.Punkt (1,0) blir da - forutsatt \\(\\alpha=30\\):\\(y'= sin(\\alpha) = 0.5\\)\\(x'= cos(\\alpha) = 0.9\\)(1,0) blir da rotert (0.5, 0.9). Slik gjør man alle datapunkter.","code":"\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, TeX('Pythagoras gir: a = 1, b = 0.46, c = 1.1'))\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\n\nlines(x=c(0,17.9), y=c(0, 4.9), col = \"orange\")\nlines(x=c(17.9, 17.2), y=c(4.9, 7.9), col = \"orange\")\ntext(9.8, 2, \"a\")\ntext(18.5, 7, \"b\")\ntext(10, 6.5, \"c\")\n\ntext(11, -1.5, \"Pythagoras gir: a = 1, b = 0.46, c = 1.1\", cex = 0.75)\ntext(11, -4, \"PCA-skalerte verdier: a = 0.91, b = 0.42, c = 1\", cex = 0.75)\ntext(11, -6.5, \"SS(Distances) = Eigenvalue for PC1\", cex = 0.75)\n# Base R\nplot(sentrert$matte, sentrert$engelsk, xlab = \"matte\", ylab = \"engelsk\")\nabline(v = 0, lty=2)\nabline(h = 0, lty=2)\nabline(lmkarakterer, col = \"red\")\ntext(3, 3, \"PC1\", srt = 20)\nabline(a = 0, b = -4.5, col = \"blue\") \ntext(-4, 8, \"PC2\", srt = 20)\n# Bruker pakken: calibrate\nkaraktererX2 <- within(karakterer2, rm(studentnr))\n# Skalerer data\nstandardize <- function(x) {(x - mean(x))}\nskalert_karaktererX2 <- apply(karaktererX2,2,function(x) (x-mean(x)))\nplot(skalert_karaktererX2, cex=0.9, xlim=c(-20,20))\n# Finner Eigenvalues fra kovariansematrisen\nmy.cov <- cov(skalert_karaktererX2)\nmy.eigen <- eigen(my.cov)\nrownames(my.eigen$vectors) <- c(\"matte\",\"engelsk\")\ncolnames(my.eigen$vectors) <- c(\"PC1\",\"PC\")\n# Sum Eigenvalues = den totale variansen i dataene\nsum(my.eigen$values)\n#> [1] 357.9778\nvar(skalert_karaktererX2[,1]) + var(skalert_karaktererX2[,2])\n#> [1] 357.9778\n# Eigenvektorene er principal components.\nloadings <- my.eigen$vectors\npc1.slope <- my.eigen$vectors[1,1]/my.eigen$vectors[2,1]\npc2.slope <- my.eigen$vectors[1,2]/my.eigen$vectors[2,2]\nabline(0,pc1.slope,col=\"red\")\nabline(0,pc2.slope,col=\"blue\")\ntextxy(12,10,\"(-0.710,-0.695)\",cx=0.9,dcol=\"red\")\ntextxy(-12,10,\"(0.695,-0.719)\",cx=0.9,dcol=\"blue\")\n# Hvor mye varians foklarer hver eigenvector\npc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=2)\npc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)\nxlab=paste(\"PC1 - \",pc1.var,\" % of variation\",sep=\"\")\nylab=paste(\"PC2 - \",pc2.var,\" % of variation\",sep=\"\")\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")\n# Base R\nscores <- skalert_karaktererX2 %*% loadings\nsd <- sqrt(my.eigen$values)\nrownames(loadings) = colnames(karaktererX2)\n\nplot(scores,ylim=c(-10,10),xlab=xlab,ylab=ylab)\nabline(0,0,col=\"red\")\nabline(0,90,col=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"litt-mer-i-detalj-om-rotasjon","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.2 Litt mer i detalj om rotasjon","text":"Det vi har gjort ovenfor med et lite eksempel kjøres på hele datasettet det verktøyet vi bruker (R, Stata, SPSS, osv.).Med utgangspunkt en datamatrise roteres dataene en gitt vinkel gjennom en rotasjonsmatrise til et ny datamatrise (PC1). Denne igjen roteres på samme måte til PC2.Vi kan se på komponentene:Og matrisen etter en rotasjon som vi kan plotte (som vi kjenner igjen fra litt lenger oppe):","code":"\n# Base R\nut1 <- princomp(karaktererX2, cor = T)\nsummary(ut1)\n#> Importance of components:\n#>                           Comp.1     Comp.2\n#> Standard deviation     1.3533352 0.41046786\n#> Proportion of Variance 0.9157581 0.08424193\n#> Cumulative Proportion  0.9157581 1.00000000\n# Base R\nutdata <- ut1$score\nutdata\n#>           Comp.1      Comp.2\n#>  [1,]  2.2283619 -0.34268382\n#>  [2,]  1.2036707  0.41133586\n#>  [3,]  1.5364460  0.25900821\n#>  [4,]  1.3202598  0.20452296\n#>  [5,] -1.5428918  0.09028815\n#>  [6,] -1.2458551  0.06392291\n#>  [7,] -0.8603493 -0.68247820\n#>  [8,] -0.7964907 -0.29521764\n#>  [9,] -0.2926411 -0.43817195\n#> [10,] -1.5505104  0.72947352\nplot(utdata)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"utvelgelse-av-antall-komponenter","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.3 Utvelgelse av antall komponenter","text":"Et sentralt element PCA er valg av antall komponenter man vil beholde. Dette kan vi gjøre gjennom variansen komponentnen (: PC1 og PC2). kommer vi tilbake til begrepet eigenvalues som vi definerte lenger opp. Vi kan se på eigenvalue en PC som hvor mange variabler som representeres av den respektive PC. Eigenvalues forholder seg til forklart varians slik:\\(Forklart\\ varians = \\frac{Eigenvalue}{antall\\ opprinnelige\\ variabler}\\)Dette kan også uttrykkes slik:\\(\\frac{SS(Distances PC1)}{n - 1} = Varians PC1\\)\\(\\frac{SS(Distances PC2)}{n - 1} = Varians PC2\\)Et viktig poeng er en PCA ikke reduserer antall variabler seg selv. Hvis du opprinnelig har 13 variabler vil du få 13 PC, men spørsmålet er hvor mange du faktisk trenger å se på å forklare variansen (hvilket du ønsker skal være færre enn det opprinnelige antall variabler). Og det er vi kommer fram til hele poenget med PCA: Hvir mange komponenter skal vi beholde?å vise uilke metoder å vurdere antall komponenter som bør beholdes bruker vi et datasett med flere variabler enn vårt eksempel ovenfor, modifisert fra DataViz (2020)) - vi går ikke inn på hva dataene er.","code":"\n# Bruker pakken: palmerpenguins\npenguins_data <- penguins[,c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")]\npenguins_data <- na.omit(penguins_data)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.3.1 Kaisers kriterium","text":"Kaiser (1960) kriterium baserer seg på å beholde alle komponenter med eigenvalue på 1,0. Enhver komponent med eigenvalue 1 forklarer mer varianse enn en enkeltvariabel. Med andre ord – ut fra denne måten å vurdere ønsker vi ikke å beholde komponenter som forklarer mindre varians enn enkeltvariabler. Det er generelt anbefalt man ikke bruker Kaisers kriterium alene da metoden har en tendens til å overvurdere antallet komponenter. Samtidig hevdes det det er umulig å tillegge en komponent med verdi 1,01 som viktig og en annen med verdi 0,99 som uviktig (Fabrigar et al. 1999).Ut fra dette bør vi beholde 1 komponent, men vi ser komponent 2 er svært nærme 1.","code":"\n# Base R\npca_obj <- prcomp(drop_na(penguins_data), scale. = TRUE)\npca_obj_eigen <- ((pca_obj$sdev)^2)\npca_obj_eigen\n#> [1] 2.77008681 0.99348866 0.77191746 0.36520940 0.09929767"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.3.2 Scree plott","text":"Et hjelpemiddel dette er scree plot (Cattell 1966). Et Scree Plot er en grafisk framstilling av komponentene langs x-aksen og de korresponderende eigenvalues på y-aksen. Et scree plot viser hvor stor del av variansen variabelen forklarer rundt f.eks. PC1.Når vi vurderer Scree Plot ønsker vi å identifisere knekkpunktet (også kalt «albuen»).La oss forutsette variansen PC1 = 13 og PC2 = 4. Dvs. den totale variansen = 17. Videre beytr det PC1 forklarer \\(\\frac{13}{17}=0.765\\) - altså 76.5% av den totale variansen. PC2 forklarer på sin side \\(\\frac{4}{17}=0.235\\). Et scree plot er en grafisk framstilling av denne variansen.vårt eksempel kan vi dermed fremstille dette scree plottet:Cattell (1966) beskriver framgangsmåten som man finner albuen og dropper alle komponenter etter komponenten som starter albuen (eller sagt på en annen måte: vi beholder alle komponentene knekkpunktet). vårt tilfelle indikerer det vi beholder 1 komponent. vårt scree plott er det et tydelig knekkpunkt, men mange tilfeller er det ikke så tydelig, og det kan være vanskelig å identifisere «det rette» knekkpunktet.En alternativ, og ofte brukt meetode å illlustrere et scree plot på, er å kombinere det med et histogram (eksempel fra Szczęsna (2022)).","code":"\n# Bruker pakken: psych\nvar_explained_df <- data.frame(PC= paste0(\"PC\",1:5),                           var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2))\nvar_explained_df %>%\n  ggplot(aes(x=PC,y=var_explained, group=1))+\n  geom_point(size=4)+\n  geom_line()+\n  labs(title=\"Scree plott\")\nsummary(pca_obj)\n#> Importance of components:\n#>                          PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     1.664 0.9967 0.8786 0.60433 0.31512\n#> Proportion of Variance 0.554 0.1987 0.1544 0.07304 0.01986\n#> Cumulative Proportion  0.554 0.7527 0.9071 0.98014 1.00000\n# Bruker pakken: factoextra\nfviz_eig(pca_obj, col.var=\"blue\")"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.2.3.3 Parallell analyse","text":"Parallell anlayse (PA) (Horn 1965) sammenlikner korrelasjonsmatrisen fra våre data med tilfeldig genererte korrelasjonsmatriser med samme antall variabler og observasjoner, å sammenlikne eigenvalues de genererte med den observerte. eksempelet ber vi om 5000 tilfeldige korrelasjonsmatriser.","code":"\n# Bruker pakken: paran\nparan(penguins_data, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           2.617826    2.770086      0.152260\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (1 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel-pca","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3 Eksempel PCA","text":"En anvendelse av PCA kan være vi ønsker å se på den underliggende strukturen en skalavariabel. skal vi bruke Pallants datasett som vi også brukte deler av kapittelet om regresjonsanalyse som du finner . PCA-eksempelet skal vi se på en av skalaene - PANAS - og har modifisert datasettet til å kun inneholde spørsmålene knyttet til denne skalaen.Download Pallant_survey_PANAS.xlsxDownload Pallant_survey_PANAS.savDownload Pallant_survey_PANAS.dtaDatasettet består av 20 spørsmål som utgjør PANAS skalaen (“Positive Negative Affect Schedule”).","code":"\n# Base R\n# Bruker pakken: readxl\nPallant_survey_PANAS <- as.data.frame(read_excel(\"Pallant_survey_PANAS.xlsx\"))\nPallant_survey_PANAS <- na.omit(Pallant_survey_PANAS)\nsummarytools::descr(Pallant_survey_PANAS)\n#> Descriptive Statistics  \n#> Pallant_survey_PANAS  \n#> N: 435  \n#> \n#>                        pn1     pn10     pn11     pn12     pn13     pn14     pn15     pn16     pn17\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.79     1.86     2.43     2.87     3.26     1.66     3.19     1.68     3.37\n#>           Std.Dev     0.91     1.07     1.04     1.19     1.00     0.95     1.12     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     2.00     3.00     1.00     2.00     1.00     3.00\n#>            Median     4.00     2.00     2.00     3.00     3.00     1.00     3.00     1.00     3.00\n#>                Q3     4.00     2.00     3.00     4.00     4.00     2.00     4.00     2.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     1.48     1.48     0.00     1.48     0.00     1.48\n#>               IQR     1.00     1.00     1.00     2.00     1.00     1.00     2.00     1.00     1.00\n#>                CV     0.24     0.58     0.43     0.41     0.31     0.57     0.35     0.60     0.30\n#>          Skewness    -0.67     1.16     0.47    -0.01    -0.33     1.59    -0.29     1.53    -0.35\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.26     0.54    -0.40    -0.94    -0.35     2.10    -0.65     1.67    -0.24\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                       pn18     pn19      pn2     pn20      pn3      pn4      pn5      pn6      pn7\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     3.42     2.19     2.56     1.72     1.74     3.16     1.41     3.72     3.60\n#>           Std.Dev     0.99     1.17     1.20     1.08     1.00     1.11     0.82     1.01     1.10\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     3.00     1.00     2.00     1.00     1.00     2.00     1.00     3.00     3.00\n#>            Median     3.00     2.00     2.00     1.00     1.00     3.00     1.00     4.00     4.00\n#>                Q3     4.00     3.00     3.00     2.00     2.00     4.00     2.00     4.00     4.00\n#>               Max     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00     5.00\n#>               MAD     1.48     1.48     1.48     0.00     0.00     1.48     0.00     1.48     1.48\n#>               IQR     1.00     2.00     1.00     1.00     1.00     2.00     1.00     1.00     1.00\n#>                CV     0.29     0.54     0.47     0.63     0.58     0.35     0.58     0.27     0.31\n#>          Skewness    -0.48     0.74     0.40     1.47     1.41    -0.27     2.33    -0.66    -0.53\n#>       SE.Skewness     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12     0.12\n#>          Kurtosis     0.02    -0.45    -0.81     1.25     1.25    -0.69     5.31     0.11    -0.33\n#>           N.Valid   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00   435.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00\n#> \n#> Table: Table continues below\n#> \n#>  \n#> \n#>                        pn8      pn9\n#> ----------------- -------- --------\n#>              Mean     2.20     3.29\n#>           Std.Dev     1.17     1.01\n#>               Min     1.00     1.00\n#>                Q1     1.00     3.00\n#>            Median     2.00     3.00\n#>                Q3     3.00     4.00\n#>               Max     5.00     5.00\n#>               MAD     1.48     1.48\n#>               IQR     2.00     1.00\n#>                CV     0.53     0.31\n#>          Skewness     0.77    -0.38\n#>       SE.Skewness     0.12     0.12\n#>          Kurtosis    -0.33    -0.12\n#>           N.Valid   435.00   435.00\n#>         Pct.Valid   100.00   100.00\n# Base R\nresultat.pca <- prcomp(Pallant_survey_PANAS, scale = TRUE)\nsummary(resultat.pca)\n#> Importance of components:\n#>                           PC1    PC2    PC3     PC4     PC5\n#> Standard deviation     2.4973 1.8443 1.1063 1.07642 0.94816\n#> Proportion of Variance 0.3118 0.1701 0.0612 0.05793 0.04495\n#> Cumulative Proportion  0.3118 0.4819 0.5431 0.60104 0.64599\n#>                            PC6    PC7     PC8     PC9\n#> Standard deviation     0.88714 0.8556 0.81001 0.80669\n#> Proportion of Variance 0.03935 0.0366 0.03281 0.03254\n#> Cumulative Proportion  0.68534 0.7219 0.75475 0.78729\n#>                           PC10    PC11    PC12    PC13\n#> Standard deviation     0.77118 0.76610 0.70723 0.70109\n#> Proportion of Variance 0.02974 0.02935 0.02501 0.02458\n#> Cumulative Proportion  0.81702 0.84637 0.87138 0.89595\n#>                           PC14    PC15    PC16    PC17\n#> Standard deviation     0.62728 0.61274 0.57483 0.54724\n#> Proportion of Variance 0.01967 0.01877 0.01652 0.01497\n#> Cumulative Proportion  0.91563 0.93440 0.95092 0.96589\n#>                           PC18    PC19    PC20\n#> Standard deviation     0.53320 0.47228 0.41805\n#> Proportion of Variance 0.01422 0.01115 0.00874\n#> Cumulative Proportion  0.98011 0.99126 1.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"valg-av-antall-komponenter","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.1 Valg av antall komponenter","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-og-eigenvalues","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.1.1 Kaisers kriterium og eigenvalues","text":"Ut fra Kaisers kriterium beholder vi fire komponenter.","code":"\n# Base R\neig.val <- as.data.frame(get_eigenvalue(resultat.pca))\neig.val\n#>        eigenvalue variance.percent\n#> Dim.1   6.2365752       31.1828761\n#> Dim.2   3.4015750       17.0078752\n#> Dim.3   1.2239359        6.1196796\n#> Dim.4   1.1586859        5.7934294\n#> Dim.5   0.8990120        4.4950602\n#> Dim.6   0.7870241        3.9351204\n#> Dim.7   0.7320491        3.6602453\n#> Dim.8   0.6561184        3.2805918\n#> Dim.9   0.6507440        3.2537200\n#> Dim.10  0.5947166        2.9735829\n#> Dim.11  0.5869064        2.9345320\n#> Dim.12  0.5001778        2.5008892\n#> Dim.13  0.4915331        2.4576653\n#> Dim.14  0.3934826        1.9674130\n#> Dim.15  0.3754471        1.8772354\n#> Dim.16  0.3304249        1.6521243\n#> Dim.17  0.2994713        1.4973567\n#> Dim.18  0.2843020        1.4215102\n#> Dim.19  0.2230511        1.1152553\n#> Dim.20  0.1747675        0.8738377\n#>        cumulative.variance.percent\n#> Dim.1                     31.18288\n#> Dim.2                     48.19075\n#> Dim.3                     54.31043\n#> Dim.4                     60.10386\n#> Dim.5                     64.59892\n#> Dim.6                     68.53404\n#> Dim.7                     72.19429\n#> Dim.8                     75.47488\n#> Dim.9                     78.72860\n#> Dim.10                    81.70218\n#> Dim.11                    84.63671\n#> Dim.12                    87.13760\n#> Dim.13                    89.59527\n#> Dim.14                    91.56268\n#> Dim.15                    93.43992\n#> Dim.16                    95.09204\n#> Dim.17                    96.58940\n#> Dim.18                    98.01091\n#> Dim.19                    99.12616\n#> Dim.20                   100.00000"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-1","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.1.2 Scree plott","text":"Dette skulle indikere vi beholder komponenter.","code":"\n# Bruker pakken: factoextra\nfviz_eig(resultat.pca, addlabels = TRUE)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-1","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.1.3 Parallell analyse","text":"Dette peker også mot vi bør beholde komponenter.","code":"\n# Bruker pakken: paran\nparan(Pallant_survey_PANAS, iterations=5000)\n#> \n#> Using eigendecomposition of correlation matrix.\n#> Computing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n#> \n#> \n#> Results of Horn's Parallel Analysis for component retention\n#> 5000 iterations, using the mean estimate\n#> \n#> -------------------------------------------------- \n#> Component   Adjusted    Unadjusted    Estimated \n#>             Eigenvalue  Eigenvalue    Bias \n#> -------------------------------------------------- \n#> 1           5.836032    6.236575      0.400542\n#> 2           3.074520    3.401575      0.327054\n#> -------------------------------------------------- \n#> \n#> Adjusted eigenvalues > 1 indicate dimensions to retain.\n#> (2 components retained)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"pca-låst-til-to-komponenter","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.2 PCA låst til to komponenter","text":"Vi ønsker å se på modellen med komponenter på tre parametere. Først ønsker vi mindre enn 50% av residualene skal ha absoluttverdi > 0.05.vårt tilfelle er 12.5% av residualene > 0.05.Den neste parameteren er model fit som bør være > 0.9.er verdien 0.8705564.Til slutt ser vi på “communalities”.Pallant (2010) foreslår å se etter verdier på 0.3. En lav verdi indikerer den respektive variabelen ikke passer godt sammen med de andre variablene sin respektive komponent. Man kan vurdere å se om modellen blir bedre ved å ta vekk variabler med lav verdi (f.eks. 0.3). vårt tilfelle er variabelen pn5 terskelverdien på 0.3. Vi kan prøve å ta den bort. Fra før ser vi modellen forklarer 48% (se “Cumulative Var” tabellen).Vi ser ingen forbedring kumulativ varians forklart.Model fit er marginalt bedre.","code":"\n# Bruker pakken: psych\npca2 <- psych::principal(Pallant_survey_PANAS, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca2\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.14 0.50 0.50 1.1\n#> pn2  -0.15  0.70 0.52 0.48 1.1\n#> pn3  -0.11  0.73 0.54 0.46 1.0\n#> pn4   0.54 -0.12 0.31 0.69 1.1\n#> pn5  -0.12  0.49 0.26 0.74 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.16  0.73 0.55 0.45 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.2\n#> pn10 -0.01  0.60 0.35 0.65 1.0\n#> pn11 -0.15  0.65 0.44 0.56 1.1\n#> pn12  0.76 -0.04 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.73 0.55 0.45 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.58 0.35 0.65 1.1\n#> pn17  0.82 -0.12 0.69 0.31 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.04  0.79 0.62 0.38 1.0\n#> pn20 -0.08  0.71 0.51 0.49 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.89 4.75\n#> Proportion Var        0.24 0.24\n#> Cumulative Var        0.24 0.48\n#> Proportion Explained  0.51 0.49\n#> Cumulative Proportion 0.51 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  830.29  with prob <  2.5e-94 \n#> \n#> Fit based upon off diagonal values = 0.95\n# Base R\nantall_over <- length(pca2$residual[pca2$residual>0.05])\nantall_residualverdier <- nrow(pca2$residual)*ncol(pca2$residual)\n(antall_over/antall_residualverdier)*100\n#> [1] 12.5\n# Base R\npca2$fit\n#> [1] 0.8705564\n# Base R\nsort(pca2$communality)\n#>       pn5       pn4      pn16      pn10       pn6      pn11 \n#> 0.2574873 0.3064322 0.3512776 0.3542041 0.3751836 0.4394038 \n#>       pn7      pn15       pn9       pn1      pn20       pn2 \n#> 0.4503231 0.4608460 0.4746693 0.5034580 0.5064648 0.5160759 \n#>      pn13       pn3      pn14       pn8      pn18      pn12 \n#> 0.5377154 0.5422017 0.5471146 0.5534260 0.5717316 0.5848691 \n#>      pn19      pn17 \n#> 0.6201048 0.6851615\n# Base R\nPallant_survey_PANAS2 <- subset(Pallant_survey_PANAS, select = -(pn5))\npca3 <- psych::principal(Pallant_survey_PANAS2, nfactors=2, scores = TRUE, rotate = \"varimax\")\npca3\n#> Principal Components Analysis\n#> Call: psych::principal(r = Pallant_survey_PANAS2, nfactors = 2, rotate = \"varimax\", \n#>     scores = TRUE)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>        RC1   RC2   h2   u2 com\n#> pn1   0.70 -0.15 0.51 0.49 1.1\n#> pn2  -0.14  0.71 0.53 0.47 1.1\n#> pn3  -0.11  0.74 0.56 0.44 1.0\n#> pn4   0.54 -0.11 0.31 0.69 1.1\n#> pn6   0.61  0.02 0.38 0.62 1.0\n#> pn7   0.62 -0.25 0.45 0.55 1.3\n#> pn8  -0.15  0.74 0.57 0.43 1.1\n#> pn9   0.66 -0.18 0.47 0.53 1.1\n#> pn10 -0.01  0.60 0.36 0.64 1.0\n#> pn11 -0.14  0.65 0.45 0.55 1.1\n#> pn12  0.76 -0.05 0.58 0.42 1.0\n#> pn13  0.72 -0.12 0.54 0.46 1.1\n#> pn14 -0.11  0.74 0.56 0.44 1.0\n#> pn15  0.68  0.02 0.46 0.54 1.0\n#> pn16 -0.10  0.55 0.31 0.69 1.1\n#> pn17  0.82 -0.13 0.68 0.32 1.0\n#> pn18  0.74 -0.15 0.57 0.43 1.1\n#> pn19 -0.03  0.80 0.64 0.36 1.0\n#> pn20 -0.08  0.72 0.52 0.48 1.0\n#> \n#>                        RC1  RC2\n#> SS loadings           4.87 4.55\n#> Proportion Var        0.26 0.24\n#> Cumulative Var        0.26 0.50\n#> Proportion Explained  0.52 0.48\n#> Cumulative Proportion 0.52 1.00\n#> \n#> Mean item complexity =  1.1\n#> Test of the hypothesis that 2 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.07 \n#>  with the empirical chi square  708.34  with prob <  6.1e-79 \n#> \n#> Fit based upon off diagonal values = 0.95\n# Base R\npca2$fit\n#> [1] 0.8705564\npca3$fit\n#> [1] 0.8785797"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"forutsetninger","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.3 Forutsetninger","text":"Så langt har vi ikke sett på hvilke forutsetninger som må ligge til grunn å kunne kjøre en PCA. Det skal vi nå.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"størrelse-på-datasettetutvalgsstørrelse","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.3.1 Størrelse på datasettet/utvalgsstørrelse","text":"Antall cases (sample size) og forholdstallet mellom antall respondenter og antall variabler kan være av betydning en faktoranalyse. små utvalg er korrelasjonskoeffisientene mellom variablene mindre pålitelige/konsistente.Det finnes ulike anbefalinger, og Hogarty et al. (2005) hevder det ikke finnes et minimum hva angår \\(N\\) og \\(\\frac{N}{variabler}\\) å oppnå en god faktoranalyse. Arrindell van der Ende (1985) fant verken et bestemt forholdstall eller et minimumsantall observasjoner hadde påvirkning på faktorstabiliteten. Guadagnoli Velicer (1988) viste en faktor med fire eller flere faktorladninger på 0,6 eller høyere er stabil uavhengig av utvalgsstørrelsen. En faktor med 10 eller flere ladninger større enn 0,4 var stabil dersom utvalgsstørrelsen er minst 150. Antallet caser kan imidlertid ses opp mot hvor sterkt variablene lader på faktorene (Tabachnik Fidell 2007) og korrelasjonene (MacCallum et al. 1999) – høyere korrelasjoner (>.80) krever mindre sample size (Guadagnoli Velicer 1988).vårt datasett har vi 435 caser/observasjoner. Vi får da forholdstallet 22.9. Antallet og forholdstallet skulle utgangspunktet ikke være til hinder en faktoranalyse . Både Hair Jr. et al. (2010) og Nunally (1978) anbefaler et forholdstall på 10:1.\n##### Sphericity - er datasettet “faktoriserbart”Vi gjennomfører tester: Bartletts test sfæritet og KMO. Først Bartletts:ser vi p-verdien er terskelverdi på 0.05, så vi får dermed indikert dataene er egnet PCA etter dette kriteriet.Deretter KMO. KMO er en utregning som indikerer andelen av varians skalavariabelen som kan forklares av de underliggende faktorene. En høy verdi indikerer en faktoranalyse er mulig.Pallant (2010) anbefaler en grenseverdi på 0.60.\nKaiser (1974) anbefaler følgende retningslinjer KMO-verdier:KMO er derfor “respektabel”.","code":"\n# Bruker pakken: psych\nkorrelasjonsmatrise <- cor(Pallant_survey_PANAS2)\ncortest.bartlett(korrelasjonsmatrise, n = nrow(Pallant_survey_PANAS2))\n#> $chisq\n#> [1] 3781.425\n#> \n#> $p.value\n#> [1] 0\n#> \n#> $df\n#> [1] 171\n# Bruker pakken: psych\nKMO(Pallant_survey_PANAS2)\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> \n#> -- Kaiser-Meyer-Olkin criterion (KMO) ----------------------\n#> \n#> v The overall KMO value for your data is meritorious.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.876\n#> \n#>   For each variable:\n#>   pn1   pn2   pn3   pn4   pn6   pn7   pn8   pn9  pn10  pn11 \n#> 0.936 0.863 0.795 0.947 0.884 0.950 0.889 0.936 0.820 0.868 \n#>  pn12  pn13  pn14  pn15  pn16  pn17  pn18  pn19  pn20 \n#> 0.893 0.899 0.800 0.870 0.918 0.905 0.915 0.827 0.803"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"anti-image-korrelasjonsmatrise","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.2.3.3.2 Anti-image korrelasjonsmatrise","text":"resultatet har vi kun hentet ut de diagonale verdiene anti-image matrisen (som er de vi er interessert ) og sortert disse synkende rekkefølge. Disse verdiene er KMO verdier de individuelle variablene. Disse bør ifølge Field (2009) være på 0,5. Verdier 0,5 kan bety vi bør ta denne variabelen ut.Siden denne matrisen blir stor har vi kun hentet ut antall korrelasjonsverdier som er 0.9 (og 1.0 siden det matrisen alltid vil være 1.0 diagonalene - der variablene korrelerer med seg selv). Field (2009) peker på ingen korrelasjo-ner bør være 0.9. Samtidig bør det være godt med korrelasjoner 0.3. Det finnes ingen absolutte krav til hvor mange/hvor stor andel av korrelasjonene som bør være 0.3, men hvis man har få 0.3 indikerer det dataene kanskje ikke egner seg PCA.tabellen har vi fjernet alle korrelasjonsverdier +/- 0.3 og 1 å gjøre det litt lettere å se.Det kan se ut som vi har et greit antall korrelasjoner 0.3.","code":"\n# Bruker pakken: multiUS\nantiimage <- antiImage(Pallant_survey_PANAS2)$AIR\nantiimage2 <- as.matrix(diag(antiimage), row.names = FALSE)\nsort(antiimage2, decreasing = TRUE)\n#>  [1] 0.9499796 0.9469195 0.9364141 0.9359988 0.9182026\n#>  [6] 0.9146945 0.9048130 0.8987604 0.8931080 0.8887645\n#> [11] 0.8837913 0.8703518 0.8682341 0.8629180 0.8265146\n#> [16] 0.8197264 0.8028766 0.8001013 0.7945513\n# Base R\ncorPallant_survey_PANAS2 <- cor(Pallant_survey_PANAS2)\ncorPallant_survey_PANAS2 <- round(corPallant_survey_PANAS2, 2)\nlength(corPallant_survey_PANAS2[corPallant_survey_PANAS2>0.9 & corPallant_survey_PANAS2<1])\n#> [1] 0\n# Base R \nunder03<- as.data.frame(apply(corPallant_survey_PANAS2, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\nunder03\n#>       pn1  pn2  pn3  pn4  pn6  pn7  pn8  pn9 pn10 pn11 pn12\n#> pn1                 0.34 0.35 0.41      0.41           0.48\n#> pn2            0.46                0.64      0.41  0.5     \n#> pn3       0.46                     0.49           0.33     \n#> pn4  0.34                     0.33       0.4           0.31\n#> pn6  0.35                     0.33      0.43            0.4\n#> pn7  0.41           0.33 0.33           0.48            0.4\n#> pn8       0.64 0.49                          0.38 0.46     \n#> pn9  0.41            0.4 0.43 0.48                     0.41\n#> pn10      0.41                     0.38           0.58     \n#> pn11       0.5 0.33                0.46      0.58          \n#> pn12 0.48           0.31  0.4  0.4      0.41               \n#> pn13 0.49           0.33 0.33 0.39      0.43           0.58\n#> pn14      0.41 0.81                0.46           0.32     \n#> pn15 0.41           0.32      0.33      0.36           0.51\n#> pn16      0.31 0.33                0.38      0.31 0.35     \n#> pn17 0.56           0.37 0.39 0.49      0.46           0.64\n#> pn18 0.47           0.34 0.45 0.46      0.46           0.47\n#> pn19      0.46 0.56                0.48      0.34 0.39     \n#> pn20      0.42 0.42                0.43      0.37 0.37     \n#>      pn13 pn14 pn15 pn16 pn17 pn18 pn19 pn20\n#> pn1  0.49      0.41      0.56 0.47          \n#> pn2       0.41      0.31           0.46 0.42\n#> pn3       0.81      0.33           0.56 0.42\n#> pn4  0.33      0.32      0.37 0.34          \n#> pn6  0.33                0.39 0.45          \n#> pn7  0.39      0.33      0.49 0.46          \n#> pn8       0.46      0.38           0.48 0.43\n#> pn9  0.43      0.36      0.46 0.46          \n#> pn10                0.31           0.34 0.37\n#> pn11      0.32      0.35           0.39 0.37\n#> pn12 0.58      0.51      0.64 0.47          \n#> pn13           0.36      0.55 0.58          \n#> pn14                0.36           0.56 0.45\n#> pn15 0.36                0.62 0.41          \n#> pn16      0.36                     0.35     \n#> pn17 0.55      0.62           0.58          \n#> pn18 0.58      0.41      0.58               \n#> pn19      0.56      0.35                0.75\n#> pn20      0.45                     0.75"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"faktoranalyse","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3 Faktoranalyse","text":"Det er flere teknikker assosiert med begrepet faktoranalyse, men hovedsak kan vi dele disse inn typer: eksplorerende og konfirmerende (Hoyle 2000; Hurley et al. 1997).Eksplorerende faktoranalyse som søker å gruppere variabler et datasett ut fra korrelasjon uten spesifikke hypoteser på forhånd om hvordan strukturen ser ut. Denne tilnærmingen er således induktiv og mindre grad drevet av teori - “one can always subject data set EFA necessarily CFA” (Schriesheim Hurley et al. 1997, s.672).Konfirmerende faktoranalyse starter andre enden - med forhåndshypoteser om dataene og strukturen. ønsker vi å bekrefte (konfirmere) en antatt datastruktur vårt datasett, f.eks. ut fra teoretiske forventninger og tidligere undersøkelser.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksplorerende-faktoranalyse-efa","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.1 Eksplorerende faktoranalyse (EFA)","text":"Hensikten med en ekspolerende faktoranalyse er altså å undersøke om vi har variabler som korrelerer med hverandre og se om disse kan grupperes på en meningsfull måte. Vi ser på graden av korrelasjon - variabler som er sterkt korrelerte grupperes og skilles fra andre som er mindre korrelerte (som igjen kan inneholde grupper av relativt sterkt korrelerte variabler). Målet er altså å få grupper av variabler som internt er sterkt korrelerte med hverandre, og lite korrelert med variabler utenfor gruppen.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"hva-er-en-faktor","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.1.1 Hva er en faktor?","text":"En faktor kan ses på som en skjult variabel - en variabel vi ikke kan observere eller måle direkte, men som påvirker flere andre synlige/målbare variabler.Variablene , B, C osv er med andre ord observerbare/målbare fenomen underliggende, skjulte faktorer. Når vi grupperer variabler som er høyt korrelerte antar vi deres variasjon og korrelasjon skyldes den underliggende/skjulte variabelen.Et typisk eksempel er den såkalte “five-factor model” (McCrae John 1992) som antar personlighetstrekk kan grupperes fem faktorer: Åpenhet, planmessighet, ekstroversjon, omgjengelighet og nevrotisisme (se f.eks. Kennair (2021) en kort introduksjon til modellen på norsk). Man kan imidlertid ikke måle disse fem faktorene direkte, men man antar de påvirker en rekke målbare fohold. Disse kan man spørre om/måle/observere. Faktoren planmessighet kan eksempel påvirke spørsmål/atferd som “Jeg er alltid forberedt”, “Jeg følger en plan” eller “Jeg utfører mine oppgaver med en gang de er gitt”.La oss se på dette visuelt på en forenklet framstilling. Vi har en teoretisk modell, der vi sier positive tilbakemeldinger på jobben predikerer jobbtilfredshet, økonomi predikerer tilfredshet hjemmet, og tilsammen predikerer jobbtilfredshet og tilfredshet hjemmet den totale personlige tilfredsheten:Dette er det vi teoretisk forventer og vår modell. Når vi samler data ser vi alltid dataene (selvsagt) aldri passer perfekt inn vår teoretiske modell. figuren er våre faktiske (empiriske) data fra vår undersøkelse representert gjennom de fargede sirklene som knyttes til sin respektive variabel.Så det vi faktisk ser - empirisk - er egentlig dette:Ikke alle målte variabler måler sterkest på den underliggende faktoret, det er overlapping mellom variablene og hva de måler (og det kan se langt verre ut enn figuren ).Faktoranalysen vil hjelpe oss å rydde litt opp dette, ved å se på hvilke variabler som faktisk (ikker teoretisk) korrelerer sterkt med hvilke, og hvilke som korrelerer svakt, så å hjelpe oss strukturere modellen vi tester.figuren kan vi se vi nok har en struktur av sterkt korrelerte variabler som måler “sine” underliggende faktorer, men vi ser også det er en gruppe som teoretisk burde ligge nørmere sine respektive faktorer, men som ser ut til å klumpe seg midten. Kanskje dette er en bedre representasjon?må vi trolig gå tilbake til vårt teoretiske utgangspunkt og spørreskjemate å se på om vi har funnet en ny faktor, eller om vi skal utelate enkelte målinger/sørsmål å få en bedre modell.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"eksempel","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.1.2 Eksempel","text":"Vi skal bruke et generert datasett gjennomgang av eksplorerende faktoranalyse.Download fa_spm.xlsxDownload fa_spm.savDownload fa_spm.dtaVi kan se oss vi har stilt en rekke mennesker 9 spørsmål der de har svart på en skala fra 1-4. Det vi ønsker å se er om disse spørsmålene kan si noe om en eller flere latente variabler. De 9 spørsmålene er altså direkte målt, og vi vil se om vi kan si noe om latente variabler ut fra dette.Inngangsverderdiene en faktoranalyse er korrelasjonsmatrisen som “tygges” (programvare) til en struktur/et mønster.","code":"\n# Base R\n# Bruker pakken: readxl\nfa_spm <- as.data.frame(read_excel(\"fa_spm.xlsx\"))\nsummarytools::descr(fa_spm)\n#> Descriptive Statistics  \n#> fa_spm  \n#> N: 366  \n#> \n#>                      spm_1    spm_2    spm_3    spm_4    spm_5    spm_6    spm_7    spm_8    spm_9\n#> ----------------- -------- -------- -------- -------- -------- -------- -------- -------- --------\n#>              Mean     1.42     1.28     1.58     2.10     2.86     2.64     1.94     2.48     1.95\n#>           Std.Dev     0.66     0.59     0.72     0.98     0.91     0.89     0.91     1.01     1.01\n#>               Min     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00\n#>                Q1     1.00     1.00     1.00     1.00     2.00     2.00     1.00     2.00     1.00\n#>            Median     1.00     1.00     1.00     2.00     3.00     3.00     2.00     2.50     2.00\n#>                Q3     2.00     1.00     2.00     3.00     4.00     3.00     3.00     3.00     3.00\n#>               Max     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00     4.00\n#>               MAD     0.00     0.00     0.00     1.48     1.48     1.48     1.48     0.74     1.48\n#>               IQR     1.00     0.00     1.00     2.00     2.00     1.00     2.00     1.00     2.00\n#>                CV     0.46     0.47     0.46     0.47     0.32     0.34     0.47     0.41     0.52\n#>          Skewness     1.50     2.24     1.04     0.32    -0.35     0.05     0.59     0.00     0.64\n#>       SE.Skewness     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13     0.13\n#>          Kurtosis     1.86     4.63     0.43    -1.08    -0.74    -0.84    -0.62    -1.11    -0.86\n#>           N.Valid   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00   366.00\n#>         Pct.Valid   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00   100.00"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"korrelasjonsmatrise","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.1.3 Korrelasjonsmatrise","text":"Vi ser på korrelasjonsmatrisen.Henter fram antall korrelasjoner 0.9.Videre tar vi vekk verdier 0.3 og de som = 1:","code":"\n# Bruker pakken: rstatix\nfa_spm_cor <- round(cor(fa_spm, use=\"complete.obs\"),2)\npull_lower_triangle(fa_spm_cor, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4  0.19  0.22  0.21                              \n#> 5   spm_5  0.07  0.17  0.12  0.46                        \n#> 6   spm_6  0.17  0.23  0.21  0.47   0.7                  \n#> 7   spm_7  0.34  0.33  0.27  0.18  0.11  0.26            \n#> 8   spm_8  0.26  0.27  0.25  0.25  0.29  0.34  0.52      \n#> 9   spm_9  0.35   0.3  0.23  0.17  0.22  0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9\nlength(fa_spm_cor[fa_spm_cor>0.9 & fa_spm_cor<1])\n#> [1] 0\n# Bruker pakken: rstatix\nunder03x<- as.data.frame(apply(fa_spm_cor, 2, function(x) ifelse (abs(x) > 0.3 & (x) < 1,x,\"\")))\npull_lower_triangle(under03x, diagonal = FALSE)\n#>   rowname spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8\n#> 1   spm_1                                                \n#> 2   spm_2  0.59                                          \n#> 3   spm_3   0.5  0.54                                    \n#> 4   spm_4                                                \n#> 5   spm_5                    0.46                        \n#> 6   spm_6                    0.47   0.7                  \n#> 7   spm_7  0.34  0.33                                    \n#> 8   spm_8                                0.34  0.52      \n#> 9   spm_9  0.35                          0.32  0.63  0.54\n#>   spm_9\n#> 1      \n#> 2      \n#> 3      \n#> 4      \n#> 5      \n#> 6      \n#> 7      \n#> 8      \n#> 9"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kmo-og-bartletts","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.1.4 KMO og Bartletts","text":"Vi bruker KMO og Bartletts til å se på “faktoriserbarheten”. Kaiser (1974) anbefaler 0.60 som cut-verdi. Bartletts test er signifkant. Bartletts test sammenligner våre faktiske korrelasjonsmatrise med en “identity matrix”. “Identity matrix” er en konstruert korrelasjonsmatrise med verdi 1 diagnoalen og 0 på alle andre korrelasjoner:Vi forventer selvsagt korrelasjon vår korrelasjonsmatrise, og siden Bartletts test bruker nullhypotsene om det ikke er korrelasjon, forteller en signifikant test det er meningsfullt å gjennomføre en datareduksjonsteknikk. Hvis vår korrelasjonsmatrise ikke er signifikant forskjellig fra en matrise med null korrelasjon mellom variablene gir det ingen mening å se etter strukturer av korrelasjoner.tillegg kan vi se på “determinant”:En positiv verdi indikerer også datasettet egner seg faktoranalyse.","code":"\n# Bruker pakken: EFAtools\nKMO(fa_spm_cor)\n#> \n#> -- Kaiser-Meyer-Olkin criterion (KMO) ----------------------\n#> \n#> v The overall KMO value for your data is middling.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   Overall: 0.779\n#> \n#>   For each variable:\n#> spm_1 spm_2 spm_3 spm_4 spm_5 spm_6 spm_7 spm_8 spm_9 \n#> 0.791 0.790 0.821 0.870 0.659 0.729 0.775 0.863 0.783\nBARTLETT(fa_spm_cor, N = nrow(fa_spm))\n#> \n#> v The Bartlett's test of sphericity was significant at an alpha level of .05.\n#>   These data are probably suitable for factor analysis.\n#> \n#>   <U+0001D712>²(36) = 1157.84, p < .001#> [1] \"Identity Matrix\"\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]    1    0    0    0    0    0    0\n#> [2,]    0    1    0    0    0    0    0\n#> [3,]    0    0    1    0    0    0    0\n#> [4,]    0    0    0    1    0    0    0\n#> [5,]    0    0    0    0    1    0    0\n#> [6,]    0    0    0    0    0    1    0\n#> [7,]    0    0    0    0    0    0    1\n# Bruker pakken: Matrix\ndet(fa_spm_cor)\n#> [1] 0.04052472"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"antall-faktorer","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.2 Antall faktorer","text":"","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"kaisers-kriterium-1","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.2.1 Kaisers kriterium","text":"Kaisers kriterium tilsier 3 faktorer.","code":"\n# Bruker pakken: nFactors\neigenComputes(fa_spm)\n#> [1] 3.5289174 1.6219116 1.2294074 0.6032709 0.5332548\n#> [6] 0.4581451 0.4093023 0.3446454 0.2711450"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"scree-plott-2","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.2.2 Scree plott","text":"Det kan se ut til albuen/knekkpunktet tilsier 3 faktorer.","code":"\n# Bruker pakken: EFAtools\nSCREE(fa_spm_cor, eigen_type = \"EFA\")\n#> \n#> Eigenvalues were found using EFA."},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"parallell-analyse-2","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.2.3 Parallell analyse","text":"Dette peker mot tre faktorer.","code":"\n# Bruker pakken: EFAtools\nPARALLEL(fa_spm, eigen_type = \"EFA\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\n#> Parallel Analysis performed using 1000 simulated random data sets\n#> Eigenvalues were found using EFA\n#> \n#> Decision rule used: means\n#> \n#> -- Number of factors to retain according to ----------------\n#> \n#> ( ) EFA-determined eigenvalues:  3"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"analyse","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.3 Analyse","text":"Vi kjører en faktoranalyse med 3 faktorer uten rotasjon.Vi kan se på Unrotated loadings hvordan de ulike spørsmålene lader på de tre faktorene. Det er ikke gitt bildet er helt enkelt å tolke. noen spørsmål - som 1, 2, og 3 - ser vi loadings på alle tre faktorene. andre er det klarere loading på en faktor, eller positivt på en og negativt på en annen. Et hjelpemiddler å tolke modellen er rotasjon. Rotasjon innebærer egnetlig brae å se på variablene og faktorene fra en annen vinkelt. Som Hartmann, Krois, Waske (2018) påpeker: “purpose rotation produce factors mix high low loadings moderate-sized loadings. idea give meaning factors, helps interpret . mathematical viewpoint, difference rotated unrotated matrix. fitted model , uniquenesses , proportion variance explained ”.Det finnes hovedgrupper rotasjoner: ortogonal og oblikk. Blant ortogonale rotasjonsteknikker finner vi varimax, quartimax og equimax. (Direct) oblimin og promax er vanlige oblikke rotasjoner. En hovedforskjell er ved ortogonal rotasjon tillates ikke faktorene er korrelerte, mens ved oblikk rotasjon kan faktorene korrelere. må vi altså gå tilbake til vår teoretiske forståelse av hva vi undersøker. Svært ofte samfunnsvitenskapene (vil vi hevde) ønsker vi å tillate faktorene kan korrelere ved rotasjon (vi antar veldig mange tilfeller vil dette være teoretisk fornuftig). så fall bør vi bruke oblikk rotasjon. Hvis vi har teoretiske vurderinger som tilsier faktorene ikke korrelerer velger vi ortogonalt.kapittelet om PCA viste vi ortogonal rotasjon. Dette innebærer aksene forblir ortogonale på hverandre, mens ved oblikk rotasjon kan aksenens vinkler på hverandre variere.Når vi kjører en ny faktoranalyse med 3 faktorer ser det slik ut:Vi ser en klar struktur hvilke spørsmål som lader på hvilke faktorer (dette eksempelet er konstruert å vise en veldig klar faktorstruktur, ofte vil det være større rom tolkning).","code":"\n# Bruker pakken: EFAtool\nurotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\nurotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'none'.\n#> \n#> -- Unrotated Loadings --------------------------------------\n#> \n#>           F1      F2      F3  \n#> spm_1     .502    .454    .338\n#> spm_2     .556    .371    .418\n#> spm_3     .470    .318    .378\n#> spm_4     .499   -.241    .119\n#> spm_5     .635   -.592    .056\n#> spm_6     .711   -.428    .016\n#> spm_7     .594    .395   -.364\n#> spm_8     .605    .151   -.282\n#> spm_9     .638    .289   -.386\n#> \n#> -- Variances Accounted for ---------------------------------\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> -- Model Fit -----------------------------------------------\n#> \n#> <U+0001D712>²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50\n# Bruker pakken: EFAtool\nmrotasjon <- EFA(fa_spm, n_factors = 3, method = \"ML\", rotation = \"promax\")\n#> i 'x' was not a correlation matrix. Correlations are found from entered raw data.\nmrotasjon\n#> \n#> EFA performed with type = 'EFAtools', method = 'ML', and rotation = 'promax'.\n#> \n#> -- Rotated Loadings ----------------------------------------\n#> \n#>           F1      F3      F2  \n#> spm_1    -.066    .076    .732\n#> spm_2     .056   -.022    .782\n#> spm_3     .049   -.043    .688\n#> spm_4     .521   -.018    .140\n#> spm_5     .903   -.055   -.069\n#> spm_6     .788    .094    .004\n#> spm_7    -.103    .814    .039\n#> spm_8     .142    .623   -.010\n#> spm_9     .013    .808   -.027\n#> \n#> -- Factor Intercorrelations --------------------------------\n#> \n#>       F1      F2      F3  \n#> F1    1.000   0.380   0.268\n#> F2    0.380   1.000   0.504\n#> F3    0.268   0.504   1.000\n#> \n#> -- Variances Accounted for ---------------------------------\n#> \n#>                       F1      F2      F3  \n#> SS loadings           3.063   1.299   0.810\n#> Prop Tot Var          0.340   0.144   0.090\n#> Cum Prop Tot Var      0.340   0.485   0.575\n#> Prop Comm Var         0.592   0.251   0.157\n#> Cum Prop Comm Var     0.592   0.843   1.000\n#> \n#> -- Model Fit -----------------------------------------------\n#> \n#> <U+0001D712>²(12) = 11.82, p = .460\n#> CFI = 1.00\n#> RMSEA [90% CI] = .00 [.00; .05]\n#> AIC = -12.18\n#> BIC = -59.01\n#> CAF = .50"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"model-fit","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.3.1 Model fit","text":"Vi ser vi presenteres flere mål på model fit (se f.eks. Finch (2020)). De ulike indeksene måler ulike aspekter av model fit. Vi skal gå inn på av dem. Husk dette datasettet er generert å vise en utmerket modell - med andre data vil du sjeldent oppleve så gode verdier på model fit som .CFI = Comparative Fit Index. Verdiene kan være mellom 0 og 1, og verdier 0.9 regnes som en god fit (Hu Bentler 1999) (en mer konservativ terskel kan være 0.95).RMSEA = Root Mean Square Error Approximation. oppgis ofte verdiene 0.01, 0.05 og 0.08 som henhodsvis utmerket, god og middels. Finch (2020) viser til 0.05 som en cut-verdi.","code":""},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"sammenlikning-uten-rotasjon-med-ortogonal-varimax-og-oblikk-promax-rotasjon","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.3.2 Sammenlikning uten rotasjon, med ortogonal (varimax) og oblikk (promax) rotasjon","text":"","code":"\nfa_ingenrot <- factanal(fa_spm, factors = 3, rotation = \"none\")\nfa_varimax <- factanal(fa_spm, factors = 3, rotation = \"varimax\")\nfa_promax <- factanal(fa_spm, factors = 3, rotation = \"promax\")\n\npar(mfrow = c(1,3))\nplot(fa_ingenrot$loadings[,1], \n     fa_ingenrot$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Uten rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_varimax$loadings[,1], \n     fa_varimax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med varimax rotasjon\")\nabline(h = 0, v = 0)\n\nplot(fa_promax$loadings[,1], \n     fa_promax$loadings[,2],\n     xlab = \"Faktor 1\", \n     ylab = \"Faktor 2\", \n     ylim = c(-1,1),\n     xlim = c(-1,1),\n     main = \"Med promax rotasjon\")\nabline(h = 0, v = 0)"},{"path":"dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html","id":"konfirmerende-faktoranalyse-cfa","chapter":"Kapittel 4 Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)","heading":"4.3.4 Konfirmerende faktoranalyse (CFA)","text":"","code":""},{"path":"referanser.html","id":"referanser","chapter":"Referanser","heading":"Referanser","text":"","code":""},{"path":"vedlegg-x---session-info.html","id":"vedlegg-x---session-info","chapter":"Vedlegg X - Session Info","heading":"Vedlegg X - Session Info","text":"","code":"#> R version 4.1.2 (2021-11-01)\n#> Platform: x86_64-w64-mingw32/x64 (64-bit)\n#> Running under: Windows 10 x64 (build 19044)\n#> \n#> Locale:\n#>   LC_COLLATE=Norwegian Bokmål_Norway.1252 \n#>   LC_CTYPE=Norwegian Bokmål_Norway.1252   \n#>   LC_MONETARY=Norwegian Bokmål_Norway.1252\n#>   LC_NUMERIC=C                            \n#>   LC_TIME=Norwegian Bokmål_Norway.1252    \n#> \n#> Package version:\n#>   base64enc_0.1.3 bookdown_0.26   brio_1.1.3     \n#>   bslib_0.3.1     cachem_1.0.6    cli_3.2.0      \n#>   compiler_4.1.2  desc_1.4.1      digest_0.6.29  \n#>   downlit_0.4.0   evaluate_0.15   fansi_1.0.3    \n#>   fastmap_1.1.0   fs_1.5.2        glue_1.6.2     \n#>   graphics_4.1.2  grDevices_4.1.2 highr_0.9      \n#>   htmltools_0.5.2 jquerylib_0.1.4 jsonlite_1.8.0 \n#>   knitr_1.39      magrittr_2.0.3  memoise_2.0.1  \n#>   methods_4.1.2   R6_2.5.1        rappdirs_0.3.3 \n#>   rlang_1.0.2     rmarkdown_2.14  rprojroot_2.0.3\n#>   rstudioapi_0.13 sass_0.4.1      stats_4.1.2    \n#>   stringi_1.7.6   stringr_1.4.0   tinytex_0.38   \n#>   tools_4.1.2     utils_4.1.2     vctrs_0.4.1    \n#>   xfun_0.30       xml2_1.3.3      yaml_2.3.5"}]
