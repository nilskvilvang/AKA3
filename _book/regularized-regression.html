<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kapittel 11 Regularized regression | Anvendt kvantitativ analyse</title>
<meta name="author" content="Nils Kvilvang">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Kapittel 11 Regularized regression | Anvendt kvantitativ analyse">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kapittel 11 Regularized regression | Anvendt kvantitativ analyse">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet">
<link href="libs/table1-1.0/table1_defaults.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/viz-1.8.2/viz.js"></script><link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="libs/grViz-binding-1.0.9/grViz.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="I kapittelet om regresjonsanalyse (ikke medregnet logistisk regresjon) har vi brukt minste kvadratsums metode (“Ordinary Least Squares”). Dersom vi har mange forklaringsvariabler i forhold til...">
<meta property="og:description" content="I kapittelet om regresjonsanalyse (ikke medregnet logistisk regresjon) har vi brukt minste kvadratsums metode (“Ordinary Least Squares”). Dersom vi har mange forklaringsvariabler i forhold til...">
<meta name="twitter:description" content="I kapittelet om regresjonsanalyse (ikke medregnet logistisk regresjon) har vi brukt minste kvadratsums metode (“Ordinary Least Squares”). Dersom vi har mange forklaringsvariabler i forhold til...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Anvendt kvantitativ analyse</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduksjon</a></li>
<li><a class="" href="hvorfor-denne-boka.html"><span class="header-section-number">1</span> Hvorfor denne boka?</a></li>
<li><a class="" href="grunnleggende-begreper-og-sammenhenger.html"><span class="header-section-number">2</span> Grunnleggende begreper og sammenhenger</a></li>
<li><a class="" href="samvariasjon-og-korrelasjon.html"><span class="header-section-number">3</span> Samvariasjon og korrelasjon</a></li>
<li><a class="" href="univariat-analyse.html"><span class="header-section-number">4</span> Univariat analyse</a></li>
<li><a class="" href="kjikvadrattest---analyse-av-kategoriske-data.html"><span class="header-section-number">5</span> Kjikvadrattest - Analyse av kategoriske data</a></li>
<li><a class="" href="t-tester.html"><span class="header-section-number">6</span> T-tester</a></li>
<li><a class="" href="variansanalyse---anova-analysis-of-variance.html"><span class="header-section-number">7</span> Variansanalyse - ANOVA (“Analysis of Variance”)</a></li>
<li><a class="" href="variansanalyse---manova-multivariate-analysis-of-variance.html"><span class="header-section-number">8</span> Variansanalyse - MANOVA (“Multivariate Analysis of Variance”)</a></li>
<li><a class="" href="regresjonsanalyse---ols.html"><span class="header-section-number">9</span> Regresjonsanalyse - OLS</a></li>
<li><a class="" href="iv-regresjon.html"><span class="header-section-number">10</span> IV regresjon</a></li>
<li><a class="active" href="regularized-regression.html"><span class="header-section-number">11</span> Regularized regression</a></li>
<li><a class="" href="polynomisk-regresjon.html"><span class="header-section-number">12</span> Polynomisk regresjon</a></li>
<li><a class="" href="logistisk-regresjon.html"><span class="header-section-number">13</span> Logistisk regresjon</a></li>
<li><a class="" href="structural-equation-modelling-sem---strukturelle-regresjonsmodeller.html"><span class="header-section-number">14</span> Structural Equation Modelling (SEM - Strukturelle Regresjonsmodeller)</a></li>
<li><a class="" href="dimension-reduction-factor-analysis-fa-og-principal-component-analysis-pca.html"><span class="header-section-number">15</span> Dimension reduction: Factor Analysis (FA) og Principal Component Analysis (PCA)</a></li>
<li><a class="" href="maskinl%C3%A6ring-machine-learning.html"><span class="header-section-number">16</span> Maskinlæring (Machine Learning)</a></li>
<li><a class="" href="tidsserieanalyser.html"><span class="header-section-number">17</span> Tidsserieanalyser</a></li>
<li><a class="" href="referanser.html">Referanser</a></li>
<li><a class="" href="vedlegg-a---sentralgrenseteoremet-central-limit-theorem.html">Vedlegg A - Sentralgrenseteoremet (Central Limit Theorem)</a></li>
<li><a class="" href="vedlegg-b---chebyshevs-teorem.html">Vedlegg B - Chebyshevs teorem</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nilskvilvang/AKA3">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="regularized-regression" class="section level1" number="11">
<h1>
<span class="header-section-number">Kapittel 11</span> Regularized regression<a class="anchor" aria-label="anchor" href="#regularized-regression"><i class="fas fa-link"></i></a>
</h1>
<p>I kapittelet om regresjonsanalyse (ikke medregnet logistisk regresjon) har vi brukt minste kvadratsums metode (“Ordinary Least Squares”). Dersom vi har mange forklaringsvariabler i forhold til antall observasjoner eller stor korrelasjon mellom forklaringsvariablene (multikolinearitet) kan det være aktuelt å bruke alternative metoder fordi minste kvadratsums metode vil medføre stor varians i estimatorene. <span class="citation">ProjectPro (2021)</span> beskriver dette slik:</p>
<p>“The subset selection methods use ordinary least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. This shrinkage is also known as regularisation.”</p>
<p>I OLS-regresjon (“Ordinary Least Squares”) - der vi altså søker å predikere gjennom en kombinasjon av prediktorer (uavhengige variabler) (og forutsetter normalfordelte feilledd). Vi estimerer parametrene fra et utvalg siden vi ikke kjenner populasjonens “ekte” parametere gjennom å minimalisere de kvadrerte residualene (jfr kapittel om lineær regresjon). De estimerte koeffisientene kan være skjeve/“feil” på to måter: bias og varians. Variansen er et mål på usikkerheten i estimatene gjennom spredningen. Bias er forskjellen mellom populasjonenens sanne parameter og den forventede parameteren. <span class="citation">Arya (2022)</span> illustrerer dette slik:</p>
<div class="figure">
<img src="biasvarians.png" alt=""><p class="caption">Bilde fra kdnuggets.com</p>
</div>
<p>Vi ønsker selvsagt at både varians og bias skal være så liten som mulig siden det vil gi oss gode prediksjoner, men like selvsagt er det at det åpenbart varierer. En OLS-estimator er “unbiased”, men kan ha en stor varians - spesielt ved multikolinearitet og et høyt antall uavhengige variabler. Det regularized regression søker å gjøre er å redusere variansen, med den implisitte effekten at bias økes noe. Vi kan se på forholdet mellom bias, varians og modellkompleksitet slik:</p>
<div class="figure">
<img src="biasvarianstradeoff.png" style="width:85.0%" alt=""><p class="caption">Bilde fra Wikipedia</p>
</div>
<p>Ettersom modellens kompleksitet øker reduseres bias (fordi vi antar at våre estimater vil ligge nærmere populasjonens sanne verdi med økt antall variabler). Regularisering er altså et forsøk på å finne det optimale punketet der bias og varians tilsammen gir lavest mulig total feil.</p>
<p>Som <span class="citation">Boehmke and Greenwell (2020)</span> uttrykker det:</p>
<p>“Regularization methods provide a means to constrain or regularize the estimated coefficients, which can reduce the variance and decrease out of sample error.”</p>
<p>Regularisering innebærer dermed at man legger begrensninger på ikke-betydningsfulle regresjonskoeffiseinter mot 0. Hensikten med dette er bl.a. lettere tolkbare resultater (for L2/lasso - L1/ridge gir ikke denne effekten). Hovedhensikten er også å gjøre modellen mer generaliserbar fordi effekten av ikke-betydningsfulle prediktorer reduseres (ved at koeffisientene til disse reduseres mot 0 eller fjernes). Det primære anvendelsesområdet kan derfor sies å være tilfeller der vi har komplekse modeller - modeller med mange variabler. En effekt - som vi ikke går nærmere inn på her - er også at metodene er raskere enn mange alternativer ved svært store datasett. Dette kan være spesielt interessant i maskinlæringstilfeller. En siste effekt vi kan nevne her er at disse metodene vil fungere også når antallet prediktorer er høyere enn antall observasjoner - noe som er problematisk i andre regresjonsmetoder.</p>
<p>Ofte snakker man om tre metoder for såkalt “Regularized Regression”:</p>
<ol style="list-style-type: decimal">
<li><p>Ridge regression - også kalt L2-regresjon - søker å minimere koeffisientenes kvadratsum. L2 vil påvirke store koeffisienter mer enn små, siden de koeffisientene som er små/nærme 0 ikke vil reduseres relativt sett like mye som større koeffisienter. Man får dermed en modell med flere koeffisienter nærme 0 (men ikke 0). Ridge/L2 vil normalt fungere bedre enn L1 dersom vi har et stort antall uavhengige variabler som alle bidrar i mdoellen (få/ingen prediktorer som har vesentlig høyere påvirkning enn øvrige prediktorer).</p></li>
<li><p>Lasso regression (“Least Absolute Shrinkage and Selection Operator”) - også kalt L1-regresjon - søker å minimere den absolutte verdien av regresjonskoeffisientene. L1 vil altså redusere noen koeffisienter til 0 (eller i praksis eksludere dem fra modellen), noe som i realiteten er en implisitt varaibelseleksjon. L1 - lasso regression - er dermed egnet dersom man f.eks. i en eksplorerende analyse ønsker å foreta et utvalg blant mange variabler. Lasso regresjon/L1 vil gi bedre resultater/bedre prediktiv evne enn ridge/L2 dersom man har relativt få uavhengige variabler som er gode prediktorer og øvrige uavhengige variabler ikke har stor påvirkning.</p></li>
<li><p>Elastic-Net regression - er en kombinasjon av ridge og lasso.</p></li>
</ol>
<p>I disse tilnærmingene tilfører vi en “shrinkage” på koeffisientene gjennom en straffeparameter som som regel bete3gnes <span class="math inline">\(\lambda\)</span> (lambda). Jo større <span class="math inline">\(\lambda\)</span>-verdien er, jo mer reduserer vi koeffisientene. Som vi vil vise under kan vi teste for hva den optimale <span class="math inline">\(\lambda\)</span>-verdien er, f.eks. gjennom kryssvalidering (som vi omtaler mer i detalj andre steder i boka).</p>
<div id="ridge-regresjon" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Ridge regresjon<a class="anchor" aria-label="anchor" href="#ridge-regresjon"><i class="fas fa-link"></i></a>
</h2>
<p>En av disse er “ridge regression” <span class="citation">(Hoerl and Kennard 1970a, 1970b)</span>. Vi tenker det er i overkant teknisk å gå inn i utregning (se f.eks. <a href="https://www.publichealth.columbia.edu/research/population-health-methods/ridge-regression">her</a> for en rimelig bra forklaring (på engelsk), men poenget er å få estimatorer som er mer “korrekt” under de betingelsene som er nevnt over. Ridge regresjon utvider lineær regresjon ved å ta hensyn til kompleksiteten i modellen ved å introdusere en “straffeparameter” (“penalty parameter”) som utgjøres av den kvadrerte størrelsen av koeffisientene.</p>
<p>For å illustrere dette bruker vi et eksempel fra <span class="citation">Zach (2020)</span> og et innebygd og ofte brukt datasett i R (“mtcars”). Den avhengige variabelen er “hp” (“gross horsepower”), og “mpg” (“Miles pr gallon”), “wt” (“weight”), “drat” (“Rear axle ratio”) og “qsec” (“1/4 mile time”) som de uavhengige variablene.
Den avhengige variabelen er da:</p>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.matrix.html">data.matrix</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">hp</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="co">#&gt;      [,1]</span></span>
<span><span class="co">#&gt; [1,]  110</span></span>
<span><span class="co">#&gt; [2,]  110</span></span>
<span><span class="co">#&gt; [3,]   93</span></span>
<span><span class="co">#&gt; [4,]  110</span></span>
<span><span class="co">#&gt; [5,]  175</span></span>
<span><span class="co">#&gt; [6,]  105</span></span></code></pre></div>
<p>Og de uavhengige variablene:</p>
<div class="sourceCode" id="cb374"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.matrix.html">data.matrix</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'mpg'</span>, <span class="st">'wt'</span>, <span class="st">'drat'</span>, <span class="st">'qsec'</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">x</span></span>
<span><span class="co">#&gt;                      mpg    wt drat  qsec</span></span>
<span><span class="co">#&gt; Mazda RX4           21.0 2.620 3.90 16.46</span></span>
<span><span class="co">#&gt; Mazda RX4 Wag       21.0 2.875 3.90 17.02</span></span>
<span><span class="co">#&gt; Datsun 710          22.8 2.320 3.85 18.61</span></span>
<span><span class="co">#&gt; Hornet 4 Drive      21.4 3.215 3.08 19.44</span></span>
<span><span class="co">#&gt; Hornet Sportabout   18.7 3.440 3.15 17.02</span></span>
<span><span class="co">#&gt; Valiant             18.1 3.460 2.76 20.22</span></span>
<span><span class="co">#&gt; Duster 360          14.3 3.570 3.21 15.84</span></span>
<span><span class="co">#&gt; Merc 240D           24.4 3.190 3.69 20.00</span></span>
<span><span class="co">#&gt; Merc 230            22.8 3.150 3.92 22.90</span></span>
<span><span class="co">#&gt; Merc 280            19.2 3.440 3.92 18.30</span></span>
<span><span class="co">#&gt; Merc 280C           17.8 3.440 3.92 18.90</span></span>
<span><span class="co">#&gt; Merc 450SE          16.4 4.070 3.07 17.40</span></span>
<span><span class="co">#&gt; Merc 450SL          17.3 3.730 3.07 17.60</span></span>
<span><span class="co">#&gt; Merc 450SLC         15.2 3.780 3.07 18.00</span></span>
<span><span class="co">#&gt; Cadillac Fleetwood  10.4 5.250 2.93 17.98</span></span>
<span><span class="co">#&gt; Lincoln Continental 10.4 5.424 3.00 17.82</span></span>
<span><span class="co">#&gt; Chrysler Imperial   14.7 5.345 3.23 17.42</span></span>
<span><span class="co">#&gt; Fiat 128            32.4 2.200 4.08 19.47</span></span>
<span><span class="co">#&gt; Honda Civic         30.4 1.615 4.93 18.52</span></span>
<span><span class="co">#&gt; Toyota Corolla      33.9 1.835 4.22 19.90</span></span>
<span><span class="co">#&gt; Toyota Corona       21.5 2.465 3.70 20.01</span></span>
<span><span class="co">#&gt; Dodge Challenger    15.5 3.520 2.76 16.87</span></span>
<span><span class="co">#&gt; AMC Javelin         15.2 3.435 3.15 17.30</span></span>
<span><span class="co">#&gt; Camaro Z28          13.3 3.840 3.73 15.41</span></span>
<span><span class="co">#&gt; Pontiac Firebird    19.2 3.845 3.08 17.05</span></span>
<span><span class="co">#&gt; Fiat X1-9           27.3 1.935 4.08 18.90</span></span>
<span><span class="co">#&gt; Porsche 914-2       26.0 2.140 4.43 16.70</span></span>
<span><span class="co">#&gt; Lotus Europa        30.4 1.513 3.77 16.90</span></span>
<span><span class="co">#&gt; Ford Pantera L      15.8 3.170 4.22 14.50</span></span>
<span><span class="co">#&gt; Ferrari Dino        19.7 2.770 3.62 15.50</span></span>
<span><span class="co">#&gt; Maserati Bora       15.0 3.570 3.54 14.60</span></span>
<span><span class="co">#&gt; Volvo 142E          21.4 2.780 4.11 18.60</span></span></code></pre></div>
<p>Modellen blir da:</p>
<div class="sourceCode" id="cb375"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridgemodell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ridgemodell</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Length Class     Mode   </span></span>
<span><span class="co">#&gt; a0        100    -none-    numeric</span></span>
<span><span class="co">#&gt; beta      400    dgCMatrix S4     </span></span>
<span><span class="co">#&gt; df        100    -none-    numeric</span></span>
<span><span class="co">#&gt; dim         2    -none-    numeric</span></span>
<span><span class="co">#&gt; lambda    100    -none-    numeric</span></span>
<span><span class="co">#&gt; dev.ratio 100    -none-    numeric</span></span>
<span><span class="co">#&gt; nulldev     1    -none-    numeric</span></span>
<span><span class="co">#&gt; npasses     1    -none-    numeric</span></span>
<span><span class="co">#&gt; jerr        1    -none-    numeric</span></span>
<span><span class="co">#&gt; offset      1    -none-    logical</span></span>
<span><span class="co">#&gt; call        4    -none-    call   </span></span>
<span><span class="co">#&gt; nobs        1    -none-    numeric</span></span></code></pre></div>
<p>Vi kan legge merke til at i koden velger vi alpha = 0. Glmnet = 0 gir ridge regresjon, alpha = 1 gir lasso regresjon, og alpha mellom 0 og 1 gir elastic-net regresjon. Vi kan også merke også at disse typene regresjon forutsetter standardisering, men dette ivaretar glmnet for oss.
Vi kommenterer ikke modellen på dette stadiet. Neste steg blir å velge en såkalt lambdaverdi (<span class="math inline">\(\lambda\)</span>) som gir lavest MSE (Mean Square Error). Dette gjør vi gjennom en kryssvalidering (“cv.glmnet” som default velger k=10 folds).</p>
<div class="sourceCode" id="cb376"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_ridgemodell</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">beste_lambda_ridge</span> <span class="op">&lt;-</span> <span class="va">cv_ridgemodell</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">beste_lambda_ridge</span></span>
<span><span class="co">#&gt; [1] 9.153244</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_ridgemodell</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<p>Vi kan da optimalisere modellen gjennom den identifiserte beste <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beste_modell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span>, lambda <span class="op">=</span> <span class="va">beste_lambda_ridge</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">beste_modell</span><span class="op">)</span></span>
<span><span class="co">#&gt; 5 x 1 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">#&gt;                      s0</span></span>
<span><span class="co">#&gt; (Intercept) 476.3006630</span></span>
<span><span class="co">#&gt; mpg          -3.3010159</span></span>
<span><span class="co">#&gt; wt           19.7334277</span></span>
<span><span class="co">#&gt; drat         -0.8298609</span></span>
<span><span class="co">#&gt; qsec        -18.1411255</span></span></code></pre></div>
<p>Et interessant plott er et “trace-plott” som viser hvordan koeffisientene endrer seg som følge av endret lamda-verdi:</p>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ridgemodell</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"bottomright"</span>, lwd <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, cex <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-7-1.png" width="672"></div>
<p>Til slutt kan vi se på <span class="math inline">\(R^2\)</span> for beste modell:</p>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_prediksjon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridgemodell</span>, s <span class="op">=</span> <span class="va">beste_lambda_ridge</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="va">sst</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">sse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_prediksjon</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">rsq</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">sse</span><span class="op">/</span><span class="va">sst</span></span>
<span><span class="va">rsq</span></span>
<span><span class="co">#&gt; [1] 0.8009616</span></span></code></pre></div>
<p>Beste modell kan dermed forklare 80.1 % av variansen i den avhengige variabelen.</p>
</div>
<div id="lasso-regression" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Lasso regression<a class="anchor" aria-label="anchor" href="#lasso-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Vi fortsetter med eksempelet ovenfor, og som nevnt kan vi endre alpha-verdien fra 0 til 1 for å skifte fra ridge regresjon til lasso regresjon <span class="citation">(Tibshirani 1996)</span>.</p>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lassomodell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lassomodell</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Length Class     Mode   </span></span>
<span><span class="co">#&gt; a0         64    -none-    numeric</span></span>
<span><span class="co">#&gt; beta      256    dgCMatrix S4     </span></span>
<span><span class="co">#&gt; df         64    -none-    numeric</span></span>
<span><span class="co">#&gt; dim         2    -none-    numeric</span></span>
<span><span class="co">#&gt; lambda     64    -none-    numeric</span></span>
<span><span class="co">#&gt; dev.ratio  64    -none-    numeric</span></span>
<span><span class="co">#&gt; nulldev     1    -none-    numeric</span></span>
<span><span class="co">#&gt; npasses     1    -none-    numeric</span></span>
<span><span class="co">#&gt; jerr        1    -none-    numeric</span></span>
<span><span class="co">#&gt; offset      1    -none-    logical</span></span>
<span><span class="co">#&gt; call        4    -none-    call   </span></span>
<span><span class="co">#&gt; nobs        1    -none-    numeric</span></span></code></pre></div>
<p>Identifisering av optimal lambda:</p>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_lassomodell</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">beste_lambda_lasso</span> <span class="op">&lt;-</span> <span class="va">cv_lassomodell</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">beste_lambda_lasso</span></span>
<span><span class="co">#&gt; [1] 2.215202</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_lassomodell</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-10-1.png" width="672"></div>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lassomodell</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"bottomright"</span>, lwd <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, cex <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-11-1.png" width="672"></div>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beste_lassomodell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span>, lambda <span class="op">=</span> <span class="va">beste_lambda_lasso</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">beste_lassomodell</span><span class="op">)</span></span>
<span><span class="co">#&gt; 5 x 1 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">#&gt;                     s0</span></span>
<span><span class="co">#&gt; (Intercept) 486.013958</span></span>
<span><span class="co">#&gt; mpg          -2.916499</span></span>
<span><span class="co">#&gt; wt           21.989615</span></span>
<span><span class="co">#&gt; drat          .       </span></span>
<span><span class="co">#&gt; qsec        -19.692037</span></span></code></pre></div>
<p>Her kan vi se at lassoregresjonen har redusert koeffisienten til “drat” til 0 (variabelen er dermed ikke i beste modell). Lassoregresjon kan altså redusere variablers koeffisienter <strong>til</strong> 0, mens ridgeregresjon reduserer kan redusere koeffisientene <strong>mot</strong> 0.</p>
<p>Vi kan til slutt se på <span class="math inline">\(R^2\)</span> for beste modell:</p>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_prediksjon2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lassomodell</span>, s <span class="op">=</span> <span class="va">beste_lambda_lasso</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="va">sst2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">sse2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_prediksjon2</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">rsq2</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">sse2</span><span class="op">/</span><span class="va">sst2</span></span>
<span><span class="va">rsq2</span></span>
<span><span class="co">#&gt; [1] 0.8047073</span></span></code></pre></div>
<p>Beste modell kan dermed forklare 80.5 % av variansen i den avhengige variabelen.</p>
</div>
<div id="elastic-net-regresjon" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Elastic-net regresjon<a class="anchor" aria-label="anchor" href="#elastic-net-regresjon"><i class="fas fa-link"></i></a>
</h2>
<p>Som vi har sett over vil lasso regresjon fungere som “variabel selection”. En utfordring med denne metoden kan oppstå når man to sterkt korrelerte variabler “dyttes” mot null er at den ene kan falle ut av modellen mens den andre forblir i modellen (selv om de altså er sterkt korrelerte). Dette vil også kunne opptre usystematisk. Ridge regresjon kan i slike tilfeller fungere noe bedre, men denne vil altså <em>ikke</em> utelukke variabler helt. Kombinasjonen av de to - elastic-net <span class="citation">(Zou and Hastie 2005)</span> - søker å kombinere egenskapene for de to.</p>
<p>I modellen under setter vi en alpha-verdi mellom 0 og 1. Dersom vi setter den til 0.5 ber vi om at ridge og lasso vektes like tungt. Hvis alpha settes &lt; 0.5 vil modellen lene seg tyngre mot ridge, og med alpha &gt; 0.5 vil den lene seg tyngre mot lasso.</p>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">en_modell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">en_modell</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Length Class     Mode   </span></span>
<span><span class="co">#&gt; a0         69    -none-    numeric</span></span>
<span><span class="co">#&gt; beta      276    dgCMatrix S4     </span></span>
<span><span class="co">#&gt; df         69    -none-    numeric</span></span>
<span><span class="co">#&gt; dim         2    -none-    numeric</span></span>
<span><span class="co">#&gt; lambda     69    -none-    numeric</span></span>
<span><span class="co">#&gt; dev.ratio  69    -none-    numeric</span></span>
<span><span class="co">#&gt; nulldev     1    -none-    numeric</span></span>
<span><span class="co">#&gt; npasses     1    -none-    numeric</span></span>
<span><span class="co">#&gt; jerr        1    -none-    numeric</span></span>
<span><span class="co">#&gt; offset      1    -none-    logical</span></span>
<span><span class="co">#&gt; call        4    -none-    call   </span></span>
<span><span class="co">#&gt; nobs        1    -none-    numeric</span></span></code></pre></div>
<p>Identifisering av optimal lambda:</p>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_en_modell</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="va">beste_lambda_en</span> <span class="op">&lt;-</span> <span class="va">cv_en_modell</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">beste_lambda_en</span></span>
<span><span class="co">#&gt; [1] 5.08951</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_en_modell</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-15-1.png" width="672"></div>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">en_modell</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"bottomright"</span>, lwd <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, cex <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11_files/figure-html/unnamed-chunk-16-1.png" width="672"></div>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beste_en_modell</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, lambda <span class="op">=</span> <span class="va">beste_lambda_en</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">beste_en_modell</span><span class="op">)</span></span>
<span><span class="co">#&gt; 5 x 1 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">#&gt;                     s0</span></span>
<span><span class="co">#&gt; (Intercept) 481.323552</span></span>
<span><span class="co">#&gt; mpg          -3.180651</span></span>
<span><span class="co">#&gt; wt           20.477663</span></span>
<span><span class="co">#&gt; drat          .       </span></span>
<span><span class="co">#&gt; qsec        -18.859391</span></span></code></pre></div>
<div class="sourceCode" id="cb389"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_prediksjon3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">en_modell</span>, s <span class="op">=</span> <span class="va">beste_lambda_en</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="va">sst3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">sse3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_prediksjon3</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">rsq3</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">sse3</span><span class="op">/</span><span class="va">sst3</span></span>
<span><span class="va">rsq3</span></span>
<span><span class="co">#&gt; [1] 0.8030335</span></span></code></pre></div>
<p>Beste modell kan dermed forklare 80.3 % av variansen i den avhengige variabelen.</p>
</div>
<div id="ridge-vs-lasso-vs-elastic-net" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Ridge vs Lasso vs Elastic-net<a class="anchor" aria-label="anchor" href="#ridge-vs-lasso-vs-elastic-net"><i class="fas fa-link"></i></a>
</h2>
<p>Vi kan sammenblikne <span class="math inline">\(R^2\)</span> for de tre teknikkene:</p>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rsq4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="st">"R-squared"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">rsq</span>, <span class="va">rsq2</span>, <span class="va">rsq3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">rsq4</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Ridge regression"</span>, <span class="st">"Lasso regression"</span>, <span class="st">"Elastic-net"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">rsq4</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  R-squared</span></span>
<span><span class="co">#&gt; Ridge regression 0.8009616</span></span>
<span><span class="co">#&gt; Lasso regression 0.8047073</span></span>
<span><span class="co">#&gt; Elastic-net      0.8030335</span></span></code></pre></div>
<p>De gir realtivt like <span class="math inline">\(R^2\)</span>-verdier. Som tidligere nevnt kan man ikke si at den ene er generelt bedre enn de andre. De lar oss håndtere multikolinearitet, og lasso kan fungere som teknikk for å redusere antall variabler (ridge kan ikke det). Elastic-net søker altså å kombinere de gode egenskapene for de to, men likevel kan man ikke si på generelt grunnlag at den alltid vil være bedre.</p>
<p>Det kan til slutt også være på sin plass å beskrive noen av de utfordringene med disse teknikkene som ofte nevnes.
Slik teknikkene fungerer må alle variablene være numeriske. Mange pakker/program som kjører teknikkene håndterer manglende verdier dårlig/ikke, så det kan kreve endel bearbeiding av dataene før analyse (på den andre siden - deskriptiv statistikk og databearbeiding bør være et must uansett…). Uteliggere, f.eks., kan være en utfordring som for OLS og her viser vi til grundig diskusjon om tematikken uteliggere i OLS-kapittelet. Og - det er en lineær teknikk med de svakheter dette medfører generelt.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="iv-regresjon.html"><span class="header-section-number">10</span> IV regresjon</a></div>
<div class="next"><a href="polynomisk-regresjon.html"><span class="header-section-number">12</span> Polynomisk regresjon</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#regularized-regression"><span class="header-section-number">11</span> Regularized regression</a></li>
<li><a class="nav-link" href="#ridge-regresjon"><span class="header-section-number">11.1</span> Ridge regresjon</a></li>
<li><a class="nav-link" href="#lasso-regression"><span class="header-section-number">11.2</span> Lasso regression</a></li>
<li><a class="nav-link" href="#elastic-net-regresjon"><span class="header-section-number">11.3</span> Elastic-net regresjon</a></li>
<li><a class="nav-link" href="#ridge-vs-lasso-vs-elastic-net"><span class="header-section-number">11.4</span> Ridge vs Lasso vs Elastic-net</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nilskvilvang/AKA3/blob/master/11.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nilskvilvang/AKA3/edit/master/11.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Anvendt kvantitativ analyse</strong>" was written by Nils Kvilvang. It was last built on 27 september, 2022.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
